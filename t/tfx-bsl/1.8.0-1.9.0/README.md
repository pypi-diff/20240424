# Comparing `tmp/tfx_bsl-1.8.0-cp39-cp39-win_amd64.whl.zip` & `tmp/tfx_bsl-1.9.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,95 +1,95 @@
-Zip file size: 1964484 bytes, number of entries: 93
--rw-rw-rw-  2.0 fat      663 b- defN 22-May-12 19:06 tfx_bsl/__init__.py
--rw-rw-rw-  2.0 fat     1469 b- defN 22-May-12 19:06 tfx_bsl/types_compat.py
--rw-rw-rw-  2.0 fat      701 b- defN 22-May-12 19:06 tfx_bsl/version.py
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-12 19:06 tfx_bsl/arrow/__init__.py
--rw-rw-rw-  2.0 fat     4659 b- defN 22-May-12 19:06 tfx_bsl/arrow/array_util.py
--rw-rw-rw-  2.0 fat    27538 b- defN 22-May-12 19:06 tfx_bsl/arrow/array_util_test.py
--rw-rw-rw-  2.0 fat     4311 b- defN 22-May-12 19:06 tfx_bsl/arrow/path.py
--rw-rw-rw-  2.0 fat     1072 b- defN 22-May-12 19:06 tfx_bsl/arrow/sql_util.py
--rw-rw-rw-  2.0 fat    14026 b- defN 22-May-12 19:06 tfx_bsl/arrow/sql_util_test.py
--rw-rw-rw-  2.0 fat     5842 b- defN 22-May-12 19:06 tfx_bsl/arrow/table_util.py
--rw-rw-rw-  2.0 fat    22351 b- defN 22-May-12 19:06 tfx_bsl/arrow/table_util_test.py
--rw-rw-rw-  2.0 fat      655 b- defN 22-May-12 19:06 tfx_bsl/beam/__init__.py
--rw-rw-rw-  2.0 fat    47952 b- defN 22-May-12 19:06 tfx_bsl/beam/run_inference.py
--rw-rw-rw-  2.0 fat     9032 b- defN 22-May-12 19:06 tfx_bsl/beam/run_inference_base.py
--rw-rw-rw-  2.0 fat     5440 b- defN 22-May-12 19:06 tfx_bsl/beam/run_inference_base_test.py
--rw-rw-rw-  2.0 fat    39682 b- defN 22-May-12 19:06 tfx_bsl/beam/run_inference_test.py
--rw-rw-rw-  2.0 fat      816 b- defN 22-May-12 19:06 tfx_bsl/beam/test_helpers.py
--rw-rw-rw-  2.0 fat     1003 b- defN 22-May-12 19:06 tfx_bsl/cc/__init__.py
--rw-rw-rw-  2.0 fat  5683712 b- defN 22-May-12 19:09 tfx_bsl/cc/tfx_bsl_extension.pyd
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-12 19:06 tfx_bsl/coders/__init__.py
--rw-rw-rw-  2.0 fat     1243 b- defN 22-May-12 19:06 tfx_bsl/coders/batch_util.py
--rw-rw-rw-  2.0 fat     1146 b- defN 22-May-12 19:06 tfx_bsl/coders/batch_util_test.py
--rw-rw-rw-  2.0 fat    20295 b- defN 22-May-12 19:06 tfx_bsl/coders/csv_decoder.py
--rw-rw-rw-  2.0 fat    29400 b- defN 22-May-12 19:06 tfx_bsl/coders/csv_decoder_test.py
--rw-rw-rw-  2.0 fat     3466 b- defN 22-May-12 19:06 tfx_bsl/coders/example_coder.py
--rw-rw-rw-  2.0 fat    23706 b- defN 22-May-12 19:06 tfx_bsl/coders/example_coder_test.py
--rw-rw-rw-  2.0 fat     4690 b- defN 22-May-12 19:06 tfx_bsl/coders/example_numpy_decoder_test.py
--rw-rw-rw-  2.0 fat     1145 b- defN 22-May-12 19:06 tfx_bsl/coders/sequence_example_coder.py
--rw-rw-rw-  2.0 fat    28811 b- defN 22-May-12 19:06 tfx_bsl/coders/sequence_example_coder_test.py
--rw-rw-rw-  2.0 fat     7706 b- defN 22-May-12 19:06 tfx_bsl/coders/tf_graph_record_decoder.py
--rw-rw-rw-  2.0 fat     5919 b- defN 22-May-12 19:06 tfx_bsl/coders/tf_graph_record_decoder_test.py
--rw-rw-rw-  2.0 fat      657 b- defN 22-May-12 19:06 tfx_bsl/public/__init__.py
--rw-rw-rw-  2.0 fat      717 b- defN 22-May-12 19:06 tfx_bsl/public/beam/__init__.py
--rw-rw-rw-  2.0 fat     8585 b- defN 22-May-12 19:06 tfx_bsl/public/beam/run_inference.py
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-12 19:06 tfx_bsl/public/proto/__init__.py
--rw-rw-rw-  2.0 fat     9724 b- defN 22-May-12 19:09 tfx_bsl/public/proto/model_spec_pb2.py
--rw-rw-rw-  2.0 fat     1842 b- defN 22-May-12 19:06 tfx_bsl/public/tfxio/__init__.py
--rw-rw-rw-  2.0 fat     1639 b- defN 22-May-12 19:06 tfx_bsl/public/tfxio/tfxio_import_test.py
--rw-rw-rw-  2.0 fat     1326 b- defN 22-May-12 19:06 tfx_bsl/sketches/__init__.py
--rw-rw-rw-  2.0 fat     3670 b- defN 22-May-12 19:06 tfx_bsl/sketches/kmv_sketch_test.py
--rw-rw-rw-  2.0 fat    11921 b- defN 22-May-12 19:06 tfx_bsl/sketches/misragries_sketch_test.py
--rw-rw-rw-  2.0 fat    14479 b- defN 22-May-12 19:06 tfx_bsl/sketches/quantiles_sketch_test.py
--rw-rw-rw-  2.0 fat     1206 b- defN 22-May-12 19:06 tfx_bsl/statistics/__init__.py
--rw-rw-rw-  2.0 fat     2385 b- defN 22-May-12 19:06 tfx_bsl/statistics/merge_util.py
--rw-rw-rw-  2.0 fat     2201 b- defN 22-May-12 19:06 tfx_bsl/statistics/merge_util_test.py
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-12 19:06 tfx_bsl/telemetry/__init__.py
--rw-rw-rw-  2.0 fat     2393 b- defN 22-May-12 19:06 tfx_bsl/telemetry/collection.py
--rw-rw-rw-  2.0 fat     3904 b- defN 22-May-12 19:06 tfx_bsl/telemetry/collection_test.py
--rw-rw-rw-  2.0 fat     1114 b- defN 22-May-12 19:06 tfx_bsl/telemetry/util.py
--rw-rw-rw-  2.0 fat     1283 b- defN 22-May-12 19:06 tfx_bsl/telemetry/util_test.py
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-12 19:06 tfx_bsl/test_util/__init__.py
--rw-rw-rw-  2.0 fat     7354 b- defN 22-May-12 19:06 tfx_bsl/test_util/run_all_tests.py
--rw-rw-rw-  2.0 fat      776 b- defN 22-May-12 19:06 tfx_bsl/tfxio/__init__.py
--rw-rw-rw-  2.0 fat    11496 b- defN 22-May-12 19:06 tfx_bsl/tfxio/csv_tfxio.py
--rw-rw-rw-  2.0 fat    14613 b- defN 22-May-12 19:06 tfx_bsl/tfxio/csv_tfxio_test.py
--rw-rw-rw-  2.0 fat     6624 b- defN 22-May-12 19:06 tfx_bsl/tfxio/dataset_options.py
--rw-rw-rw-  2.0 fat     5377 b- defN 22-May-12 19:06 tfx_bsl/tfxio/dataset_util.py
--rw-rw-rw-  2.0 fat     4418 b- defN 22-May-12 19:06 tfx_bsl/tfxio/dataset_util_test.py
--rw-rw-rw-  2.0 fat     6047 b- defN 22-May-12 19:06 tfx_bsl/tfxio/parquet_tfxio.py
--rw-rw-rw-  2.0 fat    14652 b- defN 22-May-12 19:06 tfx_bsl/tfxio/parquet_tfxio_test.py
--rw-rw-rw-  2.0 fat     7501 b- defN 22-May-12 19:06 tfx_bsl/tfxio/raw_tf_record.py
--rw-rw-rw-  2.0 fat     6133 b- defN 22-May-12 19:06 tfx_bsl/tfxio/raw_tf_record_test.py
--rw-rw-rw-  2.0 fat    16249 b- defN 22-May-12 19:06 tfx_bsl/tfxio/record_based_tfxio.py
--rw-rw-rw-  2.0 fat     6024 b- defN 22-May-12 19:06 tfx_bsl/tfxio/record_based_tfxio_test.py
--rw-rw-rw-  2.0 fat    15697 b- defN 22-May-12 19:06 tfx_bsl/tfxio/record_to_tensor_tfxio.py
--rw-rw-rw-  2.0 fat    13103 b- defN 22-May-12 19:06 tfx_bsl/tfxio/record_to_tensor_tfxio_test.py
--rw-rw-rw-  2.0 fat    11899 b- defN 22-May-12 19:06 tfx_bsl/tfxio/telemetry.py
--rw-rw-rw-  2.0 fat     9518 b- defN 22-May-12 19:06 tfx_bsl/tfxio/telemetry_test.py
--rw-rw-rw-  2.0 fat     1643 b- defN 22-May-12 19:06 tfx_bsl/tfxio/telemetry_test_util.py
--rw-rw-rw-  2.0 fat    39526 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tensor_adapter.py
--rw-rw-rw-  2.0 fat    73516 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tensor_adapter_test.py
--rw-rw-rw-  2.0 fat    29042 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tensor_representation_util.py
--rw-rw-rw-  2.0 fat    56795 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tensor_representation_util_test.py
--rw-rw-rw-  2.0 fat    22843 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tensor_to_arrow.py
--rw-rw-rw-  2.0 fat    27007 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tensor_to_arrow_test.py
--rw-rw-rw-  2.0 fat     1276 b- defN 22-May-12 19:06 tfx_bsl/tfxio/test_util.py
--rw-rw-rw-  2.0 fat     1802 b- defN 22-May-12 19:06 tfx_bsl/tfxio/test_util_test.py
--rw-rw-rw-  2.0 fat    17212 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tf_example_record.py
--rw-rw-rw-  2.0 fat    33439 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tf_example_record_test.py
--rw-rw-rw-  2.0 fat    11670 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tf_sequence_example_record.py
--rw-rw-rw-  2.0 fat    12863 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tf_sequence_example_record_test.py
--rw-rw-rw-  2.0 fat     7965 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tfxio.py
--rw-rw-rw-  2.0 fat     2959 b- defN 22-May-12 19:06 tfx_bsl/tfxio/tfxio_test.py
--rw-rw-rw-  2.0 fat      588 b- defN 22-May-12 19:06 tfx_bsl/types/__init__.py
--rw-rw-rw-  2.0 fat      933 b- defN 22-May-12 19:06 tfx_bsl/types/common_types.py
--rw-rw-rw-  2.0 fat     2373 b- defN 22-May-12 19:06 tfx_bsl/types/tfx_namedtuple.py
--rw-rw-rw-  2.0 fat     4835 b- defN 22-May-12 19:06 tfx_bsl/types/tfx_namedtuple_test.py
--rw-rw-rw-  2.0 fat    12806 b- defN 22-May-12 19:09 tfx_bsl-1.8.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     9862 b- defN 22-May-12 19:09 tfx_bsl-1.8.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 22-May-12 19:09 tfx_bsl-1.8.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        1 b- defN 22-May-12 19:09 tfx_bsl-1.8.0.dist-info/namespace_packages.txt
--rw-rw-rw-  2.0 fat        8 b- defN 22-May-12 19:09 tfx_bsl-1.8.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     8288 b- defN 22-May-12 19:09 tfx_bsl-1.8.0.dist-info/RECORD
-93 files, 6607270 bytes uncompressed, 1951334 bytes compressed:  70.5%
+Zip file size: 1965785 bytes, number of entries: 93
+-rw-rw-rw-  2.0 fat      663 b- defN 22-Jun-28 05:56 tfx_bsl/__init__.py
+-rw-rw-rw-  2.0 fat     1469 b- defN 22-Jun-28 05:56 tfx_bsl/types_compat.py
+-rw-rw-rw-  2.0 fat      701 b- defN 22-Jun-28 05:56 tfx_bsl/version.py
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/__init__.py
+-rw-rw-rw-  2.0 fat     4659 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/array_util.py
+-rw-rw-rw-  2.0 fat    27538 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/array_util_test.py
+-rw-rw-rw-  2.0 fat     4311 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/path.py
+-rw-rw-rw-  2.0 fat     1072 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/sql_util.py
+-rw-rw-rw-  2.0 fat    14026 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/sql_util_test.py
+-rw-rw-rw-  2.0 fat     5842 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/table_util.py
+-rw-rw-rw-  2.0 fat    22351 b- defN 22-Jun-28 05:56 tfx_bsl/arrow/table_util_test.py
+-rw-rw-rw-  2.0 fat      655 b- defN 22-Jun-28 05:56 tfx_bsl/beam/__init__.py
+-rw-rw-rw-  2.0 fat    50408 b- defN 22-Jun-28 05:56 tfx_bsl/beam/run_inference.py
+-rw-rw-rw-  2.0 fat     9032 b- defN 22-Jun-28 05:56 tfx_bsl/beam/run_inference_base.py
+-rw-rw-rw-  2.0 fat     5440 b- defN 22-Jun-28 05:56 tfx_bsl/beam/run_inference_base_test.py
+-rw-rw-rw-  2.0 fat    45565 b- defN 22-Jun-28 05:56 tfx_bsl/beam/run_inference_test.py
+-rw-rw-rw-  2.0 fat      816 b- defN 22-Jun-28 05:56 tfx_bsl/beam/test_helpers.py
+-rw-rw-rw-  2.0 fat     1003 b- defN 22-Jun-28 05:56 tfx_bsl/cc/__init__.py
+-rw-rw-rw-  2.0 fat  5683712 b- defN 22-Jun-28 05:58 tfx_bsl/cc/tfx_bsl_extension.pyd
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-28 05:56 tfx_bsl/coders/__init__.py
+-rw-rw-rw-  2.0 fat     1243 b- defN 22-Jun-28 05:56 tfx_bsl/coders/batch_util.py
+-rw-rw-rw-  2.0 fat     1146 b- defN 22-Jun-28 05:56 tfx_bsl/coders/batch_util_test.py
+-rw-rw-rw-  2.0 fat    20295 b- defN 22-Jun-28 05:56 tfx_bsl/coders/csv_decoder.py
+-rw-rw-rw-  2.0 fat    29400 b- defN 22-Jun-28 05:56 tfx_bsl/coders/csv_decoder_test.py
+-rw-rw-rw-  2.0 fat     3466 b- defN 22-Jun-28 05:56 tfx_bsl/coders/example_coder.py
+-rw-rw-rw-  2.0 fat    23706 b- defN 22-Jun-28 05:56 tfx_bsl/coders/example_coder_test.py
+-rw-rw-rw-  2.0 fat     4690 b- defN 22-Jun-28 05:56 tfx_bsl/coders/example_numpy_decoder_test.py
+-rw-rw-rw-  2.0 fat     1145 b- defN 22-Jun-28 05:56 tfx_bsl/coders/sequence_example_coder.py
+-rw-rw-rw-  2.0 fat    28811 b- defN 22-Jun-28 05:56 tfx_bsl/coders/sequence_example_coder_test.py
+-rw-rw-rw-  2.0 fat     7706 b- defN 22-Jun-28 05:56 tfx_bsl/coders/tf_graph_record_decoder.py
+-rw-rw-rw-  2.0 fat     5919 b- defN 22-Jun-28 05:56 tfx_bsl/coders/tf_graph_record_decoder_test.py
+-rw-rw-rw-  2.0 fat      657 b- defN 22-Jun-28 05:56 tfx_bsl/public/__init__.py
+-rw-rw-rw-  2.0 fat      717 b- defN 22-Jun-28 05:56 tfx_bsl/public/beam/__init__.py
+-rw-rw-rw-  2.0 fat    11365 b- defN 22-Jun-28 05:56 tfx_bsl/public/beam/run_inference.py
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-28 05:56 tfx_bsl/public/proto/__init__.py
+-rw-rw-rw-  2.0 fat     9724 b- defN 22-Jun-28 05:58 tfx_bsl/public/proto/model_spec_pb2.py
+-rw-rw-rw-  2.0 fat     1842 b- defN 22-Jun-28 05:56 tfx_bsl/public/tfxio/__init__.py
+-rw-rw-rw-  2.0 fat     1639 b- defN 22-Jun-28 05:56 tfx_bsl/public/tfxio/tfxio_import_test.py
+-rw-rw-rw-  2.0 fat     1326 b- defN 22-Jun-28 05:56 tfx_bsl/sketches/__init__.py
+-rw-rw-rw-  2.0 fat     3670 b- defN 22-Jun-28 05:56 tfx_bsl/sketches/kmv_sketch_test.py
+-rw-rw-rw-  2.0 fat    11921 b- defN 22-Jun-28 05:56 tfx_bsl/sketches/misragries_sketch_test.py
+-rw-rw-rw-  2.0 fat    14479 b- defN 22-Jun-28 05:56 tfx_bsl/sketches/quantiles_sketch_test.py
+-rw-rw-rw-  2.0 fat     1206 b- defN 22-Jun-28 05:56 tfx_bsl/statistics/__init__.py
+-rw-rw-rw-  2.0 fat     2385 b- defN 22-Jun-28 05:56 tfx_bsl/statistics/merge_util.py
+-rw-rw-rw-  2.0 fat     2201 b- defN 22-Jun-28 05:56 tfx_bsl/statistics/merge_util_test.py
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-28 05:56 tfx_bsl/telemetry/__init__.py
+-rw-rw-rw-  2.0 fat     2393 b- defN 22-Jun-28 05:56 tfx_bsl/telemetry/collection.py
+-rw-rw-rw-  2.0 fat     3904 b- defN 22-Jun-28 05:56 tfx_bsl/telemetry/collection_test.py
+-rw-rw-rw-  2.0 fat     1114 b- defN 22-Jun-28 05:56 tfx_bsl/telemetry/util.py
+-rw-rw-rw-  2.0 fat     1283 b- defN 22-Jun-28 05:56 tfx_bsl/telemetry/util_test.py
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-28 05:56 tfx_bsl/test_util/__init__.py
+-rw-rw-rw-  2.0 fat     7354 b- defN 22-Jun-28 05:56 tfx_bsl/test_util/run_all_tests.py
+-rw-rw-rw-  2.0 fat      776 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/__init__.py
+-rw-rw-rw-  2.0 fat    11496 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/csv_tfxio.py
+-rw-rw-rw-  2.0 fat    14613 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/csv_tfxio_test.py
+-rw-rw-rw-  2.0 fat     6624 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/dataset_options.py
+-rw-rw-rw-  2.0 fat     5377 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/dataset_util.py
+-rw-rw-rw-  2.0 fat     4418 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/dataset_util_test.py
+-rw-rw-rw-  2.0 fat     6047 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/parquet_tfxio.py
+-rw-rw-rw-  2.0 fat    14652 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/parquet_tfxio_test.py
+-rw-rw-rw-  2.0 fat     7501 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/raw_tf_record.py
+-rw-rw-rw-  2.0 fat     6133 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/raw_tf_record_test.py
+-rw-rw-rw-  2.0 fat    16249 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/record_based_tfxio.py
+-rw-rw-rw-  2.0 fat     6024 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/record_based_tfxio_test.py
+-rw-rw-rw-  2.0 fat    15697 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/record_to_tensor_tfxio.py
+-rw-rw-rw-  2.0 fat    13103 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/record_to_tensor_tfxio_test.py
+-rw-rw-rw-  2.0 fat    11899 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/telemetry.py
+-rw-rw-rw-  2.0 fat     9518 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/telemetry_test.py
+-rw-rw-rw-  2.0 fat     1643 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/telemetry_test_util.py
+-rw-rw-rw-  2.0 fat    39526 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tensor_adapter.py
+-rw-rw-rw-  2.0 fat    73516 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tensor_adapter_test.py
+-rw-rw-rw-  2.0 fat    29042 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tensor_representation_util.py
+-rw-rw-rw-  2.0 fat    56795 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tensor_representation_util_test.py
+-rw-rw-rw-  2.0 fat    22843 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tensor_to_arrow.py
+-rw-rw-rw-  2.0 fat    27007 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tensor_to_arrow_test.py
+-rw-rw-rw-  2.0 fat     1276 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/test_util.py
+-rw-rw-rw-  2.0 fat     1802 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/test_util_test.py
+-rw-rw-rw-  2.0 fat    17212 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tf_example_record.py
+-rw-rw-rw-  2.0 fat    33439 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tf_example_record_test.py
+-rw-rw-rw-  2.0 fat    11670 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tf_sequence_example_record.py
+-rw-rw-rw-  2.0 fat    12863 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tf_sequence_example_record_test.py
+-rw-rw-rw-  2.0 fat     7965 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tfxio.py
+-rw-rw-rw-  2.0 fat     2959 b- defN 22-Jun-28 05:56 tfx_bsl/tfxio/tfxio_test.py
+-rw-rw-rw-  2.0 fat      588 b- defN 22-Jun-28 05:56 tfx_bsl/types/__init__.py
+-rw-rw-rw-  2.0 fat      933 b- defN 22-Jun-28 05:56 tfx_bsl/types/common_types.py
+-rw-rw-rw-  2.0 fat     2373 b- defN 22-Jun-28 05:56 tfx_bsl/types/tfx_namedtuple.py
+-rw-rw-rw-  2.0 fat     4835 b- defN 22-Jun-28 05:56 tfx_bsl/types/tfx_namedtuple_test.py
+-rw-rw-rw-  2.0 fat    12806 b- defN 22-Jun-28 05:58 tfx_bsl-1.9.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    10061 b- defN 22-Jun-28 05:58 tfx_bsl-1.9.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 22-Jun-28 05:58 tfx_bsl-1.9.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        1 b- defN 22-Jun-28 05:58 tfx_bsl-1.9.0.dist-info/namespace_packages.txt
+-rw-rw-rw-  2.0 fat        8 b- defN 22-Jun-28 05:58 tfx_bsl-1.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     8290 b- defN 22-Jun-28 05:58 tfx_bsl-1.9.0.dist-info/RECORD
+93 files, 6618590 bytes uncompressed, 1952635 bytes compressed:  70.5%
```

## zipnote {}

```diff
@@ -255,26 +255,26 @@
 
 Filename: tfx_bsl/types/tfx_namedtuple.py
 Comment: 
 
 Filename: tfx_bsl/types/tfx_namedtuple_test.py
 Comment: 
 
-Filename: tfx_bsl-1.8.0.dist-info/LICENSE
+Filename: tfx_bsl-1.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: tfx_bsl-1.8.0.dist-info/METADATA
+Filename: tfx_bsl-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: tfx_bsl-1.8.0.dist-info/WHEEL
+Filename: tfx_bsl-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: tfx_bsl-1.8.0.dist-info/namespace_packages.txt
+Filename: tfx_bsl-1.9.0.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: tfx_bsl-1.8.0.dist-info/top_level.txt
+Filename: tfx_bsl-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: tfx_bsl-1.8.0.dist-info/RECORD
+Filename: tfx_bsl-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tfx_bsl/version.py

```diff
@@ -10,8 +10,8 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Contains the version string of tfx_bsl."""
 
 # Note that setup.py uses this version.
-__version__ = '1.8.0'
+__version__ = '1.9.0'
```

## tfx_bsl/beam/run_inference.py

```diff
@@ -15,15 +15,15 @@
 
 import abc
 import base64
 from concurrent import futures
 import functools
 import importlib
 import os
-from typing import Any, Dict, Iterable, List, Mapping, NamedTuple, Optional, Sequence, Text, Tuple, TypeVar, Union
+from typing import Any, Callable, Dict, Iterable, List, Mapping, NamedTuple, Optional, Sequence, Text, Tuple, TypeVar, Union
 
 from absl import logging
 import apache_beam as beam
 from apache_beam.options.pipeline_options import GoogleCloudOptions
 from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.transforms import resources
 from apache_beam.utils import retry
@@ -102,16 +102,28 @@
 
 # Output type is inferred from input.
 @beam.typehints.with_input_types(Union[_INPUT_TYPE, Tuple[_K, _INPUT_TYPE],
                                        Tuple[_K, List[_INPUT_TYPE]]])
 class RunInferenceImpl(beam.PTransform):
   """Implementation of RunInference API."""
 
-  def __init__(self, inference_spec_type: model_spec_pb2.InferenceSpecType):
+  def __init__(self,
+               inference_spec_type: model_spec_pb2.InferenceSpecType,
+               load_override_fn: Optional[Callable[[str, Sequence[str]],
+                                                   Any]] = None):
+    """Initializes transform.
+
+    Args:
+      inference_spec_type: InferenceSpecType proto.
+      load_override_fn: If provided, overrides the model loader fn of the
+        underlying ModelLoader. This takes a model path and sequence of tags,
+        and should return a model with interface compatible with tf.SavedModel.
+    """
     self._inference_spec_type = inference_spec_type
+    self._load_override_fn = load_override_fn
 
   # LINT.IfChange(close_to_resources)
   @staticmethod
   def _model_size_bytes(path: str) -> int:
     # We might be unable to compute the size of the model during pipeline
     # construction, but the model might still be accessible during pipeline
     # execution. In such cases we will provide a default value for the model
@@ -182,61 +194,74 @@
       examples |= (
           'CloseToResources' >> beam.Map(lambda x: x).with_resource_hints(
               close_to_resources=self._make_close_to_resources(
                   self._inference_spec_type)))
 
     # pylint: disable=no-value-for-parameter
     if _using_in_process_inference(self._inference_spec_type):
-      spec = _get_saved_model_spec(self._inference_spec_type)
+      spec = _get_saved_model_spec(self._inference_spec_type,
+                                   self._load_override_fn)
     else:
       spec = _get_remote_model_spec(self._inference_spec_type, examples)
     spec = _ModelLoaderWrapper(spec)
     return examples | 'BulkInference' >> run_inference_base.RunInference(
         spec).with_output_types(output_type)
 
 
 def _get_saved_model_spec(
-    inference_spec_type: model_spec_pb2.InferenceSpecType
+    inference_spec_type: model_spec_pb2.InferenceSpecType,
+    load_override_fn: Optional[Callable[[str, Sequence[str]], Any]]
 ) -> run_inference_base.ModelLoader:
   """Get an in-process ModelLoader."""
   operation_type = _get_operation_type(inference_spec_type)
   if operation_type == _OperationType.CLASSIFICATION:
-    return _ClassifyModelSpec(inference_spec_type)
+    return _ClassifyModelSpec(inference_spec_type, load_override_fn)
   elif operation_type == _OperationType.REGRESSION:
-    return _RegressModelSpec(inference_spec_type)
+    return _RegressModelSpec(inference_spec_type, load_override_fn)
   elif operation_type == _OperationType.MULTI_INFERENCE:
-    return _MultiInferenceModelSpec(inference_spec_type)
+    return _MultiInferenceModelSpec(inference_spec_type, load_override_fn)
   elif operation_type == _OperationType.PREDICTION:
-    return _PredictModelSpec(inference_spec_type)
+    return _PredictModelSpec(inference_spec_type, load_override_fn)
   else:
     raise ValueError('Unsupported operation_type %s' % operation_type)
 
 
 def _get_remote_model_spec(
     inference_spec_type: model_spec_pb2.InferenceSpecType,
     pcoll: beam.PCollection) -> run_inference_base.ModelLoader:
   """Get a remote prediction ModelLoader."""
   return _RemotePredictModelSpec(inference_spec_type, pcoll.pipeline.options)
 
 
-# TODO(b/231328769): Consider supporting Tuple[K, List[E]] here as well.
 # Output type is inferred from input.
-@beam.typehints.with_input_types(Union[_INPUT_TYPE, Tuple[_K, _INPUT_TYPE]])
+@beam.typehints.with_input_types(Union[_INPUT_TYPE, Tuple[_K, _INPUT_TYPE],
+                                       Tuple[_K, List[_INPUT_TYPE]]])
 class RunInferencePerModelImpl(beam.PTransform):
   """Implementation of the vectorized variant of the RunInference API."""
 
-  def __init__(
-      self, inference_spec_types: Iterable[model_spec_pb2.InferenceSpecType]):
+  def __init__(self,
+               inference_spec_types: Iterable[model_spec_pb2.InferenceSpecType],
+               load_override_fn: Optional[Callable[[str, Sequence[str]],
+                                                   Any]] = None):
+    """Initializes transform.
+
+    Args:
+      inference_spec_types: InferenceSpecType proto.
+      load_override_fn: If provided, overrides the model loader fn of the
+        underlying ModelLoader. This takes a model path and sequence of tags,
+        and should return a model with interface compatible with tf.SavedModel.
+    """
     self._inference_spec_types = tuple(inference_spec_types)
+    self._load_override_fn = load_override_fn
 
   def infer_output_type(self, input_type):
     key_type, result_type = _key_and_result_type(input_type)
     result_type = beam.typehints.Tuple[(result_type,) *
                                        len(self._inference_spec_types)]
-    if key_type:
+    if key_type is not None:
       return beam.typehints.Tuple[key_type, result_type]
     return result_type
 
   def expand(self, examples: beam.PCollection) -> beam.PCollection:
     output_type = self.infer_output_type(examples.element_type)
 
     # TODO(b/217442215): Obviate the need for this block (and instead rely
@@ -256,20 +281,41 @@
       # drop the dummy key afterwards.
       return (examples
               | 'PairWithNone' >> beam.Map(lambda x: (None, x))
               | 'ApplyOnKeyedInput' >> RunInferencePerModelImpl(
                   self._inference_spec_types)
               | 'DropNone' >> beam.Values().with_output_types(output_type))
 
+    def infer_iteration_output_type(input_type):
+      """Infers ouput typehint for Iteration Ptransform based on input_type."""
+      tuple_types = getattr(input_type, 'tuple_types', None)
+      output_tuple_components = []
+      if tuple_types is not None:
+        output_tuple_components.extend(tuple_types)
+        example_type = tuple_types[1]
+      else:
+        output_tuple_components.append(input_type)
+        example_type = input_type
+
+      if _is_list_type(example_type):
+        inference_result_type = beam.typehints.List[_OUTPUT_TYPE]
+      else:
+        inference_result_type = _OUTPUT_TYPE
+      output_tuple_components.append(inference_result_type)
+      return beam.typehints.Tuple[output_tuple_components]
+
     @beam.ptransform_fn
     def Iteration(pcoll, inference_spec_type):  # pylint: disable=invalid-name
       return (pcoll
               | 'PairWithInput' >> beam.Map(lambda x: (x, x[1]))
-              | 'RunInferenceImpl' >> RunInferenceImpl(inference_spec_type)
-              | 'ExtendResults' >> beam.MapTuple(lambda k, v: k + (v,)))
+              | 'RunInferenceImpl' >> RunInferenceImpl(inference_spec_type,
+                                                       self._load_override_fn)
+              | 'ExtendResults' >>
+              beam.MapTuple(lambda k, v: k + (v,)).with_output_types(
+                  infer_iteration_output_type(pcoll.element_type)))
 
     result = examples
     for i, inference_spec_type in enumerate(self._inference_spec_types):
       result |= f'Model[{i}]' >> Iteration(inference_spec_type)  # pylint: disable=no-value-for-parameter
     result |= 'ExtractResults' >> beam.Map(
         lambda tup: (tup[0], tuple(tup[2:]))).with_output_types(output_type)
     return result
@@ -515,28 +561,30 @@
     Models need to have the required serving signature as mentioned in
     [Tensorflow Serving](https://www.tensorflow.org/tfx/serving/signature_defs)
 
     This function will check model signatures first. Then it will load and run
     model inference in batch.
   """
 
-  def __init__(self, inference_spec_type: model_spec_pb2.InferenceSpecType):
+  def __init__(self, inference_spec_type: model_spec_pb2.InferenceSpecType,
+               load_override_fn: Optional[Callable[[str, Sequence[str]], Any]]):
     super().__init__(inference_spec_type)
     self._inference_spec_type = inference_spec_type
     self._model_path = inference_spec_type.saved_model_spec.model_path
     if not self._model_path:
       raise ValueError('Model path is not valid.')
     self._tags = _get_tags(inference_spec_type)
     self._signatures = _get_signatures(
         inference_spec_type.saved_model_spec.model_path,
         inference_spec_type.saved_model_spec.signature_name, self._tags)
     self._io_tensor_spec = self._make_io_tensor_spec()
     if self._has_tpu_tag():
       # TODO(b/161563144): Support TPU inference.
       raise NotImplementedError('TPU inference is not supported yet.')
+    self._load_override_fn = load_override_fn
 
   def _has_tpu_tag(self) -> bool:
     return (len(self._tags) == 2 and tf.saved_model.SERVING in self._tags and
             tf.saved_model.TPU in self._tags)
 
   # TODO(b/159982957): Replace this with a mechinism that registers any custom
   # op.
@@ -549,14 +597,16 @@
         logging.info('%s is not available.', name)
 
     _try_import('tensorflow_text')
     _try_import('tensorflow_decision_forests')
     _try_import('struct2tensor')
 
   def load_model(self):
+    if self._load_override_fn:
+      return self._load_override_fn(self._model_path, self._tags)
     self._maybe_register_addon_ops()
     result = tf.compat.v1.Session(graph=tf.compat.v1.Graph())
     tf.compat.v1.saved_model.loader.load(result, self._tags, self._model_path)
     return result
 
   def get_inference_runner(self):
     return self
```

## tfx_bsl/beam/run_inference_test.py

```diff
@@ -14,20 +14,23 @@
 """Tests for tfx_bsl.run_inference."""
 
 import base64
 from http import client as http_client
 import json
 import os
 import pickle
+from typing import Any, Callable, Optional
 from unittest import mock
 
 import apache_beam as beam
 from apache_beam.metrics.metric import MetricsFilter
 from apache_beam.testing.util import assert_that
 from apache_beam.testing.util import equal_to
+from apache_beam.utils import shared
+
 from googleapiclient import discovery
 from googleapiclient import http
 import tensorflow as tf
 from tensorflow.compat.v1 import estimator as tf_estimator
 from tfx_bsl.beam import run_inference
 from tfx_bsl.beam import test_helpers
 from tfx_bsl.public.proto import model_spec_pb2
@@ -220,15 +223,16 @@
   def _run_inference_with_beam(
       self,
       example_path: str,
       inference_spec_type: model_spec_pb2.InferenceSpecType,
       prediction_log_path: str,
       keyed_input: bool,
       decode_examples: bool,
-      pre_batch_inputs=False):
+      pre_batch_inputs=False,
+      load_override_fn: Optional[Callable[[], Any]] = None):
     with self._make_beam_pipeline() as pipeline:
       if keyed_input:
         key = 'TheKey'
         def verify_key(k, v):
           if k != key:
             raise RuntimeError('Wrong Key %s' % k)
           return v
@@ -248,43 +252,44 @@
         maybe_batch = beam.Map(lambda x: x)
       _ = (
           pipeline
           | 'ReadExamples' >> beam.io.ReadFromTFRecord(example_path)
           | 'MaybeDecode' >> beam.Map(maybe_decode)
           | 'MaybeBatch' >> maybe_batch
           | maybe_pair_with_key
-          |
-          'RunInference' >> run_inference.RunInferenceImpl(inference_spec_type)
+          | 'RunInference' >> run_inference.RunInferenceImpl(
+              inference_spec_type, load_override_fn)
           | maybe_verify_key
           | 'WritePredictions' >> beam.io.WriteToTFRecord(
               prediction_log_path, coder=beam.coders.PickleCoder()))
 
   def _get_results(self, prediction_log_path):
     result = []
     for f in tf.io.gfile.glob(prediction_log_path + '-?????-of-?????'):
       record_iterator = tf.compat.v1.io.tf_record_iterator(path=f)
       for record_string in record_iterator:
         result.append(pickle.loads(record_string))
     return result
 
   @parameterized.named_parameters(_RUN_OFFLINE_INFERENCE_TEST_CASES)
-  def testModelPathInvalid(self, keyed_input: bool, decode_examples: bool):
+  def test_model_path_invalid(self, keyed_input: bool, decode_examples: bool):
     example_path = self._get_output_data_dir('examples')
     self._prepare_predict_examples(example_path)
     prediction_log_path = self._get_output_data_dir('predictions')
     with self.assertRaisesRegex(IOError, 'SavedModel file does not exist.*'):
       self._run_inference_with_beam(
           example_path,
           model_spec_pb2.InferenceSpecType(
               saved_model_spec=model_spec_pb2.SavedModelSpec(
                   model_path=self._get_output_data_dir())), prediction_log_path,
           keyed_input, decode_examples)
 
   @parameterized.named_parameters(_RUN_OFFLINE_INFERENCE_TEST_CASES)
-  def testEstimatorModelPredict(self, keyed_input: bool, decode_examples: bool):
+  def test_estimator_model_predict(self, keyed_input: bool,
+                                   decode_examples: bool):
     example_path = self._get_output_data_dir('examples')
     self._prepare_predict_examples(example_path)
     model_path = self._get_output_data_dir('model')
     self._build_predict_model(model_path)
     prediction_log_path = self._get_output_data_dir('predictions')
     self._run_inference_with_beam(
         example_path,
@@ -302,15 +307,15 @@
     outputs = results[0].predict_log.response.outputs['y']
     self.assertEqual(outputs.dtype, tf.float32)
     self.assertLen(outputs.tensor_shape.dim, 2)
     self.assertEqual(outputs.tensor_shape.dim[0].size, 1)
     self.assertEqual(outputs.tensor_shape.dim[1].size, 1)
 
   @parameterized.named_parameters(_RUN_OFFLINE_INFERENCE_TEST_CASES)
-  def testClassifyModel(self, keyed_input: bool, decode_examples: bool):
+  def test_classify_model(self, keyed_input: bool, decode_examples: bool):
     example_path = self._get_output_data_dir('examples')
     self._prepare_multihead_examples(example_path)
     model_path = self._get_output_data_dir('model')
     self._build_multihead_model(model_path)
     prediction_log_path = self._get_output_data_dir('predictions')
     self._run_inference_with_beam(
         example_path,
@@ -327,15 +332,15 @@
                      self._multihead_examples[0])
     self.assertLen(classify_log.response.result.classifications, 1)
     self.assertLen(classify_log.response.result.classifications[0].classes, 1)
     self.assertAlmostEqual(
         classify_log.response.result.classifications[0].classes[0].score, 1.0)
 
   @parameterized.named_parameters(_RUN_OFFLINE_INFERENCE_TEST_CASES)
-  def testRegressModel(self, keyed_input: bool, decode_examples: bool):
+  def test_regress_model(self, keyed_input: bool, decode_examples: bool):
     example_path = self._get_output_data_dir('examples')
     self._prepare_multihead_examples(example_path)
     model_path = self._get_output_data_dir('model')
     self._build_multihead_model(model_path)
     prediction_log_path = self._get_output_data_dir('predictions')
     self._run_inference_with_beam(
         example_path,
@@ -351,15 +356,63 @@
     self.assertEqual(regress_log.request.input.example_list.examples[0],
                      self._multihead_examples[0])
     self.assertLen(regress_log.response.result.regressions, 1)
     self.assertAlmostEqual(regress_log.response.result.regressions[0].value,
                            0.6)
 
   @parameterized.named_parameters(_RUN_OFFLINE_INFERENCE_TEST_CASES)
-  def testMultiInferenceModel(self, keyed_input: bool, decode_examples: bool):
+  def testRegressModelSideloaded(self, keyed_input: bool,
+                                 decode_examples: bool):
+    example_path = self._get_output_data_dir('examples')
+    self._prepare_multihead_examples(example_path)
+    model_path = self._get_output_data_dir('model')
+    self._build_multihead_model(model_path)
+
+    called_count = 0
+
+    def load_model():
+      nonlocal called_count
+      called_count += 1
+      result = tf.compat.v1.Session(graph=tf.compat.v1.Graph())
+      tf.compat.v1.saved_model.loader.load(result, [tf.saved_model.SERVING],
+                                           model_path)
+      return result
+
+    shared_handle = shared.Shared()
+    def _load_override_fn(unused_path, unused_tags):
+      return shared_handle.acquire(load_model)
+    # Load the model the first time. It should not be loaded again.
+    _ = _load_override_fn('', [])
+
+    prediction_log_path = self._get_output_data_dir('predictions')
+    self._run_inference_with_beam(
+        example_path,
+        model_spec_pb2.InferenceSpecType(
+            saved_model_spec=model_spec_pb2.SavedModelSpec(
+                model_path=model_path, signature_name=['regress_diff'])),
+        prediction_log_path,
+        keyed_input,
+        decode_examples,
+        load_override_fn=_load_override_fn)
+
+    results = self._get_results(prediction_log_path)
+    self.assertLen(results, 2)
+    regress_log = results[0].regress_log
+    self.assertLen(regress_log.request.input.example_list.examples, 1)
+    self.assertEqual(regress_log.request.input.example_list.examples[0],
+                     self._multihead_examples[0])
+    self.assertLen(regress_log.response.result.regressions, 1)
+    self.assertAlmostEqual(regress_log.response.result.regressions[0].value,
+                           0.6)
+    # Ensure that the model load only happened once.
+    self.assertEqual(called_count, 1)
+
+  @parameterized.named_parameters(_RUN_OFFLINE_INFERENCE_TEST_CASES)
+  def test_multi_inference_model(self, keyed_input: bool,
+                                 decode_examples: bool):
     example_path = self._get_output_data_dir('examples')
     self._prepare_multihead_examples(example_path)
     model_path = self._get_output_data_dir('model')
     self._build_multihead_model(model_path)
     prediction_log_path = self._get_output_data_dir('predictions')
     self._run_inference_with_beam(
         example_path,
@@ -388,15 +441,15 @@
     self.assertEqual(result.model_spec.signature_name, 'classify_sum')
     self.assertLen(result.classification_result.classifications, 1)
     self.assertLen(result.classification_result.classifications[0].classes, 1)
     self.assertAlmostEqual(
         result.classification_result.classifications[0].classes[0].score, 1.0)
 
   @parameterized.named_parameters(_RUN_OFFLINE_INFERENCE_TEST_CASES)
-  def testKerasModelPredict(self, keyed_input: bool, decode_examples: bool):
+  def test_keras_model_predict(self, keyed_input: bool, decode_examples: bool):
     inputs = tf.keras.Input(shape=(1,), name='input1')
     output1 = tf.keras.layers.Dense(
         1, activation=tf.nn.sigmoid, name='output1')(
             inputs)
     output2 = tf.keras.layers.Dense(
         1, activation=tf.nn.sigmoid, name='output2')(
             inputs)
@@ -446,15 +499,15 @@
   @parameterized.named_parameters([{
       'testcase_name': 'decoded_examples',
       'decode_examples': True
   }, {
       'testcase_name': 'encoded_examples',
       'decode_examples': False
   }])
-  def testTelemetry(self, decode_examples: bool):
+  def test_telemetry(self, decode_examples: bool):
     example_path = self._get_output_data_dir('examples')
     self._prepare_multihead_examples(example_path)
     model_path = self._get_output_data_dir('model')
     self._build_multihead_model(model_path)
     inference_spec_type = model_spec_pb2.InferenceSpecType(
         saved_model_spec=model_spec_pb2.SavedModelSpec(
             model_path=model_path, signature_name=['classify_sum']))
@@ -490,29 +543,29 @@
         inference_batch_latency_micro_secs['distributions'][0].result.sum, 0)
     load_model_latency_milli_secs = run_result.metrics().query(
         MetricsFilter().with_name('load_model_latency_milli_secs'))
     self.assertTrue(load_model_latency_milli_secs['distributions'])
     self.assertGreaterEqual(
         load_model_latency_milli_secs['distributions'][0].result.sum, 0)
 
-  def testModelSizeBytes(self):
+  def test_model_size_bytes(self):
     self.assertEqual(
         1 << 30,
         run_inference.RunInferenceImpl._model_size_bytes(
             '/the/non-existent-or-inaccesible-file'))
 
     model_path = self._get_output_data_dir('model')
     self._build_predict_model(model_path)
     # The actual model_size is ~2K, but it might fluctuate a bit (eg due to
     # TF version changes).
     model_size = run_inference.RunInferenceImpl._model_size_bytes(model_path)
     self.assertGreater(model_size, 1000)
     self.assertLess(model_size, 5000)
 
-  def testEstimatorModelPredictBatched(self):
+  def test_estimator_model_predict_batched(self):
     example_path = self._get_output_data_dir('examples')
     self._prepare_predict_examples(example_path)
     model_path = self._get_output_data_dir('model')
     self._build_predict_model(model_path)
     prediction_log_path = self._get_output_data_dir('predictions')
     self._run_inference_with_beam(
         example_path,
@@ -567,24 +620,25 @@
                                            prediction_log_pb2.PredictionLog]),
       dict(
           testcase_name='keyed_example_list',
           input_element=('key', [tf.train.Example()]),
           output_type=beam.typehints.Tuple[
               str, beam.typehints.List[prediction_log_pb2.PredictionLog]]),
   ])
-  def testInfersElementType(self, input_element, output_type):
+  def test_infers_element_type(self, input_element, output_type):
     # TODO(zwestrick): Skip building the model, which is not actually used, or
     # stop using parameterized tests if performance becomes an issue.
     model_path = self._get_output_data_dir('model')
     self._build_predict_model(model_path)
     spec = model_spec_pb2.InferenceSpecType(
         saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=model_path))
     inference_transform = run_inference.RunInferenceImpl(spec)
-    with beam.Pipeline() as p:
-      inference = (p | beam.Create([input_element]) | inference_transform)
+    with self._make_beam_pipeline() as pipeline:
+      inference = (
+          pipeline | beam.Create([input_element]) | inference_transform)
       self.assertEqual(inference.element_type, output_type)
 
 
 _RUN_REMOTE_INFERENCE_TEST_CASES = [{
     'testcase_name': 'keyed_input_false',
     'keyed_input': False
 }, {
@@ -830,15 +884,15 @@
     self.assertEqual(
         result,
         [{'b64': base64.b64encode(se).decode()} for se in serialized_examples])
 
 
 class RunInferencePerModelTest(RunInferenceFixture, parameterized.TestCase):
 
-  def test_basic(self):
+  def test_nonkeyed_nonbatched_input(self):
     examples = [
         text_format.Parse(
             """
               features {
                 feature { key: "x" value { float_list { value: 0 }}}
               }
               """, tf.train.Example()),
@@ -862,26 +916,105 @@
           pipeline
           | beam.Create(examples)
           | run_inference.RunInferencePerModelImpl(specs)
           | beam.MapTuple(
               lambda _, p2: p2.predict_log.response.outputs['y'].float_val[0]))
       assert_that(predictions, equal_to([0.0, 2.0]))
 
+  def test_keyed_nonbatched_input(self):
+    keyed_examples = [
+        ('key1', text_format.Parse(
+            """
+              features {
+                feature { key: "x" value { float_list { value: 0 }}}
+              }
+              """, tf.train.Example())),
+        ('key2', text_format.Parse(
+            """
+              features {
+                feature { key: "x" value { float_list { value: 1 }}}
+              }
+              """, tf.train.Example()))
+    ]
+    model_paths = [self._get_output_data_dir(m) for m in ('model1', 'model2')]
+    for model_path in model_paths:
+      self._build_predict_model(model_path)
+    specs = [
+        model_spec_pb2.InferenceSpecType(
+            saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=p))
+        for p in model_paths
+    ]
+    with self._make_beam_pipeline() as pipeline:
+      predictions_table = (
+          pipeline
+          | beam.Create(keyed_examples)
+          | 'RunInferencePerModelTable' >>
+          run_inference.RunInferencePerModelImpl(specs)
+          | beam.MapTuple(lambda k, predict_logs:  # pylint: disable=g-long-lambda
+                          (k, [
+                              p.predict_log.response.outputs['y'].float_val[0]
+                              for p in predict_logs
+                          ])))
+      assert_that(
+          predictions_table,
+          equal_to([('key1', [0.0, 0.0]), ('key2', [2.0, 2.0])]),
+          label='AssertTable')
+
+  def test_keyed_batched_input(self):
+    keyed_batched_examples = [('key_batch_1', [
+        text_format.Parse(
+            """
+                features {
+                  feature { key: "x" value { float_list { value: 0 }}}
+                }
+              """, tf.train.Example()),
+        text_format.Parse(
+            """
+                features {
+                  feature { key: "x" value { float_list { value: 1 }}}
+                }
+              """, tf.train.Example())
+    ]),
+                              ('key_batch_2', [
+                                  text_format.Parse(
+                                      """
+                features {
+                  feature { key: "x" value { float_list { value: 2 }}}
+                }
+              """, tf.train.Example()),
+                                  text_format.Parse(
+                                      """
+                features {
+                  feature { key: "x" value { float_list { value: 3 }}}
+                }
+              """, tf.train.Example())
+                              ])]
+    model_paths = [self._get_output_data_dir(m) for m in ('model1', 'model2')]
+    for model_path in model_paths:
+      self._build_predict_model(model_path)
+    specs = [
+        model_spec_pb2.InferenceSpecType(
+            saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=p))
+        for p in model_paths
+    ]
+    with self._make_beam_pipeline() as pipeline:
       predictions_table = (
           pipeline
-          |
-          'CreateTable' >> beam.Create([(i, e) for i, e in enumerate(examples)])
+          | beam.Create(keyed_batched_examples)
           | 'RunInferencePerModelTable' >>
           run_inference.RunInferencePerModelImpl(specs)
-          | beam.MapTuple(lambda k, v:  # pylint: disable=g-long-lambda
-                          (k, v[1].predict_log.response.outputs['y'].float_val[
-                              0])))
+          | beam.MapTuple(lambda k, batched_predicit_logs:  # pylint: disable=g-long-lambda
+                          (k, [[  # pylint: disable=g-complex-comprehension
+                              pl.predict_log.response.outputs['y'].float_val[
+                                  0] for pl in pls
+                          ] for pls in batched_predicit_logs])))
       assert_that(
           predictions_table,
-          equal_to([(0, 0.0), (1, 2.0)]),
+          equal_to([('key_batch_1', [[0.0, 2.0], [0.0, 2.0]]),
+                    ('key_batch_2', [[4.0, 6.0], [4.0, 6.0]])]),
           label='AssertTable')
 
   @parameterized.named_parameters([
       dict(
           testcase_name='example',
           input_element=tf.train.Example(),
           output_type=beam.typehints.Tuple[prediction_log_pb2.PredictionLog,
@@ -910,26 +1043,33 @@
                                         prediction_log_pb2.PredictionLog]]),
       dict(
           testcase_name='keyed_sequence_example',
           input_element=('key', tf.train.SequenceExample()),
           output_type=beam.typehints.Tuple[
               str, beam.typehints.Tuple[prediction_log_pb2.PredictionLog,
                                         prediction_log_pb2.PredictionLog]]),
+      dict(
+          testcase_name='keyed_batched_examples',
+          input_element=('key', [tf.train.Example()]),
+          output_type=beam.typehints.Tuple[str, beam.typehints.Tuple[
+              beam.typehints.List[prediction_log_pb2.PredictionLog],
+              beam.typehints.List[prediction_log_pb2.PredictionLog]]]),
   ])
-  def testInfersElementType(self, input_element, output_type):
+  def test_infers_element_type(self, input_element, output_type):
     # TODO(zwestrick): Skip building the model, which is not actually used, or
     # stop using parameterized tests if performance becomes an issue.
     model_paths = [self._get_output_data_dir(m) for m in ('model1', 'model2')]
     for model_path in model_paths:
       self._build_predict_model(model_path)
     specs = [
         model_spec_pb2.InferenceSpecType(
             saved_model_spec=model_spec_pb2.SavedModelSpec(model_path=p))
         for p in model_paths
     ]
     inference_transform = run_inference.RunInferencePerModelImpl(specs)
-    with beam.Pipeline() as p:
-      inference = (p | beam.Create([input_element]) | inference_transform)
+    with self._make_beam_pipeline() as pipeline:
+      inference = (
+          pipeline | beam.Create([input_element]) | inference_transform)
       self.assertEqual(inference.element_type, output_type)
 
 if __name__ == '__main__':
   tf.test.main()
```

## tfx_bsl/public/beam/run_inference.py

```diff
@@ -9,15 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Publich API of batch inference."""
 
-from typing import Iterable, List, Tuple, TypeVar, Union
+from typing import Any, Callable, Iterable, List, Optional, Sequence, Tuple, TypeVar, Union
 
 import apache_beam as beam
 import tensorflow as tf
 from tfx_bsl.beam import run_inference
 from tfx_bsl.public.proto import model_spec_pb2
 
 from tensorflow_serving.apis import prediction_log_pb2
@@ -158,15 +158,16 @@
 
 
 @beam.ptransform_fn
 @beam.typehints.with_input_types(Tuple[_K, List[_INPUT_TYPE]])
 @beam.typehints.with_output_types(Tuple[_K, List[_OUTPUT_TYPE]])
 def RunInferenceOnKeyedBatches(
     examples: beam.pvalue.PCollection,
-    inference_spec_type: model_spec_pb2.InferenceSpecType
+    inference_spec_type: model_spec_pb2.InferenceSpecType,
+    load_override_fn: Optional[Callable[[str, Sequence[str]], Any]] = None
 ) -> beam.pvalue.PCollection:
   """Run inference over pre-batched keyed inputs.
 
   This API is experimental and may change in the future.
 
   Supports the same inference specs as RunInference. Inputs must consist of a
   keyed list of examples, and outputs consist of keyed list of prediction logs
@@ -176,15 +177,64 @@
     examples: A PCollection of keyed, batched inputs of type Example,
       SequenceExample, or bytes. Each type support inference specs corresponding
       to the unbatched cases described in RunInference. Supports
         - PCollection[Tuple[K, List[Example]]]
         - PCollection[Tuple[K, List[SequenceExample]]]
         - PCollection[Tuple[K, List[Bytes]]]
     inference_spec_type: Model inference endpoint.
+    load_override_fn: Optional function taking a model path and sequence of
+      tags, and returning a tf SavedModel. The loaded model must be equivalent
+      in interface to the model that would otherwise be loaded. It is up to the
+      caller to ensure compatibility. This argument is experimental and subject
+      to change.
 
   Returns:
     A PCollection of Tuple[K, List[PredictionLog]].
   """
-  return (
-      examples
-      |
-      'RunInferenceImpl' >> run_inference.RunInferenceImpl(inference_spec_type))
+  return (examples
+          | 'RunInferenceOnKeyedBatchesImpl' >> run_inference.RunInferenceImpl(
+              inference_spec_type, load_override_fn))
+
+
+@beam.ptransform_fn
+@beam.typehints.with_input_types(Tuple[_K, List[_INPUT_TYPE]])
+@beam.typehints.with_output_types(Tuple[_K, Tuple[List[_OUTPUT_TYPE]]])
+def RunInferencePerModelOnKeyedBatches(
+    examples: beam.pvalue.PCollection,
+    inference_spec_types: Iterable[model_spec_pb2.InferenceSpecType],
+    load_override_fn: Optional[Callable[[str, Sequence[str]], Any]] = None
+) -> beam.pvalue.PCollection:
+  """Run inference over pre-batched keyed inputs on multiple models.
+
+  This API is experimental and may change in the future.
+
+  Supports the same inference specs as RunInferencePerModel. Inputs must consist
+  of a keyed list of examples, and outputs consist of keyed list of prediction
+  logs corresponding by index.
+
+  Args:
+    examples: A PCollection of keyed, batched inputs of type Example,
+      SequenceExample, or bytes. Each type support inference specs corresponding
+      to the unbatched cases described in RunInferencePerModel. Supports -
+      PCollection[Tuple[K, List[Example]]] - PCollection[Tuple[K,
+      List[SequenceExample]]] - PCollection[Tuple[K, List[Bytes]]]
+    inference_spec_types: A flat iterable of Model inference endpoints.
+      Inference will happen in a fused fashion (ie without data
+      materialization), sequentially across Models within a Beam thread (but in
+      parallel across threads and workers).
+    load_override_fn: Optional function taking a model path and sequence of
+      tags, and returning a tf SavedModel. The loaded model must be equivalent
+      in interface to the model that would otherwise be loaded. It is up to the
+      caller to ensure compatibility. This argument is experimental and subject
+      to change.
+
+  Returns:
+    A PCollection containing Tuples of a key and lists of batched prediction
+    logs from each model provided in inference_spec_types. The Tuple of batched
+    prediction logs is 1-1 aligned with inference_spec_types. The individual
+    prediction logs in the batch are 1-1 aligned with the rows of data in the
+    batch key.
+  """
+  return (examples
+          | 'RunInferencePerModelOnKeyedBatchesImpl' >>
+          run_inference.RunInferencePerModelImpl(inference_spec_types,
+                                                 load_override_fn))
```

## Comparing `tfx_bsl-1.8.0.dist-info/LICENSE` & `tfx_bsl-1.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tfx_bsl-1.8.0.dist-info/METADATA` & `tfx_bsl-1.9.0.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tfx-bsl
-Version: 1.8.0
+Version: 1.9.0
 Summary: tfx_bsl (TFX Basic Shared Libraries) contains libraries shared by many TFX (TensorFlow eXtended) libraries and components.
 Home-page: https://www.tensorflow.org/tfx
 Download-URL: https://github.com/tensorflow/tfx-bsl/tags
 Author: Google LLC
 Author-email: tensorflow-extended-dev@googlegroups.com
 License: Apache 2.0
 Keywords: tfx bsl
@@ -33,19 +33,19 @@
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: absl-py (<2.0.0,>=0.9)
 Requires-Dist: apache-beam[gcp] (<3,>=2.38)
 Requires-Dist: google-api-python-client (<2,>=1.7.11)
 Requires-Dist: numpy (<2,>=1.16)
 Requires-Dist: pandas (<2,>=1.0)
-Requires-Dist: protobuf (<4,>=3.13)
+Requires-Dist: protobuf (<3.21,>=3.13)
 Requires-Dist: pyarrow (<6,>=1)
-Requires-Dist: tensorflow (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5)
-Requires-Dist: tensorflow-metadata (<1.9.0,>=1.8.0)
-Requires-Dist: tensorflow-serving-api (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15)
+Requires-Dist: tensorflow (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5)
+Requires-Dist: tensorflow-metadata (<1.10.0,>=1.9.0)
+Requires-Dist: tensorflow-serving-api (!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15)
 
 # TFX Basic Shared Libraries
 
 [![Python](https://img.shields.io/badge/python%20-3.7%7C3.8%7C3.9-blue)](https://github.com/tensorflow/tfx-bsl)
 [![PyPI](https://badge.fury.io/py/tfx-bsl.svg)](https://badge.fury.io/py/tfx-bsl)
 
 TFX Basic Shared Libraries (`tfx_bsl`) contains libraries shared by many
@@ -189,15 +189,16 @@
 
 The following table is the `tfx_bsl` package versions that are compatible with
 each other. This is determined by our testing framework, but other *untested*
 combinations may also work.
 
 tfx-bsl                                                                         | apache-beam[gcp] | pyarrow  | tensorflow        | tensorflow-metadata | tensorflow-serving-api |
 ------------------------------------------------------------------------------- | -----------------| ---------|-------------------|---------------------|------------------------|
-[GitHub master](https://github.com/tensorflow/tfx-bsl/blob/master/RELEASE.md)   | 2.38.0           | 5.0.0    | nightly (1.x/2.x) | 1.8.0               | 2.8.0                  |
+[GitHub master](https://github.com/tensorflow/tfx-bsl/blob/master/RELEASE.md)   | 2.38.0           | 5.0.0    | nightly (1.x/2.x) | 1.9.0               | 2.9.0                  |
+[1.9.0](https://github.com/tensorflow/tfx-bsl/blob/v1.9.0/RELEASE.md)           | 2.38.0           | 5.0.0    | 1.15 / 2.9        | 1.9.0               | 2.9.0                  |
 [1.8.0](https://github.com/tensorflow/tfx-bsl/blob/v1.8.0/RELEASE.md)           | 2.38.0           | 5.0.0    | 1.15 / 2.8        | 1.8.0               | 2.8.0                  |
 [1.7.0](https://github.com/tensorflow/tfx-bsl/blob/v1.7.0/RELEASE.md)           | 2.36.0           | 5.0.0    | 1.15 / 2.8        | 1.7.0               | 2.8.0                  |
 [1.6.0](https://github.com/tensorflow/tfx-bsl/blob/v1.6.0/RELEASE.md)           | 2.35.0           | 5.0.0    | 1.15 / 2.7        | 1.6.0               | 2.7.0                  |
 [1.5.0](https://github.com/tensorflow/tfx-bsl/blob/v1.4.0/RELEASE.md)           | 2.34.0           | 5.0.0    | 1.15 / 2.7        | 1.5.0               | 2.7.0                  |
 [1.4.0](https://github.com/tensorflow/tfx-bsl/blob/v1.4.0/RELEASE.md)           | 2.31.0           | 5.0.0    | 1.15 / 2.6        | 1.4.0               | 2.6.0                  |
 [1.3.0](https://github.com/tensorflow/tfx-bsl/blob/v1.3.0/RELEASE.md)           | 2.31.0           | 2.0.0    | 1.15 / 2.6        | 1.2.0               | 2.6.0                  |
 [1.2.0](https://github.com/tensorflow/tfx-bsl/blob/v1.2.0/RELEASE.md)           | 2.31.0           | 2.0.0    | 1.15 / 2.5        | 1.2.0               | 2.5.1                  |
```

## Comparing `tfx_bsl-1.8.0.dist-info/RECORD` & `tfx_bsl-1.9.0.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,41 +1,41 @@
 tfx_bsl/__init__.py,sha256=w8-SFTUKmPjJ1NEcaDidi-IUq68BpzHnc9SmSmFgLlw,663
 tfx_bsl/types_compat.py,sha256=DMkWXRp00_hfinRkTqsgg68ATODz1Wu-pbBHFlUlXyc,1469
-tfx_bsl/version.py,sha256=EElT-X0xR3Xs8ZTLt1-Fld6Y7ZdZtYl5q2rdiAIDrME,701
+tfx_bsl/version.py,sha256=-GSjS6JH6itan9SrQRoKu2Oq15UYKv5zjL09EbL3CpY,701
 tfx_bsl/arrow/__init__.py,sha256=xjQcxuGgPUfxt1tJqPAztbX33ez_2M4QobBSs-V2z5s,588
 tfx_bsl/arrow/array_util.py,sha256=O2zRUuCM8E2zJQtOlZgccWGwhxUyGgNHmuj1qovfUzc,4659
 tfx_bsl/arrow/array_util_test.py,sha256=dMkw5vb0YapVRAmG43JVVfCw4p5ru-ndnGiORceKhZE,27538
 tfx_bsl/arrow/path.py,sha256=hNrGA6qumwxM6YVHbQksyDEOlSMYPwSwin8k6mjr9-I,4311
 tfx_bsl/arrow/sql_util.py,sha256=x7Vc9rUfbWHhIBvLEo7B-Q74FHFpf_jLDGqtdglB4rw,1072
 tfx_bsl/arrow/sql_util_test.py,sha256=7RjM2HafymdI5mR4-toKt9Vfzg_FLcGJ0PWOPz3Nyao,14026
 tfx_bsl/arrow/table_util.py,sha256=O1VeI9k-qTnykv0Zrv1nPIqLJCPN6sGLT5vjVVRLURw,5842
 tfx_bsl/arrow/table_util_test.py,sha256=_oegt9Y1ZFFLCfbiO2kERGObnrWez6QRqcKeSq1pgW0,22351
 tfx_bsl/beam/__init__.py,sha256=-8vGONbVo604JbIbqHezFhj2OFiMq0kQVqC1LPmrDbM,655
-tfx_bsl/beam/run_inference.py,sha256=hkCP2n3uhU0n21J2AslHTZh33sQ5A89OTaoaP6YlmMs,47952
+tfx_bsl/beam/run_inference.py,sha256=ntyJ8cwljD-9dwA24g2P1cyYNZcVjHC8dnlPke3gETE,50408
 tfx_bsl/beam/run_inference_base.py,sha256=vcUFgmlYzROFETl1qiUQqfHOC-avjpMW0ehGQvseeIo,9032
 tfx_bsl/beam/run_inference_base_test.py,sha256=vaMxGGWWBPvDdoqR6tOTggQibxRxM9oRarCij7B_iSk,5440
-tfx_bsl/beam/run_inference_test.py,sha256=p62EJtZsR8fROGOtT8KUYTDiFI3tBm9Ym3YR2TYw7y0,39682
+tfx_bsl/beam/run_inference_test.py,sha256=ads0S7BlWXCMdw5BsOTRKiz2x2NxhLmDyqWZ4Np2C3I,45565
 tfx_bsl/beam/test_helpers.py,sha256=5WfQRchxab18gLelyu_ff7DSmN2U5OK9WY0G8vxIa1w,816
 tfx_bsl/cc/__init__.py,sha256=dVYJ1zN-zFHDj9Jd_CQMeTgt-yC2SEUUo35BYlI0Dn8,1003
-tfx_bsl/cc/tfx_bsl_extension.pyd,sha256=0ZR28tXXwYJ-dPlOyauX5B-SXNOVwH89WYPZlH8-n7Y,5683712
+tfx_bsl/cc/tfx_bsl_extension.pyd,sha256=j6t7Yg0moiIo1MAqrZOx-t6qYC4ftyxCOBk_cpFqDXw,5683712
 tfx_bsl/coders/__init__.py,sha256=xjQcxuGgPUfxt1tJqPAztbX33ez_2M4QobBSs-V2z5s,588
 tfx_bsl/coders/batch_util.py,sha256=-p4nqV6ZF20pnoKZmLVxPjPxytAUb1JqyVgm_eFglW4,1243
 tfx_bsl/coders/batch_util_test.py,sha256=4LO3k1Oz7scUDyED9jkTwc5aEQ4pGk-gHZsJbU7lHpc,1146
 tfx_bsl/coders/csv_decoder.py,sha256=xEq03ZhjZdRmg50Qw-c40bEEV9lOOpEehtb4EKxoS3w,20295
 tfx_bsl/coders/csv_decoder_test.py,sha256=a-Jw6XHDFHZUhkTD5Gd2Bgbf48_QJPOn5_Zpw6w620Y,29400
 tfx_bsl/coders/example_coder.py,sha256=Zmq_5J4AJ8s8hEGLTmEyLPiBnkuk_5DibaNX8gVyULI,3466
 tfx_bsl/coders/example_coder_test.py,sha256=yfkCoeSbN4UPZRn-uFW_x2NYcpnzmxvaPPfpZWotBZY,23706
 tfx_bsl/coders/example_numpy_decoder_test.py,sha256=F_2ax7Ekf7HgQSKZ4mAsZp7qO3JK-FXJjH_whBvk-sY,4690
 tfx_bsl/coders/sequence_example_coder.py,sha256=_V2oBBo_lz5eJTsR9GV8wWPRqYN1vJoIv9tKjfHNv-I,1145
 tfx_bsl/coders/sequence_example_coder_test.py,sha256=mBDeFxTQwJtnXIr61R4kONegPff1e0qvnA_t7DywLQw,28811
 tfx_bsl/coders/tf_graph_record_decoder.py,sha256=m8gmZ1Edm0METvo1ulqxwR4b035cI1koXniF6odiPp0,7706
 tfx_bsl/coders/tf_graph_record_decoder_test.py,sha256=l9fcK1JmyfVw45f0mH6jHThvX41ny5gdDo8pGKGWcnw,5919
 tfx_bsl/public/__init__.py,sha256=0-5Otda69Ih32lKgw3XMJCOUjGp8JUfeFVkvPF4JWbY,657
 tfx_bsl/public/beam/__init__.py,sha256=hMAUoPXY5IrkZ3ZaCu2eTgpsNeE9u_BL8jH0z2bYdvg,717
-tfx_bsl/public/beam/run_inference.py,sha256=nGXP-FDtVdQKGYXyvwMBwJlAh0Df-_H3nOP_XROIIXw,8585
+tfx_bsl/public/beam/run_inference.py,sha256=diZV2ClNF7jPDIdyVAiW3_0EqyQ_b6EqgNN6eTorJug,11365
 tfx_bsl/public/proto/__init__.py,sha256=xjQcxuGgPUfxt1tJqPAztbX33ez_2M4QobBSs-V2z5s,588
 tfx_bsl/public/proto/model_spec_pb2.py,sha256=rit5_obqUWHU8a6YiVHY8jICK46Fp14Q63PmqEM09MY,9724
 tfx_bsl/public/tfxio/__init__.py,sha256=IhlYJMeI33LstoIL7Dz199chUR2qvEPJfUEC2N8vI68,1842
 tfx_bsl/public/tfxio/tfxio_import_test.py,sha256=TR4rlQu-d0Tdh7tQ6tqXXi9sLjNJvlamlrGWUAgVi2Q,1639
 tfx_bsl/sketches/__init__.py,sha256=wPDrH4iL3efrHNlGcpuRbu8PJOiwOn15juFJ0LaCbJc,1326
 tfx_bsl/sketches/kmv_sketch_test.py,sha256=RtY8AC8OSRQussKZvLSLtqlXq0l-ckD5fcmT7piCiwM,3670
 tfx_bsl/sketches/misragries_sketch_test.py,sha256=MVn_CIXoZ_JTWMK_WjpkMu0ZbVT-gLeYXQKUOBMcyMM,11921
@@ -81,13 +81,13 @@
 tfx_bsl/tfxio/tf_sequence_example_record_test.py,sha256=tus1Av79kM2HgSycCmfrYqvKERHJy8G3Ee5towq97qM,12863
 tfx_bsl/tfxio/tfxio.py,sha256=mGOMpv8QRocj8MsZtxvzxgkeXN_YcuAgWUDMT4kmOWE,7965
 tfx_bsl/tfxio/tfxio_test.py,sha256=BgSTUKV-Dbq3n56zjZCSiQYGa36P2hMReFwNpoTGW-Q,2959
 tfx_bsl/types/__init__.py,sha256=DmMYJykvWZgPQZxfSNsvH4uNrj6WVr49pHkR_HVRBPE,588
 tfx_bsl/types/common_types.py,sha256=JapdJncGxyZdQxaH-phvrcuHgJZEOMZkT_fbNfAIEZI,933
 tfx_bsl/types/tfx_namedtuple.py,sha256=hNS8Bx_Cpr1Iv9m6wPpoFSYZnEYmeLHtE_BYIKLPFII,2373
 tfx_bsl/types/tfx_namedtuple_test.py,sha256=UPSERwWWORCBbGjojQOYwGquy1vE1XKe0fWhF6CMVzg,4835
-tfx_bsl-1.8.0.dist-info/LICENSE,sha256=2padWTY4cdbwrhXyjGmE25fqaS-v5TZue0Jcmb2vHso,12806
-tfx_bsl-1.8.0.dist-info/METADATA,sha256=4C9aHTV9qOR2UjjUMC1J1yQ3vUQvFR53w5a7asK6Pu0,9862
-tfx_bsl-1.8.0.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
-tfx_bsl-1.8.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-tfx_bsl-1.8.0.dist-info/top_level.txt,sha256=jWNP_PK7bhIibnqrcI2pgKzLwoHyqOqOf-DH8zxV4tw,8
-tfx_bsl-1.8.0.dist-info/RECORD,,
+tfx_bsl-1.9.0.dist-info/LICENSE,sha256=2padWTY4cdbwrhXyjGmE25fqaS-v5TZue0Jcmb2vHso,12806
+tfx_bsl-1.9.0.dist-info/METADATA,sha256=dMImRFTzWXPmbX2ksYBQZdzU2mQBipOBrG7csj8cgrA,10061
+tfx_bsl-1.9.0.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
+tfx_bsl-1.9.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+tfx_bsl-1.9.0.dist-info/top_level.txt,sha256=jWNP_PK7bhIibnqrcI2pgKzLwoHyqOqOf-DH8zxV4tw,8
+tfx_bsl-1.9.0.dist-info/RECORD,,
```

