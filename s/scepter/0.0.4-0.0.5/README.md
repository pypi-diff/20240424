# Comparing `tmp/scepter-0.0.4-py3-none-any.whl.zip` & `tmp/scepter-0.0.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,300 +1,316 @@
-Zip file size: 571837 bytes, number of entries: 298
--rw-r--r--  2.0 unx      602 b- defN 24-Apr-10 07:40 scepter/__init__.py
--rw-r--r--  2.0 unx      205 b- defN 24-Apr-10 07:39 scepter/version.py
--rw-r--r--  2.0 unx    12618 b- defN 24-Mar-30 17:03 scepter/methods/examples/classification/example.yaml
--rw-r--r--  2.0 unx     5383 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_1.5_512.yaml
--rw-r--r--  2.0 unx     5568 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_1.5_512_lora.yaml
--rw-r--r--  2.0 unx     4792 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_512.yaml
--rw-r--r--  2.0 unx     4982 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_512_lora.yaml
--rw-r--r--  2.0 unx     4789 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_768.yaml
--rw-r--r--  2.0 unx     4978 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_768_lora.yaml
--rw-r--r--  2.0 unx     9088 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_xl_1024.yaml
--rw-r--r--  2.0 unx     9277 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_xl_1024_lora.yaml
--rw-r--r--  2.0 unx     6440 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sd15_512_sce_ctr_hed.yaml
--rw-r--r--  2.0 unx     6315 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sd21_768_sce_ctr_canny.yaml
--rw-r--r--  2.0 unx     6434 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml
--rw-r--r--  2.0 unx    10042 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_canny.yaml
--rw-r--r--  2.0 unx    10058 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color.yaml
--rw-r--r--  2.0 unx    10420 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color_datatxt.yaml
--rw-r--r--  2.0 unx    10135 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_depth.yaml
--rw-r--r--  2.0 unx     5639 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd15_512_sce_t2i.yaml
--rw-r--r--  2.0 unx     5611 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd15_512_sce_t2i_swift.yaml
--rw-r--r--  2.0 unx     5045 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml
--rw-r--r--  2.0 unx     5017 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd21_768_sce_t2i_swift.yaml
--rw-r--r--  2.0 unx     9370 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml
--rw-r--r--  2.0 unx     9514 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_datatxt.yaml
--rw-r--r--  2.0 unx     9324 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_swift.yaml
--rw-r--r--  2.0 unx     2539 b- defN 24-Apr-10 07:39 scepter/methods/studio/scepter_ui.yaml
--rw-r--r--  2.0 unx     1707 b- defN 24-Mar-30 17:03 scepter/methods/studio/extensions/controllers/official_controllers.yaml
--rw-r--r--  2.0 unx   294732 b- defN 24-Mar-30 17:03 scepter/methods/studio/extensions/mantra_book/mantra_book.yaml
--rw-r--r--  2.0 unx    29045 b- defN 24-Mar-30 17:03 scepter/methods/studio/extensions/tuners/official_tuners.yaml
--rw-r--r--  2.0 unx     2833 b- defN 24-Mar-30 17:03 scepter/methods/studio/home/home.yaml
--rw-r--r--  2.0 unx     3317 b- defN 24-Apr-10 07:39 scepter/methods/studio/inference/inference.yaml
--rw-r--r--  2.0 unx    10894 b- defN 24-Apr-10 07:39 scepter/methods/studio/inference/largen/largen_pro.yaml
--rw-r--r--  2.0 unx    10136 b- defN 24-Mar-30 17:03 scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml
--rw-r--r--  2.0 unx     2880 b- defN 24-Mar-30 17:03 scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml
--rw-r--r--  2.0 unx     2788 b- defN 24-Mar-30 17:03 scepter/methods/studio/inference/stable_diffusion/sd21_pro.yaml
--rw-r--r--  2.0 unx      148 b- defN 24-Mar-30 17:03 scepter/methods/studio/preprocess/preprocess.yaml
--rw-r--r--  2.0 unx      519 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/self_train.yaml
--rw-r--r--  2.0 unx    28594 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml
--rw-r--r--  2.0 unx     8290 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml
--rw-r--r--  2.0 unx     6781 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml
--rw-r--r--  2.0 unx       61 b- defN 24-Apr-10 07:39 scepter/methods/studio/tuner_manager/tuner_manager.yaml
--rw-r--r--  2.0 unx      187 b- defN 24-Mar-30 17:03 scepter/modules/__init__.py
--rw-r--r--  2.0 unx      628 b- defN 24-Mar-30 17:03 scepter/modules/annotator/__init__.py
--rw-r--r--  2.0 unx     2095 b- defN 24-Mar-30 17:03 scepter/modules/annotator/base_annotator.py
--rw-r--r--  2.0 unx     2474 b- defN 24-Mar-30 17:03 scepter/modules/annotator/canny.py
--rw-r--r--  2.0 unx     2159 b- defN 24-Mar-30 17:03 scepter/modules/annotator/color.py
--rw-r--r--  2.0 unx     6346 b- defN 24-Mar-30 17:03 scepter/modules/annotator/hed.py
--rw-r--r--  2.0 unx      767 b- defN 24-Mar-30 17:03 scepter/modules/annotator/identity.py
--rw-r--r--  2.0 unx      769 b- defN 24-Mar-30 17:03 scepter/modules/annotator/invert.py
--rw-r--r--  2.0 unx     3261 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas_op.py
--rw-r--r--  2.0 unx     2891 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd_op.py
--rw-r--r--  2.0 unx    35386 b- defN 24-Mar-30 17:03 scepter/modules/annotator/openpose.py
--rw-r--r--  2.0 unx     1223 b- defN 24-Mar-30 17:03 scepter/modules/annotator/registry.py
--rw-r--r--  2.0 unx     3503 b- defN 24-Mar-30 17:03 scepter/modules/annotator/utils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/__init__.py
--rw-r--r--  2.0 unx     5214 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/api.py
--rw-r--r--  2.0 unx      391 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/base_model.py
--rw-r--r--  2.0 unx    11717 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/blocks.py
--rw-r--r--  2.0 unx     3205 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/dpt_depth.py
--rw-r--r--  2.0 unx     2769 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/midas_net.py
--rw-r--r--  2.0 unx     6019 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/midas_net_custom.py
--rw-r--r--  2.0 unx     8178 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/transforms.py
--rw-r--r--  2.0 unx     4691 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/utils.py
--rw-r--r--  2.0 unx    15580 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/vit.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/__init__.py
--rw-r--r--  2.0 unx    10166 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/mbv2_mlsd_large.py
--rw-r--r--  2.0 unx     9706 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/mbv2_mlsd_tiny.py
--rw-r--r--  2.0 unx    25395 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/utils.py
--rw-r--r--  2.0 unx      125 b- defN 24-Mar-30 17:03 scepter/modules/data/__init__.py
--rw-r--r--  2.0 unx      599 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/__init__.py
--rw-r--r--  2.0 unx     3941 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/base_dataset.py
--rw-r--r--  2.0 unx     9148 b- defN 24-Apr-10 07:39 scepter/modules/data/dataset/dataset.py
--rw-r--r--  2.0 unx    11650 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/ms_dataset.py
--rw-r--r--  2.0 unx    15112 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/registry.py
--rw-r--r--  2.0 unx      771 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/utils.py
--rw-r--r--  2.0 unx      407 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/__init__.py
--rw-r--r--  2.0 unx     1002 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/base_sampler.py
--rw-r--r--  2.0 unx     2013 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/registry.py
--rw-r--r--  2.0 unx    20503 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/sampler.py
--rw-r--r--  2.0 unx      151 b- defN 24-Mar-30 17:03 scepter/modules/inference/__init__.py
--rw-r--r--  2.0 unx     5117 b- defN 24-Apr-10 07:39 scepter/modules/inference/control_inference.py
--rw-r--r--  2.0 unx    31050 b- defN 24-Apr-10 07:39 scepter/modules/inference/diffusion_inference.py
--rw-r--r--  2.0 unx    25815 b- defN 24-Apr-10 07:39 scepter/modules/inference/largen_inference.py
--rw-r--r--  2.0 unx    11183 b- defN 24-Mar-30 17:03 scepter/modules/inference/tuner_inference.py
--rw-r--r--  2.0 unx      218 b- defN 24-Mar-30 17:03 scepter/modules/model/__init__.py
--rw-r--r--  2.0 unx     4022 b- defN 24-Mar-30 17:03 scepter/modules/model/base_model.py
--rw-r--r--  2.0 unx     1653 b- defN 24-Mar-30 17:03 scepter/modules/model/registry.py
--rw-r--r--  2.0 unx      202 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/__init__.py
--rw-r--r--  2.0 unx      300 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/autoencoder/__init__.py
--rw-r--r--  2.0 unx    12971 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/autoencoder/ae_module.py
--rw-r--r--  2.0 unx     9569 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/autoencoder/ae_utils.py
--rw-r--r--  2.0 unx      445 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/__init__.py
--rw-r--r--  2.0 unx     2791 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/mlp.py
--rw-r--r--  2.0 unx     3235 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/resnet.py
--rw-r--r--  2.0 unx    21928 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/resnet_impl.py
--rw-r--r--  2.0 unx     1625 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/timm_model.py
--rw-r--r--  2.0 unx     8783 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/vit_modify.py
--rw-r--r--  2.0 unx      133 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/__init__.py
--rw-r--r--  2.0 unx    21405 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/clip.py
--rw-r--r--  2.0 unx     5109 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/simple_tokenizer.py
--rw-r--r--  2.0 unx    21302 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/vit.py
--rw-r--r--  2.0 unx      303 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/unet/__init__.py
--rw-r--r--  2.0 unx    56311 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/unet/unet_module.py
--rw-r--r--  2.0 unx    46227 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/unet/unet_utils.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/utils/__init__.py
--rw-r--r--  2.0 unx     2031 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/utils/transformer.py
--rw-r--r--  2.0 unx      318 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/__init__.py
--rw-r--r--  2.0 unx     7081 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/init_helper.py
--rw-r--r--  2.0 unx    30685 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/resnet_3d.py
--rw-r--r--  2.0 unx    13371 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/video/video_transformer.py
--rw-r--r--  2.0 unx      683 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/__init__.py
--rw-r--r--  2.0 unx     1797 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/base_branch.py
--rw-r--r--  2.0 unx     4466 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/csn_branch.py
--rw-r--r--  2.0 unx     3405 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/non_local.py
--rw-r--r--  2.0 unx     5942 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/r2d3d_branch.py
--rw-r--r--  2.0 unx     8046 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/r2plus1d_branch.py
--rw-r--r--  2.0 unx    11916 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/tada_conv.py
--rw-r--r--  2.0 unx    12202 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/transformer_branch.py
--rw-r--r--  2.0 unx     2530 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/visualize_3d_module.py
--rw-r--r--  2.0 unx      575 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/__init__.py
--rw-r--r--  2.0 unx     3006 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/base_2d_stem.py
--rw-r--r--  2.0 unx     3160 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/base_3d_stem.py
--rw-r--r--  2.0 unx     1240 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/down_sample_stem.py
--rw-r--r--  2.0 unx     5534 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/embedding_stem.py
--rw-r--r--  2.0 unx     2576 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/r2plus1d_stem.py
--rw-r--r--  2.0 unx      602 b- defN 24-Apr-10 07:39 scepter/modules/model/embedder/__init__.py
--rw-r--r--  2.0 unx      915 b- defN 24-Mar-30 17:03 scepter/modules/model/embedder/base_embedder.py
--rw-r--r--  2.0 unx    28394 b- defN 24-Apr-10 07:39 scepter/modules/model/embedder/embedder.py
--rw-r--r--  2.0 unx     5133 b- defN 24-Apr-10 07:39 scepter/modules/model/embedder/resampler.py
--rw-r--r--  2.0 unx      524 b- defN 24-Apr-10 07:39 scepter/modules/model/head/__init__.py
--rw-r--r--  2.0 unx    11687 b- defN 24-Mar-30 17:03 scepter/modules/model/head/classifier_head.py
--rw-r--r--  2.0 unx      214 b- defN 24-Mar-30 17:03 scepter/modules/model/loss/__init__.py
--rw-r--r--  2.0 unx     3955 b- defN 24-Mar-30 17:03 scepter/modules/model/loss/base_losses.py
--rw-r--r--  2.0 unx     2178 b- defN 24-Mar-30 17:03 scepter/modules/model/loss/rec_loss.py
--rw-r--r--  2.0 unx      286 b- defN 24-Apr-01 03:43 scepter/modules/model/metric/__init__.py
--rw-r--r--  2.0 unx      783 b- defN 24-Mar-30 17:03 scepter/modules/model/metric/base_metric.py
--rw-r--r--  2.0 unx     5076 b- defN 24-Mar-30 17:03 scepter/modules/model/metric/classification.py
--rw-r--r--  2.0 unx      183 b- defN 24-Mar-30 17:03 scepter/modules/model/metric/registry.py
--rw-r--r--  2.0 unx      220 b- defN 24-Mar-30 17:03 scepter/modules/model/neck/__init__.py
--rw-r--r--  2.0 unx     1870 b- defN 24-Mar-30 17:03 scepter/modules/model/neck/global_average_pooling.py
--rw-r--r--  2.0 unx      918 b- defN 24-Mar-30 17:03 scepter/modules/model/neck/identity.py
--rw-r--r--  2.0 unx      402 b- defN 24-Mar-30 17:03 scepter/modules/model/network/__init__.py
--rw-r--r--  2.0 unx     7188 b- defN 24-Apr-01 03:43 scepter/modules/model/network/classifier.py
--rw-r--r--  2.0 unx     1189 b- defN 24-Mar-30 17:03 scepter/modules/model/network/train_module.py
--rw-r--r--  2.0 unx      148 b- defN 24-Mar-30 17:03 scepter/modules/model/network/autoencoder/__init__.py
--rw-r--r--  2.0 unx     9601 b- defN 24-Apr-10 07:39 scepter/modules/model/network/autoencoder/ae_kl.py
--rw-r--r--  2.0 unx      211 b- defN 24-Mar-30 17:03 scepter/modules/model/network/diffusion/__init__.py
--rw-r--r--  2.0 unx    25265 b- defN 24-Apr-10 07:39 scepter/modules/model/network/diffusion/diffusion.py
--rw-r--r--  2.0 unx     6226 b- defN 24-Mar-30 17:03 scepter/modules/model/network/diffusion/schedules.py
--rw-r--r--  2.0 unx    22561 b- defN 24-Mar-30 17:03 scepter/modules/model/network/diffusion/solvers.py
--rw-r--r--  2.0 unx      385 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/__init__.py
--rw-r--r--  2.0 unx    19522 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/ldm.py
--rw-r--r--  2.0 unx     5860 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/ldm_sce.py
--rw-r--r--  2.0 unx    21293 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/ldm_xl.py
--rw-r--r--  2.0 unx      368 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/__init__.py
--rw-r--r--  2.0 unx      770 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/base_tokenizer.py
--rw-r--r--  2.0 unx     5670 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/tokenizer.py
--rw-r--r--  2.0 unx     1757 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/tokenizer_component.py
--rw-r--r--  2.0 unx      260 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/__init__.py
--rw-r--r--  2.0 unx      742 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/base_tuner.py
--rw-r--r--  2.0 unx     4381 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/swift_tuner.py
--rw-r--r--  2.0 unx     6852 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/tuner_component.py
--rw-r--r--  2.0 unx      965 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/tuner_utils.py
--rw-r--r--  2.0 unx      222 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/sce/__init__.py
--rw-r--r--  2.0 unx     6716 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/sce/scetuning.py
--rw-r--r--  2.0 unx     2485 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/sce/scetuning_component.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/modules/model/utils/__init__.py
--rw-r--r--  2.0 unx     3344 b- defN 24-Mar-30 17:03 scepter/modules/model/utils/basic_utils.py
--rw-r--r--  2.0 unx     3776 b- defN 24-Apr-10 07:39 scepter/modules/model/utils/data_utils.py
--rw-r--r--  2.0 unx      133 b- defN 24-Mar-30 17:03 scepter/modules/opt/__init__.py
--rw-r--r--  2.0 unx      298 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/__init__.py
--rw-r--r--  2.0 unx      227 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/base_scheduler.py
--rw-r--r--  2.0 unx     3005 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/define_schedulers.py
--rw-r--r--  2.0 unx    14312 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/official_schedulers.py
--rw-r--r--  2.0 unx     1405 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/registry.py
--rw-r--r--  2.0 unx     5420 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/warmup.py
--rw-r--r--  2.0 unx      608 b- defN 24-Apr-10 07:39 scepter/modules/opt/optimizers/__init__.py
--rw-r--r--  2.0 unx      226 b- defN 24-Mar-30 17:03 scepter/modules/opt/optimizers/base_optimizer.py
--rw-r--r--  2.0 unx    19032 b- defN 24-Mar-30 17:03 scepter/modules/opt/optimizers/official_optimizers.py
--rw-r--r--  2.0 unx     1388 b- defN 24-Mar-30 17:03 scepter/modules/opt/optimizers/registry.py
--rw-r--r--  2.0 unx      314 b- defN 24-Mar-30 17:03 scepter/modules/solver/__init__.py
--rw-r--r--  2.0 unx    35587 b- defN 24-Mar-30 17:03 scepter/modules/solver/base_solver.py
--rw-r--r--  2.0 unx    31755 b- defN 24-Apr-10 07:39 scepter/modules/solver/diffusion_solver.py
--rw-r--r--  2.0 unx     1351 b- defN 24-Mar-30 17:03 scepter/modules/solver/registry.py
--rw-r--r--  2.0 unx     7538 b- defN 24-Mar-30 17:03 scepter/modules/solver/train_val_solver.py
--rw-r--r--  2.0 unx     1654 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/__init__.py
--rw-r--r--  2.0 unx     3617 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/backward.py
--rw-r--r--  2.0 unx    11993 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/checkpoint.py
--rw-r--r--  2.0 unx     4944 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/data_probe.py
--rw-r--r--  2.0 unx     2090 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/ema.py
--rw-r--r--  2.0 unx      611 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/hook.py
--rw-r--r--  2.0 unx    10821 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/log.py
--rw-r--r--  2.0 unx     5202 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/lr.py
--rw-r--r--  2.0 unx      154 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/registry.py
--rw-r--r--  2.0 unx     2192 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/safetensors.py
--rw-r--r--  2.0 unx     1544 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/sampler.py
--rw-r--r--  2.0 unx     1718 b- defN 24-Mar-30 17:03 scepter/modules/transform/__init__.py
--rw-r--r--  2.0 unx    19927 b- defN 24-Mar-30 17:03 scepter/modules/transform/augmention.py
--rw-r--r--  2.0 unx     1011 b- defN 24-Mar-30 17:03 scepter/modules/transform/compose.py
--rw-r--r--  2.0 unx      725 b- defN 24-Mar-30 17:03 scepter/modules/transform/identity.py
--rw-r--r--  2.0 unx    22592 b- defN 24-Apr-10 07:39 scepter/modules/transform/image.py
--rw-r--r--  2.0 unx    12249 b- defN 24-Apr-10 07:39 scepter/modules/transform/io.py
--rw-r--r--  2.0 unx    18995 b- defN 24-Mar-30 17:03 scepter/modules/transform/io_video.py
--rw-r--r--  2.0 unx     1984 b- defN 24-Mar-30 17:03 scepter/modules/transform/registry.py
--rw-r--r--  2.0 unx     9951 b- defN 24-Mar-30 17:03 scepter/modules/transform/tensor.py
--rw-r--r--  2.0 unx     3201 b- defN 24-Mar-30 17:03 scepter/modules/transform/transform_xl.py
--rw-r--r--  2.0 unx     1865 b- defN 24-Mar-30 17:03 scepter/modules/transform/utils.py
--rw-r--r--  2.0 unx    18831 b- defN 24-Mar-30 17:03 scepter/modules/transform/video.py
--rw-r--r--  2.0 unx      154 b- defN 24-Mar-30 17:03 scepter/modules/utils/__init__.py
--rw-r--r--  2.0 unx    24691 b- defN 24-Apr-10 07:39 scepter/modules/utils/config.py
--rw-r--r--  2.0 unx     2865 b- defN 24-Mar-30 17:03 scepter/modules/utils/data.py
--rw-r--r--  2.0 unx      482 b- defN 24-Mar-30 17:03 scepter/modules/utils/directory.py
--rw-r--r--  2.0 unx    15775 b- defN 24-Mar-30 17:03 scepter/modules/utils/distribute.py
--rw-r--r--  2.0 unx     3755 b- defN 24-Apr-10 07:39 scepter/modules/utils/export_model.py
--rw-r--r--  2.0 unx    17112 b- defN 24-Apr-10 07:39 scepter/modules/utils/file_system.py
--rw-r--r--  2.0 unx     1684 b- defN 24-Apr-10 07:39 scepter/modules/utils/index.py
--rw-r--r--  2.0 unx     5554 b- defN 24-Mar-30 17:03 scepter/modules/utils/logger.py
--rw-r--r--  2.0 unx     2735 b- defN 24-Mar-30 17:03 scepter/modules/utils/math_plot.py
--rw-r--r--  2.0 unx     5548 b- defN 24-Mar-30 17:03 scepter/modules/utils/model.py
--rw-r--r--  2.0 unx    17482 b- defN 24-Apr-10 07:39 scepter/modules/utils/probe.py
--rw-r--r--  2.0 unx     7995 b- defN 24-Mar-30 17:03 scepter/modules/utils/registry.py
--rw-r--r--  2.0 unx      423 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/__init__.py
--rw-r--r--  2.0 unx    44598 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/aliyun_oss_fs.py
--rw-r--r--  2.0 unx    11136 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/base_fs.py
--rw-r--r--  2.0 unx     4610 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/http_fs.py
--rw-r--r--  2.0 unx     6249 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/huggingface_fs.py
--rw-r--r--  2.0 unx    13560 b- defN 24-Apr-10 07:39 scepter/modules/utils/file_clients/local_fs.py
--rw-r--r--  2.0 unx     7207 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/modelscope_fs.py
--rw-r--r--  2.0 unx      168 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/registry.py
--rw-r--r--  2.0 unx     1067 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/utils.py
--rw-r--r--  2.0 unx      324 b- defN 24-Mar-30 17:03 scepter/modules/utils/video_reader/__init__.py
--rw-r--r--  2.0 unx     6201 b- defN 24-Mar-30 17:03 scepter/modules/utils/video_reader/frame_sampler.py
--rw-r--r--  2.0 unx     5133 b- defN 24-Mar-30 17:03 scepter/modules/utils/video_reader/video_reader.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/__init__.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/home/__init__.py
--rw-r--r--  2.0 unx     1751 b- defN 24-Mar-30 17:03 scepter/studio/home/home.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/__init__.py
--rw-r--r--  2.0 unx      387 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/component_names.py
--rw-r--r--  2.0 unx      719 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/desc_ui.py
--rw-r--r--  2.0 unx      728 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/guide_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/inference/__init__.py
--rw-r--r--  2.0 unx    10711 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_manager/__init__.py
--rw-r--r--  2.0 unx     7003 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_manager/infer_runer.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/__init__.py
--rw-r--r--  2.0 unx    21707 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/component_names.py
--rw-r--r--  2.0 unx     8185 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/control_ui.py
--rw-r--r--  2.0 unx     8631 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/diffusion_ui.py
--rw-r--r--  2.0 unx    13683 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/gallery_ui.py
--rw-r--r--  2.0 unx    22326 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/largen_ui.py
--rw-r--r--  2.0 unx     8373 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/mantra_ui.py
--rw-r--r--  2.0 unx    11945 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/model_manage_ui.py
--rw-r--r--  2.0 unx     4281 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/refiner_ui.py
--rw-r--r--  2.0 unx     9619 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/tuner_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/__init__.py
--rw-r--r--  2.0 unx     2815 b- defN 24-Apr-10 07:39 scepter/studio/preprocess/preprocess.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/__init__.py
--rw-r--r--  2.0 unx     7823 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/component_names.py
--rw-r--r--  2.0 unx    32565 b- defN 24-Apr-10 07:39 scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py
--rw-r--r--  2.0 unx    13405 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py
--rw-r--r--  2.0 unx     6875 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/__init__.py
--rw-r--r--  2.0 unx     2575 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/scripts/__init__.py
--rw-r--r--  2.0 unx     7438 b- defN 24-Apr-10 07:39 scepter/studio/self_train/scripts/run_task.py
--rw-r--r--  2.0 unx      148 b- defN 24-Apr-10 07:39 scepter/studio/self_train/scripts/sleep.py
--rw-r--r--  2.0 unx    14891 b- defN 24-Apr-10 07:39 scepter/studio/self_train/scripts/trainer.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/self_train_ui/__init__.py
--rw-r--r--  2.0 unx     8907 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train_ui/component_names.py
--rw-r--r--  2.0 unx    22179 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train_ui/model_ui.py
--rw-r--r--  2.0 unx    32081 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train_ui/trainer_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/utils/__init__.py
--rw-r--r--  2.0 unx    12071 b- defN 24-Apr-10 07:39 scepter/studio/self_train/utils/config_parser.py
--rw-r--r--  2.0 unx       74 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/__init__.py
--rw-r--r--  2.0 unx     1249 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/tuner_manager.py
--rw-r--r--  2.0 unx       74 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/__init__.py
--rw-r--r--  2.0 unx    14704 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/browser_ui.py
--rw-r--r--  2.0 unx     1632 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/component_names.py
--rw-r--r--  2.0 unx     3226 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/info_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/__init__.py
--rw-r--r--  2.0 unx      452 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/dict.py
--rw-r--r--  2.0 unx      217 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/path.py
--rw-r--r--  2.0 unx      327 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/yaml.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/utils/__init__.py
--rw-r--r--  2.0 unx      589 b- defN 24-Mar-30 17:03 scepter/studio/utils/env.py
--rw-r--r--  2.0 unx      567 b- defN 24-Mar-30 17:03 scepter/studio/utils/file.py
--rw-r--r--  2.0 unx      276 b- defN 24-Mar-30 17:03 scepter/studio/utils/singleton.py
--rw-r--r--  2.0 unx      545 b- defN 24-Mar-30 17:03 scepter/studio/utils/uibase.py
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/tools/__init__.py
--rw-r--r--  2.0 unx     3371 b- defN 24-Apr-10 07:39 scepter/tools/helper.py
--rw-r--r--  2.0 unx     9409 b- defN 24-Apr-10 07:39 scepter/tools/run_inference.py
--rw-r--r--  2.0 unx     2114 b- defN 24-Apr-10 07:39 scepter/tools/run_train.py
--rw-r--r--  2.0 unx     6671 b- defN 24-Apr-10 07:39 scepter/tools/webui.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/LICENSE
--rw-r--r--  2.0 unx    20162 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    30347 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/RECORD
-298 files, 2329931 bytes uncompressed, 522241 bytes compressed:  77.6%
+Zip file size: 616464 bytes, number of entries: 314
+-rw-r--r--  2.0 unx      602 b- defN 24-Apr-24 03:03 scepter/__init__.py
+-rw-r--r--  2.0 unx      205 b- defN 24-Apr-24 03:02 scepter/version.py
+-rw-r--r--  2.0 unx    12618 b- defN 24-Apr-24 03:02 scepter/methods/examples/classification/example.yaml
+-rw-r--r--  2.0 unx     5383 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_1.5_512.yaml
+-rw-r--r--  2.0 unx     5568 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_1.5_512_lora.yaml
+-rw-r--r--  2.0 unx     5790 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_1.5_512_textlora.yaml
+-rw-r--r--  2.0 unx     4792 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_2.1_512.yaml
+-rw-r--r--  2.0 unx     4982 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_2.1_512_lora.yaml
+-rw-r--r--  2.0 unx     4789 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_2.1_768.yaml
+-rw-r--r--  2.0 unx     4978 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_2.1_768_lora.yaml
+-rw-r--r--  2.0 unx     9088 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_xl_1024.yaml
+-rw-r--r--  2.0 unx     9277 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_xl_1024_lora.yaml
+-rw-r--r--  2.0 unx     9511 b- defN 24-Apr-24 03:02 scepter/methods/examples/generation/stable_diffusion_xl_1024_textlora.yaml
+-rw-r--r--  2.0 unx     6440 b- defN 24-Apr-24 03:02 scepter/methods/scedit/ctr/sd15_512_sce_ctr_hed.yaml
+-rw-r--r--  2.0 unx     6315 b- defN 24-Apr-24 03:02 scepter/methods/scedit/ctr/sd21_768_sce_ctr_canny.yaml
+-rw-r--r--  2.0 unx     6434 b- defN 24-Apr-24 03:02 scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml
+-rw-r--r--  2.0 unx    10042 b- defN 24-Apr-24 03:02 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_canny.yaml
+-rw-r--r--  2.0 unx    10058 b- defN 24-Apr-24 03:02 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color.yaml
+-rw-r--r--  2.0 unx    10420 b- defN 24-Apr-24 03:02 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color_datatxt.yaml
+-rw-r--r--  2.0 unx    10135 b- defN 24-Apr-24 03:02 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_depth.yaml
+-rw-r--r--  2.0 unx     5639 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sd15_512_sce_t2i.yaml
+-rw-r--r--  2.0 unx     5611 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sd15_512_sce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     5831 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sd15_512_textsce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     5045 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml
+-rw-r--r--  2.0 unx     5017 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sd21_768_sce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     9370 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml
+-rw-r--r--  2.0 unx     9514 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_datatxt.yaml
+-rw-r--r--  2.0 unx     9324 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     9556 b- defN 24-Apr-24 03:02 scepter/methods/scedit/t2i/sdxl_1024_textsce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     2539 b- defN 24-Apr-24 03:02 scepter/methods/studio/scepter_ui.yaml
+-rw-r--r--  2.0 unx     1707 b- defN 24-Apr-24 03:02 scepter/methods/studio/extensions/controllers/official_controllers.yaml
+-rw-r--r--  2.0 unx   294732 b- defN 24-Apr-24 03:02 scepter/methods/studio/extensions/mantra_book/mantra_book.yaml
+-rw-r--r--  2.0 unx    29069 b- defN 24-Apr-24 03:02 scepter/methods/studio/extensions/tuners/official_tuners.yaml
+-rw-r--r--  2.0 unx     2833 b- defN 24-Apr-24 03:02 scepter/methods/studio/home/home.yaml
+-rw-r--r--  2.0 unx     3317 b- defN 24-Apr-24 03:02 scepter/methods/studio/inference/inference.yaml
+-rw-r--r--  2.0 unx     2913 b- defN 24-Apr-24 03:02 scepter/methods/studio/inference/edit/stylebooth_tb_pro.yaml
+-rw-r--r--  2.0 unx    10894 b- defN 24-Apr-24 03:02 scepter/methods/studio/inference/largen/largen_pro.yaml
+-rw-r--r--  2.0 unx    10136 b- defN 24-Apr-24 03:02 scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml
+-rw-r--r--  2.0 unx     2880 b- defN 24-Apr-24 03:02 scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml
+-rw-r--r--  2.0 unx     2788 b- defN 24-Apr-24 03:02 scepter/methods/studio/inference/stable_diffusion/sd21_pro.yaml
+-rw-r--r--  2.0 unx     3605 b- defN 24-Apr-24 03:02 scepter/methods/studio/preprocess/preprocess.yaml
+-rw-r--r--  2.0 unx      519 b- defN 24-Apr-24 03:02 scepter/methods/studio/self_train/self_train.yaml
+-rw-r--r--  2.0 unx    28610 b- defN 24-Apr-24 03:02 scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml
+-rw-r--r--  2.0 unx     8343 b- defN 24-Apr-24 03:02 scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml
+-rw-r--r--  2.0 unx     6834 b- defN 24-Apr-24 03:02 scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml
+-rw-r--r--  2.0 unx      515 b- defN 24-Apr-24 03:02 scepter/methods/studio/tuner_manager/tuner_manager.yaml
+-rw-r--r--  2.0 unx      187 b- defN 24-Apr-24 03:02 scepter/modules/__init__.py
+-rw-r--r--  2.0 unx      628 b- defN 24-Apr-24 03:02 scepter/modules/annotator/__init__.py
+-rw-r--r--  2.0 unx     2095 b- defN 24-Apr-24 03:02 scepter/modules/annotator/base_annotator.py
+-rw-r--r--  2.0 unx     2474 b- defN 24-Apr-24 03:02 scepter/modules/annotator/canny.py
+-rw-r--r--  2.0 unx     2159 b- defN 24-Apr-24 03:02 scepter/modules/annotator/color.py
+-rw-r--r--  2.0 unx     6346 b- defN 24-Apr-24 03:02 scepter/modules/annotator/hed.py
+-rw-r--r--  2.0 unx      767 b- defN 24-Apr-24 03:02 scepter/modules/annotator/identity.py
+-rw-r--r--  2.0 unx      769 b- defN 24-Apr-24 03:02 scepter/modules/annotator/invert.py
+-rw-r--r--  2.0 unx     3261 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas_op.py
+-rw-r--r--  2.0 unx     2891 b- defN 24-Apr-24 03:02 scepter/modules/annotator/mlsd_op.py
+-rw-r--r--  2.0 unx    35386 b- defN 24-Apr-24 03:02 scepter/modules/annotator/openpose.py
+-rw-r--r--  2.0 unx     1223 b- defN 24-Apr-24 03:02 scepter/modules/annotator/registry.py
+-rw-r--r--  2.0 unx     3503 b- defN 24-Apr-24 03:02 scepter/modules/annotator/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/__init__.py
+-rw-r--r--  2.0 unx     5214 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/api.py
+-rw-r--r--  2.0 unx      391 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/base_model.py
+-rw-r--r--  2.0 unx    11717 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/blocks.py
+-rw-r--r--  2.0 unx     3205 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/dpt_depth.py
+-rw-r--r--  2.0 unx     2769 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/midas_net.py
+-rw-r--r--  2.0 unx     6019 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/midas_net_custom.py
+-rw-r--r--  2.0 unx     8178 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/transforms.py
+-rw-r--r--  2.0 unx     4691 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/utils.py
+-rw-r--r--  2.0 unx    15580 b- defN 24-Apr-24 03:02 scepter/modules/annotator/midas/vit.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-24 03:02 scepter/modules/annotator/mlsd/__init__.py
+-rw-r--r--  2.0 unx    10166 b- defN 24-Apr-24 03:02 scepter/modules/annotator/mlsd/mbv2_mlsd_large.py
+-rw-r--r--  2.0 unx     9706 b- defN 24-Apr-24 03:02 scepter/modules/annotator/mlsd/mbv2_mlsd_tiny.py
+-rw-r--r--  2.0 unx    25395 b- defN 24-Apr-24 03:02 scepter/modules/annotator/mlsd/utils.py
+-rw-r--r--  2.0 unx      125 b- defN 24-Apr-24 03:02 scepter/modules/data/__init__.py
+-rw-r--r--  2.0 unx      599 b- defN 24-Apr-24 03:02 scepter/modules/data/dataset/__init__.py
+-rw-r--r--  2.0 unx     3941 b- defN 24-Apr-24 03:02 scepter/modules/data/dataset/base_dataset.py
+-rw-r--r--  2.0 unx     9290 b- defN 24-Apr-24 03:02 scepter/modules/data/dataset/dataset.py
+-rw-r--r--  2.0 unx    11650 b- defN 24-Apr-24 03:02 scepter/modules/data/dataset/ms_dataset.py
+-rw-r--r--  2.0 unx    15274 b- defN 24-Apr-24 03:02 scepter/modules/data/dataset/registry.py
+-rw-r--r--  2.0 unx      771 b- defN 24-Apr-24 03:02 scepter/modules/data/dataset/utils.py
+-rw-r--r--  2.0 unx      431 b- defN 24-Apr-24 03:02 scepter/modules/data/sampler/__init__.py
+-rw-r--r--  2.0 unx     1002 b- defN 24-Apr-24 03:02 scepter/modules/data/sampler/base_sampler.py
+-rw-r--r--  2.0 unx     2013 b- defN 24-Apr-24 03:02 scepter/modules/data/sampler/registry.py
+-rw-r--r--  2.0 unx    25333 b- defN 24-Apr-24 03:02 scepter/modules/data/sampler/sampler.py
+-rw-r--r--  2.0 unx      140 b- defN 24-Apr-24 03:02 scepter/modules/data/utils/__init__.py
+-rw-r--r--  2.0 unx     8453 b- defN 24-Apr-24 03:02 scepter/modules/data/utils/data_bucket.py
+-rw-r--r--  2.0 unx      151 b- defN 24-Apr-24 03:02 scepter/modules/inference/__init__.py
+-rw-r--r--  2.0 unx     5141 b- defN 24-Apr-24 03:02 scepter/modules/inference/control_inference.py
+-rw-r--r--  2.0 unx    31593 b- defN 24-Apr-24 03:02 scepter/modules/inference/diffusion_inference.py
+-rw-r--r--  2.0 unx    13923 b- defN 24-Apr-24 03:02 scepter/modules/inference/largen_inference.py
+-rw-r--r--  2.0 unx    14708 b- defN 24-Apr-24 03:02 scepter/modules/inference/stylebooth_inference.py
+-rw-r--r--  2.0 unx    11183 b- defN 24-Apr-24 03:02 scepter/modules/inference/tuner_inference.py
+-rw-r--r--  2.0 unx      218 b- defN 24-Apr-24 03:02 scepter/modules/model/__init__.py
+-rw-r--r--  2.0 unx     4022 b- defN 24-Apr-24 03:02 scepter/modules/model/base_model.py
+-rw-r--r--  2.0 unx     1653 b- defN 24-Apr-24 03:02 scepter/modules/model/registry.py
+-rw-r--r--  2.0 unx      202 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/__init__.py
+-rw-r--r--  2.0 unx      300 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/autoencoder/__init__.py
+-rw-r--r--  2.0 unx    12971 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/autoencoder/ae_module.py
+-rw-r--r--  2.0 unx     9569 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/autoencoder/ae_utils.py
+-rw-r--r--  2.0 unx      445 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/__init__.py
+-rw-r--r--  2.0 unx     2791 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/mlp.py
+-rw-r--r--  2.0 unx     3235 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/resnet.py
+-rw-r--r--  2.0 unx    21928 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/resnet_impl.py
+-rw-r--r--  2.0 unx     1625 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/timm_model.py
+-rw-r--r--  2.0 unx     8783 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/vit_modify.py
+-rw-r--r--  2.0 unx      133 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/utils/__init__.py
+-rw-r--r--  2.0 unx    21405 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/utils/clip.py
+-rw-r--r--  2.0 unx     5109 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/utils/simple_tokenizer.py
+-rw-r--r--  2.0 unx    21302 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/image/utils/vit.py
+-rw-r--r--  2.0 unx      303 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/unet/__init__.py
+-rw-r--r--  2.0 unx    56311 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/unet/unet_module.py
+-rw-r--r--  2.0 unx    46328 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/unet/unet_utils.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/utils/__init__.py
+-rw-r--r--  2.0 unx     2031 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/utils/transformer.py
+-rw-r--r--  2.0 unx      318 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/__init__.py
+-rw-r--r--  2.0 unx     7081 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/init_helper.py
+-rw-r--r--  2.0 unx    30685 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/resnet_3d.py
+-rw-r--r--  2.0 unx    13371 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/video_transformer.py
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/__init__.py
+-rw-r--r--  2.0 unx     1797 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/base_branch.py
+-rw-r--r--  2.0 unx     4466 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/csn_branch.py
+-rw-r--r--  2.0 unx     3405 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/non_local.py
+-rw-r--r--  2.0 unx     5942 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/r2d3d_branch.py
+-rw-r--r--  2.0 unx     8046 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/r2plus1d_branch.py
+-rw-r--r--  2.0 unx    11916 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/tada_conv.py
+-rw-r--r--  2.0 unx    12202 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/transformer_branch.py
+-rw-r--r--  2.0 unx     2530 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/visualize_3d_module.py
+-rw-r--r--  2.0 unx      575 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/stems/__init__.py
+-rw-r--r--  2.0 unx     3006 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/stems/base_2d_stem.py
+-rw-r--r--  2.0 unx     3160 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/stems/base_3d_stem.py
+-rw-r--r--  2.0 unx     1240 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/stems/down_sample_stem.py
+-rw-r--r--  2.0 unx     5534 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/stems/embedding_stem.py
+-rw-r--r--  2.0 unx     2576 b- defN 24-Apr-24 03:02 scepter/modules/model/backbone/video/bricks/stems/r2plus1d_stem.py
+-rw-r--r--  2.0 unx      297 b- defN 24-Apr-24 03:02 scepter/modules/model/embedder/__init__.py
+-rw-r--r--  2.0 unx      915 b- defN 24-Apr-24 03:02 scepter/modules/model/embedder/base_embedder.py
+-rw-r--r--  2.0 unx    28394 b- defN 24-Apr-24 03:02 scepter/modules/model/embedder/embedder.py
+-rw-r--r--  2.0 unx     5133 b- defN 24-Apr-24 03:02 scepter/modules/model/embedder/resampler.py
+-rw-r--r--  2.0 unx      253 b- defN 24-Apr-24 03:02 scepter/modules/model/head/__init__.py
+-rw-r--r--  2.0 unx    11687 b- defN 24-Apr-24 03:02 scepter/modules/model/head/classifier_head.py
+-rw-r--r--  2.0 unx      214 b- defN 24-Apr-24 03:02 scepter/modules/model/loss/__init__.py
+-rw-r--r--  2.0 unx     3955 b- defN 24-Apr-24 03:02 scepter/modules/model/loss/base_losses.py
+-rw-r--r--  2.0 unx     2178 b- defN 24-Apr-24 03:02 scepter/modules/model/loss/rec_loss.py
+-rw-r--r--  2.0 unx      286 b- defN 24-Apr-24 03:02 scepter/modules/model/metric/__init__.py
+-rw-r--r--  2.0 unx      783 b- defN 24-Apr-24 03:02 scepter/modules/model/metric/base_metric.py
+-rw-r--r--  2.0 unx     5076 b- defN 24-Apr-24 03:02 scepter/modules/model/metric/classification.py
+-rw-r--r--  2.0 unx      183 b- defN 24-Apr-24 03:02 scepter/modules/model/metric/registry.py
+-rw-r--r--  2.0 unx      220 b- defN 24-Apr-24 03:02 scepter/modules/model/neck/__init__.py
+-rw-r--r--  2.0 unx     1870 b- defN 24-Apr-24 03:02 scepter/modules/model/neck/global_average_pooling.py
+-rw-r--r--  2.0 unx      918 b- defN 24-Apr-24 03:02 scepter/modules/model/neck/identity.py
+-rw-r--r--  2.0 unx      402 b- defN 24-Apr-24 03:02 scepter/modules/model/network/__init__.py
+-rw-r--r--  2.0 unx     7188 b- defN 24-Apr-24 03:02 scepter/modules/model/network/classifier.py
+-rw-r--r--  2.0 unx     1189 b- defN 24-Apr-24 03:02 scepter/modules/model/network/train_module.py
+-rw-r--r--  2.0 unx      148 b- defN 24-Apr-24 03:02 scepter/modules/model/network/autoencoder/__init__.py
+-rw-r--r--  2.0 unx     9534 b- defN 24-Apr-24 03:02 scepter/modules/model/network/autoencoder/ae_kl.py
+-rw-r--r--  2.0 unx      211 b- defN 24-Apr-24 03:02 scepter/modules/model/network/diffusion/__init__.py
+-rw-r--r--  2.0 unx    25395 b- defN 24-Apr-24 03:02 scepter/modules/model/network/diffusion/diffusion.py
+-rw-r--r--  2.0 unx     6226 b- defN 24-Apr-24 03:02 scepter/modules/model/network/diffusion/schedules.py
+-rw-r--r--  2.0 unx    22561 b- defN 24-Apr-24 03:02 scepter/modules/model/network/diffusion/solvers.py
+-rw-r--r--  2.0 unx      385 b- defN 24-Apr-24 03:02 scepter/modules/model/network/ldm/__init__.py
+-rw-r--r--  2.0 unx    19559 b- defN 24-Apr-24 03:02 scepter/modules/model/network/ldm/ldm.py
+-rw-r--r--  2.0 unx     5860 b- defN 24-Apr-24 03:02 scepter/modules/model/network/ldm/ldm_sce.py
+-rw-r--r--  2.0 unx    21293 b- defN 24-Apr-24 03:02 scepter/modules/model/network/ldm/ldm_xl.py
+-rw-r--r--  2.0 unx      368 b- defN 24-Apr-24 03:02 scepter/modules/model/tokenizer/__init__.py
+-rw-r--r--  2.0 unx      770 b- defN 24-Apr-24 03:02 scepter/modules/model/tokenizer/base_tokenizer.py
+-rw-r--r--  2.0 unx     5670 b- defN 24-Apr-24 03:02 scepter/modules/model/tokenizer/tokenizer.py
+-rw-r--r--  2.0 unx     1757 b- defN 24-Apr-24 03:02 scepter/modules/model/tokenizer/tokenizer_component.py
+-rw-r--r--  2.0 unx      260 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/__init__.py
+-rw-r--r--  2.0 unx      742 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/base_tuner.py
+-rw-r--r--  2.0 unx     4381 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/swift_tuner.py
+-rw-r--r--  2.0 unx     6852 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/tuner_component.py
+-rw-r--r--  2.0 unx      965 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/tuner_utils.py
+-rw-r--r--  2.0 unx      222 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/sce/__init__.py
+-rw-r--r--  2.0 unx     6716 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/sce/scetuning.py
+-rw-r--r--  2.0 unx     2485 b- defN 24-Apr-24 03:02 scepter/modules/model/tuner/sce/scetuning_component.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/modules/model/utils/__init__.py
+-rw-r--r--  2.0 unx     1176 b- defN 24-Apr-24 03:02 scepter/modules/model/utils/basic_utils.py
+-rw-r--r--  2.0 unx     3776 b- defN 24-Apr-24 03:02 scepter/modules/model/utils/data_utils.py
+-rw-r--r--  2.0 unx      133 b- defN 24-Apr-24 03:02 scepter/modules/opt/__init__.py
+-rw-r--r--  2.0 unx      298 b- defN 24-Apr-24 03:02 scepter/modules/opt/lr_schedulers/__init__.py
+-rw-r--r--  2.0 unx      227 b- defN 24-Apr-24 03:02 scepter/modules/opt/lr_schedulers/base_scheduler.py
+-rw-r--r--  2.0 unx     3005 b- defN 24-Apr-24 03:02 scepter/modules/opt/lr_schedulers/define_schedulers.py
+-rw-r--r--  2.0 unx    14312 b- defN 24-Apr-24 03:02 scepter/modules/opt/lr_schedulers/official_schedulers.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-Apr-24 03:02 scepter/modules/opt/lr_schedulers/registry.py
+-rw-r--r--  2.0 unx     5420 b- defN 24-Apr-24 03:02 scepter/modules/opt/lr_schedulers/warmup.py
+-rw-r--r--  2.0 unx      297 b- defN 24-Apr-24 03:02 scepter/modules/opt/optimizers/__init__.py
+-rw-r--r--  2.0 unx      226 b- defN 24-Apr-24 03:02 scepter/modules/opt/optimizers/base_optimizer.py
+-rw-r--r--  2.0 unx    19032 b- defN 24-Apr-24 03:02 scepter/modules/opt/optimizers/official_optimizers.py
+-rw-r--r--  2.0 unx     1388 b- defN 24-Apr-24 03:02 scepter/modules/opt/optimizers/registry.py
+-rw-r--r--  2.0 unx      314 b- defN 24-Apr-24 03:02 scepter/modules/solver/__init__.py
+-rw-r--r--  2.0 unx    35587 b- defN 24-Apr-24 03:02 scepter/modules/solver/base_solver.py
+-rw-r--r--  2.0 unx    31755 b- defN 24-Apr-24 03:02 scepter/modules/solver/diffusion_solver.py
+-rw-r--r--  2.0 unx     1351 b- defN 24-Apr-24 03:02 scepter/modules/solver/registry.py
+-rw-r--r--  2.0 unx     7538 b- defN 24-Apr-24 03:02 scepter/modules/solver/train_val_solver.py
+-rw-r--r--  2.0 unx     1654 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/__init__.py
+-rw-r--r--  2.0 unx     3617 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/backward.py
+-rw-r--r--  2.0 unx    12356 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/checkpoint.py
+-rw-r--r--  2.0 unx     4944 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/data_probe.py
+-rw-r--r--  2.0 unx     2090 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/ema.py
+-rw-r--r--  2.0 unx      611 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/hook.py
+-rw-r--r--  2.0 unx    10821 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/log.py
+-rw-r--r--  2.0 unx     5202 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/lr.py
+-rw-r--r--  2.0 unx      154 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/registry.py
+-rw-r--r--  2.0 unx     2192 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/safetensors.py
+-rw-r--r--  2.0 unx     1544 b- defN 24-Apr-24 03:02 scepter/modules/solver/hooks/sampler.py
+-rw-r--r--  2.0 unx     1718 b- defN 24-Apr-24 03:02 scepter/modules/transform/__init__.py
+-rw-r--r--  2.0 unx    19927 b- defN 24-Apr-24 03:02 scepter/modules/transform/augmention.py
+-rw-r--r--  2.0 unx     1011 b- defN 24-Apr-24 03:02 scepter/modules/transform/compose.py
+-rw-r--r--  2.0 unx      725 b- defN 24-Apr-24 03:02 scepter/modules/transform/identity.py
+-rw-r--r--  2.0 unx    22204 b- defN 24-Apr-24 03:02 scepter/modules/transform/image.py
+-rw-r--r--  2.0 unx    12249 b- defN 24-Apr-24 03:02 scepter/modules/transform/io.py
+-rw-r--r--  2.0 unx    18995 b- defN 24-Apr-24 03:02 scepter/modules/transform/io_video.py
+-rw-r--r--  2.0 unx     1984 b- defN 24-Apr-24 03:02 scepter/modules/transform/registry.py
+-rw-r--r--  2.0 unx    10216 b- defN 24-Apr-24 03:02 scepter/modules/transform/tensor.py
+-rw-r--r--  2.0 unx     3201 b- defN 24-Apr-24 03:02 scepter/modules/transform/transform_xl.py
+-rw-r--r--  2.0 unx     1865 b- defN 24-Apr-24 03:02 scepter/modules/transform/utils.py
+-rw-r--r--  2.0 unx    18831 b- defN 24-Apr-24 03:02 scepter/modules/transform/video.py
+-rw-r--r--  2.0 unx      154 b- defN 24-Apr-24 03:02 scepter/modules/utils/__init__.py
+-rw-r--r--  2.0 unx    25847 b- defN 24-Apr-24 03:02 scepter/modules/utils/config.py
+-rw-r--r--  2.0 unx     2865 b- defN 24-Apr-24 03:02 scepter/modules/utils/data.py
+-rw-r--r--  2.0 unx      482 b- defN 24-Apr-24 03:02 scepter/modules/utils/directory.py
+-rw-r--r--  2.0 unx    15775 b- defN 24-Apr-24 03:02 scepter/modules/utils/distribute.py
+-rw-r--r--  2.0 unx     3755 b- defN 24-Apr-24 03:02 scepter/modules/utils/export_model.py
+-rw-r--r--  2.0 unx    17112 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_system.py
+-rw-r--r--  2.0 unx     1684 b- defN 24-Apr-24 03:02 scepter/modules/utils/index.py
+-rw-r--r--  2.0 unx     5554 b- defN 24-Apr-24 03:02 scepter/modules/utils/logger.py
+-rw-r--r--  2.0 unx     2735 b- defN 24-Apr-24 03:02 scepter/modules/utils/math_plot.py
+-rw-r--r--  2.0 unx     5548 b- defN 24-Apr-24 03:02 scepter/modules/utils/model.py
+-rw-r--r--  2.0 unx    17482 b- defN 24-Apr-24 03:02 scepter/modules/utils/probe.py
+-rw-r--r--  2.0 unx     7995 b- defN 24-Apr-24 03:02 scepter/modules/utils/registry.py
+-rw-r--r--  2.0 unx      423 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/__init__.py
+-rw-r--r--  2.0 unx    44598 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/aliyun_oss_fs.py
+-rw-r--r--  2.0 unx    11136 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/base_fs.py
+-rw-r--r--  2.0 unx     4628 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/http_fs.py
+-rw-r--r--  2.0 unx     6267 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/huggingface_fs.py
+-rw-r--r--  2.0 unx    13560 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/local_fs.py
+-rw-r--r--  2.0 unx     7225 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/modelscope_fs.py
+-rw-r--r--  2.0 unx      168 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/registry.py
+-rw-r--r--  2.0 unx     1067 b- defN 24-Apr-24 03:02 scepter/modules/utils/file_clients/utils.py
+-rw-r--r--  2.0 unx      324 b- defN 24-Apr-24 03:02 scepter/modules/utils/video_reader/__init__.py
+-rw-r--r--  2.0 unx     6201 b- defN 24-Apr-24 03:02 scepter/modules/utils/video_reader/frame_sampler.py
+-rw-r--r--  2.0 unx     5133 b- defN 24-Apr-24 03:02 scepter/modules/utils/video_reader/video_reader.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/__init__.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/home/__init__.py
+-rw-r--r--  2.0 unx     1751 b- defN 24-Apr-24 03:02 scepter/studio/home/home.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/home/home_ui/__init__.py
+-rw-r--r--  2.0 unx      387 b- defN 24-Apr-24 03:02 scepter/studio/home/home_ui/component_names.py
+-rw-r--r--  2.0 unx      719 b- defN 24-Apr-24 03:02 scepter/studio/home/home_ui/desc_ui.py
+-rw-r--r--  2.0 unx      728 b- defN 24-Apr-24 03:02 scepter/studio/home/home_ui/guide_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/inference/__init__.py
+-rw-r--r--  2.0 unx    11904 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_manager/__init__.py
+-rw-r--r--  2.0 unx     7179 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_manager/infer_runer.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/__init__.py
+-rw-r--r--  2.0 unx    26742 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/component_names.py
+-rw-r--r--  2.0 unx     8520 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/control_ui.py
+-rw-r--r--  2.0 unx     9251 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/diffusion_ui.py
+-rw-r--r--  2.0 unx    12423 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/gallery_ui.py
+-rw-r--r--  2.0 unx    23616 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/largen_ui.py
+-rw-r--r--  2.0 unx     8674 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/mantra_ui.py
+-rw-r--r--  2.0 unx    12250 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/model_manage_ui.py
+-rw-r--r--  2.0 unx     4281 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/refiner_ui.py
+-rw-r--r--  2.0 unx     8675 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/stylebooth_ui.py
+-rw-r--r--  2.0 unx     9959 b- defN 24-Apr-24 03:02 scepter/studio/inference/inference_ui/tuner_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/__init__.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/preprocess.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/caption_editor_ui/__init__.py
+-rw-r--r--  2.0 unx    19501 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/caption_editor_ui/component_names.py
+-rw-r--r--  2.0 unx    27907 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py
+-rw-r--r--  2.0 unx    70517 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py
+-rw-r--r--  2.0 unx     3013 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/processors/__init__.py
+-rw-r--r--  2.0 unx     5014 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/processors/base_processor.py
+-rw-r--r--  2.0 unx    10504 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/processors/caption_processors.py
+-rw-r--r--  2.0 unx     1351 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/processors/image_processors.py
+-rw-r--r--  2.0 unx     2475 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/processors/processor_manager.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/utils/__init__.py
+-rw-r--r--  2.0 unx    48911 b- defN 24-Apr-24 03:02 scepter/studio/preprocess/utils/data_card.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/self_train/__init__.py
+-rw-r--r--  2.0 unx     2575 b- defN 24-Apr-24 03:02 scepter/studio/self_train/self_train.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/self_train/scripts/__init__.py
+-rw-r--r--  2.0 unx     7452 b- defN 24-Apr-24 03:02 scepter/studio/self_train/scripts/run_task.py
+-rw-r--r--  2.0 unx      148 b- defN 24-Apr-24 03:02 scepter/studio/self_train/scripts/sleep.py
+-rw-r--r--  2.0 unx    15170 b- defN 24-Apr-24 03:02 scepter/studio/self_train/scripts/trainer.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/self_train/self_train_ui/__init__.py
+-rw-r--r--  2.0 unx    10123 b- defN 24-Apr-24 03:02 scepter/studio/self_train/self_train_ui/component_names.py
+-rw-r--r--  2.0 unx    22717 b- defN 24-Apr-24 03:02 scepter/studio/self_train/self_train_ui/model_ui.py
+-rw-r--r--  2.0 unx    44050 b- defN 24-Apr-24 03:02 scepter/studio/self_train/self_train_ui/trainer_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/self_train/utils/__init__.py
+-rw-r--r--  2.0 unx    12144 b- defN 24-Apr-24 03:02 scepter/studio/self_train/utils/config_parser.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/__init__.py
+-rw-r--r--  2.0 unx     1392 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/tuner_manager.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/manager_ui/__init__.py
+-rw-r--r--  2.0 unx    39679 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/manager_ui/browser_ui.py
+-rw-r--r--  2.0 unx     4528 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/manager_ui/component_names.py
+-rw-r--r--  2.0 unx    11052 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/manager_ui/info_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/utils/__init__.py
+-rw-r--r--  2.0 unx      452 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/utils/dict.py
+-rw-r--r--  2.0 unx      217 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/utils/path.py
+-rw-r--r--  2.0 unx      327 b- defN 24-Apr-24 03:02 scepter/studio/tuner_manager/utils/yaml.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/studio/utils/__init__.py
+-rw-r--r--  2.0 unx      731 b- defN 24-Apr-24 03:02 scepter/studio/utils/env.py
+-rw-r--r--  2.0 unx      567 b- defN 24-Apr-24 03:02 scepter/studio/utils/file.py
+-rw-r--r--  2.0 unx      276 b- defN 24-Apr-24 03:02 scepter/studio/utils/singleton.py
+-rw-r--r--  2.0 unx      545 b- defN 24-Apr-24 03:02 scepter/studio/utils/uibase.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-24 03:02 scepter/tools/__init__.py
+-rw-r--r--  2.0 unx     3371 b- defN 24-Apr-24 03:02 scepter/tools/helper.py
+-rw-r--r--  2.0 unx     9409 b- defN 24-Apr-24 03:02 scepter/tools/run_inference.py
+-rw-r--r--  2.0 unx     2114 b- defN 24-Apr-24 03:02 scepter/tools/run_train.py
+-rw-r--r--  2.0 unx     6742 b- defN 24-Apr-24 03:02 scepter/tools/webui.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-24 03:03 scepter-0.0.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx    14203 b- defN 24-Apr-24 03:03 scepter-0.0.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-24 03:03 scepter-0.0.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 24-Apr-24 03:03 scepter-0.0.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    32130 b- defN 24-Apr-24 03:03 scepter-0.0.5.dist-info/RECORD
+314 files, 2574901 bytes uncompressed, 563918 bytes compressed:  78.1%
```

## zipnote {}

```diff
@@ -9,14 +9,17 @@
 
 Filename: scepter/methods/examples/generation/stable_diffusion_1.5_512.yaml
 Comment: 
 
 Filename: scepter/methods/examples/generation/stable_diffusion_1.5_512_lora.yaml
 Comment: 
 
+Filename: scepter/methods/examples/generation/stable_diffusion_1.5_512_textlora.yaml
+Comment: 
+
 Filename: scepter/methods/examples/generation/stable_diffusion_2.1_512.yaml
 Comment: 
 
 Filename: scepter/methods/examples/generation/stable_diffusion_2.1_512_lora.yaml
 Comment: 
 
 Filename: scepter/methods/examples/generation/stable_diffusion_2.1_768.yaml
@@ -27,14 +30,17 @@
 
 Filename: scepter/methods/examples/generation/stable_diffusion_xl_1024.yaml
 Comment: 
 
 Filename: scepter/methods/examples/generation/stable_diffusion_xl_1024_lora.yaml
 Comment: 
 
+Filename: scepter/methods/examples/generation/stable_diffusion_xl_1024_textlora.yaml
+Comment: 
+
 Filename: scepter/methods/scedit/ctr/sd15_512_sce_ctr_hed.yaml
 Comment: 
 
 Filename: scepter/methods/scedit/ctr/sd21_768_sce_ctr_canny.yaml
 Comment: 
 
 Filename: scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml
@@ -54,14 +60,17 @@
 
 Filename: scepter/methods/scedit/t2i/sd15_512_sce_t2i.yaml
 Comment: 
 
 Filename: scepter/methods/scedit/t2i/sd15_512_sce_t2i_swift.yaml
 Comment: 
 
+Filename: scepter/methods/scedit/t2i/sd15_512_textsce_t2i_swift.yaml
+Comment: 
+
 Filename: scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml
 Comment: 
 
 Filename: scepter/methods/scedit/t2i/sd21_768_sce_t2i_swift.yaml
 Comment: 
 
 Filename: scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml
@@ -69,14 +78,17 @@
 
 Filename: scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_datatxt.yaml
 Comment: 
 
 Filename: scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_swift.yaml
 Comment: 
 
+Filename: scepter/methods/scedit/t2i/sdxl_1024_textsce_t2i_swift.yaml
+Comment: 
+
 Filename: scepter/methods/studio/scepter_ui.yaml
 Comment: 
 
 Filename: scepter/methods/studio/extensions/controllers/official_controllers.yaml
 Comment: 
 
 Filename: scepter/methods/studio/extensions/mantra_book/mantra_book.yaml
@@ -87,14 +99,17 @@
 
 Filename: scepter/methods/studio/home/home.yaml
 Comment: 
 
 Filename: scepter/methods/studio/inference/inference.yaml
 Comment: 
 
+Filename: scepter/methods/studio/inference/edit/stylebooth_tb_pro.yaml
+Comment: 
+
 Filename: scepter/methods/studio/inference/largen/largen_pro.yaml
 Comment: 
 
 Filename: scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml
 Comment: 
 
 Filename: scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml
@@ -231,26 +246,35 @@
 
 Filename: scepter/modules/data/sampler/registry.py
 Comment: 
 
 Filename: scepter/modules/data/sampler/sampler.py
 Comment: 
 
+Filename: scepter/modules/data/utils/__init__.py
+Comment: 
+
+Filename: scepter/modules/data/utils/data_bucket.py
+Comment: 
+
 Filename: scepter/modules/inference/__init__.py
 Comment: 
 
 Filename: scepter/modules/inference/control_inference.py
 Comment: 
 
 Filename: scepter/modules/inference/diffusion_inference.py
 Comment: 
 
 Filename: scepter/modules/inference/largen_inference.py
 Comment: 
 
+Filename: scepter/modules/inference/stylebooth_inference.py
+Comment: 
+
 Filename: scepter/modules/inference/tuner_inference.py
 Comment: 
 
 Filename: scepter/modules/model/__init__.py
 Comment: 
 
 Filename: scepter/modules/model/base_model.py
@@ -753,14 +777,17 @@
 
 Filename: scepter/studio/inference/inference_ui/model_manage_ui.py
 Comment: 
 
 Filename: scepter/studio/inference/inference_ui/refiner_ui.py
 Comment: 
 
+Filename: scepter/studio/inference/inference_ui/stylebooth_ui.py
+Comment: 
+
 Filename: scepter/studio/inference/inference_ui/tuner_ui.py
 Comment: 
 
 Filename: scepter/studio/preprocess/__init__.py
 Comment: 
 
 Filename: scepter/studio/preprocess/preprocess.py
@@ -777,14 +804,35 @@
 
 Filename: scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py
 Comment: 
 
 Filename: scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py
 Comment: 
 
+Filename: scepter/studio/preprocess/processors/__init__.py
+Comment: 
+
+Filename: scepter/studio/preprocess/processors/base_processor.py
+Comment: 
+
+Filename: scepter/studio/preprocess/processors/caption_processors.py
+Comment: 
+
+Filename: scepter/studio/preprocess/processors/image_processors.py
+Comment: 
+
+Filename: scepter/studio/preprocess/processors/processor_manager.py
+Comment: 
+
+Filename: scepter/studio/preprocess/utils/__init__.py
+Comment: 
+
+Filename: scepter/studio/preprocess/utils/data_card.py
+Comment: 
+
 Filename: scepter/studio/self_train/__init__.py
 Comment: 
 
 Filename: scepter/studio/self_train/self_train.py
 Comment: 
 
 Filename: scepter/studio/self_train/scripts/__init__.py
@@ -873,23 +921,23 @@
 
 Filename: scepter/tools/run_train.py
 Comment: 
 
 Filename: scepter/tools/webui.py
 Comment: 
 
-Filename: scepter-0.0.4.dist-info/LICENSE
+Filename: scepter-0.0.5.dist-info/LICENSE
 Comment: 
 
-Filename: scepter-0.0.4.dist-info/METADATA
+Filename: scepter-0.0.5.dist-info/METADATA
 Comment: 
 
-Filename: scepter-0.0.4.dist-info/WHEEL
+Filename: scepter-0.0.5.dist-info/WHEEL
 Comment: 
 
-Filename: scepter-0.0.4.dist-info/top_level.txt
+Filename: scepter-0.0.5.dist-info/top_level.txt
 Comment: 
 
-Filename: scepter-0.0.4.dist-info/RECORD
+Filename: scepter-0.0.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## scepter/version.py

```diff
@@ -1,8 +1,8 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-__version__ = '0.0.4'
+__version__ = '0.0.5'
 
 version_info = tuple(int(x) for x in __version__.split('.')[0:3])
 
 __all__ = ['__version__', version_info]
```

## scepter/methods/studio/extensions/tuners/official_tuners.yaml

```diff
@@ -1,74 +1,74 @@
 TUNERS:
   - NAME: Azure-Dragon
     NAME_ZH: 青龙
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/azure_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/azure_dragon/xl_azure_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: Azure Dragon, 8K, high quality,Ultra High Detail.One of the Four Divine Creatures in Charge of Water.
   - NAME: Gold-Dragon
     NAME_ZH: 金龙
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/gold_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/gold_dragon/xl_gold_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: Chinese Gold Dragon in the clouds. Translucent Texture. Zbrush.  Fuzzy Art. Exquisite Craftsmanship. 3D. 8K. Ultra High Detail
   - NAME: SpringFestival-Dragon
     NAME_ZH: 春节龙
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/spring_festival_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/spring_festival_dragon/xl_spring_festival_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: Chinese dragon. Spring Festival.Festive.Street.Lanterns.32K.High quality.expressive, dramatic, dreamlike and mysterious, Surrealism
   - NAME: Red-Dragon
     NAME_ZH: 红龙
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/red_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/red_dragon/xl_red_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: Traditional Red Dragon of China. Low Water Level. Studio Ghibli Style. Mural Illustration. White Background. High Detail
   - NAME: ChinesePunk-Dragon
     NAME_ZH: 中国朋克龙
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/chinese_punk_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/chinese_punk_dragon/xl_chinese_punk_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: uhd Image,Dragon,Chinese Dragon, Dunhuang Mural Style, Traditional Maritime Art Style
   - NAME: Cute-Dragon
     NAME_ZH: 喜庆龙
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/cute_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/cute_dragon/xl_kawaii_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: China Kawaii Dragon. Contest Winner. Minimalist Illustration. White Background. Flat Style. Digital Painting Style. Red. 32k uhd. Fun Comics. Fuzzy Art. Bold. Comic-Inspired Characters
   - NAME: Dragon-Baby
     NAME_ZH: 龙宝宝
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/baby_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/baby_dragon/xl_baby_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: Warm Colors, Soft,Chinese Dragon Baby, Felt Style,Dragon Baby, Best Quality, 3D Doll, Macaron Tones, Glittering Big Eyes, Winter,Dragon
   - NAME: Sloppy-Dragon
     NAME_ZH: 潦草龙
-    SOURCE: wanx
+    SOURCE: scepter
     DESCRIPTION: None
     BASE_MODEL: SD_XL1.0
     MODEL_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/sloppy_dragon/
     IMAGE_PATH: ms://damo/scepter_scedit@tuners_model/SD_XL1.0/sloppy_dragon/xl_sloppy_dragon.png
     TUNER_TYPE: SwiftSCE
     PROMPT_EXAMPLE: Messy Chinese Dragon,Cute, Wu Guanzhong, Rough
   -
```

## scepter/methods/studio/preprocess/preprocess.yaml

```diff
@@ -1,7 +1,173 @@
 WORK_DIR: datasets
 EXPORT_DIR: export_datasets
 FILE_SYSTEM:
   -
     # NAME DESCRIPTION:  TYPE:  default: ''
     NAME: LocalFs
     AUTO_CLEAN: False
+
+PROCESSORS:
+  - NAME: BlipImageBase
+    TYPE: caption
+    MODEL_PATH: ms://cubeai/blip-image-captioning-base
+    DEVICE: "gpu"
+    MEMORY: 1200
+    PARAS:
+      - LANGUAGE_NAME: English
+        LANGUAGE_ZH_NAME: 英语
+  - NAME: QWVL
+    TYPE: caption
+    MODEL_PATH: ms://qwen/Qwen-VL:v1.0.3
+    DEVICE: "gpu"
+    MEMORY: 19968
+    PARAS:
+      - PROMPT: 用中文描述这张图片
+        LANGUAGE_NAME: Chinese
+        LANGUAGE_ZH_NAME: 中文
+        MAX_NEW_TOKENS:
+          VALUE: 1024
+          MAX: 2048
+          STEP: 128
+          MIN: 256
+        MIN_NEW_TOKENS:
+          VALUE: 16
+          MAX: 1024
+          STEP: 16
+          MIN: 0
+        NUM_BEAMS:
+          VALUE: 1
+          MAX: 12
+          STEP: 1
+          MIN: 1
+        REPETITION_PENALTY:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+        TEMPERATURE:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+      - PROMPT: Generate the caption in English
+        LANGUAGE_NAME: English
+        LANGUAGE_ZH_NAME: 英语
+        MAX_NEW_TOKENS:
+          VALUE: 1024
+          MAX: 2048
+          STEP: 128
+          MIN: 256
+        MIN_NEW_TOKENS:
+          VALUE: 16
+          MAX: 1024
+          STEP: 16
+          MIN: 0
+        NUM_BEAMS:
+          VALUE: 1
+          MAX: 12
+          STEP: 1
+          MIN: 1
+        REPETITION_PENALTY:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+        TEMPERATURE:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+  -
+    NAME: QWVLQuantize
+    TYPE: caption
+    DEVICE: "gpu"
+    MEMORY: 7885
+    MODEL_PATH: ms://qwen/Qwen-VL:v1.0.3
+    PARAS:
+      - PROMPT: 用中文描述这张图片
+        LANGUAGE_NAME: Chinese
+        LANGUAGE_ZH_NAME: 中文
+        MAX_NEW_TOKENS:
+          VALUE: 1024
+          MAX: 2048
+          STEP: 128
+          MIN: 256
+        MIN_NEW_TOKENS:
+          VALUE: 16
+          MAX: 1024
+          STEP: 16
+          MIN: 0
+        NUM_BEAMS:
+          VALUE: 1
+          MAX: 12
+          STEP: 1
+          MIN: 1
+        REPETITION_PENALTY:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+        TEMPERATURE:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+      - PROMPT: Generate the caption in English
+        LANGUAGE_NAME: English
+        LANGUAGE_ZH_NAME: 英语
+        MAX_NEW_TOKENS:
+          VALUE: 1024
+          MAX: 2048
+          STEP: 128
+          MIN: 256
+        MIN_NEW_TOKENS:
+          VALUE: 16
+          MAX: 1024
+          STEP: 16
+          MIN: 0
+        NUM_BEAMS:
+          VALUE: 1
+          MAX: 12
+          STEP: 1
+          MIN: 1
+        REPETITION_PENALTY:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+        TEMPERATURE:
+          VALUE: 1.0
+          MAX: 100.0
+          STEP: 1.0
+          MIN: 1.0
+  -
+    NAME: CenterCrop
+    TYPE: image
+    DEVICE: "cpu"
+    MEMORY: 10
+    PARAS:
+      HEIGHT_RATIO:
+        VALUE: 1
+        MAX: 20
+        STEP: 1
+        MIN: 1
+      WIDTH_RATIO:
+        VALUE: 1
+        MAX: 20
+        STEP: 1
+        MIN: 1
+#  - NAME: PaddingCrop
+#    TYPE: image
+#    DEVICE: "cpu"
+#    MEMORY: 10
+#    PARAS:
+#      HEIGHT_RATIO:
+#        VALUE: 3
+#        MAX: 25
+#        STEP: 1
+#        MIN: 1
+#      WIDTH_RATIO:
+#        VALUE: 4
+#        MAX: 20
+#        STEP: 1
+#        MIN: 1
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml

```diff
@@ -1,14 +1,15 @@
 ENV:
   BACKEND: nccl
 
 META:
   VERSION: 'SD_XL1.0'
   DESCRIPTION: "Stable Diffusion XL1.0"
   IS_DEFAULT: True
+  IS_SHARE: True
   INFERENCE_PARAS:
     INFERENCE_BATCH_SIZE: 1
     INFERENCE_PREFIX: ""
     DEFAULT_SAMPLER: "dpmpp_2s_ancestral"
     DEFAULT_SAMPLE_STEPS: 40
     INFERENCE_N_PROMPT: ""
     RESOLUTION: [1024, 1024]
@@ -528,15 +529,14 @@
   SAMPLE_ARGS:
     SAMPLER: ddim
     SAMPLE_STEPS: 50
     SEED: 2023
     GUIDE_SCALE: 5.0
     GUIDE_RESCALE:
     DISCRETIZATION: linspace
-    IMAGE_SIZE: [ 1024, 1024]
     RUN_TRAIN_N: False
   # OPTIMIZER DESCRIPTION:  TYPE:  default: ''
   OPTIMIZER:
     # NAME DESCRIPTION:  TYPE:  default: ''
     NAME: AdamW
     LEARNING_RATE: 0.0064
     EPS: 1e-8
@@ -617,14 +617,15 @@
       NAME: TensorboardLogHook
     -
       NAME: CheckpointHook
       INTERVAL: 10000
       PRIORITY: 200
       SAVE_LAST: True
       SAVE_NAME_PREFIX: 'step'
+      DISABLE_SNAPSHOT: True
   #
   EVAL_HOOKS:
     -
       NAME: ProbeDataHook
       PROB_INTERVAL: 100
       SAVE_LAST: True
       SAVE_NAME_PREFIX: 'step'
```

## scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml

```diff
@@ -1,13 +1,14 @@
 ENV:
   BACKEND: nccl
 META:
   VERSION: 'SD1.5'
   DESCRIPTION: "Stable Diffusion v1.5"
   IS_DEFAULT: False
+  IS_SHARE: True
   INFERENCE_PARAS:
     INFERENCE_BATCH_SIZE: 1
     INFERENCE_PREFIX: ""
     DEFAULT_SAMPLER: "ddim"
     DEFAULT_SAMPLE_STEPS: 40
     INFERENCE_N_PROMPT: ""
     RESOLUTION: [512, 512]
@@ -240,15 +241,14 @@
   SAMPLE_ARGS:
     SAMPLER: ddim
     SAMPLE_STEPS: 50
     SEED: 2023
     GUIDE_SCALE: 7.5
     GUIDE_RESCALE:
     DISCRETIZATION: trailing
-    IMAGE_SIZE: [512, 512]
     RUN_TRAIN_N: False
   #
   OPTIMIZER:
     NAME: AdamW
     LEARNING_RATE: 0.064
     BETAS: [ 0.9, 0.999 ]
     EPS: 1e-8
@@ -270,22 +270,22 @@
     NUM_WORKERS: 4
     SAMPLER:
       NAME: LoopSampler
     TRANSFORMS:
       - NAME: LoadImageFromFile
         RGB_ORDER: RGB
         BACKEND: pillow
-      - NAME: Resize
-        SIZE: 512
+      - NAME: FlexibleResize
         INTERPOLATION: bilinear
+        SIZE: [ 512, 512 ]
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'img' ]
         BACKEND: pillow
-      - NAME: CenterCrop
-        SIZE: 512
+      - NAME: FlexibleCenterCrop
+        SIZE: [ 512, 512 ]
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'img' ]
         BACKEND: pillow
       - NAME: ImageToTensor
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'img' ]
         BACKEND: pillow
@@ -328,14 +328,15 @@
       NAME: TensorboardLogHook
     -
       NAME: CheckpointHook
       INTERVAL: 10000
       PRIORITY: 200
       SAVE_LAST: True
       SAVE_NAME_PREFIX: 'step'
+      DISABLE_SNAPSHOT: True
   #
   EVAL_HOOKS:
     -
       NAME: ProbeDataHook
       PROB_INTERVAL: 100
       SAVE_LAST: True
       SAVE_NAME_PREFIX: 'step'
```

## scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml

```diff
@@ -1,13 +1,14 @@
 ENV:
   BACKEND: nccl
 META:
   VERSION: 'SD2.1'
   DESCRIPTION: "Stable Diffusion v2.1"
   IS_DEFAULT: False
+  IS_SHARE: True
   INFERENCE_PARAS:
     INFERENCE_BATCH_SIZE: 1
     INFERENCE_PREFIX: ""
     DEFAULT_SAMPLER: "ddim"
     DEFAULT_SAMPLE_STEPS: 40
     INFERENCE_N_PROMPT: ""
     RESOLUTION: [768, 768]
@@ -182,15 +183,14 @@
   SAMPLE_ARGS:
     SAMPLER: ddim
     SAMPLE_STEPS: 50
     SEED: 2023
     GUIDE_SCALE: 7.5
     GUIDE_RESCALE:
     DISCRETIZATION: trailing
-    IMAGE_SIZE: [768, 768]
     RUN_TRAIN_N: False
   #
   OPTIMIZER:
     NAME: AdamW
     LEARNING_RATE: 0.064
     BETAS: [ 0.9, 0.999 ]
     EPS: 1e-8
@@ -212,22 +212,22 @@
     NUM_WORKERS: 4
     SAMPLER:
       NAME: LoopSampler
     TRANSFORMS:
       - NAME: LoadImageFromFile
         RGB_ORDER: RGB
         BACKEND: pillow
-      - NAME: Resize
-        SIZE: 768
+      - NAME: FlexibleResize
         INTERPOLATION: bilinear
+        SIZE: [ 768, 768 ]
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'img' ]
         BACKEND: pillow
-      - NAME: CenterCrop
-        SIZE: 768
+      - NAME: FlexibleCenterCrop
+        SIZE: [ 768, 768 ]
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'img' ]
         BACKEND: pillow
       - NAME: ImageToTensor
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'img' ]
         BACKEND: pillow
@@ -270,14 +270,15 @@
       NAME: TensorboardLogHook
     -
       NAME: CheckpointHook
       INTERVAL: 10000
       PRIORITY: 200
       SAVE_LAST: True
       SAVE_NAME_PREFIX: 'step'
+      DISABLE_SNAPSHOT: True
   #
   EVAL_HOOKS:
     -
       NAME: ProbeDataHook
       PROB_INTERVAL: 100
       SAVE_LAST: True
       SAVE_NAME_PREFIX: 'step'
```

## scepter/methods/studio/tuner_manager/tuner_manager.yaml

```diff
@@ -1,2 +1,14 @@
 WORK_DIR: "tuner_manager"
+SELF_TRAIN_DIR: "self_train"
+EXPORT_DIR: "export_model"
 TUNER_LIST_YAML: "tuner_list.yaml"
+README_EN: "scepter/methods/studio/tuner_manager/readme_en.md"
+README_ZH: "scepter/methods/studio/tuner_manager/readme_zh.md"
+
+BASE_MODEL_VERSION:
+  - BASE_MODEL: 'SD_XL1.0'
+    TUNER_TYPE: [ 'TEXT_SCE', 'SCE', 'LORA', 'TEXT_LORA', 'FULL' ]
+  - BASE_MODEL: 'SD1.5'
+    TUNER_TYPE: [ 'TEXT_SCE', 'SCE', 'LORA', 'TEXT_LORA', 'FULL' ]
+  - BASE_MODEL: 'SD2.1'
+    TUNER_TYPE: [ 'SCE', 'LORA', 'FULL' ]
```

## scepter/modules/data/dataset/dataset.py

```diff
@@ -97,21 +97,26 @@
         'P_ZERO': {
             'value': 0.0,
             'description': '',
         },
         'NEGTIVE_PROMPT': {
             'value': '',
             'description': 'The default negtive prompt',
+        },
+        'DATA_NUM': {
+            'value': '',
+            'description': '',
         }
     }
     para_dict.update(BaseDataset.para_dict)
 
     def __init__(self, cfg, logger=None):
         super(ImageTextPairDataset, self).__init__(cfg, logger=logger)
         self.p_zero = cfg.get('P_ZERO', 0.0)
+        self.real_number = cfg.get('DATA_NUM', None)
         self._default_item = {
             'meta': {},
             'prompt':
             'Plants in the Water, Nature, Lake, Horizontal, Reflection, Photography, Backgrounds, Swamp, No People'
         }
 
     def _get(self, index):
```

## scepter/modules/data/dataset/registry.py

```diff
@@ -284,16 +284,20 @@
                     self.data_sampler_config, self.batch_size, rank, seed)
             elif sampler_name == 'TorchDefault':
                 self.sampler = None
             else:
                 self.shuffle = False
                 self.data_sampler_config.SEED = seed
                 self.data_sampler_config.BATCH_SIZE = self.batch_size
-                self.sampler = SAMPLERS.build(self.data_sampler_config,
-                                              logger=self.logger)
+                sampler = SAMPLERS.build(self.data_sampler_config,
+                                         logger=self.logger)
+                if sampler_name.endswith('BatchSampler'):
+                    self.batch_sampler = sampler
+                else:
+                    self.sampler = sampler
 
     def _instantiate_multi_level_batch_sampler(self, sampler_config,
                                                batch_size, rank, seed):
         index_file = sampler_config.INDEX_FILE
         image_size = sampler_config.get('IMAGE_SIZE', 512)
         fields = sampler_config.get('FIELDS', ['img_path', 'prompt'])
         delimiter = sampler_config.get('DELIMITER', ',')
```

## scepter/modules/data/sampler/__init__.py

```diff
@@ -2,8 +2,8 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 from scepter.modules.data.sampler.base_sampler import BaseSampler
 from scepter.modules.data.sampler.registry import SAMPLERS
 from scepter.modules.data.sampler.sampler import (
     EvalDistributedSampler, LoopSampler, MixtureOfSamplers,
     MultiFoldDistributedSampler, MultiLevelBatchSampler,
-    MultiLevelBatchSamplerMultiSource)
+    MultiLevelBatchSamplerMultiSource, ResolutionBatchSampler)
```

## scepter/modules/data/sampler/sampler.py

```diff
@@ -3,22 +3,24 @@
 
 import json
 import math
 import numbers
 import os
 import sys
 from collections.abc import Iterable
-from typing import Optional
+from typing import List, Optional
 
 import numpy as np
 import torch
 import torch.distributed as dist
 
 from scepter.modules.data.sampler.base_sampler import BaseSampler
 from scepter.modules.data.sampler.registry import SAMPLERS
+from scepter.modules.data.utils.data_bucket import (BucketBatchIndex,
+                                                    BucketManager)
 from scepter.modules.utils.config import dict_to_yaml
 from scepter.modules.utils.directory import osp_path
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
 
 
 @SAMPLERS.register_class()
@@ -585,7 +587,109 @@
         }
         :return:
         '''
         return dict_to_yaml('SAMPLERS',
                             __class__.__name__,
                             LoopSampler.para_dict,
                             set_name=True)
+
+
+@SAMPLERS.register_class()
+class ResolutionBatchSampler(BaseSampler):
+    para_dict = {}
+
+    def __init__(self, cfg, logger):
+        super().__init__(cfg, logger)
+        self.data_file = cfg.DATA_FILE
+        self.fields = cfg.get('FIELDS', [])
+        self.num_fields = len(self.fields)
+        self.delimiter = cfg.get('DELIMITER', ',')
+        self.path_prefix = cfg.get('PATH_PREFIX', '')
+        self.batch_size = cfg.BATCH_SIZE
+        max_reso = cfg.get('MAX_RESO', (1024, 1024))
+        min_bucket_reso = cfg.get('MIN_BUCKET_RESO', 256)
+        max_bucket_reso = cfg.get('MAX_BUCKET_RESO', 1024)
+        bucket_reso_steps = cfg.get('BUCKET_RESO_STEPS', 64)
+        bucket_no_upscale = cfg.get('BUCKET_NO_UPSCALE', False)
+        rank = we.rank
+        self.rng = np.random.default_rng(self.seed + rank)
+        assert 'img_path' in self.fields and 'width' in self.fields and 'height' in self.fields
+
+        self.bucket_manager = BucketManager(max_reso=max_reso,
+                                            min_size=min_bucket_reso,
+                                            max_size=max_bucket_reso,
+                                            reso_steps=bucket_reso_steps,
+                                            no_upscale=bucket_no_upscale)
+        if not bucket_no_upscale:
+            self.bucket_manager.make_buckets()
+        else:
+            self.logger.info(
+                'min_bucket_reso and max_bucket_reso are ignored if bucket_no_upscale is set, '
+                'because bucket reso is defined by image size automatically / bucket_no_upscale'
+            )
+
+        self.data_map = {}
+        img_path_idx, width_idx, height_idx = self.fields.index(
+            'img_path'), self.fields.index('width'), self.fields.index(
+                'height')
+        with FS.get_from(self.data_file) as local_path:
+            with open(local_path) as f:
+                for i, line in enumerate(f):
+                    items = line.strip()
+                    item_sp = items.split(self.delimiter, self.num_fields - 1)
+                    img_path, width, height = item_sp[img_path_idx], int(
+                        item_sp[width_idx]), int(item_sp[height_idx])
+                    item_sp[img_path_idx] = os.path.join(
+                        self.path_prefix, img_path)
+                    bucket_reso, resized_size, ar_error = self.bucket_manager.select_bucket(
+                        width, height)
+                    self.bucket_manager.add_image(reso=bucket_reso, image=i)
+                    self.data_map[i] = item_sp
+
+        for i, (reso, bucket) in enumerate(
+                zip(self.bucket_manager.resos, self.bucket_manager.buckets)):
+            count = len(bucket)
+            if count > 0:
+                # self.logger.info(f"bucket {i}: resolution {reso}, bucket {bucket}, count: {len(bucket)}")
+                self.logger.info(
+                    f'bucket {i}: resolution {reso}, count: {len(bucket)}')
+
+        self.buckets_indices: List[BucketBatchIndex] = []
+        for bucket_index, (reso, bucket) in enumerate(
+                zip(self.bucket_manager.resos, self.bucket_manager.buckets)):
+            batch_count = int(math.ceil(len(bucket) / self.batch_size))
+            for batch_index in range(batch_count):
+                self.buckets_indices.append(
+                    BucketBatchIndex(bucket_index, self.batch_size,
+                                     batch_index, reso))
+        self.shuffle_buckets()
+
+    def shuffle_buckets(self):
+        np.random.shuffle(self.buckets_indices)
+        self.bucket_manager.shuffle()
+
+    def __iter__(self):
+        while True:
+            index = self.rng.choice(len(self.buckets_indices))
+            bucket_reso = self.buckets_indices[index].bucket_reso
+            bucket_width, bucket_height = bucket_reso
+            bucket = self.bucket_manager.buckets[
+                self.buckets_indices[index].bucket_index]
+            batches = self.rng.choice(bucket, self.batch_size)
+            # image_index = self.buckets_indices[index].batch_index * self.batch_size
+            # batch = bucket[image_index : image_index + self.batch_size]
+            fields = self.fields + ['image_size', 'prompt_prefix']
+            batches = [
+                self.data_map[idx] + [[bucket_height, bucket_width], fields]
+                for idx in batches
+            ]
+            yield batches
+
+    def __len__(self):
+        return sys.maxsize
+
+    @staticmethod
+    def get_config_template():
+        return dict_to_yaml('SAMPLERS',
+                            __class__.__name__,
+                            ResolutionBatchSampler.para_dict,
+                            set_name=True)
```

## scepter/modules/inference/control_inference.py

```diff
@@ -90,15 +90,15 @@
             diffusion_model['model'].control_name = control_model_folder
         self.is_register = True
 
     @classmethod
     def get_control_input(self, control_model, control_cond_image, height,
                           width):
         hints = []
-        if control_cond_image and control_model:
+        if control_cond_image is not None and control_model is not None:
             if not isinstance(control_model, list):
                 control_model = [control_model]
             if not isinstance(control_cond_image, list):
                 control_cond_image = [control_cond_image]
             assert len(control_cond_image) == len(control_model)
             for img in control_cond_image:
                 if isinstance(img, Image):
```

## scepter/modules/inference/diffusion_inference.py

```diff
@@ -11,14 +11,15 @@
 
 from scepter.modules.model.network.diffusion.diffusion import GaussianDiffusion
 from scepter.modules.model.network.diffusion.schedules import noise_schedule
 from scepter.modules.model.registry import (BACKBONES, EMBEDDERS, MODELS,
                                             TOKENIZERS)
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
+from scepter.studio.utils.env import get_available_memory
 
 from .control_inference import ControlInference
 from .tuner_inference import TunerInference
 
 
 def get_model(model_tuple):
     assert 'model' in model_tuple
@@ -239,16 +240,27 @@
             module['device'] = we.device_id
             module['model'] = module['model'].to(we.device_id)
         return module
 
     def unload(self, module):
         if module is None:
             return module
-        module['model'] = module['model'].to('cpu')
-        module['device'] = 'cpu'
+        mem = get_available_memory()
+        free_mem = int(mem['available'] / (1024**2))
+        total_mem = int(mem['total'] / (1024**2))
+        if free_mem < 0.5 * total_mem:
+            if module['model'] is not None:
+                module['model'] = module['model'].to('cpu')
+                del module['model']
+            module['model'] = None
+            module['device'] = 'offline'
+            print('delete module')
+        else:
+            module['model'] = module['model'].to('cpu')
+            module['device'] = 'cpu'
         torch.cuda.empty_cache()
         torch.cuda.ipc_collect()
         return module
 
     def dynamic_load(self, module=None, name=''):
         self.logger.info('Loading {} model'.format(name))
         if name == 'all':
@@ -258,15 +270,15 @@
         elif name in self.loaded_model_name:
             if name in self.loaded_model:
                 if module['cfg'] != self.loaded_model[name]['cfg']:
                     self.unload(self.loaded_model[name])
                     module = self.load(module)
                     self.loaded_model[name] = module
                     return module
-                elif module['device'] == 'cpu':
+                elif module['device'] == 'cpu' or module['device'] == "offline":
                     module = self.load(module)
                     return module
                 else:
                     return module
             else:
                 module = self.load(module)
                 self.loaded_model[name] = module
```

## scepter/modules/inference/largen_inference.py

```diff
@@ -1,84 +1,42 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import copy
 import os.path
 import random
 from collections import OrderedDict
 
+import gradio as gr
 import torch
 import torchvision.transforms.functional as TF
-from PIL.Image import Image
-
-from scepter.modules.model.network.diffusion.diffusion import GaussianDiffusion
-from scepter.modules.model.network.diffusion.schedules import noise_schedule
-from scepter.modules.model.registry import (BACKBONES, EMBEDDERS, MODELS,
-                                            TOKENIZERS)
 from scepter.modules.model.utils.data_utils import crop_back
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
 
+from .diffusion_inference import DiffusionInference
+
 
 def get_model(model_tuple):
     assert 'model' in model_tuple
     return model_tuple['model']
 
 
-class LargenInference():
+class LargenInference(DiffusionInference):
     '''
         define vae, unet, text-encoder, tuner, refiner components
         support to load the components dynamicly.
         create and load model when run this model at the first time.
     '''
     def __init__(self, logger=None):
         self.logger = logger
         self.loaded_model = {}
         self.loaded_model_name = [
             'diffusion_model', 'first_stage_model', 'cond_stage_model'
         ]
 
-    def init_from_cfg(self, cfg):
-        self.name = cfg.NAME
-        self.is_default = cfg.get('IS_DEFAULT', False)
-        module_paras = self.load_default(cfg.get('DEFAULT_PARAS', None))
-        assert cfg.have('MODEL')
-        cfg.MODEL = self.redefine_paras(cfg.MODEL)
-        self.diffusion = self.load_schedule(cfg.MODEL.SCHEDULE)
-        self.diffusion_model = self.infer_model(
-            cfg.MODEL.DIFFUSION_MODEL, module_paras.get(
-                'DIFFUSION_MODEL',
-                None)) if cfg.MODEL.have('DIFFUSION_MODEL') else None
-        self.first_stage_model = self.infer_model(
-            cfg.MODEL.FIRST_STAGE_MODEL,
-            module_paras.get(
-                'FIRST_STAGE_MODEL',
-                None)) if cfg.MODEL.have('FIRST_STAGE_MODEL') else None
-        self.cond_stage_model = self.infer_model(
-            cfg.MODEL.COND_STAGE_MODEL,
-            module_paras.get(
-                'COND_STAGE_MODEL',
-                None)) if cfg.MODEL.have('COND_STAGE_MODEL') else None
-        self.refiner_cond_model = self.infer_model(
-            cfg.MODEL.REFINER_COND_MODEL,
-            module_paras.get(
-                'REFINER_COND_MODEL',
-                None)) if cfg.MODEL.have('REFINER_COND_MODEL') else None
-        self.refiner_diffusion_model = self.infer_model(
-            cfg.MODEL.REFINER_MODEL, module_paras.get(
-                'REFINER_MODEL',
-                None)) if cfg.MODEL.have('REFINER_MODEL') else None
-        self.tokenizer = TOKENIZERS.build(
-            cfg.MODEL.TOKENIZER,
-            logger=self.logger) if cfg.MODEL.have('TOKENIZER') else None
-
-        if self.tokenizer is not None:
-            self.cond_stage_model['cfg'].KWARGS = {
-                'vocab_size': self.tokenizer.vocab_size
-            }
-
     def redefine_paras(self, cfg):
         if cfg.get('PRETRAINED_MODEL', None):
             assert FS.isfile(cfg.PRETRAINED_MODEL)
             with FS.get_from(cfg.PRETRAINED_MODEL,
                              wait_finish=True) as local_path:
                 if local_path.endswith('safetensors'):
                     from safetensors.torch import load_file as load_safetensors
@@ -154,271 +112,28 @@
                     cfg.COND_STAGE_MODEL.RELOAD_MODEL = cond_stage_model_path
                 if not cfg.DIFFUSION_MODEL.get('PRETRAINED_MODEL', None):
                     cfg.DIFFUSION_MODEL.PRETRAINED_MODEL = diffusion_model_path
                 else:
                     cfg.DIFFUSION_MODEL.RELOAD_MODEL = diffusion_model_path
         return cfg
 
-    def init_from_modules(self, modules):
-        for k, v in modules.items():
-            self.__setattr__(k, v)
-
-    def infer_model(self, cfg, module_paras=None):
-        module = {
-            'model': None,
-            'cfg': cfg,
-            'device': 'offline',
-            'name': cfg.NAME,
-            'function_info': {},
-            'paras': {}
-        }
-        if module_paras is None:
-            return module
-        function_info = {}
-        paras = {
-            k.lower(): v
-            for k, v in module_paras.get('PARAS', {}).items()
-        }
-        for function in module_paras.get('FUNCTION', []):
-            input_dict = {}
-            for inp in function.get('INPUT', []):
-                if inp.lower() in self.input:
-                    input_dict[inp.lower()] = self.input[inp.lower()]
-            function_info[function.NAME] = {
-                'dtype': function.get('DTYPE', 'float32'),
-                'input': input_dict
-            }
-        module['paras'] = paras
-        module['function_info'] = function_info
-        return module
-
-    def init_from_ckpt(self, path, model, ignore_keys=list()):
-        if path.endswith('safetensors'):
-            from safetensors.torch import load_file as load_safetensors
-            sd = load_safetensors(path)
-        else:
-            sd = torch.load(path, map_location='cpu')
-
-        new_sd = OrderedDict()
-        for k, v in sd.items():
-            ignored = False
-            for ik in ignore_keys:
-                if ik in k:
-                    if we.rank == 0:
-                        self.logger.info(
-                            'Ignore key {} from state_dict.'.format(k))
-                    ignored = True
-                    break
-            if not ignored:
-                new_sd[k] = v
-
-        missing, unexpected = model.load_state_dict(new_sd, strict=False)
-        if we.rank == 0:
-            self.logger.info(
-                f'Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys'
-            )
-            if len(missing) > 0:
-                self.logger.info(f'Missing Keys:\n {missing}')
-            if len(unexpected) > 0:
-                self.logger.info(f'\nUnexpected Keys:\n {unexpected}')
-
-    def load(self, module):
-        if module['device'] == 'offline':
-            if module['cfg'].NAME in MODELS.class_map:
-                model = MODELS.build(module['cfg'], logger=self.logger).eval()
-            elif module['cfg'].NAME in BACKBONES.class_map:
-                model = BACKBONES.build(module['cfg'],
-                                        logger=self.logger).eval()
-            elif module['cfg'].NAME in EMBEDDERS.class_map:
-                model = EMBEDDERS.build(module['cfg'],
-                                        logger=self.logger).eval()
-            else:
-                raise NotImplementedError
-            if module['cfg'].get('RELOAD_MODEL', None):
-                self.init_from_ckpt(module['cfg'].RELOAD_MODEL, model)
-            module['model'] = model
-            module['device'] = 'cpu'
-        if module['device'] == 'cpu':
-            module['device'] = we.device_id
-            module['model'] = module['model'].to(we.device_id)
-        return module
-
-    def unload(self, module):
-        if module is None:
-            return module
-        module['model'] = module['model'].to('cpu')
-        module['device'] = 'cpu'
-        torch.cuda.empty_cache()
-        torch.cuda.ipc_collect()
-        return module
-
-    def dynamic_load(self, module=None, name=''):
-        self.logger.info('Loading {} model'.format(name))
-        if name == 'all':
-            for subname in self.loaded_model_name:
-                self.loaded_model[subname] = self.dynamic_load(
-                    getattr(self, subname), subname)
-        elif name in self.loaded_model_name:
-            if name in self.loaded_model:
-                if module['cfg'] != self.loaded_model[name]['cfg']:
-                    self.unload(self.loaded_model[name])
-                    module = self.load(module)
-                    self.loaded_model[name] = module
-                    return module
-                elif module['device'] == 'cpu':
-                    module = self.load(module)
-                    return module
-                else:
-                    return module
-            else:
-                module = self.load(module)
-                self.loaded_model[name] = module
-                return module
-        else:
-            return self.load(module)
-
-    def dynamic_unload(self, module=None, name='', skip_loaded=False):
-        self.logger.info('Unloading {} model'.format(name))
-        if name == 'all':
-            for name, module in self.loaded_model.items():
-                module = self.unload(self.loaded_model[name])
-                self.loaded_model[name] = module
-        elif name in self.loaded_model_name:
-            if name in self.loaded_model:
-                if not skip_loaded:
-                    module = self.unload(self.loaded_model[name])
-                    self.loaded_model[name] = module
-            else:
-                self.unload(module)
-        else:
-            self.unload(module)
-
-    def load_default(self, cfg):
-        module_paras = {}
-        if cfg is not None:
-            self.paras = cfg.PARAS
-            self.input = {k.lower(): v for k, v in cfg.INPUT.items()}
-            self.output = {k.lower(): v for k, v in cfg.OUTPUT.items()}
-            module_paras = cfg.MODULES_PARAS
-        return module_paras
-
-    def load_schedule(self, cfg):
-        parameterization = cfg.get('PARAMETERIZATION', 'eps')
-        assert parameterization in [
-            'eps', 'x0', 'v'
-        ], 'currently only supporting "eps" and "x0" and "v"'
-        num_timesteps = cfg.get('TIMESTEPS', 1000)
-
-        schedule_args = {
-            k.lower(): v
-            for k, v in cfg.get('SCHEDULE_ARGS', {
-                'NAME': 'logsnr_cosine_interp',
-                'SCALE_MIN': 2.0,
-                'SCALE_MAX': 4.0
-            }).items()
-        }
-
-        zero_terminal_snr = cfg.get('ZERO_TERMINAL_SNR', False)
-        if zero_terminal_snr:
-            assert parameterization == 'v', 'Now zero_terminal_snr only support v-prediction mode.'
-        sigmas = noise_schedule(schedule=schedule_args.pop('name'),
-                                n=num_timesteps,
-                                zero_terminal_snr=zero_terminal_snr,
-                                **schedule_args)
-        diffusion = GaussianDiffusion(sigmas=sigmas,
-                                      prediction_type=parameterization)
-        return diffusion
-
-    def get_batch(self, value_dict, num_samples=1):
-        batch = {}
-        batch_uc = {}
-        N = num_samples
-        device = we.device_id
-        for key in value_dict:
-            if key == 'prompt':
-                if not self.tokenizer:
-                    batch['prompt'] = value_dict['prompt']
-                    batch_uc['prompt'] = value_dict['negative_prompt']
-                else:
-                    batch['tokens'] = self.tokenizer(value_dict['prompt']).to(
-                        we.device_id)
-                    batch_uc['tokens'] = self.tokenizer(
-                        value_dict['negative_prompt']).to(we.device_id)
-            elif key == 'original_size_as_tuple':
-                batch['original_size_as_tuple'] = (torch.tensor(
-                    value_dict['original_size_as_tuple']).to(device).repeat(
-                        N, 1))
-            elif key == 'crop_coords_top_left':
-                batch['crop_coords_top_left'] = (torch.tensor(
-                    value_dict['crop_coords_top_left']).to(device).repeat(
-                        N, 1))
-            elif key == 'aesthetic_score':
-                batch['aesthetic_score'] = (torch.tensor(
-                    [value_dict['aesthetic_score']]).to(device).repeat(N, 1))
-                batch_uc['aesthetic_score'] = (torch.tensor([
-                    value_dict['negative_aesthetic_score']
-                ]).to(device).repeat(N, 1))
-
-            elif key == 'target_size_as_tuple':
-                batch['target_size_as_tuple'] = (torch.tensor(
-                    value_dict['target_size_as_tuple']).to(device).repeat(
-                        N, 1))
-            elif key == 'image':
-                batch[key] = self.load_image(value_dict[key], num_samples=N)
-            else:
-                batch[key] = value_dict[key]
-
-        for key in batch.keys():
-            if key not in batch_uc and isinstance(batch[key], torch.Tensor):
-                batch_uc[key] = torch.clone(batch[key])
-        return batch, batch_uc
-
-    def load_image(self, image, num_samples=1):
-        if isinstance(image, torch.Tensor):
-            pass
-        elif isinstance(image, Image):
-            pass
-        elif isinstance(image, Image):
-            pass
-
-    def get_function_info(self, module, function_name=None):
-        all_function = module['function_info']
-        if function_name in all_function:
-            return function_name, all_function[function_name]['dtype']
-        if function_name is None and len(all_function) == 1:
-            for k, v in all_function.items():
-                return k, v['dtype']
-
-    def encode_first_stage(self, x, **kwargs):
-        _, dtype = self.get_function_info(self.first_stage_model, 'encode')
-        with torch.autocast('cuda',
-                            enabled=dtype == 'float16',
-                            dtype=getattr(torch, dtype)):
-            z = get_model(self.first_stage_model).encode(x)
-            return self.first_stage_model['paras']['scale_factor'] * z
-
-    def decode_first_stage(self, z):
-        _, dtype = self.get_function_info(self.first_stage_model, 'encode')
-        with torch.autocast('cuda',
-                            enabled=dtype == 'float16',
-                            dtype=getattr(torch, dtype)):
-            z = 1. / self.first_stage_model['paras']['scale_factor'] * z
-            return get_model(self.first_stage_model).decode(z)
-
     @torch.no_grad()
     def __call__(self,
                  input,
                  num_samples=1,
                  intermediate_callback=None,
                  refine_strength=0,
                  img_to_img_strength=0,
                  cat_uc=True,
                  tuner_model=None,
                  control_model=None,
+                 largen_state=False,
                  **kwargs):
+        if not largen_state:
+            raise gr.Error('LARGEN model must be used with LAR-Gen settings')
 
         value_input = copy.deepcopy(self.input)
         value_input.update(input)
         print(value_input)
         height, width = value_input['target_size_as_tuple']
         value_output = copy.deepcopy(self.output)
         batch, batch_uc = self.get_batch(value_input, num_samples=1)
@@ -482,30 +197,30 @@
                 'ref_x0': ref_x0,
                 'ref_mask': ref_mask,
                 'image_scale': image_scale,
             })
 
         self.dynamic_unload(self.first_stage_model,
                             'first_stage_model',
-                            skip_loaded=True)
+                            skip_loaded=False)
 
         # cond stage
         self.dynamic_load(self.cond_stage_model, 'cond_stage_model')
         function_name, dtype = self.get_function_info(self.cond_stage_model)
         with torch.autocast('cuda',
                             enabled=dtype == 'float16',
                             dtype=getattr(torch, dtype)):
             context = getattr(get_model(self.cond_stage_model),
                               function_name)(batch)
             null_context = getattr(get_model(self.cond_stage_model),
                                    function_name)(batch_uc)
 
         self.dynamic_unload(self.cond_stage_model,
                             'cond_stage_model',
-                            skip_loaded=True)
+                            skip_loaded=False)
 
         # get noise
         seed = kwargs.pop('seed', -1)
         g = torch.Generator(device=we.device_id)
         seed = seed if seed >= 0 else random.randint(0, 2**32 - 1)
         g.manual_seed(seed)
         if 'seed' in value_output:
@@ -566,15 +281,15 @@
                     value_output['latent'] = []
                 value_output['latent'].append(latent)
 
             self.dynamic_load(self.first_stage_model, 'first_stage_model')
             x_samples = self.decode_first_stage(latent).float()
             self.dynamic_unload(self.first_stage_model,
                                 'first_stage_model',
-                                skip_loaded=True)
+                                skip_loaded=False)
             images = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)
             if base_image is not None:
                 stitch_images = []
                 for img in images:
                     stitch_img = crop_back(img, copy.deepcopy(base_image),
                                            extra_sizes, bbox_yyxx)
                     stitch_images.append(stitch_img)
```

## scepter/modules/model/backbone/unet/unet_utils.py

```diff
@@ -9,16 +9,17 @@
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torchvision.transforms.functional as TF
 from einops import rearrange, repeat
 from packaging import version
+from torch.utils.checkpoint import checkpoint
 
-from scepter.modules.model.utils.basic_utils import checkpoint, default, exists
+from scepter.modules.model.utils.basic_utils import default, exists
 
 try:
     import xformers
     import xformers.ops
     XFORMERS_IS_AVAILBLE = True
 except Exception as e:
     XFORMERS_IS_AVAILBLE = False
@@ -356,16 +357,18 @@
     def forward(self, x, emb):
         """
         Apply the block to a Tensor, conditioned on a timestep embedding.
         :param x: an [N x C x ...] Tensor of features.
         :param emb: an [N x emb_channels] Tensor of timestep embeddings.
         :return: an [N x C x ...] Tensor of outputs.
         """
-        return checkpoint(self._forward, (x, emb), self.parameters(),
-                          self.use_checkpoint)
+        if self.use_checkpoint:
+            return checkpoint(self._forward, x, emb)
+        else:
+            return self._forward(x, emb)
 
     def _forward(self, x, emb):
         if self.updown:
             in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]
             h = in_rest(x)
             h = self.h_upd(h)
             x = self.x_upd(x)
@@ -418,16 +421,18 @@
         else:
             # split head before split qkv
             self.attention = QKVAttentionLegacy(self.num_heads)
 
         self.proj_out = zero_module(conv_nd(1, channels, channels, 1))
 
     def forward(self, x):
-        return checkpoint(self._forward, (x, ), self.parameters(),
-                          self.use_checkpoint)
+        if self.use_checkpoint:
+            return checkpoint(self._forward, x)
+        else:
+            return self._forward(x)
 
     def _forward(self, x):
         b, c, *spatial = x.shape
         x = x.reshape(b, c, -1)
         qkv = self.qkv(self.norm(x))
         h = self.attention(qkv)
         h = self.proj_out(h)
@@ -982,16 +987,19 @@
             dropout=dropout)  # is self-attn if context is none
         self.norm1 = nn.LayerNorm(dim)
         self.norm2 = nn.LayerNorm(dim)
         self.norm3 = nn.LayerNorm(dim)
         self.use_checkpoint = use_checkpoint
 
     def forward(self, x, context=None):
-        return checkpoint(self._forward, (x, context), self.parameters(),
-                          self.use_checkpoint)
+
+        if self.use_checkpoint:
+            return checkpoint(self._forward, x, context)
+        else:
+            return self._forward(x, context)
 
     def _forward(self, x, context=None):
         x = self.attn1(self.norm1(x),
                        context=context if self.disable_self_attn else None) + x
         x = self.attn2(self.norm2(x), context=context) + x
         x = self.ff(self.norm3(x)) + x
         return x
```

## scepter/modules/model/embedder/__init__.py

```diff
@@ -1,10 +1,7 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-from scepter.modules.model.embedder.embedder import (ConcatTimestepEmbedderND,
-                                                     FrozenCLIPEmbedder,
-                                                     FrozenOpenCLIPEmbedder,
-                                                     FrozenOpenCLIPEmbedder2,
-                                                     GeneralConditioner,
-                                                     IPAdapterPlusEmbedder,
-                                                     RefCrossEmbedder)
+from scepter.modules.model.embedder.embedder import (
+    ConcatTimestepEmbedderND, FrozenCLIPEmbedder, FrozenOpenCLIPEmbedder,
+    FrozenOpenCLIPEmbedder2, GeneralConditioner, IPAdapterPlusEmbedder,
+    RefCrossEmbedder)
```

## scepter/modules/model/head/__init__.py

```diff
@@ -1,8 +1,5 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
-from scepter.modules.model.head.classifier_head import (ClassifierHead,
-                                                        CosineLinearHead,
-                                                        TransformerHead,
-                                                        TransformerHeadx2,
-                                                        VideoClassifierHead,
-                                                        VideoClassifierHeadx2)
+from scepter.modules.model.head.classifier_head import (
+    ClassifierHead, CosineLinearHead, TransformerHead, TransformerHeadx2,
+    VideoClassifierHead, VideoClassifierHeadx2)
```

## scepter/modules/model/network/autoencoder/ae_kl.py

```diff
@@ -48,15 +48,14 @@
             return torch.Tensor([0.])
         logtwopi = np.log(2.0 * np.pi)
         return 0.5 * torch.sum(logtwopi + self.logvar +
                                torch.pow(sample - self.mean, 2) / self.var,
                                dim=dims)
 
     def mode(self):
-        # print('*** use DiagonalGaussianDistribution.mode() ***')
         return self.mean
 
 
 @MODELS.register_class()
 class AutoencoderKL(TrainModule):
     para_dict = {
         'ENCODER': {},
```

## scepter/modules/model/network/diffusion/diffusion.py

```diff
@@ -444,30 +444,31 @@
         # function for denoising xt to get x0
         intermediates = []
 
         def model_fn(xt, sigma):
             # denoising
             t = self._sigma_to_t(sigma).repeat(len(xt)).round().long()
 
-            if isinstance(
-                    model_kwargs[0]['cond'], dict) and \
-                    'tar_x0' in model_kwargs[0]['cond'] and \
-                    'tar_mask_latent' in model_kwargs[0]['cond']:
-                tar_x0 = model_kwargs[0]['cond']['tar_x0']
-                tar_mask = model_kwargs[0]['cond']['tar_mask_latent']
+            if isinstance(model_kwargs, list) and len(model_kwargs) == 2:
+                if isinstance(
+                        model_kwargs[0]['cond'], dict) and \
+                        'tar_x0' in model_kwargs[0]['cond'] and \
+                        'tar_mask_latent' in model_kwargs[0]['cond']:
+                    tar_x0 = model_kwargs[0]['cond']['tar_x0']
+                    tar_mask = model_kwargs[0]['cond']['tar_mask_latent']
 
-                tar_xt = self.diffuse(x0=tar_x0, t=t)
-                xt = tar_xt * (1.0 - tar_mask) + xt * tar_mask
+                    tar_xt = self.diffuse(x0=tar_x0, t=t)
+                    xt = tar_xt * (1.0 - tar_mask) + xt * tar_mask
 
-            if isinstance(model_kwargs[0]['cond'],
-                          dict) and 'ref_x0' in model_kwargs[0]['cond']:
-                model_kwargs[0]['cond']['ref_xt'] = self.diffuse(
-                    x0=model_kwargs[0]['cond']['ref_x0'], t=t)
-                model_kwargs[1]['cond']['ref_xt'] = self.diffuse(
-                    x0=model_kwargs[1]['cond']['ref_x0'], t=t)
+                if isinstance(model_kwargs[0]['cond'],
+                              dict) and 'ref_x0' in model_kwargs[0]['cond']:
+                    model_kwargs[0]['cond']['ref_xt'] = self.diffuse(
+                        x0=model_kwargs[0]['cond']['ref_x0'], t=t)
+                    model_kwargs[1]['cond']['ref_xt'] = self.diffuse(
+                        x0=model_kwargs[1]['cond']['ref_x0'], t=t)
 
             if solver in ('onestep', 'multistep', 'multistep2', 'multistep3'):
                 x0 = self.denoise(xt,
                                   t,
                                   None,
                                   model,
                                   model_kwargs,
```

## scepter/modules/model/network/ldm/ldm.py

```diff
@@ -332,15 +332,15 @@
         image_size = None
         if 'meta' in kwargs:
             meta = kwargs.pop('meta')
             if 'image_size' in meta:
                 h = int(meta['image_size'][0][0])
                 w = int(meta['image_size'][1][0])
                 image_size = [h, w]
-        if 'image_size' in kwargs:
+        if 'image_size' in kwargs and kwargs['image_size'] is not None:
             image_size = kwargs.pop('image_size')
         if isinstance(image_size, numbers.Number):
             image_size = [image_size, image_size]
         if image_size is None:
             image_size = [1024, 1024]
         height, width = image_size
         noise = self.noise_sample(num_samples, height // self.size_factor,
```

## scepter/modules/model/utils/basic_utils.py

```diff
@@ -1,80 +1,22 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from inspect import isfunction
 
-import torch
-
 
 def exists(x):
     return x is not None
 
 
 def default(val, d):
     if exists(val):
         return val
     return d() if isfunction(d) else d
 
 
-def checkpoint(func, inputs, params, flag):
-    """
-    Evaluate a function without caching intermediate activations, allowing for
-    reduced memory at the expense of extra compute in the backward pass.
-    :param func: the function to evaluate.
-    :param inputs: the argument sequence to pass to `func`.
-    :param params: a sequence of parameters `func` depends on but does not
-                   explicitly take as arguments.
-    :param flag: if False, disable gradient checkpointing.
-    """
-    if flag:
-        args = tuple(inputs) + tuple(params)
-        return CheckpointFunction.apply(func, len(inputs), *args)
-    else:
-        return func(*inputs)
-
-
-class CheckpointFunction(torch.autograd.Function):
-    @staticmethod
-    def forward(ctx, run_function, length, *args):
-        ctx.run_function = run_function
-        ctx.input_tensors = list(args[:length])
-        ctx.input_params = list(args[length:])
-        ctx.gpu_autocast_kwargs = {
-            'enabled': torch.is_autocast_enabled(),
-            'dtype': torch.get_autocast_gpu_dtype(),
-            'cache_enabled': torch.is_autocast_cache_enabled()
-        }
-        with torch.no_grad():
-            output_tensors = ctx.run_function(*ctx.input_tensors)
-        return output_tensors
-
-    @staticmethod
-    def backward(ctx, *output_grads):
-        ctx.input_tensors = [
-            x.detach().requires_grad_(True) for x in ctx.input_tensors
-        ]
-        with torch.enable_grad(), \
-                torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs):
-            # Fixes a bug where the first op in run_function modifies the
-            # Tensor storage in place, which is not allowed for detach()'d
-            # Tensors.
-            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]
-            output_tensors = ctx.run_function(*shallow_copies)
-        input_grads = torch.autograd.grad(
-            output_tensors,
-            ctx.input_tensors + ctx.input_params,
-            output_grads,
-            allow_unused=True,
-        )
-        del ctx.input_tensors
-        del ctx.input_params
-        del output_tensors
-        return (None, None) + input_grads
-
-
 def disabled_train(self, mode=True):
     """Overwrite model.train with this function to make sure train/eval mode
     does not change anymore."""
     return self
 
 
 def transfer_size(para_num):
```

## scepter/modules/opt/optimizers/__init__.py

```diff
@@ -1,10 +1,7 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-from scepter.modules.opt.optimizers.official_optimizers import (ASGD, LBFGS,
-                                                                SGD, Adadelta,
-                                                                Adagrad, Adam,
-                                                                Adamax, AdamW,
-                                                                RMSprop, Rprop,
-                                                                SparseAdam)
+from scepter.modules.opt.optimizers.official_optimizers import (
+    ASGD, LBFGS, SGD, Adadelta, Adagrad, Adam, Adamax, AdamW, RMSprop, Rprop,
+    SparseAdam)
 from scepter.modules.opt.optimizers.registry import OPTIMIZERS
```

## scepter/modules/solver/hooks/checkpoint.py

```diff
@@ -45,27 +45,35 @@
             'description': 'If save the best model or not!'
         },
         'SAVE_BEST_BY': {
             'value':
             '',
             'description':
             'If save the best model, which order should be sorted, +/-!'
+        },
+        'DISABLE_SNAPSHOT': {
+            'value':
+            False,
+            'description':
+            'Skip to save snapshot checkpoint.'
         }
     }]
 
     def __init__(self, cfg, logger=None):
         super(CheckpointHook, self).__init__(cfg, logger=logger)
         self.priority = cfg.get('PRIORITY', _DEFAULT_CHECKPOINT_PRIORITY)
         self.interval = cfg.get('INTERVAL', 1)
         self.save_name_prefix = cfg.get('SAVE_NAME_PREFIX', 'ldm_step')
         self.save_last = cfg.get('SAVE_LAST', False)
         self.save_best = cfg.get('SAVE_BEST', False)
         self.save_best_by = cfg.get('SAVE_BEST_BY', '')
         self.push_to_hub = cfg.get('PUSH_TO_HUB', False)
         self.hub_model_id = cfg.get('HUB_MODEL_ID', None)
+        self.hub_private = cfg.get('HUB_PRIVATE', False)
+        self.disable_save_snapshot = cfg.get('DISABLE_SNAPSHOT', False)
         self.last_ckpt = None
         if self.save_best and not self.save_best_by:
             warnings.warn(
                 "CheckpointHook: Parameter 'save_best_by' is not set, turn off save_best function."
             )
             self.save_best = False
         self.higher_the_best = True
@@ -106,18 +114,19 @@
             solver.logger.info(
                 f'Saving checkpoint after {solver.total_iter + 1} steps')
             if we.rank == 0:
                 save_path = osp.join(
                     solver.work_dir,
                     'checkpoints/{}-{}.pth'.format(self.save_name_prefix,
                                                    solver.total_iter + 1))
-                with FS.put_to(save_path) as local_path:
-                    with open(local_path, 'wb') as f:
-                        checkpoint = solver.save_checkpoint()
-                        torch.save(checkpoint, f)
+                if not self.disable_save_snapshot:
+                    with FS.put_to(save_path) as local_path:
+                        with open(local_path, 'wb') as f:
+                            checkpoint = solver.save_checkpoint()
+                            torch.save(checkpoint, f)
 
                 from swift import SwiftModel
                 if isinstance(solver.model, SwiftModel):
                     save_path = osp.join(
                         solver.work_dir,
                         'checkpoints/{}-{}'.format(self.save_name_prefix,
                                                    solver.total_iter + 1))
@@ -242,15 +251,15 @@
                                 os.path.join(save_path, base_file))
                     except OSError:
                         shutil.copyfile(self.last_ckpt,
                                         os.path.join(save_path, base_file))
 
                 push_to_hub(repo_name=self.hub_model_id,
                             output_dir=self.last_ckpt,
-                            private=False)
+                            private=self.hub_private)
                 current_dir = os.path.dirname(__file__)
                 base_path = os.sep.join(current_dir.split(os.sep)[:-4])
                 base_path = os.path.join(base_path, 'config')
                 file_name = self.hub_model_id.replace(os.sep, '_')
                 content = {'name': self.hub_model_id}
                 import json
                 with open(os.path.join(base_path, file_name), 'w') as f:
```

## scepter/modules/transform/image.py

```diff
@@ -6,24 +6,19 @@
 import opencv_transforms.functional as cv2_TF
 import opencv_transforms.transforms as cv2_transforms
 import torch
 import torchvision.transforms as transforms
 import torchvision.transforms.functional as TF
 
 from scepter.modules.transform.registry import TRANSFORMS
-from scepter.modules.transform.utils import (BACKEND_CV2, BACKEND_PILLOW,
-                                             BACKEND_TORCHVISION,
-                                             INPUT_CV2_TYPE_WARNING,
-                                             INPUT_PIL_TYPE_WARNING,
-                                             INPUT_TENSOR_TYPE_WARNING,
-                                             INTERPOLATION_STYLE,
-                                             INTERPOLATION_STYLE_CV2,
-                                             TORCHVISION_CAPABILITY,
-                                             is_cv2_image, is_pil_image,
-                                             is_tensor)
+from scepter.modules.transform.utils import (
+    BACKEND_CV2, BACKEND_PILLOW, BACKEND_TORCHVISION, INPUT_CV2_TYPE_WARNING,
+    INPUT_PIL_TYPE_WARNING, INPUT_TENSOR_TYPE_WARNING, INTERPOLATION_STYLE,
+    INTERPOLATION_STYLE_CV2, TORCHVISION_CAPABILITY, is_cv2_image,
+    is_pil_image, is_tensor)
 from scepter.modules.utils.config import dict_to_yaml
 
 if TORCHVISION_CAPABILITY:
     BACKENDS = (BACKEND_PILLOW, BACKEND_CV2, BACKEND_TORCHVISION)
 else:
     BACKENDS = (BACKEND_PILLOW, BACKEND_CV2)
```

## scepter/modules/transform/tensor.py

```diff
@@ -256,28 +256,35 @@
 
 @TRANSFORMS.register_class()
 class RenameMeta(object):
     def __init__(self, cfg, logger=None):
         self.input_key = cfg.INPUT_KEY
         self.output_key = cfg.OUTPUT_KEY
         self.force = cfg.get('FORCE', False)
+        self.move = cfg.get('MOVE', False)
 
     def __call__(self, item):
         if 'meta' in item:
             data = {}
             for idx, key in enumerate(self.input_key):
                 data[self.output_key[idx]] = item['meta'][key]
             if not self.force:
                 have_key_set = set(self.input_key)
             else:
                 have_key_set = set(self.input_key + self.output_key)
-            for k, v in item['meta'].items():
-                if k not in have_key_set:
-                    data[k] = v
-            item['meta'] = data
+            if not self.move:
+                for k, v in item['meta'].items():
+                    if k not in have_key_set:
+                        data[k] = v
+                item['meta'] = data
+            else:
+                for k, v in item.items():
+                    if k not in have_key_set:
+                        data[k] = v
+                item.update(data)
         return item
 
     @staticmethod
     def get_config_template():
         '''
         { "ENV" :
             { "description" : "",
```

## scepter/modules/utils/config.py

```diff
@@ -7,14 +7,21 @@
 import os
 import sys
 
 import yaml
 
 from scepter.modules.utils.model import StdMsg
 
+_SECURE_KEYWORDS = [
+    'ENDPOINT', 'BUCKET', 'OSS_AK', 'OSS_SK', 'OSS', 'TOKEN', 'APPKEY'
+    'SECRET', 'ACCESS_ID', 'ACCESS_KEY', 'PASSWORD', 'TEMP_DIR'
+]  # -> "*****"
+
+_SECURE_VALUEWORDS = ['oss://', 'oss-']  # -> "#####"
+
 
 def dict_to_yaml(module_name, name, json_config, set_name=False):
     '''
     { "ENV" :
         { "description" : "",
           "A" : {
                 "value": 1.0,
@@ -513,16 +520,37 @@
                     key_split[3]] = vals[idx]
 
         return cfg
 
     def __repr__(self):
         return '{}\n'.format(self.dump())
 
-    def dump(self):
-        return json.dumps(self.cfg_dict, indent=2)
+    def dump(self, is_secure=False):
+        if not is_secure:
+            return json.dumps(self.cfg_dict, indent=2)
+        else:
+
+            def make_secure(cfg):
+                if isinstance(cfg, dict):
+                    for key, val in cfg.items():
+                        if key in _SECURE_KEYWORDS and type(val) is str:
+                            cfg[key] = '*****'
+                        else:
+                            cfg[key] = make_secure(cfg[key])
+                elif isinstance(cfg, list):
+                    cfg = [make_secure(t) for t in cfg]
+                elif isinstance(cfg, str):
+                    for sval in _SECURE_VALUEWORDS:
+                        if sval in cfg:
+                            cfg = '#####'
+                return cfg
+
+            cfg_dict_copy = copy.deepcopy(self.cfg_dict)
+            cfg_dict_copy = make_secure(cfg_dict_copy)
+            return json.dumps(cfg_dict_copy, indent=2)
 
     def deep_copy(self):
         return copy.deepcopy(self)
 
     def have(self, name):
         if name in self.__dict__:
             return True
@@ -600,9 +628,12 @@
                     cfg_new.append(Config.get_plain_cfg(val))
                 elif isinstance(val, (str, numbers.Number)):
                     cfg_new.append(val)
             return cfg_new
         else:
             return cfg
 
+    def __len__(self):
+        return len(self.cfg_dict)
+
     def pop(self, name):
         self.cfg_dict.pop(name)
```

## scepter/modules/utils/file_clients/http_fs.py

```diff
@@ -127,15 +127,15 @@
                              local_path=None,
                              wait_finish=False,
                              multi_thread=False,
                              timeout=3600,
                              worker_id=0) -> Optional[str]:
         raise NotImplementedError
 
-    def get_url(self, target_path, lifecycle=3600 * 100):
+    def get_url(self, target_path, set_public=False, lifecycle=3600 * 100):
         return target_path
 
     def exists(self, target_path) -> bool:
         req = urllib.request.Request(target_path)
         req.get_method = lambda: 'HEAD'
 
         try:
```

## scepter/modules/utils/file_clients/huggingface_fs.py

```diff
@@ -177,15 +177,15 @@
             self,
             target_path,
             start,
             size=10000,
             delimiter=None) -> (Union[bytes, str, None], Optional[int]):
         raise NotImplementedError
 
-    def get_url(self, target_path, lifecycle=3600 * 100):
+    def get_url(self, target_path, set_public=False, lifecycle=3600 * 100):
         return target_path
 
     def exists(self, target_path) -> bool:
         req = urllib.request.Request(target_path)
         req.get_method = lambda: 'HEAD'
 
         try:
```

## scepter/modules/utils/file_clients/modelscope_fs.py

```diff
@@ -195,15 +195,15 @@
             self,
             target_path,
             start,
             size=10000,
             delimiter=None) -> (Union[bytes, str, None], Optional[int]):
         raise NotImplementedError
 
-    def get_url(self, target_path, lifecycle=3600 * 100):
+    def get_url(self, target_path, set_public=False, lifecycle=3600 * 100):
         return target_path
 
     def exists(self, target_path) -> bool:
         req = urllib.request.Request(target_path)
         req.get_method = lambda: 'HEAD'
 
         try:
```

## scepter/studio/inference/inference.py

```diff
@@ -16,19 +16,21 @@
 from scepter.studio.inference.inference_ui.control_ui import ControlUI
 from scepter.studio.inference.inference_ui.diffusion_ui import DiffusionUI
 from scepter.studio.inference.inference_ui.gallery_ui import GalleryUI
 from scepter.studio.inference.inference_ui.largen_ui import LargenUI
 from scepter.studio.inference.inference_ui.mantra_ui import MantraUI
 from scepter.studio.inference.inference_ui.model_manage_ui import ModelManageUI
 from scepter.studio.inference.inference_ui.refiner_ui import RefinerUI
+from scepter.studio.inference.inference_ui.stylebooth_ui import StyleboothUI
 from scepter.studio.inference.inference_ui.tuner_ui import TunerUI
 from scepter.studio.utils.env import init_env
 
 UI_MAP = [('diffusion', DiffusionUI), ('mantra', MantraUI), ('tuner', TunerUI),
-          ('control', ControlUI), ('refiner', RefinerUI), ('largen', LargenUI)]
+          ('control', ControlUI), ('refiner', RefinerUI), ('largen', LargenUI),
+          ('stylebooth', StyleboothUI)]
 
 
 class InferenceUI():
     def __init__(self,
                  cfg_general_file,
                  is_debug=False,
                  language='en',
@@ -104,90 +106,108 @@
                     pipe_manager,
                     is_debug=is_debug,
                     language=language)
             self.tab_ui[name] = ui
             self.tab_ui_kwargs[f'{name}_ui'] = ui
             self.__setattr__(f'{name}_ui', ui)
 
-        self.check_box_controlled_tabs = ['mantra', 'tuner', 'control', 'largen']
+        self.check_box_controlled_tabs = [
+            'mantra', 'tuner', 'control', 'largen', 'stylebooth'
+        ]
         self.pipe_manager = pipe_manager
         assert len(self.component_names.check_box_for_setting) == len(
             self.check_box_controlled_tabs)
 
     def create_ui(self):
         # create model
-        self.model_manage_ui.create_ui()
+        self.model_manage_ui.create_ui(gallery_ui=self.gallery_ui)
         self.gallery_ui.create_ui()
         self.infer_info = gr.State(value=None)
 
         # create tabs
         def create_tab(name, ui):
             label = getattr(self.component_names, f'{name}_paras')
             if name in ['refiner']:
-                ui.create_ui()
+                ui.create_ui(gallery_ui=self.gallery_ui)
             else:
                 with gr.TabItem(label=label, id=f'{name}_ui'):
-                    ui.create_ui()
+                    ui.create_ui(gallery_ui=self.gallery_ui)
 
         with gr.Row(variant='panel', equal_height=True):
             with gr.Accordion(label=self.component_names.advance_block_name,
                               open=True):
                 self.check_box_for_setting = gr.CheckboxGroup(
                     choices=self.component_names.check_box_for_setting,
                     show_label=False)
                 with gr.Tabs() as self.setting_tab:
                     for name, ui in self.tab_ui.items():
                         create_tab(name, ui)
 
     def set_callbacks(self, manager):
         self.model_manage_ui.set_callbacks(**self.tab_ui_kwargs)
-        self.gallery_ui.set_callbacks(self, self.model_manage_ui,
-                                      **self.tab_ui_kwargs)
+        self.gallery_ui.set_callbacks(self,
+                                      self.model_manage_ui,
+                                      **self.tab_ui_kwargs,
+                                      manager=manager)
         for name, ui in self.tab_ui_kwargs.items():
             ui.set_callbacks(self.model_manage_ui,
                              **self.tab_ui_kwargs,
                              gallery_ui=self.gallery_ui,
                              manager=manager)
 
         def change_setting_tab(check_box, default_diffusion_model, *args):
             selected_tab = 'diffusion_ui'
             ui_tabs_state = [False] * len(args)
             largen_index = self.check_box_controlled_tabs.index('largen')
-            largen_key = self.component_names.check_box_for_setting[largen_index]
+            largen_key = self.component_names.check_box_for_setting[
+                largen_index]
             largen_status = args[largen_index]
+            stylebooth_index = self.check_box_controlled_tabs.index(
+                'stylebooth')
+            stylebooth_key = self.component_names.check_box_for_setting[
+                stylebooth_index]
+            stylebooth_status = args[stylebooth_index]
             for key in check_box:
                 i = self.component_names.check_box_for_setting.index(key)
                 ui_tabs_state[i] = True
                 if ui_tabs_state[i] != args[i]:
                     selected_tab = self.check_box_controlled_tabs[i] + '_ui'
             new_check_box_value = check_box
             for key in check_box:
                 i = self.component_names.check_box_for_setting.index(key)
                 if ui_tabs_state[i] != args[i]:
-                    if i in [largen_index]:
+                    if i in [largen_index, stylebooth_index]:
                         new_check_box_value = [key]
                         for j in range(len(ui_tabs_state)):
                             ui_tabs_state[j] = j == i
                     else:
                         new_check_box_value = [
                             k for k in check_box
-                            if k not in [largen_key]
+                            if k not in [largen_key, stylebooth_key]
                         ]
                         ui_tabs_state[largen_index] = False
+                        ui_tabs_state[stylebooth_index] = False
 
             ui_tabs_updates = [gr.update(visible=v) for v in ui_tabs_state]
 
             if ui_tabs_state[largen_index]:
                 diffusion_model = gr.Dropdown(
                     label=self.model_manage_ui.component_names.diffusion_model,
                     choices=self.model_manage_ui.
                     default_choices['diffusion_model']['choices'],
                     value='LARGEN_LargenUNetXL',
                     interactive=False)
-            elif largen_status:
+            elif ui_tabs_state[stylebooth_index]:
+                diffusion_model = gr.Dropdown(
+                    label=self.model_manage_ui.component_names.diffusion_model,
+                    choices=self.model_manage_ui.
+                    default_choices['diffusion_model']['choices'],
+                    value='EDIT_DiffusionUNet',
+                    interactive=False)
+            elif largen_status or stylebooth_status:
                 diffusion_model = gr.Dropdown(
                     label=self.model_manage_ui.component_names.diffusion_model,
                     choices=self.model_manage_ui.
                     default_choices['diffusion_model']['choices'],
                     value=self.model_manage_ui.
                     default_choices['diffusion_model']['default'],
                     interactive=True)
@@ -199,28 +219,34 @@
                     value=default_diffusion_model,
                     interactive=True)
 
             return gr.CheckboxGroup(
                 choices=self.component_names.check_box_for_setting,
                 value=new_check_box_value,
                 show_label=False), gr.update(
-                selected=selected_tab), *ui_tabs_state, *ui_tabs_updates, diffusion_model
+                    selected=selected_tab
+                ), *ui_tabs_state, *ui_tabs_updates, diffusion_model
 
         gr_states = [
             self.tab_ui[name].state for name in self.check_box_controlled_tabs
         ]
         gr_tabs = [
             self.tab_ui[name].tab for name in self.check_box_controlled_tabs
         ]
         self.check_box_for_setting.change(
             change_setting_tab,
-            inputs=[self.check_box_for_setting, self.model_manage_ui.diffusion_model, *gr_states],
-            outputs=[self.check_box_for_setting, self.setting_tab, *gr_states,
-                     *gr_tabs, self.model_manage_ui.diffusion_model],
-            queue=False)
+            inputs=[
+                self.check_box_for_setting,
+                self.model_manage_ui.diffusion_model, *gr_states
+            ],
+            outputs=[
+                self.check_box_for_setting, self.setting_tab, *gr_states,
+                *gr_tabs, self.model_manage_ui.diffusion_model
+            ],
+            queue=True)
 
 
 if __name__ == '__main__':
     infer_ins = InferenceUI('scepter/methods/studio/inference/inference.yaml',
                             is_debug=True,
                             language='en',
                             root_work_dir='./cache')
```

## scepter/studio/inference/inference_manager/infer_runer.py

```diff
@@ -1,11 +1,12 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from scepter.modules.inference.diffusion_inference import DiffusionInference
 from scepter.modules.inference.largen_inference import LargenInference
+from scepter.modules.inference.stylebooth_inference import StyleboothInference
 from scepter.modules.utils.logger import get_logger
 
 
 class PipelineManager():
     def __init__(self, logger=None):
         '''
         Args:
@@ -95,14 +96,16 @@
     def construct_new_pipeline(self):
         pass
 
     def register_pipeline(self, cfg):
         pipeline_name = cfg.NAME
         if 'LARGEN' in pipeline_name:
             PipelineBuilder = LargenInference
+        elif pipeline_name.startswith('EDIT'):
+            PipelineBuilder = StyleboothInference
         else:
             PipelineBuilder = DiffusionInference
         new_inference = PipelineBuilder(logger=self.logger)
         new_inference.init_from_cfg(cfg)
         self.contruct_models_index(cfg.NAME, new_inference)
 
     def register_tuner(self, cfg, name=None, is_customized=False):
```

## scepter/studio/inference/inference_ui/component_names.py

```diff
@@ -3,48 +3,48 @@
 # For dataset manager
 from scepter.modules.utils.directory import get_md5
 from scepter.modules.utils.file_system import FS
 
 
 def download_image(image):
     if image is not None:
-        client = FS.get_fs_client(image)
-        if client.tmp_dir.startswith('/home'):
-            name = get_md5(image)
-            local_path = FS.get_from(image,
-                                     f'/tmp/gradio/scepter_examples/{name}')
-        else:
-            local_path = FS.get_from(image)
+        name = get_md5(image)
+        local_path = FS.get_from(image, f'/tmp/gradio/scepter_examples/{name}')
         return local_path
     else:
         return image
 
 
 class InferenceUIName():
     def __init__(self, language='en'):
         if language == 'en':
             self.advance_block_name = 'Advance Setting'
             self.check_box_for_setting = [
-                'Use Mantra', 'Use Tuners', 'Use Controller', 'LAR-Gen'
+                'Use Mantra', 'Use Tuners', 'Use Controller', 'LAR-Gen',
+                'StyleBooth'
             ]
             self.diffusion_paras = 'Generation Setting'
             self.mantra_paras = 'Mantra Book'
             self.tuner_paras = 'Tuners'
             self.control_paras = 'Controlable Generation'
             self.refiner_paras = 'Refiner Setting'
             self.largen_paras = 'LAR-Gen'
+            self.stylebooth_paras = 'StyleBooth'
         elif language == 'zh':
             self.advance_block_name = '生成选项'
-            self.check_box_for_setting = ['使用咒语', '使用微调', '使用控制', 'LAR-Gen']
+            self.check_box_for_setting = [
+                '使用咒语', '使用微调', '使用控制', 'LAR-Gen', 'StyleBooth'
+            ]
             self.diffusion_paras = '生成参数设置'
             self.mantra_paras = '咒语书'
             self.tuner_paras = '微调模型'
             self.control_paras = '可控生成'
             self.refiner_paras = 'Refine设置'
             self.largen_paras = 'LAR-Gen'
+            self.stylebooth_paras = 'StyleBooth'
 
 
 class ModelManageUIName():
     def __init__(self, language='en'):
         if language == 'en':
             self.model_block_name = 'Model Management'
             self.postprocess_model_name = 'Refiners and Tuners'
@@ -495,7 +495,86 @@
                 0.45,
                 0.0,
                 '',
                 1024,
                 1024
             ],
         ]
+
+
+class StyleboothUIName():
+    def __init__(self, language='en'):
+        if language == 'en':
+            self.dropdown_name = 'Application'
+            self.apps = ['Text-based Style Editing']
+            # self.apps = ["Text-based Style Editing", "Exemplar-based Style Editing"]
+            self.source_image = 'Source Image'
+            self.exemplar_image = 'Exemplar Image'
+            self.ins_format = 'Instruction Format (select or rewrite, en only)'
+            self.style_format = '{} (select or rewrite, en only)'
+            self.guide_scale_image = 'Guide Scale For Uncondition Image'
+            self.guide_scale_text = 'Guide Scale For Uncondition Text'
+            self.guide_rescale = 'Guide Rescale'
+            self.resolution = 'Resolution of Short Edge'
+            self.compose_button = 'Assemble Style Editing Instruction to Prompt'
+        elif language == 'zh':
+            self.dropdown_name = '应用'
+            self.apps = ['根据文本编辑风格']
+            # self.apps = ["根据文本编辑风格", "根据风格样例编辑风格"]
+            self.source_image = '源图片'
+            self.exemplar_image = '样例图片'
+            self.ins_format = '指令模版 (选择或者新写，仅英文)'
+            self.style_format = '{} (选择或者新写，仅英文)'
+            self.guide_scale_image = '图片条件引导比例'
+            self.guide_scale_text = '文本条件引导比例'
+            self.guide_rescale = '引导缩放'
+            self.resolution = '短边分辨率'
+            self.compose_button = '组装风格编辑指令到Prompt栏'
+        self.tb_ins_format_choice = [
+            'Let this image be in the style of <style>',
+            'Please edit this image to embody the characteristics of <style> style.',
+            'Transform this image to reflect the distinct aesthetic of <style>.',
+            'Can you infuse this image with the signature techniques representative of <style>?',
+            'Adjust the visual elements of this image to emulate the <style> style.',
+            'Reinterpret this image through the artistic lens of <style>.',
+            'Apply the <style> style to this image to capture its unique essence.',
+            'Modify this photograph to mirror the thematic qualities of <style>.',
+            "I'd like you to rework this image to pay homage to the <style> movement.",
+            'Ensure that this image adopts the brushwork and color palette typical of <style>.',
+            'Give this image a makeover so that it aligns with the <style> stylistic approach.',
+            'Retouch this image to channel the spirit and technique of <style>.',
+            'Merge this image with the foundational elements of <style>.',
+            'Re-envision this image to fit within the <style> genre.',
+            'Adapt this image to exhibit the soft edges and vibrant light of <style>.',
+            'Craft this image to resonate with the visual themes found in <style>.',
+        ]
+        self.eb_ins_format_choice = [
+            'Let this image be in the style of <image>',
+            'Please match the aesthetic of this image to that of <image>.',
+            'Adjust the current image to mimic the visual style of <image>.',
+            'Edit this photo so that it reflects the artistic style found in <image>.',
+            'Transform this picture to be stylistically similar to <image>.',
+            'Recreate the ambiance and look of <image> in this one.',
+            'Harmonize the visual elements of this image with those in <image>.',
+            'Ensure the editing of this image captures the essence of the style in <image>.',
+            'Can you make this image resonate with the artistic flair of <image>?',
+            "I'd like this image to have the same feel and tone as the style reference in <image>.",
+            'Retouch this image to align with the creative direction of <image>.',
+            'Replicate the style characteristics of <image> onto this one.',
+            'Adapt the visual theme of this image to be consistent with <image>.',
+            "Conform this image's aesthetic to the distinctive look of <image>.",
+            'Make over this image so it conforms to the stylistic cues of <image>.',
+            "Modify this image's style to echo the artistic qualities of <image>.",
+        ]
+        self.tb_target_style = [
+            'Adorable 3D Character', 'Color Field Painting',
+            'Colored Pencil Art', 'Graffiti Art', 'futuristic-retro futurism',
+            'game-retro arcade', 'Simple Vector Art', 'Sketchup', 'mre-comic',
+            'sai-comic book', 'sai-lowpoly', 'misc-stained glass',
+            'misc-zentangle', 'papercraft-papercut collage', 'Pop Art 2',
+            'photo-silhouette', 'Adorable Kawaii', 'sai-anime', 'sai-origami',
+            'futuristic-vaporwave', 'game-retro game', 'misc-disco',
+            'papercraft-flat papercut'
+        ]
+        self.eb_target_style = ['<image>']
+        self.tb_identifier = '<style>'
+        self.eb_identifier = '<image>'
```

## scepter/studio/inference/inference_ui/control_ui.py

```diff
@@ -114,14 +114,22 @@
                     maximum=1.0,
                     step=0.05,
                     value=1.0,
                     interactive=True)
 
             self.example_block = gr.Accordion(
                 label=self.component_names.example_block_name, open=True)
+        gallery_ui = kwargs.pop('gallery_ui', None)
+        gallery_ui.register_components({
+            'control_state': self.state,
+            'control_model': self.control_model,
+            'control_scale': self.control_scale,
+            'crop_type': self.crop_type,
+            'control_cond_image': self.cond_image,
+        })
 
     def set_callbacks(self, model_manage_ui, diffusion_ui, **kwargs):
         gallery_ui = kwargs.pop('gallery_ui')
         with self.example_block:
             gr.Examples(
                 examples=self.component_names.examples,
                 inputs=[self.control_mode, self.cond_image, gallery_ui.prompt],
```

## scepter/studio/inference/inference_ui/diffusion_ui.py

```diff
@@ -149,14 +149,28 @@
             with gr.Column(scale=2):
                 self.image_seed = gr.Textbox(label=self.component_names.seed,
                                              value=-1,
                                              max_lines=1,
                                              interactive=True)
             with gr.Column(scale=1):
                 self.refresh_seed = gr.Button(value=refresh_symbol)
+        gallery_ui = kwargs.pop('gallery_ui', None)
+        gallery_ui.register_components({
+            'negative_prompt': self.negative_prompt,
+            'prompt_prefix': self.prompt_prefix,
+            'sample': self.sampler,
+            'discretization': self.discretization,
+            'output_height': self.output_height,
+            'output_width': self.output_width,
+            'image_number': self.image_number,
+            'sample_steps': self.sample_steps,
+            'guide_scale': self.guide_scale,
+            'guide_rescale': self.guide_rescale,
+            'image_seed': self.image_seed,
+        })
 
     def set_callbacks(self, model_manage_ui, **kwargs):
         gallery_ui = kwargs.pop('gallery_ui')
         with self.example_block:
             gr.Examples(label=self.component_names.example_block_name,
                         examples=self.component_names.examples,
                         inputs=gallery_ui.prompt)
```

## scepter/studio/inference/inference_ui/gallery_ui.py

```diff
@@ -1,10 +1,11 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
+from collections import OrderedDict
 
 import gradio as gr
 import numpy as np
 from PIL import Image
 
 from scepter.modules.utils.file_system import FS
 from scepter.studio.inference.inference_ui.component_names import GalleryUIName
@@ -15,14 +16,16 @@
     def __init__(self, cfg, pipe_manager, is_debug=False, language='en'):
         self.pipe_manager = pipe_manager
         self.component_names = GalleryUIName(language)
         self.cfg = cfg
         self.work_dir = cfg.WORK_DIR
         self.local_work_dir, _ = FS.map_to_local(self.work_dir)
         os.makedirs(self.local_work_dir, exist_ok=True)
+        self.component_mapping = OrderedDict()
+        self.manager = None
 
     def create_ui(self, *args, **kwargs):
         with gr.Group():
             gr.Markdown(value=self.component_names.gallery_block_name)
             with gr.Row(variant='panel', equal_height=True):
                 with gr.Column(scale=2, min_width=0,
                                visible=False) as self.before_refine_panel:
@@ -51,172 +54,176 @@
                 with gr.Column(scale=3, min_width=0):
                     self.generate_button = gr.Button(
                         label='Generate',
                         value=self.component_names.generate,
                         elem_classes='type_row',
                         elem_id='generate_button',
                         visible=True)
+        self.register_components({'prompt': self.prompt})
 
-    def generate_gallery(self,
-                         prompt,
-                         mantra_state,
-                         tuner_state,
-                         control_state,
-                         refine_state,
-                         diffusion_model,
-                         first_stage_model,
-                         cond_stage_model,
-                         refiner_cond_model,
-                         refiner_diffusion_model,
-                         tuner_model,
-                         tuner_scale,
-                         custom_tuner_model,
-                         control_model,
-                         control_scale,
-                         crop_type,
-                         control_cond_image,
-                         negative_prompt,
-                         prompt_prefix,
-                         sample,
-                         discretization,
-                         output_height,
-                         output_width,
-                         image_number,
-                         sample_steps,
-                         guide_scale,
-                         guide_rescale,
-                         refine_strength,
-                         refine_sampler,
-                         refine_discretization,
-                         refine_guide_scale,
-                         refine_guide_rescale,
-                         style_template,
-                         style_negative_template,
-                         image_seed,
-                         largen_state,
-                         largen_task,
-                         largen_image_scale,
-                         largen_tar_image,
-                         largen_tar_mask,
-                         largen_masked_image,
-                         largen_ref_image,
-                         largen_ref_mask,
-                         largen_ref_clip,
-                         largen_base_image,
-                         largen_extra_sizes,
-                         largen_bbox_yyxx,
-                         largen_history,
-                         show_jpeg_image=True):
-        if control_state and control_cond_image is None:
-            raise gr.Error(self.component_names.control_err1)
-
-        current_pipeline = self.pipe_manager.get_pipeline_given_modules({
-            'diffusion_model':
-            diffusion_model,
-            'first_stage_model':
-            first_stage_model,
-            'cond_stage_model':
-            cond_stage_model,
-            'refiner_cond_model':
-            refiner_cond_model,
-            'refiner_diffusion_model':
-            refiner_diffusion_model
-        })
-        now_pipeline = self.pipe_manager.model_level_info[diffusion_model][
-            'pipeline'][0]
-        used_tuner_model = []
-        if not isinstance(tuner_model, list):
-            tuner_model = [tuner_model]
-        for tuner_m in tuner_model:
-            if tuner_m is None or tuner_m == '':
-                continue
-            if now_pipeline in self.pipe_manager.model_level_info['tuners'] and \
-               tuner_m in self.pipe_manager.model_level_info['tuners'][now_pipeline]:
-                tuner_m = self.pipe_manager.model_level_info['tuners'][
-                    now_pipeline][tuner_m]['model_info']
-                used_tuner_model.append(tuner_m)
-        used_custom_tuner_model = []
-        if not isinstance(custom_tuner_model, list):
-            custom_tuner_model = [custom_tuner_model]
-        for tuner_m in custom_tuner_model:
-            if tuner_m is None or tuner_m == '':
-                continue
+    def register_components(self, components):
+        common_keys = self.component_mapping.keys() & components.keys()
+        assert len(
+            common_keys) == 0, f'Component key already exist: {common_keys}'
+        self.component_mapping.update(components)
+
+    def generate_gallery(self, *args, show_jpeg_image=True):
+        # unload all image preprocess model.
+        if self.manager is not None:
+            if (hasattr(self.manager, 'preprocess') and hasattr(
+                    self.manager.preprocess.dataset_gallery.processors_manager,
+                    'dynamic_unload')):
+                self.manager.preprocess.dataset_gallery.processors_manager.dynamic_unload(
+                )
+
+        def pipeline_init(args):
+            return self.pipe_manager.get_pipeline_given_modules(args)
+
+        def control_init(args):
+            control_state, tuner_state = args['control_state'], args[
+                'tuner_state']
+            control_cond_image = args.pop('control_cond_image')
+            control_model = args.pop('control_model')
+
+            if control_state and control_cond_image is None:
+                raise gr.Error(self.component_names.control_err1)
+
+            now_pipeline = self.pipe_manager.model_level_info[
+                args['diffusion_model']]['pipeline'][0]
             if (now_pipeline
-                    in self.pipe_manager.model_level_info['customized_tuners']
-                    and tuner_m in self.pipe_manager.
-                    model_level_info['customized_tuners'][now_pipeline]):
-                tuner_m = self.pipe_manager.model_level_info[
-                    'customized_tuners'][now_pipeline][tuner_m]['model_info']
-                used_custom_tuner_model.append(tuner_m)
-
-        if (now_pipeline in self.pipe_manager.model_level_info['controllers']
-                and control_model in self.pipe_manager.
-                model_level_info['controllers'][now_pipeline]):
-            control_model = self.pipe_manager.model_level_info['controllers'][
-                now_pipeline][control_model]['model_info']
-
-        prompt_rephrased = style_template.replace(
-            '{prompt}',
-            prompt) if not style_template == '' and mantra_state else prompt
-        prompt_rephrased = f'{prompt_prefix}{prompt_rephrased}' if not prompt_prefix == '' else prompt_rephrased
-        negative_prompt_rephrased = negative_prompt + style_negative_template if mantra_state else negative_prompt
-        pipeline_input = {
-            'prompt': prompt_rephrased,
-            'negative_prompt': negative_prompt_rephrased,
-            'sample': sample,
-            'sample_steps': sample_steps,
-            'discretization': discretization,
-            'original_size_as_tuple': [int(output_height),
-                                       int(output_width)],
-            'target_size_as_tuple': [int(output_height),
-                                     int(output_width)],
-            'crop_coords_top_left': [0, 0],
-            'guide_scale': guide_scale,
-            'guide_rescale': guide_rescale,
-        }
-        if refine_state:
-            pipeline_input['refine_sampler'] = refine_sampler
-            pipeline_input['refine_discretization'] = refine_discretization
-            pipeline_input['refine_guide_scale'] = refine_guide_scale
-            pipeline_input['refine_guide_rescale'] = refine_guide_rescale
-        else:
-            refine_strength = 0
-        if largen_state:
-            largen_cfg = {
-                'largen_task': largen_task,
-                'largen_image_scale': largen_image_scale,
-                'largen_tar_image': largen_tar_image,
-                'largen_tar_mask': largen_tar_mask,
-                'largen_ref_image': largen_ref_image,
-                'largen_ref_mask': largen_ref_mask,
-                'largen_masked_image': largen_masked_image,
-                'largen_ref_clip': largen_ref_clip,
-                'largen_base_image': largen_base_image,
-                'largen_extra_sizes': largen_extra_sizes,
-                'largen_bbox_yyxx': largen_bbox_yyxx,
+                    in self.pipe_manager.model_level_info['controllers']
+                    and control_model in self.pipe_manager.
+                    model_level_info['controllers'][now_pipeline]):
+                control_model = self.pipe_manager.model_level_info[
+                    'controllers'][now_pipeline][control_model]['model_info']
+
+            args.update({
+                'control_model':
+                control_model if control_state else None,
+                'control_scale':
+                args.pop('control_scale')
+                if tuner_state or control_state else None,
+                'control_cond_image':
+                control_cond_image if control_state else None,
+                'crop_type':
+                args.pop('crop_type') if control_state else None
+            })
+
+        def tuner_init(args):
+            control_state, tuner_state = args['control_state'], args[
+                'tuner_state']
+            tuner_model = args.pop('tuner_model')
+            custom_tuner_model = args.pop('custom_tuner_model')
+
+            now_pipeline = self.pipe_manager.model_level_info[
+                args['diffusion_model']]['pipeline'][0]
+            used_tuner_model = []
+            if not isinstance(tuner_model, list):
+                tuner_model = [tuner_model]
+            for tuner_m in tuner_model:
+                if tuner_m is None or tuner_m == '':
+                    continue
+                if now_pipeline in self.pipe_manager.model_level_info['tuners'] and \
+                   tuner_m in self.pipe_manager.model_level_info['tuners'][now_pipeline]:
+                    tuner_m = self.pipe_manager.model_level_info['tuners'][
+                        now_pipeline][tuner_m]['model_info']
+                    used_tuner_model.append(tuner_m)
+            used_custom_tuner_model = []
+            if not isinstance(custom_tuner_model, list):
+                custom_tuner_model = [custom_tuner_model]
+            for tuner_m in custom_tuner_model:
+                if tuner_m is None or tuner_m == '':
+                    continue
+                if (now_pipeline in
+                        self.pipe_manager.model_level_info['customized_tuners']
+                        and tuner_m in self.pipe_manager.
+                        model_level_info['customized_tuners'][now_pipeline]):
+                    tuner_m = self.pipe_manager.model_level_info[
+                        'customized_tuners'][now_pipeline][tuner_m][
+                            'model_info']
+                    used_custom_tuner_model.append(tuner_m)
+            args.update({
+                'tuner_model':
+                used_tuner_model +
+                used_custom_tuner_model if tuner_state else None,
+                'tuner_scale':
+                args.pop('tuner_scale')
+                if tuner_state or control_state else None,
+            })
+
+        def input_init(args):
+            mantra_state = args['mantra_state']
+            prompt = args.pop('prompt')
+            negative_prompt = args.pop('negative_prompt')
+            prompt_prefix = args.pop('prompt_prefix')
+            style_template = args.pop('style_template')
+            size = [
+                int(args.pop('output_height')),
+                int(args.pop('output_width'))
+            ]
+
+            prompt_rephrased = style_template.replace(
+                '{prompt}', prompt
+            ) if not style_template == '' and mantra_state else prompt
+
+            pipeline_input = {
+                'prompt':
+                f'{prompt_prefix}{prompt_rephrased}'
+                if not prompt_prefix == '' else prompt_rephrased,
+                'negative_prompt':
+                negative_prompt + args.pop('style_negative_template')
+                if mantra_state else negative_prompt,
+                'sample':
+                args.pop('sample'),
+                'sample_steps':
+                args.pop('sample_steps'),
+                'discretization':
+                args.pop('discretization'),
+                'original_size_as_tuple':
+                size,
+                'target_size_as_tuple':
+                size,
+                'crop_coords_top_left': [0, 0],
+                'guide_scale':
+                args.pop('guide_scale'),
+                'guide_rescale':
+                args.pop('guide_rescale')
             }
-        else:
-            largen_cfg = {}
+            args.update({'input': pipeline_input})
+
+        def appedix_init(args):
+            args.update({
+                'num_samples': args.pop('image_number'),
+                'intermediate_callback': None,
+                'img_to_img_strength': 0,
+                'seed': int(args.pop('image_seed')),
+            })
+
+        def load_init(args):
+            cur_pipe_name = self.pipe_manager.model_level_info[
+                args['diffusion_model']]['pipeline'][0]
+            for sub_name, sub_pipe in self.pipe_manager.pipeline_level_modules.items(
+            ):
+                if sub_name == cur_pipe_name:
+                    continue
+                if len(sub_pipe.loaded_model) > 0:
+                    sub_pipe.dynamic_unload(name='all')
+                    print(f'Unloading {sub_name} modules')
+
+        args = dict(zip(self.component_mapping.keys(), args))
+        largen_history = args.pop('largen_history')
+
+        current_pipeline = pipeline_init(args)
+        control_init(args)
+        tuner_init(args)
+        input_init(args)
+        appedix_init(args)
+        load_init(args)
+        results = current_pipeline(**args)
 
-        results = current_pipeline(
-            pipeline_input,
-            num_samples=image_number,
-            intermediate_callback=None,
-            refine_strength=refine_strength,
-            img_to_img_strength=0,
-            tuner_model=used_tuner_model +
-            used_custom_tuner_model if tuner_state else None,
-            tuner_scale=tuner_scale if tuner_state or control_state else None,
-            control_model=control_model if control_state else None,
-            control_scale=control_scale
-            if tuner_state or control_state else None,
-            control_cond_image=control_cond_image if control_state else None,
-            crop_type=crop_type if control_state else None,
-            seed=int(image_seed),
-            **largen_cfg)
         images = []
         before_images = []
         if 'images' in results:
             images_tensor = results['images'] * 255
             images = [
                 Image.fromarray(images_tensor[idx].permute(
                     1, 2, 0).cpu().numpy().astype(np.uint8))
@@ -229,17 +236,20 @@
                 Image.fromarray(before_refine_images_tensor[idx].permute(
                     1, 2, 0).cpu().numpy().astype(np.uint8))
                 for idx in range(before_refine_images_tensor.shape[0])
             ]
         if 'seed' in results:
             print(results['seed'])
         print(images, before_images)
-        largen_history.extend(images)
-        if len(largen_history) > 10:
-            largen_history = largen_history[-10:]
+
+        if args['largen_state']:
+            largen_history.extend(images)
+            if len(largen_history) > 5:
+                largen_history = largen_history[-5:]
+
         if show_jpeg_image:
             save_list = []
             for i, img in enumerate(images):
                 save_image = os.path.join(self.local_work_dir,
                                           f'cur_gallery_{i}.jpg')
                 img.save(save_image)
                 save_list.append(save_image)
@@ -254,43 +264,28 @@
         )
 
     def generate_image(self, *args, **kwargs):
         gallery_result = self.generate_gallery(*args, **kwargs)
         before_refine_panel, before_refine_gallery, output_gallery, _ = gallery_result
         return (before_refine_panel, before_refine_gallery, output_gallery[0])
 
-    def set_callbacks(self, inference_ui, model_manage_ui, diffusion_ui,
-                      mantra_ui, tuner_ui, refiner_ui, control_ui, largen_ui,
+    def set_callbacks(self,
+                      inference_ui,
+                      model_manage_ui,
+                      diffusion_ui,
+                      mantra_ui,
+                      tuner_ui,
+                      refiner_ui,
+                      control_ui,
+                      largen_ui,
+                      manager=None,
                       **kwargs):
-
-        self.gen_inputs = [
-            self.prompt, mantra_ui.state, tuner_ui.state, control_ui.state,
-            refiner_ui.state, model_manage_ui.diffusion_model,
-            model_manage_ui.first_stage_model,
-            model_manage_ui.cond_stage_model, refiner_ui.refiner_cond_model,
-            refiner_ui.refiner_diffusion_model, tuner_ui.tuner_model,
-            tuner_ui.tuner_scale, tuner_ui.custom_tuner_model,
-            control_ui.control_model, control_ui.control_scale,
-            control_ui.crop_type, control_ui.cond_image,
-            diffusion_ui.negative_prompt, diffusion_ui.prompt_prefix,
-            diffusion_ui.sampler, diffusion_ui.discretization,
-            diffusion_ui.output_height, diffusion_ui.output_width,
-            diffusion_ui.image_number, diffusion_ui.sample_steps,
-            diffusion_ui.guide_scale, diffusion_ui.guide_rescale,
-            refiner_ui.refine_strength, refiner_ui.refine_sampler,
-            refiner_ui.refine_discretization, refiner_ui.refine_guide_scale,
-            refiner_ui.refine_guide_rescale, mantra_ui.style_template,
-            mantra_ui.style_negative_template, diffusion_ui.image_seed,
-            largen_ui.state, largen_ui.task, largen_ui.image_scale,
-            largen_ui.tar_image, largen_ui.tar_mask, largen_ui.masked_image,
-            largen_ui.ref_image, largen_ui.ref_mask, largen_ui.ref_clip,
-            largen_ui.base_image, largen_ui.extra_sizes, largen_ui.bbox_yyxx,
-            largen_ui.image_history
-        ]
-
+        self.manager = manager
+        self.gen_inputs = list(self.component_mapping.values())
+        print(self.gen_inputs, len(self.gen_inputs))
         self.gen_outputs = [
             self.before_refine_panel,
             self.before_refine_gallery,
             self.output_gallery,
             largen_ui.image_history,
             largen_ui.gallery,
         ]
```

## scepter/studio/inference/inference_ui/largen_ui.py

```diff
@@ -1,19 +1,19 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-import albumentations as A
 import cv2
 import gradio as gr
 import numpy as np
 import torch
 import torchvision.transforms as T
 import torchvision.transforms.functional as TF
 from PIL import Image
 
+import albumentations as A
 from scepter.modules.annotator.registry import ANNOTATORS
 from scepter.modules.model.utils.data_utils import (box2squre, expand_bbox,
                                                     get_bbox_from_mask,
                                                     pad_to_square)
 from scepter.modules.utils.distribute import we
 from scepter.studio.inference.inference_ui.component_names import LargenUIName
 from scepter.studio.utils.uibase import UIBase
@@ -57,175 +57,207 @@
         self.extra_sizes = gr.State(value=None)
         self.bbox_yyxx = gr.State(value=None)
         self.image_history = gr.State(value=[])
 
         with gr.Column(visible=False) as self.tab:
             with gr.Row():
                 self.select_app = gr.Dropdown(
-                        label=self.component_names.dropdown_name,
-                        choices=self.component_names.apps,
-                        value=self.component_names.apps[0],
-                        type='index')
+                    label=self.component_names.dropdown_name,
+                    choices=self.component_names.apps,
+                    value=self.component_names.apps[0],
+                    type='index')
             with gr.Row(equal_height=True):
                 with gr.Column(scale=2, min_width=0):
                     with gr.Row():
                         with gr.Column(scale=1, min_width=0):
                             self.scene_image = gr.Image(
                                 label=self.component_names.scene_image,
                                 type='pil',
                                 tool='sketch',
                                 source='upload',
                                 height=400,
                                 interactive=True)
-                            self.cache_button = gr.Button(value='Use Last Generated Image', visible=True)
+                            self.cache_button = gr.Button(
+                                value='Use Last Generated Image', visible=True)
                         with gr.Column(scale=1, min_width=0):
                             self.subject_image = gr.Image(
                                 label=self.component_names.subject_image,
                                 type='pil',
                                 tool='sketch',
                                 source='upload',
                                 interactive=True,
                                 height=400,
                                 visible=False)
 
-                    self.gallery = gr.Gallery(label='Image History', value=[], columns=1, rows=1, height=500)
-                    self.clear_button = gr.Button(value='Clear History', visible=True)
+                    self.gallery = gr.Gallery(label='Image History',
+                                              value=[],
+                                              columns=1,
+                                              rows=1,
+                                              height=500)
+                    self.clear_button = gr.Button(value='Clear History',
+                                                  visible=True)
 
                 with gr.Column(scale=1, min_width=0):
                     self.image_scale = gr.Slider(label='Image Strength',
                                                  minimum=0.0,
                                                  maximum=1.0,
                                                  value=1.0,
                                                  visible=False)
                     self.image_ratio = gr.Slider(minimum=0.5,
                                                  maximum=1.0,
                                                  value=0.75,
                                                  label='Image Resize Ratio',
                                                  visible=True)
-                    self.out_direction = gr.Dropdown(label=self.component_names.out_direction_label,
-                                                     choices=self.component_names.out_directions,
-                                                     value=self.component_names.out_directions[0],
-                                                     visible=True)
+                    self.out_direction = gr.Dropdown(
+                        label=self.component_names.out_direction_label,
+                        choices=self.component_names.out_directions,
+                        value=self.component_names.out_directions[0],
+                        visible=True)
 
-                    self.proc_button = gr.Button(value=self.component_names.button_name)
+                    self.proc_button = gr.Button(
+                        value=self.component_names.button_name)
                     self.proc_status = gr.Markdown(value='', visible=False)
-                    self.task_desc = gr.Markdown(self.component_names.direction, visible=True)
+                    self.task_desc = gr.Markdown(
+                        self.component_names.direction, visible=True)
 
                     self.eg = gr.Column(visible=True)
+        gallery_ui = kwargs.pop('gallery_ui', None)
+        gallery_ui.register_components({
+            'largen_state': self.state,
+            'largen_task': self.task,
+            'largen_image_scale': self.image_scale,
+            'largen_tar_image': self.tar_image,
+            'largen_tar_mask': self.tar_mask,
+            'largen_masked_image': self.masked_image,
+            'largen_ref_image': self.ref_image,
+            'largen_ref_mask': self.ref_mask,
+            'largen_ref_clip': self.ref_clip,
+            'largen_base_image': self.base_image,
+            'largen_extra_sizes': self.extra_sizes,
+            'largen_bbox_yyxx': self.bbox_yyxx,
+            'largen_history': self.image_history
+        })
 
     def set_callbacks(self, model_manage_ui, diffusion_ui, **kwargs):
-
-        def example_data_process(select_app_id, prompt, scene_image, scene_mask, subject_image,
-                                 subject_mask, image_scale, image_ratio, out_direction,
+        def example_data_process(select_app_id, prompt, scene_image,
+                                 scene_mask, subject_image, subject_mask,
+                                 image_scale, image_ratio, out_direction,
                                  output_height, output_width):
             task = self.component_names.tasks[select_app_id]
             if scene_mask is not None:
                 scene_mask = (scene_mask > 128).astype(np.uint8)
             if subject_mask is not None:
                 subject_mask = (subject_mask > 128).astype(np.uint8)
 
             if task == 'Text_Guided_Inpainting':
-                data = self.data_preprocess_inpaint(scene_image,
-                                                    scene_mask,
-                                                    None,
-                                                    None,
-                                                    False,
-                                                    1.3,
-                                                    output_height, output_width)
+                data = self.data_preprocess_inpaint(scene_image, scene_mask,
+                                                    None, None, False, 1.3,
+                                                    output_height,
+                                                    output_width)
             elif task == 'Subject_Guided_Inpainting':
-                data = self.data_preprocess_inpaint(scene_image,
-                                                    scene_mask,
+                data = self.data_preprocess_inpaint(scene_image, scene_mask,
                                                     subject_image,
-                                                    subject_mask,
-                                                    False,
-                                                    1.3,
+                                                    subject_mask, False, 1.3,
                                                     output_height,
                                                     output_width)
             elif task == 'Text_Subject_Guided_Inpainting':
-                data = self.data_preprocess_inpaint(scene_image,
-                                                    scene_mask,
+                data = self.data_preprocess_inpaint(scene_image, scene_mask,
                                                     subject_image,
-                                                    subject_mask,
-                                                    True,
-                                                    1.3,
+                                                    subject_mask, True, 1.3,
                                                     output_height,
                                                     output_width)
             elif task == 'Text_Guided_Outpainting':
                 data = self.data_preprocess_outpaint(scene_image,
                                                      out_direction,
                                                      image_ratio,
                                                      output_height,
                                                      output_width)
 
-            subject_image_show = None if subject_image is None else Image.fromarray(subject_image.astype(np.uint8))
+            subject_image_show = None if subject_image is None else Image.fromarray(
+                subject_image.astype(np.uint8))
             return *data, gr.update(value='Data Process Succeed!', visible=True), \
                 gr.update(value=Image.fromarray(scene_image.astype(np.uint8))), \
                 gr.update(value=subject_image_show), \
                 task, gr.update(value=self.component_names.apps[select_app_id]), \
                 gr.update(value=prompt), gr.update(value=image_scale), gr.update(value=image_ratio)
 
         gallery_ui = kwargs.pop('gallery_ui')
         with self.eg:
-            self.scene_image_eg = gr.Image(label=self.component_names.scene_image, type='numpy', visible=False) # noqa
-            self.scene_mask_eg = gr.Image(label=self.component_names.scene_mask, type='numpy', image_mode='L', visible=False) # noqa
-            self.subject_image_eg = gr.Image(label=self.component_names.subject_image, type='numpy', visible=False) # noqa
-            self.subject_mask_eg = gr.Image(label=self.component_names.subject_mask, type='numpy', image_mode='L', visible=False) # noqa
-            self.prompt = gr.Textbox(label=self.component_names.prompt, visible=False)
-            self.examples = gr.Examples(
-                examples=self.component_names.examples,
-                inputs=[self.select_app, self.prompt,
-                        self.scene_image_eg, self.scene_mask_eg,
-                        self.subject_image_eg, self.subject_mask_eg,
-                        self.image_scale,
-                        self.image_ratio,
-                        self.out_direction,
-                        diffusion_ui.output_height,
-                        diffusion_ui.output_width,
-                        ],
-                outputs=[self.tar_image,
-                         self.tar_mask,
-                         self.masked_image,
-                         self.ref_image,
-                         self.ref_mask,
-                         self.ref_clip,
-                         self.base_image,
-                         self.extra_sizes,
-                         self.bbox_yyxx,
-                         self.proc_status,
-                         self.scene_image,
-                         self.subject_image,
-                         self.task,
-                         self.select_app,
-                         gallery_ui.prompt,
-                         self.image_scale,
-                         self.image_ratio,
-                         ],
-                fn=example_data_process,
-                cache_examples=False,
-                run_on_click=True)
+            self.scene_image_eg = gr.Image(
+                label=self.component_names.scene_image,
+                type='numpy',
+                visible=False)  # noqa
+            self.scene_mask_eg = gr.Image(
+                label=self.component_names.scene_mask,
+                type='numpy',
+                image_mode='L',
+                visible=False)  # noqa
+            self.subject_image_eg = gr.Image(
+                label=self.component_names.subject_image,
+                type='numpy',
+                visible=False)  # noqa
+            self.subject_mask_eg = gr.Image(
+                label=self.component_names.subject_mask,
+                type='numpy',
+                image_mode='L',
+                visible=False)  # noqa
+            self.prompt = gr.Textbox(label=self.component_names.prompt,
+                                     visible=False)
+            self.examples = gr.Examples(examples=self.component_names.examples,
+                                        inputs=[
+                                            self.select_app,
+                                            self.prompt,
+                                            self.scene_image_eg,
+                                            self.scene_mask_eg,
+                                            self.subject_image_eg,
+                                            self.subject_mask_eg,
+                                            self.image_scale,
+                                            self.image_ratio,
+                                            self.out_direction,
+                                            diffusion_ui.output_height,
+                                            diffusion_ui.output_width,
+                                        ],
+                                        outputs=[
+                                            self.tar_image,
+                                            self.tar_mask,
+                                            self.masked_image,
+                                            self.ref_image,
+                                            self.ref_mask,
+                                            self.ref_clip,
+                                            self.base_image,
+                                            self.extra_sizes,
+                                            self.bbox_yyxx,
+                                            self.proc_status,
+                                            self.scene_image,
+                                            self.subject_image,
+                                            self.task,
+                                            self.select_app,
+                                            gallery_ui.prompt,
+                                            self.image_scale,
+                                            self.image_ratio,
+                                        ],
+                                        fn=example_data_process,
+                                        cache_examples=False,
+                                        run_on_click=True)
 
         def change_app(select_app_id):
             select_task = self.component_names.tasks[select_app_id]
             return gr.update(visible=('Subject' in select_task)), \
                 gr.update(visible=('Subject' in select_task)), \
                 gr.update(visible=('Outpainting' in select_task)), \
                 gr.update(visible=('Outpainting' in select_task)), select_task
 
-        self.select_app.change(
-            change_app,
-            inputs=[self.select_app],
-            outputs=[
-                self.subject_image,
-                self.image_scale,
-                self.image_ratio,
-                self.out_direction,
-                self.task
-            ],
-            queue=False
-        )
+        self.select_app.change(change_app,
+                               inputs=[self.select_app],
+                               outputs=[
+                                   self.subject_image, self.image_scale,
+                                   self.image_ratio, self.out_direction,
+                                   self.task
+                               ],
+                               queue=False)
 
         def read_gallery_image(gallery):
             if len(gallery) == 0:
                 last_image = None
             else:
                 last_image = gallery[-1]['name']
             return gr.update(value=last_image)
@@ -235,79 +267,64 @@
                                 outputs=[self.scene_image])
 
         def clear_gallery(image_history, gallery):
             image_history.clear()
             gallery.clear()
             return image_history, gallery
 
-        self.clear_button.click(
-            fn=clear_gallery,
-            inputs=[self.image_history, self.gallery],
-            outputs=[self.image_history, self.gallery]
-        )
+        self.clear_button.click(fn=clear_gallery,
+                                inputs=[self.image_history, self.gallery],
+                                outputs=[self.image_history, self.gallery])
 
-        def data_process(scene_image, subject_image, task, image_ratio, out_direction, output_height, output_width):
+        def data_process(scene_image, subject_image, task, image_ratio,
+                         out_direction, output_height, output_width):
             tar_image = scene_image['image'].convert('RGB')
             tar_mask = scene_image['mask'].convert('L')
             tar_image = np.asarray(tar_image)
             tar_mask = np.asarray(tar_mask)
             tar_mask = np.where(tar_mask > 128, 1, 0).astype(np.uint8)
 
             if task == 'Text_Guided_Inpainting':
-                data = self.data_preprocess_inpaint(tar_image,
-                                                    tar_mask,
-                                                    None,
-                                                    None,
-                                                    False,
-                                                    1.3,
+                data = self.data_preprocess_inpaint(tar_image, tar_mask, None,
+                                                    None, False, 1.3,
                                                     output_height,
                                                     output_width)
             elif task == 'Subject_Guided_Inpainting':
                 ref_image = subject_image['image'].convert('RGB')
                 ref_mask = subject_image['mask'].convert('L')
                 ref_image = np.asarray(ref_image)
                 ref_mask = np.asarray(ref_mask)
                 ref_mask = np.where(ref_mask > 128, 1, 0).astype(np.uint8)
-                data = self.data_preprocess_inpaint(tar_image,
-                                                    tar_mask,
-                                                    ref_image,
-                                                    ref_mask,
-                                                    False,
-                                                    1.3,
-                                                    output_height,
+                data = self.data_preprocess_inpaint(tar_image, tar_mask,
+                                                    ref_image, ref_mask, False,
+                                                    1.3, output_height,
                                                     output_width)
             elif task == 'Text_Subject_Guided_Inpainting':
                 ref_image = subject_image['image'].convert('RGB')
                 ref_mask = subject_image['mask'].convert('L')
                 ref_image = np.asarray(ref_image)
                 ref_mask = np.asarray(ref_mask)
                 ref_mask = np.where(ref_mask > 128, 1, 0).astype(np.uint8)
-                data = self.data_preprocess_inpaint(tar_image,
-                                                    tar_mask,
-                                                    ref_image,
-                                                    ref_mask,
-                                                    True,
-                                                    1.3,
-                                                    output_height,
+                data = self.data_preprocess_inpaint(tar_image, tar_mask,
+                                                    ref_image, ref_mask, True,
+                                                    1.3, output_height,
                                                     output_width)
             elif task == 'Text_Guided_Outpainting':
-                data = self.data_preprocess_outpaint(tar_image,
-                                                     out_direction,
+                data = self.data_preprocess_outpaint(tar_image, out_direction,
                                                      image_ratio,
                                                      output_height,
                                                      output_width)
 
-            return *data, gr.update(value='Data Process Succeed!', visible=True)
+            return *data, gr.update(value='Data Process Succeed!',
+                                    visible=True)
 
         self.proc_button.click(data_process,
                                inputs=[
-                                   self.scene_image,
-                                   self.subject_image,
-                                   self.task,
-                                   self.image_ratio,
+                                   self.scene_image, self.subject_image,
+                                   self.task, self.image_ratio,
                                    self.out_direction,
                                    diffusion_ui.output_height,
                                    diffusion_ui.output_width
                                ],
                                outputs=[
                                    self.tar_image,
                                    self.tar_mask,
@@ -315,25 +332,19 @@
                                    self.ref_image,
                                    self.ref_mask,
                                    self.ref_clip,
                                    self.base_image,
                                    self.extra_sizes,
                                    self.bbox_yyxx,
                                    self.proc_status,
-                                ])
+                               ])
 
-    def data_preprocess_inpaint(self,
-                                tar_image,
-                                tar_mask,
-                                ref_image,
-                                ref_mask,
-                                use_rectangle_mask,
-                                tar_crop_ratio,
-                                output_height,
-                                output_width):
+    def data_preprocess_inpaint(self, tar_image, tar_mask, ref_image, ref_mask,
+                                use_rectangle_mask, tar_crop_ratio,
+                                output_height, output_width):
         tar_mask = np.expand_dims(tar_mask, 2).astype(np.float32)
 
         # Zoom-In
         tar_yyxx = get_bbox_from_mask(tar_mask)
         tar_yyxx_crop = expand_bbox(tar_mask, tar_yyxx, ratio=tar_crop_ratio)
         tar_yyxx_crop = box2squre(tar_mask, tar_yyxx_crop)
         y1, y2, x1, x2 = tar_yyxx_crop
@@ -342,119 +353,131 @@
         H1, W1 = crop_tar_image.shape[:2]
 
         if use_rectangle_mask:
             tar_bbox_yyxx = get_bbox_from_mask(crop_tar_mask)
             y1, y2, x1, x2 = tar_bbox_yyxx
             crop_tar_mask[y1:y2, x1:x2] = 1
 
-        crop_tar_image, pad1, pad2 = pad_to_square(crop_tar_image.astype(np.uint8), pad_value=0)
+        crop_tar_image, pad1, pad2 = pad_to_square(crop_tar_image.astype(
+            np.uint8),
+                                                   pad_value=0)
         crop_tar_mask, _, _ = pad_to_square(crop_tar_mask, pad_value=0)
         H2, W2 = crop_tar_image.shape[:2]
 
-        aug_tar_image = cv2.resize(crop_tar_image.astype(np.uint8), (output_width, output_height))
+        aug_tar_image = cv2.resize(crop_tar_image.astype(np.uint8),
+                                   (output_width, output_height))
         aug_tar_mask = cv2.resize(crop_tar_mask, (output_width, output_height))
 
         final_tar_image = TF.to_tensor(aug_tar_image)
-        final_tar_image = TF.normalize(final_tar_image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
+        final_tar_image = TF.normalize(final_tar_image,
+                                       mean=[0.5, 0.5, 0.5],
+                                       std=[0.5, 0.5, 0.5])
         final_tar_mask = TF.to_tensor((aug_tar_mask > 0.5).astype(np.float32))
 
         masked_image = final_tar_image.clone()
         masked_image = masked_image * (1 - final_tar_mask)
 
         final_tar_image = final_tar_image.unsqueeze(0)
         final_tar_mask = final_tar_mask.unsqueeze(0)
         masked_image = masked_image.unsqueeze(0)
 
         if ref_image is not None and ref_mask is not None:
             ref_mask = np.expand_dims(ref_mask, 2).astype(np.float32)
             # background-free
-            ref_image = ref_image * ref_mask + np.ones_like(ref_image) * 255. * (1 - ref_mask)
+            ref_image = ref_image * ref_mask + np.ones_like(
+                ref_image) * 255. * (1 - ref_mask)
 
             ref_yyxx = get_bbox_from_mask(ref_mask)
             y1, y2, x1, x2 = ref_yyxx
 
             crop_ref_image_i = ref_image[y1:y2, x1:x2, :]
             crop_ref_mask_i = ref_mask[y1:y2, x1:x2, :]
 
             h, w = crop_ref_mask_i.shape[:2]
             ref_expand_size = int(max(h, w) * 1.02)
-            pad_op = A.PadIfNeeded(ref_expand_size, ref_expand_size,
+            pad_op = A.PadIfNeeded(ref_expand_size,
+                                   ref_expand_size,
                                    border_mode=cv2.BORDER_CONSTANT,
-                                   value=(255, 255, 255), mask_value=0)
+                                   value=(255, 255, 255),
+                                   mask_value=0)
             out = pad_op(image=crop_ref_image_i, mask=crop_ref_mask_i)
             crop_ref_image = out['image']
 
             to_clip_input = T.Compose([
                 T.ToTensor(),
                 T.Resize((224, 224)),
-                T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),
+                T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),
+                            std=(0.26862954, 0.26130258, 0.27577711)),
             ])
             ref_clip = to_clip_input(crop_ref_image.astype(np.uint8))
 
             output_size = max(output_height, output_width)
             ref_resize_op = A.Compose([
                 A.LongestMaxSize(output_size),
-                A.PadIfNeeded(output_size, output_size,
+                A.PadIfNeeded(output_size,
+                              output_size,
                               border_mode=cv2.BORDER_CONSTANT,
-                              value=(255, 255, 255), mask_value=0),
+                              value=(255, 255, 255),
+                              mask_value=0),
             ])
-            aug_out = ref_resize_op(image=crop_ref_image_i.astype(np.uint8), mask=crop_ref_mask_i)
+            aug_out = ref_resize_op(image=crop_ref_image_i.astype(np.uint8),
+                                    mask=crop_ref_mask_i)
             aug_ref_image = aug_out['image']
             aug_ref_mask = aug_out['mask']
 
             final_ref_image = TF.to_tensor(aug_ref_image)
-            final_ref_image = TF.normalize(final_ref_image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
+            final_ref_image = TF.normalize(final_ref_image,
+                                           mean=[0.5, 0.5, 0.5],
+                                           std=[0.5, 0.5, 0.5])
             final_ref_mask = TF.to_tensor(aug_ref_mask)
 
             final_ref_image = final_ref_image.unsqueeze(0)
             final_ref_mask = final_ref_mask.unsqueeze(0)
             ref_clip = ref_clip.unsqueeze(0)
         else:
             final_ref_image = None
             final_ref_mask = None
             ref_clip = None
 
         return final_tar_image, final_tar_mask, masked_image, final_ref_image, final_ref_mask, ref_clip, \
             TF.to_tensor(tar_image), torch.LongTensor([H1, W1, H2, W2, pad1, pad2]), torch.LongTensor(tar_yyxx_crop)
 
-    def data_preprocess_outpaint(self,
-                                 tar_image,
-                                 direction,
-                                 img_ratio,
-                                 output_height,
-                                 output_width):
+    def data_preprocess_outpaint(self, tar_image, direction, img_ratio,
+                                 output_height, output_width):
         oh, ow = output_height, output_width
         h, w = tar_image.shape[:2]
-        ratio = max(h/(oh*img_ratio), w/(ow*img_ratio))
+        ratio = max(h / (oh * img_ratio), w / (ow * img_ratio))
 
         ih, iw = int(h / ratio), int(w / ratio)
 
         masked_image = np.zeros((oh, ow, 3), dtype=np.uint8)
         mask = np.zeros((oh, ow, 1))
 
         if direction in ['CenterAround', '中心向外']:
-            y1, x1 = (oh-ih)//2, (ow-iw)//2
+            y1, x1 = (oh - ih) // 2, (ow - iw) // 2
         elif direction in ['RightDown', '右下']:
             y1, x1 = 0, 0
         elif direction in ['LeftDown', '左下']:
-            y1, x1 = 0, ow-iw
+            y1, x1 = 0, ow - iw
         elif direction in ['RightUp', '右上']:
-            y1, x1 = oh-ih, 0
+            y1, x1 = oh - ih, 0
         elif direction in ['LeftUp', '左上']:
-            y1, x1 = oh-ih, ow-iw
+            y1, x1 = oh - ih, ow - iw
         else:
             y1, x1 = 0, 0
 
         tar_image = cv2.resize(tar_image.astype(np.uint8), (iw, ih))
-        masked_image[y1:y1+ih, x1:x1+iw] = tar_image
-        mask[y1+5:y1+ih-5, x1+5:x1+iw-5] = 1
+        masked_image[y1:y1 + ih, x1:x1 + iw] = tar_image
+        mask[y1 + 5:y1 + ih - 5, x1 + 5:x1 + iw - 5] = 1
 
         final_tar_image = TF.to_tensor(masked_image.astype(np.uint8))
-        final_tar_image = TF.normalize(final_tar_image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
-        final_tar_mask = TF.to_tensor(((1.0-mask) > 0.5).astype(np.float32))
+        final_tar_image = TF.normalize(final_tar_image,
+                                       mean=[0.5, 0.5, 0.5],
+                                       std=[0.5, 0.5, 0.5])
+        final_tar_mask = TF.to_tensor(((1.0 - mask) > 0.5).astype(np.float32))
 
         masked_image = final_tar_image.clone()
         masked_image = masked_image * (1 - final_tar_mask)
 
         final_tar_image = final_tar_image.unsqueeze(0)
         final_tar_mask = final_tar_mask.unsqueeze(0)
         masked_image = masked_image.unsqueeze(0)
```

## scepter/studio/inference/inference_ui/mantra_ui.py

```diff
@@ -107,14 +107,24 @@
                                 value='',
                                 label=self.component_names.
                                 style_example_prompt,
                                 lines=2)
             self.example_block = gr.Accordion(
                 label=self.component_names.example_block_name, open=True)
 
+        gallery_ui = kwargs.pop('gallery_ui', None)
+        gallery_ui.register_components({
+            'mantra_state':
+            self.state,
+            'style_template':
+            self.style_template,
+            'style_negative_template':
+            self.style_negative_template,
+        })
+
     def set_callbacks(self, model_manage_ui, **kwargs):
         gallery_ui = kwargs.pop('gallery_ui')
         with self.example_block:
             gr.Examples(examples=self.component_names.examples,
                         inputs=[self.style, gallery_ui.prompt])
 
         def change_style(style, diffusion_model):
```

## scepter/studio/inference/inference_ui/model_manage_ui.py

```diff
@@ -97,14 +97,23 @@
             #                     label='tuner_name')
             #         with gr.Row(variant='panel') as self.tuner_info:
             #             with gr.Accordion(label=self.component_names.
             #                               postprocess_model_name,
             #                               open=False):
             #                 self.tuner_name = gr.Text(
             #                     label='tuner_name')
+        gallery_ui = kwargs.pop('gallery_ui', None)
+        gallery_ui.register_components({
+            'diffusion_model':
+            self.diffusion_model,
+            'first_stage_model':
+            self.first_stage_model,
+            'cond_stage_model':
+            self.cond_stage_model,
+        })
 
     def set_callbacks(self, diffusion_ui, tuner_ui, control_ui, mantra_ui,
                       **kwargs):
         # def select_refine_tuner(all_select, evt: gr.SelectData):
         #     if 'Refiners' in all_select:
         #         refine_panel = gr.Row(visible=True)
         #         refine_tab = gr.Group(visible=True)
```

## scepter/studio/inference/inference_ui/tuner_ui.py

```diff
@@ -114,14 +114,25 @@
                     maximum=1.0,
                     step=0.05,
                     value=1.0,
                     interactive=True)
 
             self.example_block = gr.Accordion(
                 label=self.component_names.example_block_name, open=True)
+        gallery_ui = kwargs.pop('gallery_ui', None)
+        gallery_ui.register_components({
+            'tuner_state':
+            self.state,
+            'tuner_model':
+            self.tuner_model,
+            'tuner_scale':
+            self.tuner_scale,
+            'custom_tuner_model':
+            self.custom_tuner_model,
+        })
 
     def set_callbacks(self, model_manage_ui, **kwargs):
         manager = kwargs.pop('manager')
         gallery_ui = kwargs.pop('gallery_ui')
         with self.example_block:
             gr.Examples(examples=self.component_names.examples,
                         inputs=[self.tuner_model, gallery_ui.prompt])
```

## scepter/studio/preprocess/preprocess.py

```diff
@@ -28,30 +28,34 @@
         if not FS.exists(cfg_general.WORK_DIR):
             FS.make_dir(cfg_general.WORK_DIR)
 
         cfg_general = init_env(cfg_general)
         self.create_dataset = CreateDatasetUI.get_instance(cfg_general,
                                                            is_debug=is_debug,
                                                            language=language)
-        self.dataset_gallery = DatasetGalleryUI.get_instance(cfg_general,
-                                                             is_debug=is_debug,
-                                                             language=language)
-        self.export_dataset = ExportDatasetUI.get_instance(cfg_general,
-                                                           is_debug=is_debug,
-                                                           language=language)
+        self.dataset_gallery = DatasetGalleryUI.get_instance(
+            cfg_general,
+            is_debug=is_debug,
+            language=language,
+            create_ins=self.create_dataset)
+        self.export_dataset = ExportDatasetUI.get_instance(
+            cfg_general,
+            is_debug=is_debug,
+            language=language,
+            gallery_ins=self.dataset_gallery)
 
     def create_ui(self):
         self.create_dataset.create_ui()
-        self.dataset_gallery.create_ui()
         self.export_dataset.create_ui()
+        self.dataset_gallery.create_ui()
 
     def set_callbacks(self, manager):
         self.create_dataset.set_callbacks(self.dataset_gallery,
                                           self.export_dataset, manager)
-        self.dataset_gallery.set_callbacks(self.create_dataset)
+        self.dataset_gallery.set_callbacks(self.create_dataset, manager)
         self.export_dataset.set_callbacks(self.create_dataset, manager)
 
 
 if __name__ == '__main__':
     pre_ui = PreprocessUI('scepter/methods/studio/preprocess/preprocess.yaml',
                           root_work_dir='./cache')
     with gr.Blocks() as demo:
```

## scepter/studio/preprocess/caption_editor_ui/component_names.py

```diff
@@ -1,128 +1,332 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 # For dataset manager
 class CreateDatasetUIName():
     def __init__(self, language='en'):
         if language == 'en':
-            self.dataset_name = 'All Dataset'
-            self.btn_create_datasets = 'Create Dataset'
-            self.user_data_name = 'Current Dataset Name'
-            self.modify_data_button = 'Modify Name'
-            self.confirm_data_button = 'Confirm'
-            self.refresh_list_button = 'Refresh List'
+            self.system_log = '<span style="color: blue;">System Log: {}</span> '
+            self.btn_create_datasets = '\U00002795'  # ➕
+            self.get_data_name_button = '\U0001F3B2'  # 🎲
+            self.new_data_name = (
+                'New dataset name, replace "name" and "version" with easy-to-remember identifiers.'
+                f'Also get a random name by clicking {self.get_data_name_button}'
+            )
+            self.modify_data_button = '\U0001F4DD'  # 📝
+            self.confirm_data_button = '\U00002714'  # ✔️
+            self.cancel_create_button = '\U00002716'  # ✖️
+            self.refresh_list_button = '\U0001f504'  # 🔄
+            self.delete_dataset_button = '\U0001f5d1'  # 🗑️
+            self.dataset_name = (
+                f'All Dataset，click{self.btn_create_datasets}to create new dataset，'
+                f'click{self.delete_dataset_button}to delete this dataset.')
+            self.dataset_type = 'Dataset Type'
+            self.dataset_type_name = {
+                'scepter_txt2img': 'Text2Image Generation',
+                'scepter_img2img': 'Image Edit Generation'
+            }
+            self.user_data_name = (
+                f'Current Dataset Name. Changes of dataset name take '
+                f'effect after clicking {self.modify_data_button}')
             self.zip_file = 'Upload Dataset(Zip/Txt)'
             self.zip_file_url = 'Dataset Url'
+            self.default_dataset_repo = 'https://www.modelscope.cn/api/v1/models/iic/scepter/'
+            self.default_dataset_zip = \
+                self.default_dataset_repo + 'repo?Revision=master&FilePath=datasets/3D_example_csv.zip'
+            self.default_dataset_name = '3D_example'
             self.btn_create_datasets_from_file = 'Create Dataset From File'
             self.user_direction = (
                 '### User Guide: \n' +
                 f'* {self.btn_create_datasets} button is used to create a new dataset '
-                "from scratch. Please make sure to modify the dataset's name and version. After creation, "
+                ". Please make sure to modify the dataset's name and version. After creation, "
                 'you can upload images one by one. \n'
-                f'* The {self.btn_create_datasets_from_file} button supports creating a new dataset from '
+                f'* The "{self.btn_create_datasets_from_file}" button supports creating a new dataset from '
                 'a file, currently supporting zip files. For zip files, the format should be consistent'
                 " with the one used during training, ensuring it contains an 'images/' folder and a '"
                 "train.csv' (which will use the image paths in this file); "
                 'The first line is Target:FILE, Prompt, followed by the format of each line: image path, description.'
                 'we also surpport the zip of '
-                'one level subfolder of images whose format are in jpg, jpeg, png, webp.\n'
+                'one level subfolder of images whose format are in jpg, jpeg, png, webp.'
+                f'The ZIP example is: {self.default_dataset_zip}. \n'  # noqa
                 f'* If you have refreshed the page, please click the {self.refresh_list_button} '
                 'button to ensure all previously created datasets are visible in the dropdown menu.\n'
-                '* ZIP example: https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip \n'  # noqa
-                '* For processing and training with large-scale data, it is recommended to use the command line.'
-            )
+                '* For processing and training with large-scale data(for example more than 10K samples), '
+                'it is recommended to use the command line to train the model.'
+                '* <span style="color: blue;">Please pay attention to the output of '
+                'the system logs to help improve operations.</span> \n')
             # Error or Warning
             self.illegal_data_name_err1 = (
                 'The data name is empty or contains illegal '
                 "characters ' ' (space) or '/' (slash).")
             self.illegal_data_name_err2 = "Please follow the format '{name}-{version}-{randomstr}'"
             self.illegal_data_name_err3 = "Do not include '.' in the dataset name."
             self.illegal_data_name_err4 = 'Please do not upload files and set dataset links simultaneously.'
             self.illegal_data_name_err5 = 'Invalid dataset name, please switch datasets or create a new one.'
             self.illegal_data_err1 = 'File download failed'
-            self.illegal_data_err2 = 'Illegal file format'
             self.illegal_data_err3 = 'File decompression failed, failed to upload to storage!'
+            self.delete_data_err1 = 'The example dataset is not allowed delete!'
+            self.modify_data_err1 = 'The example dataset is not allowed modify!'
             self.modify_data_name_err1 = 'Failed to change dataset name!'
             self.refresh_data_list_info1 = (
                 'The dataset name has been changed, '
                 'please refresh the list and try again.')
             self.use_link = 'Use File Link'
         elif language == 'zh':
-            self.dataset_name = '数据集'
-            self.btn_create_datasets = '新建'
-            self.user_data_name = '当前数据集名称'
-            self.modify_data_button = '修改数据集名称'
-            self.confirm_data_button = '确认'
-            self.refresh_list_button = '刷新列表'
+            self.system_log = '<span style="color: blue;">系统日志: {}</span> '
+            self.btn_create_datasets = '\U00002795'  # ➕
+            self.get_data_name_button = '\U0001F3B2'  # 🎲
+            self.new_data_name = ('新数据集名称，替换"name"和"version"为方便记忆名称.'
+                                  f'可以通过点击{self.get_data_name_button}获取随机名称')
+            self.modify_data_button = '\U0001F4DD'  # 📝
+            self.confirm_data_button = '\U00002714'  # ✔️
+            self.cancel_create_button = '\U00002716'  # ✖️
+            self.refresh_list_button = '\U0001f504'  # 🔄
+            self.delete_dataset_button = '\U0001f5d1'  # 🗑️
+            self.dataset_name = (f'数据集，点击{self.btn_create_datasets}新建数据集，'
+                                 f'点击{self.delete_dataset_button}删除数据集')
+            self.dataset_type = '数据集类型'
+            self.dataset_type_name = {
+                'scepter_txt2img': '文生图数据',
+                'scepter_img2img': '图像编辑（图生图）数据'
+            }
+
+            self.user_data_name = f'当前数据集名称，修改后点{self.modify_data_button}生效'
             self.zip_file = '上传数据集'
             self.zip_file_url = '数据集链接'
+            self.default_dataset_repo = 'https://www.modelscope.cn/api/v1/models/iic/scepter/'
+            self.default_dataset_zip = \
+                self.default_dataset_repo + 'repo?Revision=master&FilePath=datasets/3D_example_csv.zip'
+            self.default_dataset_name = '3D_example'
             self.btn_create_datasets_from_file = '从文件新建'
             self.user_direction = (
                 '### 使用说明 \n' +
                 f'* {self.btn_create_datasets} 按钮用于从零新建数据集，请注意修改数据集的name和version，'
                 '新建完成后可以逐个上传图片。\n' +
                 f'* {self.btn_create_datasets_from_file} 按钮支持从文件中来新建数据集，目前支持zip文件，'
                 '需要保证在文件夹外进行打包，并包含images/文件夹和train.csv(会使用该文件中的图片路径)，首行为Target:FILE,Prompt，'
                 '其次每行格式为：图片路径,描述；'
-                '同时我们也支持图像文件的zip包，格式在jpg、jpeg、png或webp \n' +
+                f'同时我们也支持图像文件的zip包，格式在jpg、jpeg、png或webp。数据ZIP样例路径：{self.default_dataset_zip}. \n'
+                +
                 f'* 如果刷新了页面，请点击{self.refresh_list_button} 按钮以确保所有以往创建的数据集在下拉框中可见。\n'
-                '* ZIP样例路径：https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip \n'  # noqa
-                '* 对于大规模数据的处理和训练，建议使用命令行形式')
+                '* 对于大规模数据的处理和训练（数据规模大于1万），建议使用命令行形式\n'
+                '* <span style="color: blue;">请注意观察系统日志的输出以帮助改进操作。</span> \n')
             # Error or Warning
             self.illegal_data_name_err1 = "数据名称为空或包含非法字符' '或者'/'"
-            self.illegal_data_name_err2 = '请按照{name}-{version}-{randomstr}'
+            self.illegal_data_name_err2 = '数据名称应该按照{name}-{version}-{randomstr}'
             self.illegal_data_name_err3 = "数据集名称中不要包含'.'"
             self.illegal_data_name_err4 = '请不要同时上传文件和设置数据集链接'
             self.illegal_data_name_err5 = '不合法的数据集名称，请切换数据集或新建数据集。'
             self.illegal_data_err1 = '文件下载失败'
-            self.illegal_data_err2 = '非法的文件格式'
             self.illegal_data_err3 = '文件解压失败，上传存储器失败！'
+
+            self.delete_data_err1 = '示例数据集不允许删除!'
+            self.modify_data_err1 = '示例数据集不允许修改!'
+
             self.modify_data_name_err1 = '变更数据集名称失败！'
             self.refresh_data_list_info1 = '该数据集名称发生了变更，请刷新列表试一下。'
             self.use_link = '使用文件链接'
 
 
 class DatasetGalleryUIName():
     def __init__(self, language='en'):
         if language == 'en':
+            self.system_log = '<span style="color: blue;">System Log: {}</span> '
             self.upload_image = 'Upload Image'
-            self.upload_image_btn = 'Upload'
+            self.upload_image_btn = '\U00002714'  # ✔️
+            self.cancel_upload_btn = '\U00002716'  # ✖️
             self.image_caption = 'Image Caption'
-            self.dataset_images = 'Dataset Images'
+
             self.ori_caption = 'Original Caption'
-            self.edit_caption = 'Editable Caption'
-            self.btn_modify = 'Replace Caption'
-            self.btn_delete = 'Delete Image'
+            self.btn_modify = '\U0001F4DD'  # 📝
+            self.btn_delete = '\U0001f5d1'  # 🗑️
+            self.btn_add = '\U00002795'  # ➕
+            self.dataset_images = f'Original Images，click{self.btn_modify} into editable mode.'
+            self.edit_caption = f'Editable Caption，click{self.btn_modify} into editable mode.'
+            self.dataset_images = f'Original Images，click{self.btn_modify} into editable mode.'
+
+            self.ori_dataset = 'Original Data Height({}) * Width({}) and Image Format({})'
+            self.edit_dataset = 'Editable Data Height({}) * Width({}) and Image Format({})'
+            self.upload_image_info = 'Image Information: Height({}) * Width({}) and Image Format({})'
+
+            self.range_mode_name = [
+                'Current sample', 'All samples', 'Samples in range'
+            ]
+            self.samples_range = 'The samples range to be process.'
+            self.samples_range_placeholder = (
+                '"1,4,6" indicates to process 1st, 4th and 6th sample;'
+                '"1-6" indicates to process samples from 1st to 6th.'
+                '"1-4,6-8"indicates to process samples from 1st to 4th '
+                'and from 6th to 8th.')
+            self.set_range_name = 'Samples Range to be edited'
+            self.btn_confirm_edit = '\U00002714'  # ✔️
+            self.btn_cancel_edit = '\U00002716'  # ✖️
+            self.btn_reset_edit = '\U000021BA'  # ↺
+            self.confirm_direction = (
+                f'click{self.btn_confirm_edit} to apply all changes，'
+                f'click{self.btn_reset_edit} to reset edited data，'
+                f'click{self.btn_cancel_edit} to out of editing mode.')
+            self.preprocess_choices = [
+                'Image Preprocess', 'Caption Preprocess'
+            ]
+            self.image_processor_type = 'Image Preprocessors'
+            self.caption_processor_type = 'Caption Preprocessors'
+            self.image_preprocess_btn = 'Run'
+            self.caption_preprocess_btn = 'Run'
+            self.caption_update_mode = 'Caption Update Mode'
+            self.caption_update_choices = ['Append', 'Replace']
+
+            self.used_device = 'Used Device'
+            self.used_memory = 'Used Memory'
+            self.caption_language = "Caption's Language"
+            self.advance_setting = 'Generation Setting'
+            self.system_prompt = 'System Prompt'
+            self.max_new_tokens = 'Max New Tokens'
+            self.min_new_tokens = 'Min New Tokens'
+            self.num_beams = 'Beams Num'
+            self.repetition_penalty = 'Repetition Penalty'
+            self.temperature = 'Temperature'
+
+            self.height_ratio = 'Height side scale'
+            self.width_ratio = 'Width side scale'
             # Error or Warning
-            self.delete_err1 = 'Deletion failed, the data is already empty.'
+
         elif language == 'zh':
+            self.system_log = '<span style="color: blue;">系统日志: {}</span> '
             self.upload_image = '上传图片'
-            self.upload_image_btn = '上传'
+            self.upload_image_btn = '\U00002714'  # ✔️
+            self.cancel_upload_btn = '\U00002716'  # ✖️
             self.image_caption = '图片描述'
-            self.dataset_images = '图片集'
-            self.ori_caption = '原始描述'
+
+            # self.image_height = '高度'
+            # self.image_width = '宽度'
+            # self.image_format = '格式'
+
+            self.btn_modify = '\U0001F4DD'  # 📝
+            self.dataset_images = f'图片数据，点击{self.btn_modify}进入编辑模式'
+
+            self.btn_delete = '\U0001f5d1'  # 🗑️
+            self.btn_add = '\U00002795'  # ➕
+
+            self.ori_caption = f'原始描述，点击{self.btn_modify}进入编辑模式'
             self.edit_caption = '编辑描述'
-            self.btn_modify = '替换描述'
-            self.btn_delete = '删除图片'
+            self.batch_caption_generate = '处理范围'
+
+            self.ori_dataset = '原始数据 高({}) * 宽({}) 图像格式({})'
+            self.edit_dataset = '可编辑数据 高({}) * 宽({}) 图像格式({})'
+            self.upload_image_info = '图像信息 高({}) * 宽({})'
+
+            self.range_mode_name = ['当前样本', '全部样本', '指定范围']
+            self.samples_range = '处理样本范围'
+            self.samples_range_placeholder = (
+                '"1,4,6"代表处理第1，4，6个样本;'
+                '"1-6" 代表处理从第1个到第6个的全部样本;'
+                '"1-4,6-8" 代表处理从第1个到第4个，第6到第8个样本。')
+            self.set_range_name = '编辑数据范围'
+
+            self.btn_confirm_edit = '\U00002714'  # ✔️
+            self.btn_cancel_edit = '\U00002716'  # ✖️
+            self.btn_reset_edit = '\U000021BA'  # ↺
+            self.confirm_direction = (f'点击{self.btn_confirm_edit}使所有编辑内容生效，'
+                                      f'点击{self.btn_cancel_edit}取消编辑，'
+                                      f'点击{self.btn_reset_edit}重置数据，'
+                                      f'修改编辑范围可以批量编辑不同范围的数据。')
+            self.preprocess_choices = ['图像预处理', '描述生成']
+            self.image_processor_type = '图像预处理器'
+            self.caption_processor_type = '描述生成器'
+            self.image_preprocess_btn = '运行'
+            self.caption_preprocess_btn = '运行'
+            self.caption_update_mode = '描述更新方式'
+            self.caption_update_choices = ['追加', '替换']
+            self.used_device = '使用设备'
+            self.used_memory = '使用内存'
+            self.caption_language = '描述语言'
+            self.advance_setting = '生成设置'
+            self.system_prompt = '系统提示'
+            self.max_new_tokens = '描述最大长度'
+            self.min_new_tokens = '描述最小长度'
+            self.num_beams = 'Beams数'
+            self.repetition_penalty = '重复惩罚'
+            self.temperature = '温度系数'
+            self.height_ratio = '高度比例'
+            self.width_ratio = '宽度比例'
             # Error or Warning
-            self.delete_err1 = '删除失败，数据已经为空了'
 
 
 class ExportDatasetUIName():
     def __init__(self, language='en'):
         if language == 'en':
             self.btn_export_zip = 'Download Data'
             self.btn_export_list = 'Export List'
             self.export_file = 'Download Data'
             # Error or Warning
             self.export_err1 = 'The dataset is empty, export is not possible!'
-            self.export_zip_err1 = 'Failed to compress the file!'
+
             self.upload_err1 = 'Failed to compress the file!'
             self.go_to_train = 'Go to train...'
         elif language == 'zh':
             self.btn_export_zip = '导出数据'
             self.btn_export_list = '导出列表'
             self.export_file = '下载数据'
             self.export_err1 = '数据集为空，无法导出!'
-            self.export_zip_err1 = '压缩文件失败!'
+
             self.upload_err1 = '压缩文件失败!'
             self.go_to_train = '去训练...'
+
+
+class Text2ImageDataCardName():
+    def __init__(self, language='en'):
+        if language == 'en':
+            self.illegal_data_err1 = (
+                'The list supports only "," or "#;#" as delimiters. '
+                'The four columns represent image path, width, height, '
+                'and description, respectively.')
+            self.illegal_data_err2 = 'Illegal file format'
+            self.illegal_data_err3 = 'File decompression failed, failed to upload to storage!'
+            self.illegal_data_err4 = 'Illegal width({}),height({})'
+            self.illegal_data_err5 = (
+                'The path should not contain "{}". '
+                'It should be an OSS path (oss://) or the prefix '
+                'can be omitted (xxx/xxx)."')
+            self.illegal_data_err6 = 'Image download failed {}'
+            self.illegal_data_err7 = 'Image upload failed {}'
+            self.delete_err1 = 'Deletion failed, the data is already empty.'
+            self.export_zip_err1 = 'Failed to compress the file!'
+        elif language == 'zh':
+            self.illegal_data_err1 = '列表只支持,或#;#作为分割符，四列分别为图像路径/宽/高/描述'
+            self.illegal_data_err2 = '非法的文件格式'
+            self.illegal_data_err3 = '文件解压失败，上传存储器失败！'
+            self.illegal_data_err4 = '不合法的width({}),height({})'
+            self.illegal_data_err5 = '路径不支持{}，应该为oss路径（oss://）或者省略前缀（xxx/xxx）'
+            self.illegal_data_err6 = '下载图像失败{}'
+            self.illegal_data_err7 = '上传图像失败{}'
+            self.delete_err1 = '删除失败，数据已经为空了'
+            self.export_zip_err1 = '压缩文件失败!'
+
+
+class Image2ImageDataCardName():
+    def __init__(self, language='en'):
+        if language == 'en':
+            self.illegal_data_err1 = (
+                'The list supports only "," or "#;#" as delimiters. '
+                'The four columns represent image path, width, height, '
+                'and description, respectively.')
+            self.illegal_data_err2 = 'Illegal file format'
+            self.illegal_data_err3 = 'File decompression failed, failed to upload to storage!'
+            self.illegal_data_err4 = 'Illegal width({}),height({})'
+            self.illegal_data_err5 = (
+                'The path should not contain "{}". '
+                'It should be an OSS path (oss://) or the prefix '
+                'can be omitted (xxx/xxx)."')
+            self.illegal_data_err6 = 'Image download failed {}'
+            self.illegal_data_err7 = 'Image upload failed {}'
+            self.delete_err1 = 'Deletion failed, the data is already empty.'
+            self.export_zip_err1 = 'Failed to compress the file!'
+        elif language == 'zh':
+            self.illegal_data_err1 = '列表只支持,或#;#作为分割符，四列分别为图像路径/宽/高/描述'
+            self.illegal_data_err2 = '非法的文件格式'
+            self.illegal_data_err3 = '文件解压失败，上传存储器失败！'
+            self.illegal_data_err4 = '不合法的width({}),height({})'
+            self.illegal_data_err5 = '路径不支持{}，应该为oss路径（oss://）或者省略前缀（xxx/xxx）'
+            self.illegal_data_err6 = '下载图像失败{}'
+            self.illegal_data_err7 = '上传图像失败{}'
+            self.delete_err1 = '删除失败，数据已经为空了'
+            self.export_zip_err1 = '压缩文件失败!'
```

## scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py

```diff
@@ -1,700 +1,565 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from __future__ import annotations
 
-import copy
-import csv
 import datetime
-import json
 import os.path
+from collections import OrderedDict
 
 import gradio as gr
-from PIL import Image
-from tqdm import tqdm
 
-from scepter.modules.utils.directory import get_md5
 from scepter.modules.utils.file_system import FS
 from scepter.studio.preprocess.caption_editor_ui.component_names import \
     CreateDatasetUIName
+from scepter.studio.preprocess.utils.data_card import Text2ImageDataCard
 from scepter.studio.utils.uibase import UIBase
 
 refresh_symbol = '\U0001f504'  # 🔄
 
 
+def wget_file(file_url, save_file):
+    if 'oss' in file_url:
+        file_url = file_url.split('?')[0]
+    local_path, _ = FS.map_to_local(save_file)
+    res = os.popen(f"wget -c '{file_url}' -O '{local_path}'")
+    res.readlines()
+    FS.put_object_from_local_file(local_path, save_file)
+    return save_file, res
+
+
 class CreateDatasetUI(UIBase):
     def __init__(self, cfg, is_debug=False, language='en'):
         self.work_dir = cfg.WORK_DIR
-        self.cache_file = {}
-        self.meta_dict = {}
-        self.dataset_list = self.load_history()
+        self.language = language
+        self.dataset_type_dict = OrderedDict(
+            {'scepter_txt2img': Text2ImageDataCard})
         self.components_name = CreateDatasetUIName(language)
+        self.default_dataset_type = list(self.dataset_type_dict.keys())[0]
+        self.default_dataset_cls = Text2ImageDataCard
 
-    def load_meta(self, meta_file):
-        dataset_meta = json.load(open(meta_file, 'r'))
-        return dataset_meta
-
-    def write_csv(self, file_list, save_csv, data_folder):
-        with FS.put_to(save_csv) as local_path:
-            print(local_path)
-            with open(local_path, 'w') as f:
-                writer = csv.writer(f)
-                writer.writerow(['Target:FILE', 'Prompt'])
-                for one_file in file_list:
-                    relative_file = one_file['relative_path']
-                    if relative_file.startswith('/'):
-                        relative_file = relative_file[1:]
-                    writer.writerow([relative_file, one_file['caption']])
-        return save_csv
-
-    def write_file_list(self, file_list, save_csv):
-        with FS.put_to(save_csv) as local_path:
-            print(local_path)
-            with open(local_path, 'w') as f:
-                for one_file in file_list:
-                    is_flag, file_path = self.del_prefix(
-                        one_file['image_path'], prefix=one_file['prefix'])
-                    f.write('{},{},{},{}\n'.format(file_path,
-                                                   one_file['width'],
-                                                   one_file['height'],
-                                                   one_file['caption']))
-        return save_csv
-
-    def save_meta(self, meta, dataset_folder):
-        meta_file = os.path.join(dataset_folder, 'meta.json')
-        save_meta = copy.deepcopy(meta)
-        if 'local_work_dir' in meta:
-            save_meta.pop('local_work_dir')
-        if 'work_dir' in meta:
-            save_meta.pop('work_dir')
-        with FS.put_to(meta_file) as local_path:
-            json.dump(save_meta, open(local_path, 'w'))
-        return meta_file
-
-    def construct_meta(self, cursor, file_list, dataset_folder, user_name,
-                       login_user_name):
+        self.cache_file = {}
         '''
             {
-                "dataset_name": "xxxx",
-                "dataset_scale": 100,
-                "file_list": "xxxxx", # image_path#;#width#;#height#;#caption
-                "update_time": "",
-                "create_time": ""
+                "dataset_type": {
+                    "dataset_name" : {}
+                }
             }
         '''
-        train_csv = os.path.join(dataset_folder, 'train.csv')
-        train_csv = self.write_csv(file_list, train_csv, dataset_folder)
-        save_file_list = os.path.join(dataset_folder, 'file.csv')
-        save_file_list = self.write_file_list(file_list, save_file_list)
-        meta = {
-            'dataset_name':
-            user_name if login_user_name == '' or login_user_name is None else
-            '_'.join([login_user_name, user_name]),
-            'cursor':
-            cursor,
-            'file_list':
-            file_list,
-            'train_csv':
-            train_csv,
-            'save_file_list':
-            save_file_list
-        }
-        self.save_meta(meta, dataset_folder)
-        return meta
+        self.dataset_dict = {}
+        '''
+            {
+                "user_name": {
+                    "dataset_type" : []
+                }
+            }
+        '''
+        self.init_example()
+        self.user_level_dataset_list = self.load_history()
 
-    def load_from_list(self, save_file, dataset_folder, local_dataset_folder):
-        file_list = []
-        images_folder = os.path.join(local_dataset_folder, 'images')
-        os.makedirs(images_folder, exist_ok=True)
-        with FS.get_from(save_file) as local_path:
-            all_remote_list, all_local_list = [], []
-            all_save_list = []
-            with open(local_path, 'r') as f:
-                for line in tqdm(f):
-                    line = line.strip()
-                    if line == '':
-                        continue
-                    try:
-                        image_path, width, height, caption = line.split(
-                            '#;#', 3)
-                    except Exception:
-                        try:
-                            image_path, width, height, caption = line.split(
-                                ',', 3)
-                        except Exception:
-                            raise gr.Error('列表只支持,或#;#作为分割符，四列分别为图像路径/宽/高/描述')
-                    is_legal, new_path, prefix = self.find_prefix(image_path)
-                    try:
-                        int(width), int(height)
-                    except Exception:
-                        raise gr.Error(f'不合法的width({width}),height({height})')
-
-                    if not is_legal:
-                        raise gr.Error(
-                            f'路径不支持{image_path}，应该为oss路径（oss://）或者省略前缀（xxx/xxx）'
-                        )
-                    relative_path = os.path.join('images',
-                                                 image_path.split('/')[-1])
-
-                    all_remote_list.append(new_path)
-                    all_local_list.append(
-                        os.path.join(local_dataset_folder, relative_path))
-                    all_save_list.append(
-                        os.path.join(dataset_folder, relative_path))
-                    file_list.append({
-                        'image_path':
-                        os.path.join(dataset_folder, relative_path),
-                        'relative_path':
-                        relative_path,
-                        'width':
-                        int(width),
-                        'height':
-                        int(height),
-                        'caption':
-                        caption,
-                        'prefix':
-                        prefix,
-                        'edit_caption':
-                        caption
-                    })
-        cache_file_list = []
-        for idx, local_path in enumerate(
-                FS.get_batch_objects_from(all_remote_list)):
-            if local_path is None:
-                raise gr.Error(f'下载图像失败{all_remote_list[idx]}')
-            _ = FS.put_object_from_local_file(local_path, all_local_list[idx])
-            cache_file_list.append(local_path)
-
-        for local_path, target_path, flg in FS.put_batch_objects_to(
-                cache_file_list, all_save_list):
-            if not flg:
-                raise gr.Error(f'上传图像失败{local_path}')
-            if os.path.exists(local_path):
-                try:
-                    os.remove(local_path)
-                except Exception:
-                    pass
-
-        return file_list
-
-    def load_from_zip(self, save_file, data_folder, local_dataset_folder):
-        with FS.get_from(save_file) as local_path:
-            res = os.popen(
-                f"unzip -o '{local_path}' -d '{local_dataset_folder}'")
-            res = res.readlines()
-        if not os.path.exists(local_dataset_folder):
-            raise gr.Error(f'解压{save_file}失败{str(res)}')
-        file_folder = None
-        train_list = None
-        hit_dir = None
-        raw_list = {}
-        mac_osx = os.path.join(local_dataset_folder, '__MACOSX')
-        if os.path.exists(mac_osx):
-            res = os.popen(f"rm -rf '{mac_osx}'")
-            res = res.readlines()
-        for one_dir in FS.walk_dir(local_dataset_folder, recurse=False):
-            if one_dir.endswith('__MACOSX'):
-                res = os.popen(f"rm -rf '{one_dir}'")
-                res = res.readlines()
-                continue
-            if FS.isdir(one_dir):
-                if one_dir.endswith('images') or one_dir.endswith('images/'):
-                    file_folder = one_dir
-                    hit_dir = one_dir
-                else:
-                    sub_dir = FS.walk_dir(one_dir)
-                    for one_s_dir in sub_dir:
-                        if FS.isdir(one_s_dir) and one_s_dir.split(
-                                one_dir)[1].replace('/', '') == 'images':
-                            file_folder = one_s_dir
-                            hit_dir = one_dir
-                        if FS.isfile(one_s_dir) and one_s_dir.split(
-                                one_dir)[1].replace('/', '') == 'train.csv':
-                            train_list = one_s_dir
-                        if file_folder is not None and train_list is not None:
-                            break
-                        if (one_s_dir.endswith('.jpg')
-                                or one_s_dir.endswith('.jpeg')
-                                or one_s_dir.endswith('.png')
-                                or one_s_dir.endswith('.webp')):
-                            file_name, surfix = os.path.splitext(one_s_dir)
-                            txt_file = file_name + '.txt'
-                            if os.path.exists(txt_file):
-                                raw_list[one_s_dir] = txt_file
-                            else:
-                                raw_list[one_s_dir] = None
-            elif one_dir.endswith('train.csv'):
-                train_list = one_dir
-            else:
-                if (one_dir.endswith('.jpg') or one_dir.endswith('.jpeg')
-                        or one_dir.endswith('.png')
-                        or one_dir.endswith('.webp')):
-                    file_name, surfix = os.path.splitext(one_dir)
-                    txt_file = file_name + '.txt'
-                    if os.path.exists(txt_file):
-                        raw_list[one_dir] = txt_file
-                    else:
-                        raw_list[one_dir] = None
-            if file_folder is not None and train_list is not None:
-                break
-        if file_folder is None and len(raw_list) < 1:
-            raise gr.Error(
-                "images folder or train.csv doesn't exists, or nothing exists in your zip"
-            )
-        new_file_folder = f'{local_dataset_folder}/images'
-        os.makedirs(new_file_folder, exist_ok=True)
-        if file_folder is not None:
-            _ = FS.get_dir_to_local_dir(file_folder, new_file_folder)
-        elif len(raw_list) > 0:
-            raw_list = [[k, v] for k, v in raw_list.items()]
-            for img_id, cur_image in enumerate(raw_list):
-                image_name, surfix = os.path.splitext(cur_image[0])
-                if cur_image[1] is not None and os.path.exists(cur_image[1]):
-                    prompt = open(cur_image[1], 'r').read()
-                else:
-                    prompt = image_name.split('/')[-1]
-                try:
-                    os.rename(
-                        os.path.abspath(cur_image[0]),
-                        f'{new_file_folder}/{get_md5(cur_image[0])}{surfix}')
-                    raw_list[img_id] = [
-                        os.path.join('images',
-                                     f'{get_md5(cur_image[0])}{surfix}'),
-                        prompt
-                    ]
-                except Exception as e:
-                    print(e)
-
-        if not os.path.exists(new_file_folder):
-            raise gr.Error(f'{str(res)}')
-        new_train_list = f'{local_dataset_folder}/train.csv'
-        if train_list is None or not os.path.exists(train_list):
-            with open(new_train_list, 'w') as f:
-                writer = csv.writer(f)
-                writer.writerow(['Target:FILE', 'Prompt'])
-                for cur_image, cur_prompt in raw_list:
-                    writer.writerow([cur_image, cur_prompt])
-        else:
-            res = os.popen(f"mv '{train_list}' '{new_train_list}'")
-            res = res.readlines()
-        if not os.path.exists(new_train_list):
-            raise gr.Error(f'{str(res)}')
-        if not file_folder == hit_dir:
-            try:
-                res = os.popen(f"rm -rf '{hit_dir}/images/*'")
-                _ = res.readlines()
-                res = os.popen(f"rm -rf '{hit_dir}'")
-                _ = res.readlines()
-            except Exception:
-                pass
-        file_list = self.load_train_csv(new_train_list, data_folder)
-        return file_list
-
-    def get_image_meta(self, image):
-        img = Image.open(image)
-        return img.size
-
-    def load_train_csv(self, file_path, data_folder):
-        base_folder = os.path.dirname(file_path)
-        file_list = []
-        with open(file_path, 'r') as f:
-            reader = csv.reader(f)
-            for row in reader:
-                image_path, prompt = row[0], row[1]
-                if image_path == 'Target:FILE':
-                    continue
-                local_image_path = os.path.join(base_folder, image_path)
-                w, h = self.get_image_meta(local_image_path)
-                file_list.append({
-                    'image_path':
-                    os.path.join(data_folder, image_path),
-                    'relative_path':
-                    image_path,
-                    'width':
-                    w,
-                    'height':
-                    h,
-                    'caption':
-                    prompt,
-                    'prefix':
-                    '',
-                    'edit_caption':
-                    prompt
-                })
-        return file_list
-
-    def find_prefix(self, file_path):
-        for k in FS._prefix_to_clients.keys():
-            if file_path.startswith(k):
-                return True, file_path, ''
-            elif FS.exists(os.path.join(k, file_path)):
-                return True, os.path.join(k, file_path), ''
-            elif FS.exists(os.path.join(k, 'datasets', file_path)):
-                return True, os.path.join(k, 'datasets', file_path), 'datasets'
-        return False, None, None
-
-    def del_prefix(self, file_path, prefix=''):
-        for k in FS._prefix_to_clients.keys():
-            if file_path.startswith(k):
-                file_path = file_path.replace(k, '')
-                while file_path.startswith('/'):
-                    file_path = file_path[1:]
-                if not prefix == '' and file_path.startswith(prefix):
-                    file_path = file_path.split(prefix)[-1]
-                while file_path.startswith('/'):
-                    file_path = file_path[1:]
-                return True, file_path
-        return False, file_path
+    def init_example(self):
+        file_url = self.components_name.default_dataset_zip
+        file_name, surfix = os.path.splitext(file_url)
+        save_file = os.path.join(
+            self.work_dir,
+            f'{self.components_name.default_dataset_name}{surfix}')
+        save_file, res = wget_file(file_url, save_file)
+        if not FS.exists(save_file):
+            print('Init example eroor, skip!')
+            self.example_dataset_ins = None
+            return
+
+        dataset_folder = os.path.join(
+            self.work_dir, '_'.join([
+                self.default_dataset_type,
+                self.components_name.default_dataset_name
+            ]))
+        self.example_dataset_ins = self.default_dataset_cls(
+            dataset_folder,
+            dataset_name=self.components_name.default_dataset_name,
+            src_file=save_file,
+            surfix=surfix,
+            user_name='example',
+            language=self.language)
+        # add to list by load_history.
+
+    def user_filter(self, dataset_ins, login_user_name):
+        if not dataset_ins.is_valid:
+            return False
+        if (dataset_ins.dataset_name.startswith(login_user_name)
+                or dataset_ins.user_name == login_user_name
+                or login_user_name == 'admin'
+                or dataset_ins.user_name == 'example'):
+            return True
+        return False
 
-    def load_history(self, login_user_name=''):
-        dataset_list = []
+    def load_history(self, login_user_name='admin'):
+        dataset_list = {login_user_name: {}}
         self.dir_list = FS.walk_dir(self.work_dir, recurse=False)
+        # From v0.0.5 we use the classname of DataCard as prefix of folder to indicate the type of data.
         for one_dir in self.dir_list:
-            if FS.isdir(one_dir):
-                meta_file = os.path.join(one_dir, 'meta.json')
-                if FS.exists(meta_file):
-                    local_dataset_folder, _ = FS.map_to_local(one_dir)
-                    if not FS.exists(
-                            os.path.join(local_dataset_folder, 'meta.json')):
-                        local_dataset_folder = FS.get_dir_to_local_dir(
-                            one_dir, local_dataset_folder, multi_thread=True)
-                    meta_data = self.load_meta(
-                        os.path.join(local_dataset_folder, 'meta.json'))
-                    meta_data['local_work_dir'] = local_dataset_folder
-                    meta_data['work_dir'] = one_dir
-                    if meta_data['dataset_name'].startswith(login_user_name):
-                        dataset_list.append(meta_data['dataset_name'])
-                        self.meta_dict[meta_data['dataset_name']] = meta_data
+            if not FS.isdir(one_dir):
+                continue
+            # the meta.json is the unique sign for dataset
+            meta_file = os.path.join(one_dir, 'meta.json')
+            if not FS.exists(meta_file):
+                continue
+            dataset_type = self.default_dataset_type
+            dataset_cls = self.default_dataset_cls
+            for key, value in self.dataset_type_dict.items():
+                if one_dir.split('/')[-1].startswith(key):
+                    dataset_type = key
+                    dataset_cls = value
+                    break
+            dataset_ins = dataset_cls(dataset_folder=one_dir)
+            if dataset_type not in self.dataset_dict:
+                self.dataset_dict[dataset_type] = {}
+            if dataset_type not in dataset_list[login_user_name]:
+                dataset_list[login_user_name][dataset_type] = []
+            if self.user_filter(dataset_ins, login_user_name):
+                dataset_list[login_user_name][dataset_type].append(
+                    dataset_ins.dataset_name)
+            self.dataset_dict[dataset_type][
+                dataset_ins.dataset_name] = dataset_ins
         return dataset_list
 
     def create_ui(self):
         with gr.Box():
             gr.Markdown(self.components_name.user_direction)
         with gr.Box():
             with gr.Row():
-                with gr.Column(scale=1, min_width=0):
+                self.sys_log = gr.Markdown(
+                    self.components_name.system_log.format(''))
+            with gr.Row(variant='panel', ):
+                with gr.Column(scale=2, min_width=0):
+                    self.dataset_type = gr.Dropdown(
+                        label=self.components_name.dataset_type,
+                        choices=[
+                            self.components_name.dataset_type_name[data_type]
+                            for data_type in self.dataset_type_dict.keys()
+                        ],
+                        interactive=True,
+                        value=self.components_name.dataset_type_name[
+                            self.default_dataset_type])
+                with gr.Column(scale=3, min_width=0):
                     self.dataset_name = gr.Dropdown(
                         label=self.components_name.dataset_name,
-                        choices=self.dataset_list,
+                        choices=self.user_level_dataset_list.get(
+                            'admin', {}).get(self.default_dataset_type, []),
+                        value=None if self.example_dataset_ins is None else
+                        self.example_dataset_ins.dataset_name,
+                        interactive=True)
+                with gr.Column(scale=3, min_width=0):
+                    self.user_dataset_name = gr.Text(
+                        label=self.components_name.user_data_name,
+                        value='' if self.example_dataset_ins is None else
+                        self.example_dataset_ins.dataset_name,
                         interactive=True)
+                    self.user_data_name_state = gr.State(
+                        value=None if self.example_dataset_ins is None else
+                        self.example_dataset_ins.dataset_name)
+                    self.create_mode = gr.State(value=0)
                 with gr.Column(scale=1, min_width=0):
-                    self.refresh_dataset_name = gr.Button(
-                        value=self.components_name.refresh_list_button)
-                    self.btn_create_datasets = gr.Button(
-                        value=self.components_name.btn_create_datasets)
-                    self.btn_create_datasets_from_file = gr.Button(
-                        value=self.components_name.
-                        btn_create_datasets_from_file)
-                    self.panel_state = gr.Checkbox(label='panel_state',
-                                                   value=False,
-                                                   visible=False)
-                with gr.Column(scale=2, min_width=0):
-                    with gr.Row(equal_height=True):
-                        with gr.Column(visible=False, min_width=0) as panel:
-                            self.user_data_name = gr.Text(
-                                label=self.components_name.user_data_name,
-                                value='',
-                                interactive=True)
-                            self.user_data_name_state = gr.State(value='')
-                            self.create_mode = gr.State(value=0)
-                        with gr.Column(visible=False,
-                                       min_width=0) as file_panel:
+                    with gr.Column(scale=1, min_width=0):
+                        self.refresh_dataset_name = gr.Button(
+                            value=self.components_name.refresh_list_button)
+                    with gr.Column(scale=1, min_width=0):
+                        self.modify_data_button = gr.Button(
+                            value=self.components_name.modify_data_button)
+                with gr.Column(scale=1, min_width=0):
+                    with gr.Column(scale=1, min_width=0):
+                        self.btn_create_datasets = gr.Button(
+                            value=self.components_name.btn_create_datasets)
+                    with gr.Column(scale=1, min_width=0):
+                        self.btn_delete_datasets = gr.Button(
+                            value=self.components_name.delete_dataset_button)
+                self.panel_state = gr.Checkbox(label='panel_state',
+                                               value=False,
+                                               visible=False)
+            with gr.Row(variant='panel', ):
+                with gr.Column(scale=4, visible=False,
+                               min_width=0) as dataname_panel:
+                    self.new_dataset_name = gr.Text(
+                        label=self.components_name.new_data_name,
+                        value='',
+                        interactive=True)
+                    with gr.Row(visible=False, ) as file_panel:
+                        with gr.Column(scale=1, min_width=0):
                             self.use_link = gr.Checkbox(
                                 label=self.components_name.use_link,
                                 value=False,
                                 visible=False)
+                        with gr.Column(scale=2, min_width=0):
                             self.file_path = gr.File(
                                 label=self.components_name.zip_file,
                                 min_width=0,
                                 file_types=['.zip', '.txt', '.csv'],
                                 visible=False)
-
                             self.file_path_url = gr.Text(
                                 label=self.components_name.zip_file_url,
                                 value='',
+                                placeholder=self.components_name.
+                                default_dataset_zip,
                                 visible=False)
-                        with gr.Column(visible=False,
-                                       min_width=0) as btn_panel:
-                            self.random_data_button = gr.Button(
-                                value=refresh_symbol)
-                            self.confirm_data_button = gr.Button(
-                                value=self.components_name.confirm_data_button)
-                        with gr.Column(visible=False,
-                                       min_width=0) as modify_panel:
-                            self.modify_data_button = gr.Button(
-                                value=self.components_name.modify_data_button)
-
-        self.dataset_panel = panel
+                with gr.Column(scale=1, visible=False,
+                               min_width=0) as btn_panel:
+                    self.random_data_button = gr.Button(
+                        value=self.components_name.get_data_name_button)
+                    self.confirm_data_button = gr.Button(
+                        value=self.components_name.confirm_data_button)
+                    self.cancel_data_button = gr.Button(
+                        value=self.components_name.cancel_create_button)
+                    self.btn_create_datasets_from_file = gr.Checkbox(
+                        label=self.components_name.
+                        btn_create_datasets_from_file,
+                        value=False)
         self.btn_panel = btn_panel
         self.file_panel = file_panel
-        self.modify_panel = modify_panel
+        self.dataname_panel = dataname_panel
 
-    def set_callbacks(self, gallery_dataset, export_dataset, manager):
-        def show_dataset_panel():
-            return (gr.Column(visible=False), gr.Column(visible=True),
-                    gr.Column(visible=True),
-                    gr.Checkbox(value=False, visible=False),
-                    gr.Text(value=get_random_dataset_name(),
-                            interactive=True), 1)
-
-        def show_file_panel():
-            return (gr.Column(visible=True), gr.Column(visible=True),
-                    gr.Column(visible=True),
-                    gr.Checkbox(value=False, visible=False),
-                    gr.Text(value=get_random_dataset_name(),
-                            interactive=True), gr.File(value=None,
-                                                       visible=True),
-                    gr.Text(value='', visible=False), 2,
-                    gr.Checkbox(value=False, visible=True))
+    def get_trans_dataset_type(self, dataset_type):
+        reverse_data_type = {
+            v: k
+            for k, v in self.components_name.dataset_type_name.items()
+        }
+        trans_dataset_type = reverse_data_type[dataset_type]
+        return trans_dataset_type
 
+    def set_callbacks(self, gallery_dataset, export_dataset, manager):
         def get_random_dataset_name():
             data_name = 'name-version-{0:%Y%m%d_%H_%M_%S}'.format(
                 datetime.datetime.now())
             return data_name
 
-        def refresh(login_user_name):
-            dataset_list = self.load_history(login_user_name=login_user_name)
-            return gr.Dropdown(
-                value=dataset_list[-1] if len(dataset_list) > 0 else '',
-                choices=dataset_list)
+        def clear_file():
+            return gr.Text(visible=False)
 
-        self.refresh_dataset_name.click(refresh,
-                                        inputs=[manager.user_name],
-                                        outputs=[self.dataset_name],
-                                        queue=False)
-
-        def confirm_create_dataset(user_name, create_mode, file_url, file_path,
-                                   panel_state, login_user_name):
-            if user_name.strip() == '' or ' ' in user_name or '/' in user_name:
-                raise gr.Error(self.components_name.illegal_data_name_err1)
+        def show_dataset_panel():
+            return gr.Checkbox(
+                value=True), self.components_name.system_log.format('')
 
-            if len(user_name.split('-')) < 3:
-                raise gr.Error(self.components_name.illegal_data_name_err2)
+        # Click Create
+        self.btn_create_datasets.click(show_dataset_panel, [],
+                                       [self.panel_state, self.sys_log],
+                                       queue=False)
+
+        def unshow_dataset_panel():
+            return gr.Checkbox(
+                value=False), self.components_name.system_log.format('')
+
+        self.cancel_data_button.click(unshow_dataset_panel, [],
+                                      [self.panel_state, self.sys_log],
+                                      queue=False)
+
+        def delete_dataset(dataset_name, dataset_type, login_user_name):
+            dataset_type = self.get_trans_dataset_type(dataset_type)
+            if dataset_name in self.dataset_dict[dataset_type]:
+                dataset_ins = self.dataset_dict[dataset_type][dataset_name]
+                if dataset_ins.user_name == 'example':
+                    sys_log = self.components_name.system_log.format(
+                        self.components_name.delete_data_err1)
+                    return (dataset_name, gr.Dropdown(value=dataset_name),
+                            sys_log)
+                dataset_ins.deactive_dataset()
+            dataset_list = self.user_level_dataset_list.get(
+                login_user_name, {}).get(dataset_type, [])
+            if dataset_name in dataset_list:
+                dataset_list.remove(dataset_name)
+            if len(dataset_list) > 0:
+                now_dataset = dataset_list[-1]
+            else:
+                now_dataset = ''
+            return (now_dataset,
+                    gr.Dropdown(choices=dataset_list, value=now_dataset),
+                    self.components_name.system_log.format(''))
+
+        self.btn_delete_datasets.click(
+            delete_dataset,
+            [self.dataset_name, self.dataset_type, manager.user_name],
+            [self.user_dataset_name, self.dataset_name, self.sys_log],
+            queue=False)
 
-            if '.' in user_name:
-                raise gr.Error(self.components_name.illegal_data_name_err3)
+        def show_file_panel(show_file_panel):
+            if show_file_panel:
+                return (gr.Row(visible=True), gr.File(value=None,
+                                                      visible=True),
+                        gr.Text(value='', visible=False), 2,
+                        gr.Checkbox(value=False, visible=True),
+                        self.components_name.system_log.format(''))
+            else:
+                return (gr.Row(visible=False),
+                        gr.File(value=None,
+                                visible=False), gr.Text(value='',
+                                                        visible=False), 1,
+                        gr.Checkbox(value=False, visible=False),
+                        self.components_name.system_log.format(''))
+
+        self.btn_create_datasets_from_file.change(
+            show_file_panel, [self.btn_create_datasets_from_file], [
+                self.file_panel, self.file_path, self.file_path_url,
+                self.create_mode, self.use_link, self.sys_log
+            ],
+            queue=False)
 
+        def use_link_change(use_link):
+            if use_link:
+                create_mode = 3
+                return (gr.File(value=None, visible=False),
+                        gr.Text(value='', visible=True), create_mode,
+                        self.components_name.system_log.format(''))
+            else:
+                create_mode = 2
+                return (gr.File(value=None, visible=True),
+                        gr.Text(value='', visible=False), create_mode,
+                        self.components_name.system_log.format(''))
+
+        self.use_link.change(use_link_change, [self.use_link], [
+            self.file_path, self.file_path_url, self.create_mode, self.sys_log
+        ])
+        # Click Refresh
+        self.random_data_button.click(get_random_dataset_name, [],
+                                      [self.new_dataset_name],
+                                      queue=False)
+
+        self.file_path.clear(clear_file,
+                             outputs=[self.file_path_url],
+                             queue=False)
+
+        def confirm_create_dataset(user_dataset_name, create_mode, file_url,
+                                   file_path, login_user_name, dataset_type):
+            if user_dataset_name.strip(
+            ) == '' or ' ' in user_dataset_name or '/' in user_dataset_name:
+                sys_log = self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err1)
+                return (gr.Checkbox(), gr.Dropdown(), gr.Text(), sys_log)
+
+            if len(user_dataset_name.split('-')) < 3:
+                sys_log = self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err2)
+                return (gr.Checkbox(), gr.Dropdown(), gr.Text(), sys_log)
+            if '.' in user_dataset_name:
+                sys_log = self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err3)
+                return (gr.Checkbox(), gr.Dropdown(), gr.Text(), sys_log)
             if not file_url.strip() == '' and file_path is not None:
-                raise gr.Error(self.components_name.illegal_data_name_err4)
+                sys_log = self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err4)
+                return (gr.Checkbox(), gr.Dropdown(), gr.Text(), sys_log)
+            dataset_type = self.get_trans_dataset_type(dataset_type)
             if create_mode == 3 and not file_url.strip() == '':
-                if 'oss' in file_url:
-                    file_url = file_url.split('?')[0]
                 file_name, surfix = os.path.splitext(file_url)
-                save_file = os.path.join(self.work_dir, f'{user_name}{surfix}')
-                local_path, _ = FS.map_to_local(save_file)
-                res = os.popen(f"wget -c '{file_url}' -O '{local_path}'")
-                res.readlines()
-                FS.put_object_from_local_file(local_path, save_file)
+                save_file = os.path.join(self.work_dir,
+                                         f'{user_dataset_name}{surfix}')
+                save_file, res = wget_file(file_url, save_file)
                 if not FS.exists(save_file):
-                    raise gr.Error(
+                    sys_log = self.components_name.system_log.format(
                         f'{self.components_name.illegal_data_err1} {str(res)}')
+                    return (gr.Checkbox(), gr.Dropdown(), gr.Text(), sys_log)
             elif create_mode == 2 and file_path is not None and file_path.name:
-                self.cache_file[user_name] = {
+                self.cache_file[user_dataset_name] = {
                     'file_name': file_path.name,
                     'surfix': os.path.splitext(file_path.name)[-1]
                 }
-                cache_file = self.cache_file.pop(user_name)
+                cache_file = self.cache_file.pop(user_dataset_name)
                 surfix = cache_file['surfix']
                 ori_file = cache_file['file_name']
-                save_file = os.path.join(self.work_dir, f'{user_name}{surfix}')
+                save_file = os.path.join(self.work_dir,
+                                         f'{user_dataset_name}{surfix}')
                 with FS.put_to(save_file) as local_path:
                     res = os.popen(f"cp '{ori_file}' '{local_path}'")
                     res = res.readlines()
                 if not FS.exists(save_file):
-                    raise gr.Error(
+                    sys_log = self.components_name.system_log.format(
                         f'{self.components_name.illegal_data_err1}{str(res)}')
+                    return (gr.Checkbox(), gr.Dropdown(), gr.Text(), sys_log)
             else:
                 surfix = None
+                save_file = None
             # untar file or create blank dataset
-            dataset_folder = os.path.join(self.work_dir, user_name)
-            local_dataset_folder, _ = FS.map_to_local(dataset_folder)
-            if surfix == '.zip':
-                file_list = self.load_from_zip(save_file, dataset_folder,
-                                               local_dataset_folder)
-            elif surfix in ['.txt', '.csv']:
-                file_list = self.load_from_list(save_file, dataset_folder,
-                                                local_dataset_folder)
-            elif surfix is None:
-                file_list = []
-            else:
-                raise gr.Error(
-                    f'{self.components_name.illegal_data_err2} {surfix}')
-            is_flag = FS.put_dir_from_local_dir(local_dataset_folder,
-                                                dataset_folder,
-                                                multi_thread=True)
-            if not is_flag:
-                raise gr.Error(f'{self.components_name.illegal_data_err3}')
-
-            cursor = 0 if len(file_list) > 0 else -1
-            meta = self.construct_meta(cursor, file_list, dataset_folder,
-                                       user_name, login_user_name)
-
-            meta['local_work_dir'] = local_dataset_folder
-            meta['work_dir'] = dataset_folder
-
-            self.meta_dict[meta['dataset_name']] = meta
-            if meta['dataset_name'] not in self.dataset_list:
-                self.dataset_list.append(meta['dataset_name'])
-            return (gr.Checkbox(value=True, visible=False),
-                    gr.Dropdown(value=meta['dataset_name'],
-                                choices=self.dataset_list),
-                    gr.Text(value=meta['dataset_name']))
-
-        def clear_file():
-            return gr.Text(visible=False)
-
-        # Click Create
-        self.btn_create_datasets.click(show_dataset_panel, [], [
-            self.file_panel, self.dataset_panel, self.btn_panel,
-            self.panel_state, self.user_data_name, self.create_mode
-        ],
-                                       queue=False)
-
-        self.btn_create_datasets_from_file.click(show_file_panel, [], [
-            self.file_panel, self.dataset_panel, self.btn_panel,
-            self.panel_state, self.user_data_name, self.file_path,
-            self.file_path_url, self.create_mode, self.use_link
-        ],
-                                                 queue=False)
-
-        def use_link_change(use_link):
-            if use_link:
-                create_mode = 3
-                return (gr.File(value=None, visible=False),
-                        gr.Text(value='', visible=True), create_mode)
-            else:
-                create_mode = 2
-                return (gr.File(value=None, visible=True),
-                        gr.Text(value='', visible=False), create_mode)
-
-        self.use_link.change(
-            use_link_change, [self.use_link],
-            [self.file_path, self.file_path_url, self.create_mode])
-        # Click Refresh
-        self.random_data_button.click(get_random_dataset_name, [],
-                                      [self.user_data_name],
-                                      queue=False)
-
-        self.file_path.clear(clear_file,
-                             outputs=[self.file_path_url],
-                             queue=False)
+            dataset_folder = os.path.join(
+                self.work_dir, '_'.join([dataset_type, user_dataset_name]))
+            dataset_cls = self.dataset_type_dict[dataset_type]
+            dataset_ins = dataset_cls(dataset_folder,
+                                      dataset_name=user_dataset_name,
+                                      src_file=save_file,
+                                      surfix=surfix,
+                                      user_name=login_user_name,
+                                      language=self.language)
+            if login_user_name not in self.user_level_dataset_list:
+                self.user_level_dataset_list[login_user_name] = {}
+            if dataset_type not in self.user_level_dataset_list[
+                    login_user_name]:
+                self.user_level_dataset_list[login_user_name][
+                    dataset_type] = []
+            if dataset_ins.dataset_name not in self.user_level_dataset_list[
+                    login_user_name][dataset_type]:
+                self.user_level_dataset_list[login_user_name][
+                    dataset_type].append(dataset_ins.dataset_name)
+            if dataset_type not in self.dataset_dict:
+                self.dataset_dict[dataset_type] = {}
+            self.dataset_dict[dataset_type][
+                dataset_ins.dataset_name] = dataset_ins
+            return (gr.Checkbox(value=False, visible=False),
+                    gr.Dropdown(value=dataset_ins.dataset_name,
+                                choices=self.user_level_dataset_list.get(
+                                    login_user_name, {}).get(dataset_type,
+                                                             [])),
+                    gr.Text(value=dataset_ins.dataset_name),
+                    self.components_name.system_log.format(''))
 
         # Click Confirm
         self.confirm_data_button.click(confirm_create_dataset, [
-            self.user_data_name, self.create_mode, self.file_path_url,
-            self.file_path, self.panel_state, manager.user_name
-        ], [self.panel_state, self.dataset_name, self.user_data_name],
+            self.new_dataset_name, self.create_mode, self.file_path_url,
+            self.file_path, manager.user_name, self.dataset_type
+        ], [
+            self.panel_state, self.dataset_name, self.user_dataset_name,
+            self.sys_log
+        ],
                                        queue=False)
 
         def show_edit_panel(panel_state, data_name):
             if panel_state:
-                return (gr.Row(visible=True), gr.Row(visible=True),
-                        gr.Row(visible=True), gr.Column(visible=True),
-                        gr.Column(visible=False), gr.Column(visible=False),
-                        data_name)
+                return (gr.Row(visible=False), gr.Column(visible=True),
+                        gr.Column(visible=True),
+                        gr.Text(value=get_random_dataset_name()), 1,
+                        gr.Checkbox(value=False), data_name,
+                        self.components_name.system_log.format(''))
             else:
-                return (gr.Row(visible=False), gr.Row(visible=False),
-                        gr.Row(visible=False), gr.Column(visible=False),
-                        gr.Column(), gr.Column(), data_name)
+                return (gr.Row(visible=False), gr.Column(visible=False),
+                        gr.Column(visible=False), gr.Text(), 0,
+                        gr.Checkbox(value=False), data_name,
+                        self.components_name.system_log.format(''))
 
         self.panel_state.change(
             show_edit_panel, [self.panel_state, self.dataset_name], [
-                gallery_dataset.gallery_panel, gallery_dataset.upload_panel,
-                export_dataset.export_panel, self.modify_panel,
-                self.file_panel, self.btn_panel, self.user_data_name_state
+                self.file_panel, self.btn_panel, self.dataname_panel,
+                self.new_dataset_name, self.create_mode,
+                self.btn_create_datasets_from_file, self.user_data_name_state,
+                self.sys_log
             ],
             queue=False)
 
-        def modify_data_name(user_name, prev_data_name, login_user_name):
+        def modify_data_name(user_dataset_name, prev_data_name,
+                             login_user_name, dataset_type):
+
+            if user_dataset_name.strip(
+            ) == '' or ' ' in user_dataset_name or '/' in user_dataset_name:
+                sys_log = self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err1)
+                return prev_data_name, prev_data_name, gr.Dropdown(), sys_log
+            if len(user_dataset_name.split('-')) < 3:
+                sys_log = self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err2)
+                return prev_data_name, prev_data_name, gr.Dropdown(), sys_log
+            if '.' in user_dataset_name:
+                sys_log = self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err3)
+                return prev_data_name, prev_data_name, gr.Dropdown(), sys_log
             print(
-                f'Current file name {prev_data_name}, new file name {user_name}.'
+                f'Current file name {prev_data_name}, new file name {user_dataset_name}.'
             )
-            if user_name.strip() == '' or ' ' in user_name or '/' in user_name:
-                raise gr.Error(self.components_name.illegal_data_name_err1)
-            if len(user_name.split('-')) < 3:
-                raise gr.Error(self.components_name.illegal_data_name_err2)
-            if '.' in user_name:
-                raise gr.Error(self.components_name.illegal_data_name_err3)
-            if user_name != prev_data_name:
-                if prev_data_name in self.meta_dict:
-                    ori_meta = self.meta_dict[prev_data_name]
-                    dataset_folder = os.path.join(self.work_dir, user_name)
-                    local_dataset_folder, _ = FS.map_to_local(dataset_folder)
-                    os.makedirs(local_dataset_folder, exist_ok=True)
-                    is_flag = FS.get_dir_to_local_dir(ori_meta['work_dir'],
-                                                      local_dataset_folder,
-                                                      multi_thread=True)
-                    file_list = ori_meta['file_list']
-                    is_flag = FS.put_dir_from_local_dir(local_dataset_folder,
-                                                        dataset_folder,
-                                                        multi_thread=True)
-                    if not is_flag:
-                        raise gr.Error(self.components_name.illegal_data_err3)
-                    is_flag = FS.put_dir_from_local_dir(local_dataset_folder,
-                                                        dataset_folder,
-                                                        multi_thread=True)
-                    if not is_flag:
-                        raise gr.Error(self.components_name.illegal_data_err3)
-                    cursor = ori_meta['cursor']
-                    meta = self.construct_meta(cursor, file_list,
-                                               dataset_folder, user_name,
-                                               login_user_name)
-                    meta['local_work_dir'] = local_dataset_folder
-                    meta['work_dir'] = dataset_folder
-
-                    if prev_data_name in self.dataset_list:
-                        self.dataset_list.remove(prev_data_name)
-                        self.dataset_list.append(user_name)
-                    self.meta_dict.pop(prev_data_name)
-                    self.meta_dict[user_name] = meta
-                    _ = FS.delete_object(
-                        os.path.join(ori_meta['work_dir'], 'meta.json'))
-                    _ = FS.delete_object(
-                        os.path.join(ori_meta['local_work_dir'], 'meta.json'))
+            dataset_type = self.get_trans_dataset_type(dataset_type)
+            if user_dataset_name != prev_data_name:
+                if prev_data_name in self.dataset_dict[dataset_type]:
+                    ori_dataset_ins = self.dataset_dict[dataset_type][
+                        prev_data_name]
+                    if ori_dataset_ins.user_name == 'example':
+                        sys_log = self.components_name.system_log.format(
+                            self.components_name.modify_data_err1)
+                        return prev_data_name, prev_data_name, gr.Dropdown(
+                        ), sys_log
+                    ori_dataset_ins.modify_data_name(user_dataset_name)
+                    user_level_dataset_list = self.load_history(
+                        login_user_name)
+                    self.user_level_dataset_list.update(
+                        user_level_dataset_list)
+                    dataset_list = self.user_level_dataset_list.get(
+                        login_user_name, {}).get(dataset_type, [])
+                    if prev_data_name in dataset_list:
+                        dataset_list.remove(prev_data_name)
+                        dataset_list.append(user_dataset_name)
+                    self.user_level_dataset_list[
+                        login_user_name] = dataset_list
+                    self.dataset_dict[dataset_type].pop(prev_data_name)
+                    self.dataset_dict[dataset_type][
+                        user_dataset_name] = ori_dataset_ins
                 else:
-                    raise gr.Error(self.components_name.modify_data_name_err1)
-                return user_name, gr.Dropdown(
-                    choices=self.dataset_list,
-                    value=user_name,
-                    select_index=len(self.dataset_list) - 1)
+                    sys_log = self.components_name.system_log.format(
+                        self.components_name.modify_data_name_err1)
+                    return prev_data_name, prev_data_name, gr.Dropdown(
+                    ), sys_log
+                return user_dataset_name, user_dataset_name, gr.Dropdown(
+                    choices=dataset_list, value=user_dataset_name
+                ), self.components_name.system_log.format('')
             else:
-                return user_name, gr.Dropdown()
+                return user_dataset_name, user_dataset_name, gr.Dropdown(
+                ), self.components_name.system_log.format('')
 
-        self.modify_data_button.click(
-            modify_data_name,
-            inputs=[
-                self.user_data_name, self.user_data_name_state,
-                manager.user_name
-            ],
-            outputs=[self.user_data_name_state, self.dataset_name],
-            queue=False)
+        self.modify_data_button.click(modify_data_name,
+                                      inputs=[
+                                          self.user_dataset_name,
+                                          self.user_data_name_state,
+                                          manager.user_name, self.dataset_type
+                                      ],
+                                      outputs=[
+                                          self.user_dataset_name,
+                                          self.user_data_name_state,
+                                          self.dataset_name, self.sys_log
+                                      ],
+                                      queue=False)
 
-        def dataset_change(user_name):
-            if user_name is None or user_name == '':
-                raise gr.Error(self.components_name.illegal_data_name_err5 +
-                               f'{user_name}')
-            if user_name not in self.meta_dict:
-                raise gr.Error(self.components_name.refresh_data_list_info1)
-            return (gr.Column(visible=True), gr.Column(visible=False),
-                    gr.Checkbox(value=True, visible=False),
-                    gr.Text(value=user_name,
-                            interactive=True), gr.Text(value=user_name))
+        def dataset_change(dataset_name, dataset_type):
+            trans_dataset_type = self.get_trans_dataset_type(dataset_type)
+            if dataset_name is None or dataset_name == '':
+                sys_log = (self.components_name.system_log.format(
+                    self.components_name.illegal_data_name_err5 +
+                    f'{dataset_name}'))
+                return (gr.Row(), gr.Text(), dataset_name, gr.Text(), sys_log)
+            if trans_dataset_type not in self.dataset_dict:
+                self.dataset_dict[trans_dataset_type] = {}
+            if dataset_name not in self.dataset_dict[trans_dataset_type]:
+                sys_log = (self.components_name.system_log.format(
+                    self.components_name.refresh_data_list_info1))
+                return (gr.Row(), gr.Text(), dataset_name, gr.Text(), sys_log)
+            return (gr.Row(visible=False),
+                    gr.Text(value=dataset_name,
+                            interactive=True), dataset_name,
+                    gr.Text(value=dataset_name),
+                    self.components_name.system_log.format(''))
 
         self.dataset_name.change(dataset_change,
-                                 inputs=[self.dataset_name],
+                                 inputs=[self.dataset_name, self.dataset_type],
                                  outputs=[
-                                     self.dataset_panel, self.file_panel,
-                                     self.panel_state, self.user_data_name,
-                                     gallery_dataset.gallery_state
+                                     self.file_panel, self.user_dataset_name,
+                                     self.user_data_name_state,
+                                     gallery_dataset.gallery_state,
+                                     self.sys_log
                                  ],
                                  queue=False)
 
-        def login_user_name_change(login_user_name):
-            dataset_list = self.load_history(login_user_name=login_user_name)
+        def dataset_type_change(dataset_type, login_user_name):
+            trans_dataset_type = self.get_trans_dataset_type(dataset_type)
+            user_level_dataset_list = self.load_history(
+                login_user_name=login_user_name)
+            self.user_level_dataset_list.update(user_level_dataset_list)
+            dataset_list = user_level_dataset_list[login_user_name].get(
+                trans_dataset_type, [])
             return gr.Dropdown(
                 value=dataset_list[-1] if len(dataset_list) > 0 else '',
-                choices=dataset_list)
+                choices=dataset_list), self.components_name.system_log.format(
+                    '')
+
+        manager.user_name.change(dataset_type_change,
+                                 inputs=[self.dataset_type, manager.user_name],
+                                 outputs=[self.dataset_name, self.sys_log],
+                                 queue=False)
 
-        manager.user_name.change(login_user_name_change,
-                                 inputs=[manager.user_name],
-                                 outputs=[self.dataset_name],
+        self.dataset_type.change(dataset_type_change,
+                                 inputs=[self.dataset_type, manager.user_name],
+                                 outputs=[self.dataset_name, self.sys_log],
                                  queue=False)
+
+        self.refresh_dataset_name.click(
+            dataset_type_change,
+            inputs=[self.dataset_type, manager.user_name],
+            outputs=[self.dataset_name, self.sys_log],
+            queue=False)
```

## scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py

```diff
@@ -1,296 +1,1401 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from __future__ import annotations
 
 import os.path
+import time
 
 import gradio as gr
 import imagehash
+from PIL import Image
 
+from scepter.modules.utils.directory import get_md5
 from scepter.modules.utils.file_system import FS
 from scepter.studio.preprocess.caption_editor_ui.component_names import \
     DatasetGalleryUIName
 from scepter.studio.preprocess.caption_editor_ui.create_dataset_ui import \
     CreateDatasetUI
+from scepter.studio.preprocess.processors.processor_manager import \
+    ProcessorsManager
 from scepter.studio.utils.uibase import UIBase
 
 
 class DatasetGalleryUI(UIBase):
-    def __init__(self, cfg, is_debug=False, language='en'):
+    def __init__(self, cfg, is_debug=False, language='en', create_ins=None):
         self.selected_path = ''
         self.selected_index = -1
         self.selected_index_prev = -1
+
+        self.processors_manager = ProcessorsManager(cfg.PROCESSORS,
+                                                    language=language)
+
         self.component_names = DatasetGalleryUIName(language)
+        if create_ins is not None:
+            self.default_dataset = create_ins.example_dataset_ins
+            current_info = self.default_dataset.current_record
+            self.default_ori_caption = current_info.get('caption', '')
+            self.default_edit_caption = current_info.get('edit_caption', '')
+            self.default_image_width = current_info.get('width', -1)
+            self.default_image_height = current_info.get('height', -1)
+            self.default_image_format = os.path.splitext(
+                current_info.get('relative_path', ''))[-1]
+
+            self.default_edit_image_width = gr.Text(value=current_info.get(
+                'edit_width', current_info.get('width', -1)))
+            self.default_edit_image_height = gr.Text(value=current_info.get(
+                'edit_height', current_info.get('height', -1)))
+            self.default_edit_image_format = gr.Text(value=os.path.splitext(
+                current_info.get('edit_relative_path',
+                                 current_info.get('relative_path', '')))[-1])
 
-    def create_ui(self):
-        with gr.Row(variant='panel', visible=False,
-                    equal_height=True) as upload_panel:
-            with gr.Column():
-                self.upload_image = gr.Image(
-                    label=self.component_names.upload_image,
-                    tool='sketch',
-                    type='pil')
-            with gr.Column(min_width=80):
-                self.caption = gr.Textbox(
-                    label=self.component_names.image_caption,
-                    placeholder='',
-                    value='',
-                    lines=5)
-                self.upload_button = gr.Button(
-                    value=self.component_names.upload_image_btn)
+            self.default_select_index = self.default_dataset.cursor
+            self.default_info = f'{self.default_dataset.cursor + 1}/{len(self.default_dataset)}'
+            image_list = [
+                os.path.join(self.default_dataset.meta['local_work_dir'],
+                             v['relative_path'])
+                for v in self.default_dataset.data
+            ]
+            self.default_image_list = image_list
+        else:
+            self.default_dataset = None
+            self.default_ori_caption = ''
+            self.default_edit_caption = ''
+            self.default_image_width = -1
+            self.default_image_height = -1
+            self.default_image_format = -1
+            self.default_edit_image_width = -1
+            self.default_edit_image_height = -1
+            self.default_edit_image_format = -1
+            self.default_select_index = -1
+            self.default_image_list = []
+            self.default_info = ''
 
-        with gr.Row(visible=False, equal_height=True) as gallery_panel:
+    def create_ui(self):
+        with gr.Row(variant='panel',
+                    visible=self.default_dataset is not None,
+                    equal_height=True) as gallery_panel:
             with gr.Row(visible=False):
-                # self.gallery_state = gr.Checkbox(label='gallery_state', value=False, visible=False)
-                self.cbg_hidden_dataset_filter = gr.CheckboxGroup(
-                    label='Dataset Filter')
-                self.nb_hidden_dataset_filter_apply = gr.Number(
-                    label='Filter Apply', value=-1)
-                self.btn_hidden_set_index = gr.Button(
-                    elem_id='dataset_tag_editor_btn_hidden_set_index')
-                self.nb_hidden_image_index = gr.Number(value=None,
-                                                       label='hidden_idx_next')
-                self.nb_hidden_image_index_prev = gr.Number(
-                    value=None, label='hidden_idx_prev')
                 self.gallery_state = gr.Text(label='gallery_state',
                                              value='',
                                              visible=False)
-
-            # with gr.Row(variant='panel', equal_height=True):
-            with gr.Column(scale=1):
-                self.gl_dataset_images = gr.Gallery(
-                    label=self.component_names.dataset_images,
-                    elem_id='dataset_tag_editor_dataset_gallery',
-                    columns=4)
-            with gr.Column(scale=1):
+                self.mode_state = gr.Text(label='mode_state',
+                                          value='view',
+                                          visible=False)
+            with gr.Column(min_width=0, visible=False) as self.edit_panel:
+                with gr.Row():
+                    self.edit_image_info = gr.Markdown(
+                        self.component_names.edit_dataset.format(
+                            self.default_edit_image_height,
+                            self.default_edit_image_width,
+                            self.default_edit_image_format))
                 with gr.Row(equal_height=True):
-                    self.info = gr.Text(value='',
-                                        label=None,
-                                        show_label=False,
-                                        interactive=False)
+                    with gr.Column(scale=3, min_width=0):
+                        with gr.Row():
+                            self.edit_gl_dataset_images = gr.Gallery(
+                                label=self.component_names.dataset_images,
+                                elem_id='dataset_tag_editor_dataset_gallery',
+                                value=self.default_image_list,
+                                selected_index=self.default_select_index,
+                                columns=4,
+                                visible=False,
+                                interactive=False)
+                    with gr.Column(ariant='panel', scale=2, min_width=0):
+                        with gr.Row():
+                            self.edit_caption = gr.Textbox(
+                                label=self.component_names.edit_caption,
+                                placeholder='',
+                                value=self.default_edit_caption,
+                                lines=18,
+                                autoscroll=False,
+                                interactive=True,
+                                visible=False)
+
+            with gr.Column(min_width=0):
+                with gr.Row():
+                    self.image_info = gr.Markdown(
+                        self.component_names.ori_dataset.format(
+                            self.default_image_height,
+                            self.default_image_width,
+                            self.default_image_format))
                 with gr.Row(equal_height=True):
-                    with gr.Column(scale=1, min_width=0):
-                        self.ori_caption = gr.Textbox(
-                            label=self.component_names.ori_caption,
-                            placeholder='',
-                            value='',
-                            lines=10,
-                            autoscroll=False,
-                            interactive=False)
-                    with gr.Column(scale=1, min_width=0):
-                        self.edit_caption = gr.Textbox(
-                            label=self.component_names.edit_caption,
-                            placeholder='',
-                            value='',
-                            lines=10,
-                            autoscroll=False,
+                    with gr.Column(scale=3, min_width=0):
+                        with gr.Row():
+                            self.gl_dataset_images = gr.Gallery(
+                                label=self.component_names.dataset_images,
+                                elem_id='dataset_tag_editor_dataset_gallery',
+                                value=self.default_image_list,
+                                selected_index=self.default_select_index,
+                                columns=4)
+                    with gr.Column(variant='panel', scale=2, min_width=0):
+                        with gr.Row():
+                            self.ori_caption = gr.Textbox(
+                                label=self.component_names.ori_caption,
+                                placeholder='',
+                                value=self.default_ori_caption,
+                                lines=18,
+                                autoscroll=False,
+                                interactive=False)
+
+        # with gr.Row(visible=False) as :
+        with gr.Row(visible=False) as self.edit_setting_panel:
+            self.sys_log = gr.Markdown(
+                self.component_names.system_log.format(''))
+        with gr.Row():
+            with gr.Column(variant='panel',
+                           visible=False,
+                           scale=1,
+                           min_width=0) as self.edit_confirm_panel:
+                with gr.Box():
+                    with gr.Row():
+                        gr.Markdown(self.component_names.confirm_direction)
+                    with gr.Row():
+                        self.range_mode = gr.Dropdown(
+                            label=self.component_names.set_range_name,
+                            choices=self.component_names.range_mode_name,
+                            value=self.component_names.range_mode_name[0])
+                    with gr.Row():
+                        self.data_range = gr.Dropdown(
+                            show_label=True,
+                            label=self.component_names.
+                            samples_range_placeholder,
+                            choices=[],
+                            allow_custom_value=True,
+                            multiselect=True,
+                            visible=False,
                             interactive=True)
-                with gr.Row(equal_height=True):
-                    self.modify_button = gr.Button(
-                        value=self.component_names.btn_modify)
-                with gr.Row(equal_height=True):
-                    self.delete_button = gr.Button(
-                        value=self.component_names.btn_delete)
+                    with gr.Row():
+                        with gr.Column(scale=1, min_width=0):
+                            self.btn_confirm_edit = gr.Button(
+                                value=self.component_names.btn_confirm_edit)
+                        with gr.Column(scale=1, min_width=0):
+                            self.btn_cancel_edit = gr.Button(
+                                value=self.component_names.btn_cancel_edit)
+                        with gr.Column(scale=1, min_width=0):
+                            self.btn_reset_edit = gr.Button(
+                                value=self.component_names.btn_reset_edit)
+                    with gr.Row():
+                        self.preprocess_checkbox = gr.CheckboxGroup(
+                            show_label=False,
+                            choices=self.component_names.preprocess_choices,
+                            value=None)
+            with gr.Column(variant='panel',
+                           visible=False,
+                           scale=1,
+                           min_width=0) as self.upload_panel:
+                with gr.Row():
+                    self.upload_image = gr.Image(
+                        label=self.component_names.upload_image, type='pil')
+                with gr.Row():
+                    self.upload_image_info = gr.Markdown(value='')
+                with gr.Row():
+                    self.caption = gr.Textbox(
+                        label=self.component_names.image_caption,
+                        placeholder='',
+                        value='',
+                        lines=5)
+                with gr.Row():
+                    with gr.Column(min_width=0):
+                        self.upload_button = gr.Button(
+                            value=self.component_names.upload_image_btn)
+                    with gr.Column(min_width=0):
+                        self.cancel_button = gr.Button(
+                            value=self.component_names.cancel_upload_btn)
+            with gr.Column(variant='panel',
+                           visible=False,
+                           scale=2,
+                           min_width=0) as self.image_preprocess_panel:
+                with gr.Box():
+                    with gr.Column(variant='panel', min_width=0):
+                        with gr.Row():
+                            self.image_preprocess_method = gr.Dropdown(
+                                label=self.component_names.
+                                image_processor_type,
+                                choices=self.processors_manager.get_choices(
+                                    'image'),
+                                value=self.processors_manager.get_default(
+                                    'image'),
+                                interactive=True)
+                        image_processor_ins = self.processors_manager.get_processor(
+                            'image',
+                            self.processors_manager.get_default('image'))
+                        with gr.Row():
+                            default_height_ratio = image_processor_ins.system_para.get(
+                                'HEIGHT_RATIO', {})
+                            self.height_ratio = gr.Slider(
+                                label=self.component_names.height_ratio,
+                                minimum=default_height_ratio.get('MIN', 1),
+                                maximum=default_height_ratio.get('MAX', 10),
+                                step=default_height_ratio.get('STEP', 1),
+                                value=default_height_ratio.get('VALUE', 1),
+                                visible=len(default_height_ratio) > 0,
+                                interactive=True)
+                        with gr.Row():
+                            default_width_ratio = image_processor_ins.system_para.get(
+                                'WIDTH_RATIO', {})
+                            self.width_ratio = gr.Slider(
+                                label=self.component_names.width_ratio,
+                                minimum=default_width_ratio.get('MIN', 1),
+                                maximum=default_width_ratio.get('MAX', 10),
+                                step=default_width_ratio.get('STEP', 1),
+                                value=default_width_ratio.get('VALUE', 1),
+                                visible=len(default_width_ratio) > 0,
+                                interactive=True)
+                        with gr.Row():
+                            self.image_preprocess_btn = gr.Button(
+                                value=self.component_names.image_preprocess_btn
+                            )
+            with gr.Column(variant='panel',
+                           visible=False,
+                           scale=2,
+                           min_width=0) as self.caption_preprocess_panel:
+                with gr.Box():
+                    with gr.Column(variant='panel', min_width=0):
+                        with gr.Row():
+                            self.caption_preprocess_method = gr.Dropdown(
+                                label=self.component_names.
+                                caption_processor_type,
+                                choices=self.processors_manager.get_choices(
+                                    'caption'),
+                                value=self.processors_manager.get_default(
+                                    'caption'),
+                                interactive=True)
+                        with gr.Row():
+                            with gr.Column(scale=1, min_width=0):
+                                self.caption_use_device = gr.Text(
+                                    label=self.component_names.used_device,
+                                    value=self.processors_manager.
+                                    get_default_device('caption'),
+                                    interactive=False)
+                            with gr.Column(scale=1, min_width=0):
+                                self.caption_use_memory = gr.Text(
+                                    label=self.component_names.used_memory,
+                                    value=self.processors_manager.
+                                    get_default_memory('caption'),
+                                    interactive=False)
+                            default_processor_method = self.processors_manager.get_default(
+                                'caption')
+                            default_processor_ins = self.processors_manager.get_processor(
+                                'caption', default_processor_method)
+                            with gr.Column(scale=1, min_width=0):
+                                self.caption_language = gr.Dropdown(
+                                    label=self.component_names.
+                                    caption_language,
+                                    choices=default_processor_ins.
+                                    get_language_choice,
+                                    value=default_processor_ins.
+                                    get_language_default)
+                            with gr.Column(scale=1, min_width=0):
+                                self.caption_update_mode = gr.Dropdown(
+                                    label=self.component_names.
+                                    caption_update_mode,
+                                    choices=self.component_names.
+                                    caption_update_choices,
+                                    value=self.component_names.
+                                    caption_update_choices[0] if
+                                    len(self.component_names.
+                                        caption_update_choices) > 0 else None)
+                        with gr.Accordion(
+                                label=self.component_names.advance_setting,
+                                open=False):
+                            with gr.Row():
+                                self.sys_prompt = gr.Text(
+                                    label=self.component_names.system_prompt,
+                                    interactive=True,
+                                    value=default_processor_ins.
+                                    get_para_by_language(
+                                        default_processor_ins.
+                                        get_language_default).get(
+                                            'PROMPT', ''),
+                                    lines=2,
+                                    visible='PROMPT' in
+                                    default_processor_ins.get_para_by_language(
+                                        default_processor_ins.
+                                        get_language_default))
+                            with gr.Row():
+                                default_max_new_tokens = default_processor_ins.get_para_by_language(
+                                    default_processor_ins.get_language_default
+                                ).get('MAX_NEW_TOKENS', {})
+                                self.max_new_tokens = gr.Slider(
+                                    label=self.component_names.max_new_tokens,
+                                    minimum=default_max_new_tokens.get(
+                                        'MIN', 0),
+                                    maximum=default_max_new_tokens.get(
+                                        'MAX', 1.0),
+                                    step=default_max_new_tokens.get(
+                                        'STEP', 0.1),
+                                    value=default_max_new_tokens.get(
+                                        'VALUE', 0.1),
+                                    visible=len(default_max_new_tokens) > 0,
+                                    interactive=True)
+                            with gr.Row():
+                                default_min_new_tokens = default_processor_ins.get_para_by_language(
+                                    default_processor_ins.get_language_default
+                                ).get('MIN_NEW_TOKENS', {})
+
+                                self.min_new_tokens = gr.Slider(
+                                    label=self.component_names.min_new_tokens,
+                                    minimum=default_min_new_tokens.get(
+                                        'MIN', 0),
+                                    maximum=default_min_new_tokens.get(
+                                        'MAX', 1.0),
+                                    step=default_min_new_tokens.get(
+                                        'STEP', 0.1),
+                                    value=default_min_new_tokens.get(
+                                        'VALUE', 0.1),
+                                    visible=len(default_min_new_tokens) > 0,
+                                    interactive=True)
+                            with gr.Row():
+                                default_num_beams = default_processor_ins.get_para_by_language(
+                                    default_processor_ins.get_language_default
+                                ).get('NUM_BEAMS', {})
+
+                                self.num_beams = gr.Slider(
+                                    label=self.component_names.num_beams,
+                                    minimum=default_num_beams.get('MIN', 0),
+                                    maximum=default_num_beams.get('MAX', 1.0),
+                                    step=default_num_beams.get('STEP', 0.1),
+                                    value=default_num_beams.get('VALUE', 0.1),
+                                    visible=len(default_num_beams) > 0,
+                                    interactive=True)
+                            with gr.Row():
+                                default_repetition_penalty = default_processor_ins.get_para_by_language(
+                                    default_processor_ins.get_language_default
+                                ).get('REPETITION_PENALTY', {})
+
+                                self.repetition_penalty = gr.Slider(
+                                    label=self.component_names.
+                                    repetition_penalty,
+                                    minimum=default_repetition_penalty.get(
+                                        'MIN', 0),
+                                    maximum=default_repetition_penalty.get(
+                                        'MAX', 1.0),
+                                    step=default_repetition_penalty.get(
+                                        'STEP', 0.1),
+                                    value=default_repetition_penalty.get(
+                                        'VALUE', 0.1),
+                                    visible=len(default_repetition_penalty) >
+                                    0,
+                                    interactive=True)
+                            with gr.Row():
+                                default_temperature = default_processor_ins.get_para_by_language(
+                                    default_processor_ins.get_language_default
+                                ).get('TEMPERATURE', {})
+
+                                self.temperature = gr.Slider(
+                                    label=self.component_names.temperature,
+                                    minimum=default_temperature.get('MIN', 0),
+                                    maximum=default_temperature.get(
+                                        'MAX', 1.0),
+                                    step=default_temperature.get('STEP', 0.1),
+                                    value=default_temperature.get(
+                                        'VALUE', 0.1),
+                                    visible=len(default_temperature) > 0,
+                                    interactive=True)
+
+                        with gr.Row():
+
+                            with gr.Column(scale=1, min_width=0):
+                                self.caption_preprocess_btn = gr.Button(
+                                    value=self.component_names.
+                                    caption_preprocess_btn)
+
+        with gr.Row(variant='panel', equal_height=True):
+            with gr.Column(scale=1, min_width=0):
+                self.info = gr.Text(value=self.default_info,
+                                    label=None,
+                                    show_label=False,
+                                    container=False,
+                                    interactive=False)
+            with gr.Column(scale=1, min_width=0):
+                self.modify_button = gr.Button(
+                    value=self.component_names.btn_modify)
+            with gr.Column(scale=1, min_width=0):
+                self.add_button = gr.Button(value=self.component_names.btn_add)
+            with gr.Column(scale=1, min_width=0):
+                self.delete_button = gr.Button(
+                    value=self.component_names.btn_delete)
 
-        self.upload_panel = upload_panel
         self.gallery_panel = gallery_panel
 
-    def set_callbacks(self, create_dataset: CreateDatasetUI):
-        def change_gallery(dataset_name):
-            meta_data = create_dataset.meta_dict[dataset_name]
-            if len(meta_data['file_list']) > 0:
-                cursor = create_dataset.meta_dict[dataset_name]['cursor']
-            else:
-                cursor = -1
+    def set_callbacks(self, create_dataset: CreateDatasetUI, manager):
+        def range_state_trans(range_mode):
+            reverse_mode = {
+                v: id
+                for id, v in enumerate(self.component_names.range_mode_name)
+            }
+            hit_range_mode = reverse_mode[range_mode]
+            return hit_range_mode
+
+        def change_gallery(dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            cursor = dataset_ins.cursor
             image_list = [
-                os.path.join(meta_data['local_work_dir'], v['relative_path'])
-                for v in meta_data['file_list']
+                os.path.join(dataset_ins.meta['local_work_dir'],
+                             v['relative_path']) for v in dataset_ins.data
             ]
             if cursor >= 0:
                 return gr.Gallery(label=dataset_name,
                                   value=image_list,
                                   selected_index=cursor)
             else:
                 return gr.Gallery(label=dataset_name,
                                   value=image_list,
                                   selected_index=None)
 
-        self.gallery_state.change(change_gallery,
-                                  inputs=[create_dataset.user_data_name],
-                                  outputs=[self.gl_dataset_images],
-                                  queue=False)
-
-        def select_image(dataset_name, evt: gr.SelectData):
-            meta_data = create_dataset.meta_dict[dataset_name]
-            if len(meta_data['file_list']) > 0:
-                current_info = meta_data['file_list'][evt.index]
-                create_dataset.meta_dict[dataset_name]['cursor'] = evt.index
-                cursor = evt.index
+        self.gallery_state.change(
+            change_gallery,
+            inputs=[create_dataset.dataset_type, create_dataset.dataset_name],
+            outputs=[self.gl_dataset_images],
+            queue=False)
+
+        def select_image(dataset_type, dataset_name, evt: gr.SelectData):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            dataset_ins.set_cursor(evt.index)
+            current_info = dataset_ins.current_record
+            all_number = len(dataset_ins)
+
+            ret_image_gl = gr.Gallery(selected_index=dataset_ins.cursor) if dataset_ins.cursor >= 0 \
+                else gr.Gallery(value=[], selected_index=None)
+            ret_caption = gr.Textbox(value=current_info.get('caption', ''))
+            ret_info = gr.Text(value=f'{dataset_ins.cursor+1}/{all_number}')
+
+            ret_image_height = current_info.get('height', -1)
+            ret_image_width = current_info.get('width', -1)
+            ret_image_format = os.path.splitext(
+                current_info.get('relative_path', ''))[-1]
+
+            image_info = self.component_names.ori_dataset.format(
+                ret_image_height, ret_image_width, ret_image_format)
+
+            return (ret_image_gl, ret_caption, image_info, ret_info)
+
+        self.gl_dataset_images.select(
+            select_image,
+            inputs=[create_dataset.dataset_type, create_dataset.dataset_name],
+            outputs=[
+                self.gl_dataset_images, self.ori_caption, self.image_info,
+                self.info
+            ],
+            queue=False)
+
+        def edit_select_image(dataset_type, dataset_name, mode_state,
+                              evt: gr.SelectData):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+
+            cursor = dataset_ins.cursor_from_edit_index(evt.index)
+            dataset_ins.set_cursor(cursor)
+            current_info = dataset_ins.current_record
+            all_number = len(dataset_ins)
+
+            ret_image_gl = gr.Gallery(selected_index=dataset_ins.cursor) if dataset_ins.cursor >= 0 \
+                else gr.Gallery(value=[], selected_index=None)
+            dataset_ins.edit_cursor = evt.index
+            ret_edit_image_gl = gr.Gallery(selected_index=evt.index)
+            ret_edit_caption = gr.Textbox(value=current_info.get(
+                'edit_caption', ''),
+                                          visible=True)
+
+            ret_edit_image_width = current_info.get(
+                'edit_width', current_info.get('width', -1))
+            ret_edit_image_height = current_info.get(
+                'edit_height', current_info.get('height', -1))
+            ret_edit_image_format = os.path.splitext(
+                current_info.get('edit_relative_path',
+                                 current_info.get('relative_path', '')))[-1]
+
+            edit_image_info = self.component_names.edit_dataset.format(
+                ret_edit_image_height, ret_edit_image_width,
+                ret_edit_image_format)
+
+            ret_info = gr.Text(value=f'{dataset_ins.cursor+1}/{all_number}')
+            return (ret_image_gl, ret_edit_image_gl, ret_edit_caption,
+                    edit_image_info, ret_info)
+
+        self.edit_gl_dataset_images.select(edit_select_image,
+                                           inputs=[
+                                               create_dataset.dataset_type,
+                                               create_dataset.dataset_name,
+                                               self.mode_state
+                                           ],
+                                           outputs=[
+                                               self.gl_dataset_images,
+                                               self.edit_gl_dataset_images,
+                                               self.edit_caption,
+                                               self.edit_image_info, self.info
+                                           ],
+                                           queue=False)
+
+        def change_gallery(dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            current_info = dataset_ins.current_record
+            ret_image_height = current_info.get('height', -1)
+            ret_image_width = current_info.get('width', -1)
+            ret_image_format = os.path.splitext(
+                current_info.get('relative_path', ''))[-1]
+
+            image_info = self.component_names.ori_dataset.format(
+                ret_image_height, ret_image_width, ret_image_format)
+
+            ret_edit_image_width = current_info.get(
+                'edit_width', current_info.get('width', -1))
+            ret_edit_image_height = current_info.get(
+                'edit_height', current_info.get('height', -1))
+            ret_edit_image_format = os.path.splitext(
+                current_info.get('edit_relative_path',
+                                 current_info.get('relative_path', '')))[-1]
+
+            edit_image_info = self.component_names.edit_dataset.format(
+                ret_edit_image_height, ret_edit_image_width,
+                ret_edit_image_format)
+
+            all_number = len(dataset_ins)
+            return (gr.Textbox(value=current_info.get('caption', '')),
+                    image_info,
+                    gr.Textbox(value=current_info.get('edit_caption', '')),
+                    edit_image_info,
+                    gr.Text(value=f'{dataset_ins.cursor+1}/{all_number}'))
+
+        self.gl_dataset_images.change(
+            change_gallery,
+            inputs=[create_dataset.dataset_type, create_dataset.dataset_name],
+            outputs=[
+                self.ori_caption, self.image_info, self.edit_caption,
+                self.edit_image_info, self.info
+            ],
+            queue=False)
+
+        def edit_mode():
+            return gr.Text(value='edit')
+
+        self.modify_button.click(edit_mode,
+                                 inputs=[],
+                                 outputs=[self.mode_state],
+                                 queue=False)
+
+        def view_mode():
+            return gr.Text(value='view')
+
+        self.btn_cancel_edit.click(view_mode,
+                                   inputs=[],
+                                   outputs=[self.mode_state],
+                                   queue=False)
+
+        def reset_edit(dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            for idx, one_data in enumerate(dataset_ins.edit_samples):
+                current_cursor = dataset_ins.cursor_from_edit_index(idx)
+                dataset_ins.edit_samples[idx][
+                    'edit_relative_path'] = dataset_ins.data[current_cursor][
+                        'relative_path']
+                dataset_ins.edit_samples[idx][
+                    'edit_image_path'] = dataset_ins.data[current_cursor][
+                        'image_path']
+                dataset_ins.edit_samples[idx][
+                    'edit_caption'] = dataset_ins.data[current_cursor][
+                        'caption']
+                dataset_ins.edit_samples[idx]['edit_width'] = dataset_ins.data[
+                    current_cursor]['width']
+                dataset_ins.edit_samples[idx][
+                    'edit_height'] = dataset_ins.data[current_cursor]['height']
+
+            image_list = [
+                os.path.join(dataset_ins.meta['local_work_dir'],
+                             v.get('edit_relative_path', v['relative_path']))
+                for v in dataset_ins.edit_samples
+            ]
+            current_info = dataset_ins.current_record
+
+            ret_edit_image_width = current_info.get(
+                'edit_width', current_info.get('width', -1))
+            ret_edit_image_height = current_info.get(
+                'edit_height', current_info.get('height', -1))
+            ret_edit_image_format = os.path.splitext(
+                current_info.get('edit_relative_path',
+                                 current_info.get('relative_path', '')))[-1]
+
+            edit_image_info = self.component_names.edit_dataset.format(
+                ret_edit_image_height, ret_edit_image_width,
+                ret_edit_image_format)
+
+            return (gr.Gallery(value=image_list, visible=True),
+                    gr.Textbox(value=current_info.get('edit_caption', '')),
+                    edit_image_info)
+
+        self.btn_reset_edit.click(
+            reset_edit,
+            inputs=[create_dataset.dataset_type, create_dataset.dataset_name],
+            outputs=[
+                self.edit_gl_dataset_images, self.edit_caption,
+                self.edit_image_info
+            ],
+            queue=False)
+
+        def confirm_edit(dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            is_flg, msg = dataset_ins.apply_changes()
+            if not is_flg:
+                return (gr.Gallery(), gr.Textbox(), gr.Text(), gr.Text(),
+                        gr.Text(), self.component_names.system_log.format(msg),
+                        gr.Text())
+            image_list = [
+                os.path.join(dataset_ins.local_work_dir, v['relative_path'])
+                for v in dataset_ins.data
+            ]
+            current_record = dataset_ins.current_record
+            ret_image_height = current_record.get('height', -1)
+            ret_image_width = current_record.get('width', -1)
+            ret_image_format = os.path.splitext(
+                current_record.get('relative_path', ''))[-1]
+
+            image_info = self.component_names.ori_dataset.format(
+                ret_image_height, ret_image_width, ret_image_format)
+            return (gr.Gallery(value=image_list,
+                               selected_index=dataset_ins.cursor),
+                    gr.Textbox(value=current_record.get('caption', '')),
+                    image_info, self.component_names.system_log.format(''),
+                    gr.Text(value='view'))
+
+        self.btn_confirm_edit.click(
+            confirm_edit,
+            inputs=[create_dataset.dataset_type, create_dataset.dataset_name],
+            outputs=[
+                self.gl_dataset_images, self.ori_caption, self.image_info,
+                self.sys_log, self.mode_state
+            ],
+            queue=False)
+
+        def preprocess_box_change(preprocess_checkbox):
+            image_proc_status, caption_proc_status = False, False
+            reverse_status = {
+                v: id
+                for id, v in enumerate(self.component_names.preprocess_choices)
+            }
+            for value in preprocess_checkbox:
+                hit_status = reverse_status[value]
+                if hit_status == 0:
+                    image_proc_status = True
+                elif hit_status == 1:
+                    caption_proc_status = True
+            return gr.Column(visible=image_proc_status), gr.Column(
+                visible=caption_proc_status)
+
+        self.preprocess_checkbox.change(preprocess_box_change,
+                                        inputs=[self.preprocess_checkbox],
+                                        outputs=[
+                                            self.image_preprocess_panel,
+                                            self.caption_preprocess_panel
+                                        ],
+                                        queue=False)
+
+        def preprocess_image(mode_state, preprocess_method, upload_image,
+                             upload_caption, height_ratio, width_ratio,
+                             dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            if hasattr(manager, 'inference'):
+                for k, v in manager.inference.pipe_manager.pipeline_level_modules.items(
+                ):
+                    if hasattr(v, 'dynamic_unload'):
+                        v.dynamic_unload(name='all')
+            processor_ins = self.processors_manager.get_processor(
+                'image', preprocess_method)
+            if processor_ins is None:
+                sys_log = 'Current processor is illegal'
+                return (gr.Gallery(), '', gr.Image(), '',
+                        self.component_names.system_log.format(sys_log))
+            is_flag, msg = processor_ins.load_model()
+            if not is_flag:
+                sys_log = f'Load processor failed: {msg}'
+                return (gr.Gallery(), '', gr.Image(), '',
+                        self.component_names.system_log.format(sys_log))
+            if mode_state == 'edit':
+                edit_index_list = dataset_ins.edit_list
+                for index in edit_index_list:
+                    one_data = dataset_ins.data[index]
+                    relative_image_path = one_data.get(
+                        'edit_relative_path', one_data['relative_path'])
+                    file_name, surfix = os.path.splitext(relative_image_path)
+                    src_image = os.path.join(
+                        dataset_ins.meta['local_work_dir'],
+                        relative_image_path)
+                    target_image = processor_ins(
+                        Image.open(src_image).convert('RGB'),
+                        height_ratio=height_ratio,
+                        width_ratio=width_ratio)
+                    now_time = time.time()
+                    save_folders = 'edit_images'
+                    os.makedirs(os.path.join(
+                        dataset_ins.meta['local_work_dir'], save_folders),
+                                exist_ok=True)
+                    save_file_name = f'{os.path.basename(file_name)}_{now_time}'
+                    save_relative_image_path = os.path.join(
+                        save_folders, f'{get_md5(save_file_name)}{surfix}')
+                    save_image_path = os.path.join(
+                        dataset_ins.meta['work_dir'], save_relative_image_path)
+                    local_save_image_math = os.path.join(
+                        dataset_ins.meta['local_work_dir'],
+                        save_relative_image_path)
+                    target_image.save(local_save_image_math)
+                    FS.put_object_from_local_file(local_save_image_math,
+                                                  save_image_path)
+                    dataset_ins.data[index][
+                        'edit_relative_path'] = save_relative_image_path
+                    dataset_ins.data[index][
+                        'edit_image_path'] = save_image_path
+                    dataset_ins.data[index]['edit_width'] = target_image.size[
+                        0]
+                    dataset_ins.data[index]['edit_height'] = target_image.size[
+                        1]
+                dataset_ins.update_dataset()
+                image_list = [
+                    os.path.join(
+                        dataset_ins.meta['local_work_dir'],
+                        v.get('edit_relative_path', v['relative_path']))
+                    for v in dataset_ins.edit_samples
+                ]
+                if len(edit_index_list) > 0:
+                    current_info = dataset_ins.data[edit_index_list[
+                        dataset_ins.edit_cursor]]
+
+                    ret_edit_image_width = current_info.get(
+                        'edit_width', current_info.get('width', -1))
+                    ret_edit_image_height = current_info.get(
+                        'edit_height', current_info.get('height', -1))
+                    ret_edit_image_format = os.path.splitext(
+                        current_info.get('edit_relative_path',
+                                         current_info.get('relative_path',
+                                                          '')))[-1]
+
+                    edit_image_info = self.component_names.edit_dataset.format(
+                        ret_edit_image_height, ret_edit_image_width,
+                        ret_edit_image_format)
+
+                else:
+                    edit_image_info = ''
+
+                ret_image_gallery = gr.Gallery(value=image_list)
+                ret_upload_image = gr.Image()
+                ret_upload_image_info = ''
+            elif mode_state == 'add':
+                if isinstance(upload_image, dict):
+                    image = upload_image['image']
+                else:
+                    image = upload_image
+                target_image = processor_ins(image.convert('RGB'),
+                                             height_ratio=height_ratio,
+                                             width_ratio=width_ratio)
+                w, h = target_image.size
+                ret_image_gallery = gr.Gallery()
+                ret_upload_image = gr.Image(target_image)
+                ret_upload_image_info = self.component_names.upload_image_info.format(
+                    h, w)
+                edit_image_info = ''
             else:
-                current_info = {'caption': ''}
-                cursor = -1
+                ret_image_gallery = gr.Gallery()
+                ret_upload_image = gr.Image()
+                ret_upload_image_info = ''
+                edit_image_info = ''
+
+            is_flag, msg = processor_ins.unload_model()
+            if not is_flag:
+                sys_log = f'Unoad processor failed: {msg}'
+                return (ret_image_gallery, edit_image_info, ret_upload_image,
+                        ret_upload_image_info,
+                        self.component_names.system_log.format(sys_log))
+            return (ret_image_gallery, edit_image_info, ret_upload_image,
+                    ret_upload_image_info,
+                    self.component_names.system_log.format(''))
+
+        self.image_preprocess_btn.click(
+            preprocess_image,
+            inputs=[
+                self.mode_state, self.image_preprocess_method,
+                self.upload_image, self.caption, self.height_ratio,
+                self.width_ratio, create_dataset.dataset_type,
+                create_dataset.dataset_name
+            ],
+            outputs=[
+                self.edit_gl_dataset_images, self.edit_image_info,
+                self.upload_image, self.upload_image_info, self.sys_log
+            ],
+            queue=False)
 
-            all_number = len(meta_data['file_list'])
-            if cursor >= 0:
-                return (gr.Gallery(selected_index=cursor),
-                        gr.Textbox(value=current_info['caption']),
-                        gr.Textbox(value=current_info['edit_caption']),
-                        gr.Text(value=f'{cursor+1}/{all_number}'))
+        def image_preprocess_method_change(image_preprocess_method):
+            image_processor_ins = self.processors_manager.get_processor(
+                'image', image_preprocess_method)
+            height_ratio = image_processor_ins.system_para.get(
+                'HEIGHT_RATIO', {})
+            ret_height_ratio = gr.Slider(minimum=height_ratio.get('MIN', 1),
+                                         maximum=height_ratio.get('MAX', 10),
+                                         step=height_ratio.get('STEP', 1),
+                                         value=height_ratio.get('VALUE', 1),
+                                         visible=len(height_ratio) > 0,
+                                         interactive=True)
+            width_ratio = image_processor_ins.system_para.get(
+                'WIDTH_RATIO', {})
+            ret_width_ratio = gr.Slider(minimum=width_ratio.get('MIN', 1),
+                                        maximum=width_ratio.get('MAX', 10),
+                                        step=width_ratio.get('STEP', 1),
+                                        value=width_ratio.get('VALUE', 1),
+                                        visible=len(width_ratio) > 0,
+                                        interactive=True)
+            return (ret_height_ratio, ret_width_ratio)
+
+        self.image_preprocess_method.change(
+            image_preprocess_method_change,
+            inputs=[self.image_preprocess_method],
+            outputs=[self.height_ratio, self.width_ratio],
+            queue=False)
+
+        def caption_preprocess_method_change(caption_preprocess_method):
+            processor_ins = self.processors_manager.get_processor(
+                'caption', caption_preprocess_method)
+            if processor_ins is None:
+                return (gr.Text(), gr.Text(), gr.Dropdown(),
+                        self.component_names.system_log.format(
+                            'Load processor failed, processor is None.'))
+            language_choice = processor_ins.get_language_choice
+            language_default = processor_ins.get_language_default
+            return (gr.Text(value=processor_ins.use_device),
+                    gr.Text(value=f'{processor_ins.use_memory}M'),
+                    gr.Dropdown(choices=language_choice,
+                                value=language_default),
+                    self.component_names.system_log.format(''))
+
+        self.caption_preprocess_method.change(
+            caption_preprocess_method_change,
+            inputs=[self.caption_preprocess_method],
+            outputs=[
+                self.caption_use_device, self.caption_use_memory,
+                self.caption_language
+            ],
+            queue=False)
+
+        def caption_language_change(caption_language,
+                                    caption_preprocess_method):
+            processor_ins = self.processors_manager.get_processor(
+                'caption', caption_preprocess_method)
+            para = processor_ins.get_para_by_language(caption_language)
+            system_prompt = para.get('PROMPT', '')
+            ret_system_prompt = gr.Text(value=system_prompt,
+                                        visible='PROMPT' in para)
+
+            max_new_tokens = para.get('MAX_NEW_TOKENS', {})
+            ret_max_new_tokens = gr.Slider(
+                minimum=max_new_tokens.get('MIN', 0),
+                maximum=max_new_tokens.get('MAX', 1.0),
+                step=max_new_tokens.get('STEP', 0.1),
+                value=max_new_tokens.get('VALUE', 0.1),
+                visible=len(max_new_tokens) > 0)
+            min_new_tokens = para.get('MIN_NEW_TOKENS', {})
+            ret_min_new_tokens = gr.Slider(
+                minimum=min_new_tokens.get('MIN', 0),
+                maximum=min_new_tokens.get('MAX', 1.0),
+                step=min_new_tokens.get('STEP', 0.1),
+                value=min_new_tokens.get('VALUE', 0.1),
+                visible=len(min_new_tokens) > 0)
+
+            num_beams = para.get('NUM_BEAMS', {})
+            ret_num_beams = gr.Slider(minimum=num_beams.get('MIN', 0),
+                                      maximum=num_beams.get('MAX', 1.0),
+                                      step=num_beams.get('STEP', 0.1),
+                                      value=num_beams.get('VALUE', 0.1),
+                                      visible=len(num_beams) > 0)
+
+            repetition_penalty = para.get('REPETITION_PENALTY', {})
+            ret_repetition_penalty = gr.Slider(
+                minimum=repetition_penalty.get('MIN', 0),
+                maximum=repetition_penalty.get('MAX', 1.0),
+                step=repetition_penalty.get('STEP', 0.1),
+                value=repetition_penalty.get('VALUE', 0.1),
+                visible=len(repetition_penalty) > 0)
+
+            temperature = para.get('TEMPERATURE', {})
+            ret_temperature = gr.Slider(minimum=temperature.get('MIN', 0),
+                                        maximum=temperature.get('MAX', 1.0),
+                                        step=temperature.get('STEP', 0.1),
+                                        value=temperature.get('VALUE', 0.1),
+                                        visible=len(temperature) > 0)
+
+            return (ret_system_prompt, ret_max_new_tokens, ret_min_new_tokens,
+                    ret_num_beams, ret_repetition_penalty, ret_temperature)
+
+        self.caption_language.change(
+            caption_language_change,
+            inputs=[self.caption_language, self.caption_preprocess_method],
+            outputs=[
+                self.sys_prompt, self.max_new_tokens, self.min_new_tokens,
+                self.num_beams, self.repetition_penalty, self.temperature
+            ],
+            queue=False)
+
+        def preprocess_caption(mode_state, preprocess_method, sys_prompt,
+                               max_new_tokens, min_new_tokens, num_beams,
+                               repetition_penalty, temperature,
+                               caption_update_mode, upload_image,
+                               upload_caption, dataset_type, dataset_name):
+
+            reverse_update_mode = {
+                v: idx
+                for idx, v in enumerate(
+                    self.component_names.caption_update_choices)
+            }
+
+            update_mode = reverse_update_mode.get(caption_update_mode, -1)
+
+            processor_ins = self.processors_manager.get_processor(
+                'caption', preprocess_method)
+            if processor_ins is None:
+                sys_log = 'Current processor is illegal'
+                return gr.Textbox(), gr.Textbox(
+                ), self.component_names.system_log.format(sys_log)
+
+            is_flag, msg = processor_ins.load_model()
+            if not is_flag:
+                sys_log = f'Load processor failed: {msg}'
+                return gr.Textbox(), gr.Textbox(
+                ), self.component_names.system_log.format(sys_log)
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            if mode_state == 'edit':
+                edit_index_list = dataset_ins.edit_list
+                for index in edit_index_list:
+                    one_data = dataset_ins.data[index]
+                    relative_image_path = one_data.get(
+                        'edit_relative_path', one_data['relative_path'])
+                    src_image = os.path.join(
+                        dataset_ins.meta['local_work_dir'],
+                        relative_image_path)
+                    response = processor_ins(
+                        src_image,
+                        prompt=sys_prompt,
+                        max_new_tokens=max_new_tokens,
+                        min_new_tokens=min_new_tokens,
+                        num_beams=num_beams,
+                        repetition_penalty=repetition_penalty,
+                        temperature=temperature)
+                    if update_mode == 0:
+                        dataset_ins.data[index][
+                            'edit_caption'] += ';' + response
+                    elif update_mode == 1:
+                        dataset_ins.data[index]['edit_caption'] = response
+
+                dataset_ins.update_dataset()
+                current_info = dataset_ins.current_record
+                ret_edit_caption = gr.Textbox(value=current_info.get(
+                    'edit_caption', ''),
+                                              visible=True)
+                ret_upload_caption = gr.Textbox()
+            elif mode_state == 'add':
+                save_folder = os.path.join(dataset_ins.local_work_dir, 'cache')
+                os.makedirs(save_folder, exist_ok=True)
+                if isinstance(upload_image, dict):
+                    image = upload_image['image']
+                else:
+                    image = upload_image
+                w, h = image.size
+                image_path = os.path.join(
+                    save_folder, f'{imagehash.phash(image)}_{w}_{h}.png')
+                if not os.path.exists(image_path):
+                    image.save(image_path)
+                response = processor_ins(image_path,
+                                         prompt=sys_prompt,
+                                         max_new_tokens=max_new_tokens,
+                                         min_new_tokens=min_new_tokens,
+                                         num_beams=num_beams,
+                                         repetition_penalty=repetition_penalty,
+                                         temperature=temperature)
+                ret_edit_caption = gr.Textbox()
+                if update_mode == 0:
+                    upload_caption += ';' + response
+                elif update_mode == 1:
+                    upload_caption = response
+                ret_upload_caption = gr.Textbox(value=upload_caption)
             else:
-                return (gr.Gallery(value=[], selected_index=None),
-                        gr.Textbox(value=current_info['caption']),
-                        gr.Textbox(value=current_info['edit_caption']),
-                        gr.Text(value=f'{cursor + 1}/{all_number}'))
-
-        def change_image(dataset_name):
-            meta_data = create_dataset.meta_dict[dataset_name]
-            cursor = create_dataset.meta_dict[dataset_name]['cursor']
-            if cursor >= 0:
-                current_info = meta_data['file_list'][cursor]
+                ret_edit_caption = gr.Textbox()
+                ret_upload_caption = gr.Textbox()
+
+            is_flag, msg = processor_ins.unload_model()
+            if not is_flag:
+                sys_log = f'Unoad processor failed: {msg}'
+                return gr.Textbox(
+                ), ret_upload_caption, self.component_names.system_log.format(
+                    sys_log)
+
+            return ret_edit_caption, ret_upload_caption, self.component_names.system_log.format(
+                '')
+
+        self.caption_preprocess_btn.click(
+            preprocess_caption,
+            inputs=[
+                self.mode_state, self.caption_preprocess_method,
+                self.sys_prompt, self.max_new_tokens, self.min_new_tokens,
+                self.num_beams, self.repetition_penalty, self.temperature,
+                self.caption_update_mode, self.upload_image, self.caption,
+                create_dataset.dataset_type, create_dataset.dataset_name
+            ],
+            outputs=[self.edit_caption, self.caption, self.sys_log],
+            queue=False)
+
+        def mode_state_change(mode_state, dataset_type, dataset_name):
+            # default is editing current sample
+            if mode_state == 'edit':
+                dataset_type = create_dataset.get_trans_dataset_type(
+                    dataset_type)
+                dataset_ins = create_dataset.dataset_dict[dataset_type][
+                    dataset_name]
+                dataset_ins.set_edit_range(str(dataset_ins.cursor + 1))
+                image_list = [
+                    os.path.join(
+                        dataset_ins.meta['local_work_dir'],
+                        v.get('edit_relative_path', v['relative_path']))
+                    for v in dataset_ins.edit_samples
+                ]
+                selected_index = dataset_ins.edit_index_from_cursor(
+                    dataset_ins.cursor)
+                dataset_ins.edit_cursor = selected_index
+                return (gr.Row(visible=True), gr.Column(visible=True),
+                        gr.Column(visible=False), gr.CheckboxGroup(value=None),
+                        gr.Column(visible=False), gr.Column(visible=False),
+                        gr.Column(visible=True),
+                        gr.Gallery(value=image_list,
+                                   selected_index=selected_index,
+                                   visible=True),
+                        gr.Dropdown(
+                            value=self.component_names.range_mode_name[0]))
+            elif mode_state == 'add':
+                return (gr.Row(visible=False), gr.Column(visible=False),
+                        gr.Column(visible=True), gr.CheckboxGroup(),
+                        gr.Column(visible=True), gr.Column(visible=True),
+                        gr.Column(visible=False), gr.Gallery(visible=False),
+                        gr.Dropdown(
+                            value=self.component_names.range_mode_name[0]))
             else:
-                current_info = {'caption': '', 'edit_caption': ''}
-            all_number = len(meta_data['file_list'])
-            return (gr.Textbox(value=current_info['caption']),
-                    gr.Textbox(value=current_info['edit_caption']),
-                    gr.Text(value=f'{cursor+1}/{all_number}'))
-
-        def change_caption(dataset_name, edit_caption):
-            cursor = create_dataset.meta_dict[dataset_name]['cursor']
-            create_dataset.meta_dict[dataset_name]['file_list'][cursor][
-                'caption'] = edit_caption
-            create_dataset.save_meta(
-                create_dataset.meta_dict[dataset_name],
-                create_dataset.meta_dict[dataset_name]['work_dir'])
-            return gr.Textbox(value=edit_caption)
-
-        self.gl_dataset_images.select(select_image,
-                                      inputs=[create_dataset.user_data_name],
-                                      outputs=[
-                                          self.gl_dataset_images,
-                                          self.ori_caption, self.edit_caption,
-                                          self.info
-                                      ],
-                                      queue=False)
-        self.gl_dataset_images.change(
-            change_image,
-            inputs=[create_dataset.user_data_name],
-            outputs=[self.ori_caption, self.edit_caption, self.info],
-            queue=False)
-
-        self.modify_button.click(
-            change_caption,
-            inputs=[create_dataset.user_data_name, self.edit_caption],
-            outputs=[self.ori_caption],
-            queue=False)
-
-        def delete_file(dataset_name):
-            cursor = create_dataset.meta_dict[dataset_name]['cursor']
-            if len(create_dataset.meta_dict[dataset_name]['file_list']) < 1:
-                raise gr.Error(self.component_names.delete_err1)
-            current_file = create_dataset.meta_dict[dataset_name][
-                'file_list'].pop(cursor)
-            local_file = os.path.join(
-                create_dataset.meta_dict[dataset_name]['local_work_dir'],
-                current_file['relative_path'])
-            try:
-                os.remove(local_file)
-            except Exception:
-                print(f'remove file {local_file} error')
-            if cursor >= len(
-                    create_dataset.meta_dict[dataset_name]['file_list']):
-                cursor = 0
-            create_dataset.meta_dict[dataset_name]['cursor'] = cursor
-            create_dataset.save_meta(
-                create_dataset.meta_dict[dataset_name],
-                create_dataset.meta_dict[dataset_name]['work_dir'])
-            current_info = create_dataset.meta_dict[dataset_name]['file_list'][
-                cursor]
+                return (gr.Row(visible=False), gr.Column(visible=False),
+                        gr.Column(visible=False), gr.CheckboxGroup(value=None),
+                        gr.Column(visible=False), gr.Column(visible=False),
+                        gr.Column(visible=False), gr.Gallery(visible=False),
+                        gr.Dropdown(
+                            value=self.component_names.range_mode_name[0]))
+
+        self.mode_state.change(
+            mode_state_change,
+            inputs=[
+                self.mode_state, create_dataset.dataset_type,
+                create_dataset.dataset_name
+            ],
+            outputs=[
+                self.edit_setting_panel, self.edit_confirm_panel,
+                self.upload_panel, self.preprocess_checkbox,
+                self.image_preprocess_panel, self.caption_preprocess_panel,
+                self.edit_panel, self.edit_gl_dataset_images, self.range_mode
+            ],
+            queue=False)
+
+        def range_change(range_mode, dataset_type, dataset_name):
+            hit_range_mode = range_state_trans(range_mode)
+            if hit_range_mode == 2:
+                return (gr.Dropdown(visible=True), gr.Gallery())
+            else:
+                dataset_type = create_dataset.get_trans_dataset_type(
+                    dataset_type)
+                dataset_ins = create_dataset.dataset_dict[dataset_type][
+                    dataset_name]
+                if hit_range_mode == 1:
+                    dataset_ins.set_edit_range(-1)
+                else:
+                    dataset_ins.set_edit_range(str(dataset_ins.cursor + 1))
+                image_list = [
+                    os.path.join(
+                        dataset_ins.meta['local_work_dir'],
+                        v.get('edit_relative_path', v['relative_path']))
+                    for v in dataset_ins.edit_samples
+                ]
+                selected_index = dataset_ins.edit_index_from_cursor(
+                    dataset_ins.cursor)
+                dataset_ins.edit_cursor = selected_index
+                return (gr.Dropdown(visible=False),
+                        gr.Gallery(value=image_list,
+                                   selected_index=selected_index,
+                                   visible=True))
+
+        self.range_mode.change(
+            range_change,
+            inputs=[
+                self.range_mode, create_dataset.dataset_type,
+                create_dataset.dataset_name
+            ],
+            outputs=[self.data_range, self.edit_gl_dataset_images],
+            queue=False)
+
+        def set_range(data_range, dataset_type, dataset_name):
+            if len(data_range) == 0:
+                return (gr.Gallery(), gr.Gallery(), gr.Textbox(), gr.Text(),
+                        gr.Textbox(), gr.Text(), gr.Text(),
+                        self.component_names.system_log.format(''))
+            data_range = ','.join(data_range)
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            flg, msg = dataset_ins.set_edit_range(data_range)
+            if not flg:
+                sys_log = self.component_names.system_log.format(msg)
+                return (gr.Gallery(), gr.Gallery(), gr.Textbox(), gr.Text(),
+                        gr.Textbox(), gr.Text(), gr.Text(), sys_log)
+            image_list = [
+                os.path.join(dataset_ins.meta['local_work_dir'],
+                             v.get('edit_relative_path', v['relative_path']))
+                for v in dataset_ins.edit_samples
+            ]
+            selected_index = 0 if len(image_list) > 0 else -1
+            if selected_index >= 0:
+                cursor = dataset_ins.cursor_from_edit_index(selected_index)
+                dataset_ins.set_cursor(cursor)
+                current_info = dataset_ins.current_record
+                all_number = len(dataset_ins)
+
+                ret_image_gl = gr.Gallery(selected_index=dataset_ins.cursor) if dataset_ins.cursor >= 0 \
+                    else gr.Gallery(value=[], selected_index=None)
+                ret_edit_image_gl = gr.Gallery(value=image_list,
+                                               selected_index=selected_index,
+                                               visible=True)
+                ret_caption = gr.Textbox(value=current_info.get('caption', ''),
+                                         visible=True)
+                ret_edit_caption = gr.Textbox(value=current_info.get(
+                    'edit_caption', ''),
+                                              visible=True)
+
+                ret_image_height = current_info.get('height', -1)
+                ret_image_width = current_info.get('width', -1)
+                ret_image_format = os.path.splitext(
+                    current_info.get('relative_path', ''))[-1]
+
+                image_info = self.component_names.ori_dataset.format(
+                    ret_image_height, ret_image_width, ret_image_format)
+
+                ret_edit_image_width = current_info.get(
+                    'edit_width', current_info.get('width', -1))
+                ret_edit_image_height = current_info.get(
+                    'edit_height', current_info.get('height', -1))
+                ret_edit_image_format = os.path.splitext(
+                    current_info.get('edit_relative_path',
+                                     current_info.get('relative_path',
+                                                      '')))[-1]
+
+                edit_image_info = self.component_names.edit_dataset.format(
+                    ret_edit_image_height, ret_edit_image_width,
+                    ret_edit_image_format)
+
+                ret_info = gr.Text(
+                    value=f'{dataset_ins.cursor + 1}/{all_number}')
+            else:
+                ret_image_gl = gr.Gallery()
+                ret_edit_image_gl = gr.Gallery()
+                ret_caption = gr.Textbox()
+                ret_edit_caption = gr.Textbox()
+
+                image_info = ''
+                edit_image_info = ''
+                ret_info = gr.Text()
+            return (ret_image_gl, ret_edit_image_gl, ret_caption, image_info,
+                    ret_edit_caption, edit_image_info, ret_info,
+                    self.component_names.system_log.format(''))
+
+        self.data_range.change(set_range,
+                               inputs=[
+                                   self.data_range,
+                                   create_dataset.dataset_type,
+                                   create_dataset.dataset_name
+                               ],
+                               outputs=[
+                                   self.gl_dataset_images,
+                                   self.edit_gl_dataset_images, self.caption,
+                                   self.image_info, self.edit_caption,
+                                   self.edit_image_info, self.info,
+                                   self.sys_log
+                               ],
+                               queue=False)
+
+        def delete_file(dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            dataset_ins.delete_record()
+            current_info = dataset_ins.current_record
+
+            ret_image_height = current_info.get('height', -1)
+            ret_image_width = current_info.get('width', -1)
+            ret_image_format = os.path.splitext(
+                current_info.get('relative_path', ''))[-1]
+
+            image_info = self.component_names.ori_dataset.format(
+                ret_image_height, ret_image_width, ret_image_format)
+
+            ret_edit_image_width = current_info.get(
+                'edit_width', current_info.get('width', -1))
+            ret_edit_image_height = current_info.get(
+                'edit_height', current_info.get('height', -1))
+            ret_edit_image_format = os.path.splitext(
+                current_info.get('edit_relative_path',
+                                 current_info.get('relative_path', '')))[-1]
+
+            edit_image_info = self.component_names.edit_dataset.format(
+                ret_edit_image_height, ret_edit_image_width,
+                ret_edit_image_format)
+
             image_list = [
-                os.path.join(
-                    create_dataset.meta_dict[dataset_name]['local_work_dir'],
-                    v['relative_path'])
-                for v in create_dataset.meta_dict[dataset_name]['file_list']
+                os.path.join(dataset_ins.local_work_dir, v['relative_path'])
+                for v in dataset_ins.data
             ]
-            return (gr.Gallery(value=image_list, selected_index=cursor),
-                    gr.Textbox(value=current_info['caption']),
-                    gr.Textbox(value=current_info['caption']
-                               if current_info['edit_caption'] ==
-                               '' else current_info['edit_caption']),
-                    gr.Text(value=f'{cursor + 1}/{len(image_list)}'))
+            return (gr.Gallery(value=image_list,
+                               selected_index=dataset_ins.cursor),
+                    gr.Textbox(value=current_info.get('caption', '')),
+                    image_info,
+                    gr.Textbox(
+                        value=current_info.get('caption', '') if current_info.
+                        get('edit_caption', '') ==
+                        '' else current_info['edit_caption']), edit_image_info,
+                    gr.Text(
+                        value=f'{dataset_ins.cursor + 1}/{len(dataset_ins)}'))
 
         self.delete_button.click(delete_file,
-                                 inputs=[create_dataset.user_data_name],
+                                 inputs=[
+                                     create_dataset.dataset_type,
+                                     create_dataset.user_dataset_name
+                                 ],
                                  outputs=[
                                      self.gl_dataset_images, self.ori_caption,
-                                     self.edit_caption, self.info
+                                     self.image_info, self.edit_caption,
+                                     self.edit_image_info, self.info
                                  ],
                                  queue=False)
 
-        def add_file(dataset_name, upload_image, caption):
-            if 'image' in upload_image:
+        def image_upload(upload_image):
+            if isinstance(upload_image, dict):
                 image = upload_image['image']
-
             else:
                 image = upload_image
             w, h = image.size
-            meta = create_dataset.meta_dict[dataset_name]
-            local_work_dir = meta['local_work_dir']
-            work_dir = meta['work_dir']
-
-            save_folder = os.path.join(local_work_dir, 'images')
-            os.makedirs(save_folder, exist_ok=True)
-
-            relative_path = os.path.join('images',
-                                         f'{imagehash.phash(image)}.png')
-            image_path = os.path.join(work_dir, relative_path)
-
-            local_image_path = os.path.join(local_work_dir, relative_path)
-            with FS.put_to(image_path) as local_path:
-                image.save(local_path)
-
-            image.save(local_image_path)
-
-            meta['file_list'].append({
-                'image_path': image_path,
-                'relative_path': relative_path,
-                'width': w,
-                'height': h,
-                'caption': caption,
-                'prefix': '',
-                'edit_caption': caption
-            })
+            return self.component_names.upload_image_info.format(h, w)
+
+        self.upload_image.upload(image_upload,
+                                 inputs=[self.upload_image],
+                                 outputs=[self.upload_image_info],
+                                 queue=False)
+
+        def image_clear():
+            return ''
+
+        self.upload_image.clear(image_clear,
+                                inputs=[],
+                                outputs=[self.upload_image_info],
+                                queue=False)
+
+        def show_add_record_panel():
+            return gr.Text(value='add')
+
+        self.add_button.click(
+            show_add_record_panel,
+            inputs=[],
+            outputs=[self.mode_state],
+        )
+
+        def add_file(dataset_type, dataset_name, upload_image, caption):
+            if upload_image is None:
+                return (gr.Gallery(), gr.Textbox(), '', gr.Textbox(), '',
+                        gr.Image(), '', '', gr.Text(value=''))
+            if isinstance(upload_image, dict):
+                image = upload_image['image']
+            else:
+                image = upload_image
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            dataset_ins.add_record(image, caption)
+
+            current_info = dataset_ins.current_record
+
+            ret_image_height = current_info.get('height', -1)
+            ret_image_width = current_info.get('width', -1)
+            ret_image_format = os.path.splitext(
+                current_info.get('relative_path', ''))[-1]
+
+            image_info = self.component_names.ori_dataset.format(
+                ret_image_height, ret_image_width, ret_image_format)
 
-            meta['cursor'] = len(meta['file_list']) - 1
-            create_dataset.meta_dict[dataset_name] = meta
             image_list = [
-                os.path.join(
-                    create_dataset.meta_dict[dataset_name]['local_work_dir'],
-                    v['relative_path'])
-                for v in create_dataset.meta_dict[dataset_name]['file_list']
+                os.path.join(dataset_ins.local_work_dir, v['relative_path'])
+                for v in dataset_ins.data
             ]
+
+            ret_edit_image_width = current_info.get(
+                'edit_width', current_info.get('width', -1))
+            ret_edit_image_height = current_info.get(
+                'edit_height', current_info.get('height', -1))
+            ret_edit_image_format = os.path.splitext(
+                current_info.get('edit_relative_path',
+                                 current_info.get('relative_path', '')))[-1]
+
+            edit_image_info = self.component_names.edit_dataset.format(
+                ret_edit_image_height, ret_edit_image_width,
+                ret_edit_image_format)
+
             return (gr.Gallery(value=image_list,
-                               selected_index=meta['cursor']),
-                    gr.Textbox(value=caption), gr.Textbox(value=caption),
-                    gr.Text(value=f"{meta['cursor'] + 1}/{len(image_list)}"))
+                               selected_index=dataset_ins.cursor),
+                    gr.Textbox(value=caption), image_info,
+                    gr.Textbox(value=caption), edit_image_info,
+                    gr.Text(
+                        value=f'{dataset_ins.cursor + 1}/{len(dataset_ins)}'),
+                    gr.Image(value=None), gr.Text(value=''), '',
+                    gr.Text(value='view'))
 
         self.upload_button.click(add_file,
                                  inputs=[
-                                     create_dataset.user_data_name,
+                                     create_dataset.dataset_type,
+                                     create_dataset.dataset_name,
                                      self.upload_image, self.caption
                                  ],
                                  outputs=[
                                      self.gl_dataset_images, self.ori_caption,
-                                     self.edit_caption, self.info
+                                     self.image_info, self.edit_caption,
+                                     self.edit_image_info, self.info,
+                                     self.upload_image, self.caption,
+                                     self.upload_image_info, self.mode_state
                                  ],
                                  queue=False)
 
-        def edit_caption_change(dataset_name, edit_caption):
-            meta = create_dataset.meta_dict[dataset_name]
-            cursor = meta['cursor']
-            if cursor >= 0:
-                meta['file_list'][cursor]['edit_caption'] = edit_caption
+        def cancel_add_file():
+            return gr.Text(value='view')
 
-        self.edit_caption.change(
-            edit_caption_change,
-            inputs=[create_dataset.user_data_name, self.edit_caption],
-            queue=False)
+        self.cancel_button.click(cancel_add_file,
+                                 inputs=[],
+                                 outputs=[self.mode_state],
+                                 queue=False)
+
+        def edit_caption_change(dataset_type, dataset_name, edit_caption):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            dataset_ins.edit_caption(edit_caption)
+
+        self.edit_caption.change(edit_caption_change,
+                                 inputs=[
+                                     create_dataset.dataset_type,
+                                     create_dataset.dataset_name,
+                                     self.edit_caption
+                                 ],
+                                 queue=False)
```

## scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py

```diff
@@ -1,31 +1,34 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from __future__ import annotations
 
 import os
-import urllib.parse as parse
 
 import gradio as gr
 
-from scepter.modules.utils.file_system import FS
 from scepter.studio.preprocess.caption_editor_ui.component_names import \
     ExportDatasetUIName
 from scepter.studio.utils.uibase import UIBase
 
 
 class ExportDatasetUI(UIBase):
-    def __init__(self, cfg, is_debug=False, language='en'):
+    def __init__(self, cfg, is_debug=False, language='en', gallery_ins=None):
         self.dataset_name = ''
         self.work_dir = cfg.WORK_DIR
         self.export_folder = os.path.join(self.work_dir, cfg.EXPORT_DIR)
         self.component_names = ExportDatasetUIName(language)
+        if gallery_ins is not None:
+            self.default_dataset = gallery_ins.default_dataset
+        else:
+            self.default_dataset = None
 
     def create_ui(self):
-        with gr.Row(variant='panel', visible=False,
+        with gr.Row(variant='panel',
+                    visible=self.default_dataset is not None,
                     equal_height=True) as export_panel:
             self.data_state = gr.State(value=False)
             with gr.Column(scale=1, min_width=0):
                 self.export_to_zip = gr.Button(
                     value=self.component_names.btn_export_zip)
                 self.export_url = gr.File(
                     label=self.component_names.export_file,
@@ -35,108 +38,38 @@
                     show_label=True)
             with gr.Column(scale=1, min_width=0):
                 self.go_to_train = gr.Button(
                     value=self.component_names.go_to_train, size='lg')
         self.export_panel = export_panel
 
     def set_callbacks(self, create_dataset, manager):
-        def export_zip(dataset_name):
-            meta = create_dataset.meta_dict[dataset_name]
-            work_dir = meta['work_dir']
-            local_work_dir = meta['local_work_dir']
-            train_csv = os.path.join(work_dir, 'train.csv')
-            if len(meta['file_list']) < 1:
-                raise gr.Error(self.component_names.export_err1)
-            train_csv = create_dataset.write_csv(meta['file_list'], train_csv,
-                                                 work_dir)
-            _ = FS.get_from(train_csv, os.path.join(local_work_dir,
-                                                    'train.csv'))
-            save_file_list = work_dir + '_file.csv'
-            save_file_list = create_dataset.write_file_list(
-                meta['file_list'], save_file_list)
-            _ = FS.get_from(save_file_list,
-                            os.path.join(local_work_dir, 'file.csv'))
-            zip_path = os.path.join(self.export_folder, f'{dataset_name}.zip')
-            with FS.put_to(zip_path) as local_zip:
-                res = os.popen(
-                    f"cd '{local_work_dir}' && mkdir -p '{dataset_name}' "
-                    f"&& cp -rf images '{dataset_name}/images' "
-                    f"&& cp -rf train.csv '{dataset_name}/train.csv' "
-                    f"&& zip -r '{os.path.abspath(local_zip)}' '{dataset_name}'/* "
-                    f"&& rm -rf '{dataset_name}'")
-                print(res.readlines())
-
-            if not FS.exists(zip_path):
-                raise gr.Error(self.component_names.export_zip_err1)
-            create_dataset.save_meta(meta, work_dir)
-            local_zip = FS.get_from(zip_path)
+        def export_zip(dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            local_zip = dataset_ins.export_zip(self.export_folder)
             return gr.File(value=local_zip, visible=True)
 
-        self.export_to_zip.click(export_zip,
-                                 inputs=[create_dataset.user_data_name],
-                                 outputs=[self.export_url],
-                                 queue=False)
-
-        def export_csv(dataset_name):
-            meta = create_dataset.meta_dict[dataset_name]
-            work_dir = meta['work_dir']
-            local_work_dir = meta['local_work_dir']
-            train_csv = os.path.join(work_dir, 'train.csv')
-            if len(meta['file_list']) < 1:
-                raise gr.Error(self.component_names.export_err1)
-            train_csv = create_dataset.write_csv(meta['file_list'], train_csv,
-                                                 work_dir)
-            _ = FS.get_from(train_csv, os.path.join(local_work_dir,
-                                                    'train.csv'))
-            save_file_list = os.path.join(work_dir, 'file.csv')
-            save_file_list = create_dataset.write_file_list(
-                meta['file_list'], save_file_list)
-            local_file_csv = FS.get_from(
-                save_file_list, os.path.join(local_work_dir, 'file.csv'))
-            create_dataset.save_meta(meta, work_dir)
-            is_flag = FS.put_object_from_local_file(
-                local_file_csv,
-                os.path.join(self.export_folder, dataset_name + '_file.csv'))
-            if not is_flag:
-                raise gr.Error(self.component_names.upload_err1)
-            list_url = FS.get_url(os.path.join(self.export_folder,
-                                               dataset_name + '_file.csv'),
-                                  set_public=True)
-            list_url = parse.unquote(list_url)
-            if 'wulanchabu' in list_url:
-                list_url = list_url.replace(
-                    '.cn-wulanchabu.oss-internal.aliyun-inc.',
-                    '.oss-cn-wulanchabu.aliyuncs.')
-            else:
-                list_url = list_url.replace('.oss-internal.aliyun-inc.',
-                                            '.oss.aliyuncs.')
-            if not list_url.split('/')[-1] == dataset_name + '_file.csv':
-                list_url = os.path.join(os.path.dirname(list_url),
-                                        dataset_name + '_file.csv')
-            return gr.Text(value=list_url)
-
-        # self.export_to_list.click(export_csv,
-        #                           inputs=[create_dataset.user_data_name],
-        #                           outputs=[self.export_url])
-
-        def go_to_train(dataset_name):
-            meta = create_dataset.meta_dict[dataset_name]
-            work_dir = meta['work_dir']
-            local_work_dir = meta['local_work_dir']
-            train_csv = os.path.join(work_dir, 'train.csv')
-            if len(meta['file_list']) < 1:
-                raise gr.Error(self.component_names.export_err1)
-            train_csv = create_dataset.write_csv(meta['file_list'], train_csv,
-                                                 work_dir)
-            _ = FS.get_from(train_csv, os.path.join(local_work_dir,
-                                                    'train.csv'))
-            save_file_list = work_dir + '_file.csv'
-            _ = create_dataset.write_file_list(meta['file_list'],
-                                               save_file_list)
+        self.export_to_zip.click(
+            export_zip,
+            inputs=[create_dataset.dataset_type, create_dataset.dataset_name],
+            outputs=[self.export_url],
+            queue=False)
+
+        def go_to_train(dataset_type, dataset_name):
+            dataset_type = create_dataset.get_trans_dataset_type(dataset_type)
+            dataset_ins = create_dataset.dataset_dict[dataset_type][
+                dataset_name]
+            dataset_ins.update_dataset()
             return (gr.Tabs(selected='self_train'),
-                    gr.Textbox(value=os.path.abspath(local_work_dir)))
+                    gr.Textbox(
+                        value=os.path.abspath(dataset_ins.local_work_dir)),
+                    dataset_name)
 
         self.go_to_train.click(
             go_to_train,
-            inputs=[create_dataset.user_data_name],
-            outputs=[manager.tabs, manager.self_train.trainer_ui.ms_data_name],
+            inputs=[create_dataset.dataset_type, create_dataset.dataset_name],
+            outputs=[
+                manager.tabs, manager.self_train.trainer_ui.ms_data_name,
+                manager.self_train.trainer_ui.ori_data_name
+            ],
             queue=False)
```

## scepter/studio/self_train/scripts/run_task.py

```diff
@@ -91,15 +91,15 @@
     from scepter.modules.utils.distribute import get_dist_info
     rank, _ = get_dist_info()
     if rank == 0:
         config_path = os.path.join(cfg.SOLVER.WORK_DIR,
                                    cfg.args.cfg_file.split('/')[-1])
         with FS.put_to(config_path) as local_config_path:
             with open(local_config_path, 'w') as f_out:
-                f_out.write(cfg.dump())
+                f_out.write(cfg.dump(is_secure=True))
 
 
 def update_config(cfg):
     if cfg.args.work_dir and cfg.args.work_dir != '':
         cfg.SOLVER.WORK_DIR = cfg.args.work_dir
     return cfg
```

## scepter/studio/self_train/scripts/trainer.py

```diff
@@ -13,20 +13,26 @@
 from scepter.modules.utils.logger import as_time
 
 
 class TaskStatus():
     def __init__(self):
         self.pid = -1
         self.retcode = -999
-        self.error_msg = None
-        self.out_msg = None
+        self.error_log = None
 
     def __repr__(self):
         return f'Process {self.pid} retcode {self.retcode}, error msg: {self.error_msg}.'
 
+    @property
+    def error_msg(self):
+        if os.path.exists(self.error_log):
+            return "\n".join(open(self.error_log, "r").readlines()[-50:])
+        else:
+            return "No error msg."
+
 
 def kill_job(pid):
     try:
         current_process = psutil.Process(pid)
         children = current_process.children(recursive=True)
         for child in children:
             child.terminate()
@@ -42,30 +48,31 @@
     def __init__(self, run_script, status_message):
         self.run_script = run_script
         self.status_message = status_message
         self.proc = None
 
     def __call__(self, task_name):
         torch.cuda.empty_cache()
+        error_folder = "./error_logs"
+        os.makedirs(error_folder, exist_ok=True)
+        self.status_message.error_log = f"{error_folder}/{int(time.time())}.log"
         cmd = f'PYTHONPATH=. python {self.run_script} ' \
-              f'--cfg={task_name}/train.yaml'
+              f'--cfg={task_name}/train.yaml 2> {self.status_message.error_log}'
         # cmd = [f"python {self.run_script}"]
         print(cmd)
         try:
             # self.proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
             self.proc = subprocess.Popen(cmd, shell=True)
             self.status_message.pid = self.proc.pid
             self.status_message.retcode = self.proc.wait(
             )  # self.proc.wait(3600*24*14)
             # self.status_message.error_msg = self.proc.stderr.read()
-            self.status_message.error_msg = ''
         except Exception:
             self.status_message.retcode = -2
             # self.status_message.error_msg = self.proc.stderr.read()
-            self.status_message.error_msg = ''
 
     def terminate(self):
         print(f'Terminate {self.proc.pid} ...')
         kill_job(self.proc.pid)
         self.proc.terminate()
         print(f'Terminate {self.proc.pid} success.')
 
@@ -114,15 +121,15 @@
                                         '''
                                 task_status['msg'] = message
                                 task_status['status'] = 'success'
                             else:
                                 err_msg = now_task['train_status'].error_msg
                                 message = f'''
                                             Training failed! \n
-                                            Error msg: {err_msg[-1000:]} \n
+                                            Error msg: {err_msg} \n
                                             Take time [ {duration:.4f}s ] \n
                                             {self.check_memory()}
                                         '''
                                 task_status['msg'] = message
                                 task_status['status'] = 'failed'
                             with FS.put_to(status_file) as local_path:
                                 json.dump(task_status,
```

## scepter/studio/self_train/self_train_ui/component_names.py

```diff
@@ -80,39 +80,49 @@
                 - Training: Click on [Start Training].
                 - Testing: After completing the training, click [Go to inference ].
                 - Note: Timeouts may cause the connection to disconnect (an Error may occur).
                  After waiting for the time when the training is likely to be almost complete,
                  refresh the interface and then click [Refresh Model] at the bottom of the page.
                  The trained model should appear in the [Output Model Name] if training was successful;
                  if not, the training may be incomplete or have failed.
-                - zip example: https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip
+                - zip example: https://www.modelscope.cn/api/v1/models/iic/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip
                 - For processing and training with large-scale data, it is recommended to use the command line.
             '''  # noqa
             self.data_type_choices = ['Dataset zip', 'MaaS Dataset']
             self.data_type_value = 'Dataset zip'
             self.data_type_name = 'Data Source'
+            self.ori_data_name = 'Data Name'
             self.ms_data_name_place_hold = 'Supports MaaS dataset/local/HTTP Zip package'
             self.ms_data_space = 'ModelScope Space'
             self.ms_data_subname = 'MaaS Dataset - Subset'
             self.training_block = 'Training Parameters'
             self.base_model = 'Base Model'
-            self.tuner_name = 'Fine-tuning Method'
+            self.tuner_name = 'Tuner Method'
             self.base_model_revision = 'Model Version Number'
             self.resolution_height = 'Resolution Height'
             self.resolution_width = 'Resolution Width'
+            self.resolution_height_max = 'Resolution Height Max'
+            self.resolution_width_max = 'Resolution Width Max'
             self.train_epoch = 'Number of Training Epochs'
             self.learning_rate = 'Learning Rate'
             self.save_interval = 'Save Interval'
             self.train_batch_size = 'Training Batch Size'
             self.prompt_prefix = 'Prefix'
             self.replace_keywords = 'Trigger Keywords'
             self.work_name = 'Save Model Name (refresh to get a random value)'
             self.push_to_hub = 'Push to hub'
             self.training_button = 'Start Training'
             self.eval_prompts = 'Eval Prompts'
+            self.tuner_param = 'Tuner Parameters'
+            self.enable_resolution_bucket = 'Enable Resolution Bucket'
+            self.resolution_param = 'Resolution Parameters'
+            self.min_bucket_resolution = 'Min Bucket Resolution'
+            self.max_bucket_resolution = 'Max Bucket Resolution'
+            self.bucket_resolution_steps = 'Bucket Resolution Steps'
+            self.bucket_no_upscale = 'Bucket No Upscale'
             # Error or Warning
             self.training_err1 = 'CUDA is unavailable.'
             self.training_err2 = 'Currently insufficient VRAM, training failed!'
             self.training_err3 = 'You need to prepare training data.'
             self.training_err4 = 'Save model name already exists or is None, please regenerate this name.'
             self.training_err5 = 'Training failed.'
             self.training_err6 = "Can't process training data"
@@ -121,38 +131,48 @@
             self.user_direction = '''
                 ### 使用说明
                 - 数据: 选择Example的模版数据或可以按照样例中dog.zip包的格式准备自定义数据进行上传
                 - 参数: 可尝试进行相关参数的修改
                 - 训练: 点击【开始训练】
                 - 测试: 完成训练后点击【使用模型】
                 - 注意：超时可能导致连接断开(出现Error)，可以等差不多可能训完后，刷新界面再点击页面最后的[刷新模型]，即可在[产出模型名称中]出现已经完成训练的模型，若不存在则没有完成训练或训练失败
-                - ZIP样例：https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip
+                - ZIP样例：https://www.modelscope.cn/api/v1/models/iic/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip
                 - 对于大规模数据的处理和训练，建议使用命令行形式
                 '''  # noqa
             self.data_type_choices = ['数据集zip', 'MaaS数据集']
             self.data_type_value = '数据集zip'
             self.data_type_name = '数据集来源'
+            self.ori_data_name = '数据集名称'
             self.ms_data_name_place_hold = '支持MaaS数据集/本地/Http Zip包'
             self.ms_data_space = 'ModelScope 空间'
             self.ms_data_subname = 'MaaS数据集-子集'
             self.training_block = '训练参数'
             self.base_model = '基础模型'
             self.tuner_name = '微调方法'
             self.base_model_revision = '模型版本号'
             self.resolution_height = '训练高度'
             self.resolution_width = '训练宽度'
+            self.resolution_height_max = '最大训练高度'
+            self.resolution_width_max = '最大训练宽度'
             self.train_epoch = '训练轮数'
             self.learning_rate = '学习率'
             self.save_interval = '存储间隔'
             self.train_batch_size = '训练批次'
             self.prompt_prefix = '前缀'
             self.replace_keywords = '触发关键词'
             self.work_name = '保存模型名称（刷新获得随机值）'
             self.push_to_hub = '推送魔搭社区'
             self.eval_prompts = '评测文本'
+            self.tuner_param = '微调参数'
+            self.enable_resolution_bucket = '开启分辨率分桶'
+            self.resolution_param = '分辨率参数'
+            self.min_bucket_resolution = '最小分桶分辨率'
+            self.max_bucket_resolution = '最大分桶分辨率'
+            self.bucket_resolution_steps = '分桶分辨率步长'
+            self.bucket_no_upscale = '分桶分辨率不做放大'
             # Error or Warning
             self.training_err1 = 'CUDA不可用.'
             self.training_err2 = '目前显存不足，训练失败！'
             self.training_err3 = '您需要准备训练数据'
             self.training_err4 = '保存模型未生成或名称已经存在，请重新生成名称'
             self.training_err5 = '训练失败'
             self.training_err6 = '无法处理的数据'
```

## scepter/studio/self_train/self_train_ui/model_ui.py

```diff
@@ -177,16 +177,15 @@
                 gallery_value = []
             select_index = 0 if len(gallery_value) > 0 else None
             return (message, gr.Column(visible=status in ('running',
                                                           'success')),
                     gr.Dropdown(choices=ckpt_list, value=ckpt_value),
                     gr.Gallery(value=gallery_value,
                                preview=True,
-                               selected_index=select_index)
-                    )
+                               selected_index=select_index))
 
         self.output_model_name.change(fn=model_name_change,
                                       inputs=[self.output_model_name],
                                       outputs=[
                                           self.log_message,
                                           self.export_log_panel,
                                           self.output_ckpt_name,
@@ -249,31 +248,30 @@
                     pass
 
             message = trainer_ui.trainer_ins.get_log(model_name)
             status = trainer_ui.trainer_ins.get_status(model_name)
             ckpt_list = self.get_ckpt_list(model_name)
             ckpt_value = ckpt_list[-1] if len(ckpt_list) > 0 else ''
             ret_gallery = ckpt_name_change(model_name, ckpt_value)
-            return (message,
-                    gr.Column(visible=status in ('running', 'success')),
+            return (message, gr.Column(visible=status in ('running',
+                                                          'success')),
                     gr.Dropdown(choices=self.model_list, value=model_name),
                     gr.Dropdown(choices=ckpt_list,
                                 value=ckpt_value), ret_gallery)
 
-        self.refresh_model_gbtn.click(
-            fn=refresh_model,
-            inputs=[self.output_model_name],
-            outputs=[
-                self.log_message,
-                self.export_log_panel,
-                self.output_model_name,
-                self.output_ckpt_name,
-                self.eval_gallery
-            ],
-            queue=False)
+        self.refresh_model_gbtn.click(fn=refresh_model,
+                                      inputs=[self.output_model_name],
+                                      outputs=[
+                                          self.log_message,
+                                          self.export_log_panel,
+                                          self.output_model_name,
+                                          self.output_ckpt_name,
+                                          self.eval_gallery
+                                      ],
+                                      queue=False)
 
         def delete_model(model_name):
             index = 0
             trainer_ui.trainer_ins.stop_task(model_name)
             if model_name in self.model_list:
                 index = self.model_list.index(model_name)
                 self.model_list.remove(model_name)
@@ -302,15 +300,15 @@
 
         def go_to_inferece(output_model, output_ckpt_name):
             params_path = os.path.join(self.work_dir, output_model,
                                        'params.json')
             if os.path.exists(params_path):
                 params_info = json.loads(open(params_path).read())
                 assert params_info['work_name'] == output_model
-                # base_model = params_info['base_model']
+                base_model = params_info['base_model']
                 base_model_revision = params_info['base_model_revision']
                 tuner_name = params_info['tuner_name']
                 model_path = os.path.join(self.work_dir, output_model,
                                           'checkpoints', output_ckpt_name)
                 eval_prompts = params_info[
                     'eval_prompts'] if 'eval_prompts' in params_info else []
                 image_dir = os.path.join(self.work_dir, output_model,
@@ -337,26 +335,40 @@
                 image_path = image_path[0] if len(image_path) > 0 else None
             if isinstance(eval_prompts, list):
                 eval_prompts = eval_prompts[0] if len(eval_prompts) > 0 else ''
             cfg_file = os.path.join(self.work_dir, output_model,
                                     f'meta_{output_ckpt_name}.yaml')
             output_model = output_model + '@' + output_ckpt_name
             tuner_dict = {
-                'NAME': output_model,
-                'NAME_ZH': output_model,
+                'NAME':
+                output_model,
+                'NAME_ZH':
+                output_model,
                 # 'BASE_MODEL': base_model,
-                'BASE_MODEL': base_model_revision,
-                'TUNER_TYPE': tuner_name,
-                'DESCRIPTION': '',
-                'MODEL_PATH': model_path,
-                'IMAGE_PATH': image_path,
-                'PROMPT_EXAMPLE': eval_prompts,
-                'SOURCE': 'self_train',
-                'CKPT_NAME': output_ckpt_name,
-                'PARAMS': params_info
+                'BASE_MODEL':
+                base_model_revision,
+                'TUNER_TYPE':
+                tuner_name,
+                'DESCRIPTION':
+                '',
+                'MODEL_PATH':
+                model_path,
+                'IMAGE_PATH':
+                image_path,
+                'PROMPT_EXAMPLE':
+                eval_prompts,
+                'SOURCE':
+                'self_train',
+                'CKPT_NAME':
+                output_ckpt_name,
+                'PARAMS':
+                params_info,
+                'IS_SHARE':
+                self.BASE_CFG_VALUE[base_model][base_model_revision]
+                ['is_share']
             }
             tuner_cfg = Config(cfg_dict=tuner_dict, load=False)
 
             if not os.path.exists(model_path):
                 gr.Error(self.component_names.model_err4)
             self.manager.inference.model_manage_ui.pipe_manager.register_tuner(
                 tuner_cfg,
@@ -435,15 +447,15 @@
                 manager.inference.tuner_ui.custom_tuner_model
                 # manager.inference.tuner_ui.tuner_type,
                 # manager.inference.tuner_ui.base_model,
                 # manager.inference.tuner_ui.tuner_example,
                 # manager.inference.tuner_ui.tuner_desc,
                 # manager.inference.tuner_ui.tuner_prompt_example
             ],
-            queue=False)
+            queue=True)
 
         def export_train_log(model_name):
             current_log_folder = os.path.join(self.work_dir, model_name)
             _ = trainer_ui.trainer_ins.get_log(model_name)
             out_log = os.path.join(current_log_folder, 'output_std_log.txt')
             if os.path.exists(out_log):
                 zip_path = f'{trainer_ui.current_train_model}_log.zip'
```

## scepter/studio/self_train/self_train_ui/trainer_ui.py

```diff
@@ -43,14 +43,40 @@
 def get_work_name(model, version, tuner):
     model_prefix = f'Swift@{model}@{version}@{tuner}'
     return model_prefix + '@' + '{0:%Y%m%d%H%M%S%f}'.format(
         datetime.datetime.now()) + ''.join(
             [str(random.randint(1, 10)) for i in range(3)])
 
 
+def judge_tuner_visible(tuner_name):
+    lora_visible = tuner_name in ['LORA', 'TEXT_LORA']
+    text_lora_visible = tuner_name in ['TEXT_SCE']
+    sce_visible = tuner_name in ['SCE', 'TEXT_SCE']
+    return lora_visible, text_lora_visible, sce_visible
+
+
+def update_tuner_cfg(tuner_name, tuner_cfg, **kwargs):
+    update_info = {}
+    if tuner_name in ['LORA', 'TEXT_LORA']:
+        update_info = {
+            'LORA_ALPHA': kwargs['lora_alpha'],
+            'R': kwargs['lora_rank']
+        }
+    elif tuner_name == 'SCE' or (tuner_name == 'TEXT_SCE'
+                                 and tuner_cfg['NAME'] == 'SwiftSCETuning'):
+        update_info = {'DOWN_RATIO': kwargs['sce_ratio']}
+    elif tuner_name == 'TEXT_SCE' and tuner_cfg['NAME'] == 'SwiftLoRA':
+        update_info = {
+            'LORA_ALPHA': kwargs['text_lora_alpha'],
+            'R': kwargs['text_lora_rank']
+        }
+    tuner_cfg.update(update_info)
+    return tuner_cfg
+
+
 class TrainerUI(UIBase):
     def __init__(self, cfg, all_cfg_value, is_debug=False, language='en'):
         self.BASE_CFG_VALUE = all_cfg_value
         self.para_data = get_default(self.BASE_CFG_VALUE)
         self.train_para_data = cfg.TRAIN_PARAS
         self.run_script = os.path.join(os.path.dirname(scepter.dirname),
                                        cfg.SCRIPT_DIR, 'run_task.py')
@@ -77,14 +103,19 @@
                 with gr.Column(variant='panel'):
                     gr.Markdown(self.component_names.user_direction)
                     self.data_type = gr.Dropdown(
                         choices=self.component_names.data_type_choices,
                         value=self.component_names.data_type_value,
                         label=self.component_names.data_type_name,
                         interactive=True)
+                    self.ori_data_name = gr.Textbox(
+                        label=self.component_names.ori_data_name,
+                        max_lines=1,
+                        placeholder=self.component_names.ori_data_name,
+                        interactive=True)
                     self.ms_data_name = gr.Textbox(
                         label=' or '.join(
                             self.component_names.data_type_choices),
                         max_lines=1,
                         placeholder=self.component_names.
                         ms_data_name_place_hold,
                         interactive=True)
@@ -126,36 +157,120 @@
                                         'tuner_choices', []),
                                     value=self.para_data.get(
                                         'tuner_default', ''),
                                     label=self.component_names.tuner_name,
                                     interactive=True)
 
                         with gr.Row():
-                            with gr.Column(scale=1, min_width=0):
+                            with gr.Accordion(
+                                    label=self.component_names.tuner_param,
+                                    open=False):
+                                if 'TUNER' in self.para_data:
+                                    lora_visible, text_lora_visible, sce_visible = judge_tuner_visible(
+                                        self.para_data['TUNER'])
+                                else:
+                                    lora_visible, text_lora_visible, sce_visible = False, False, False
+                                with gr.Row(visible=lora_visible
+                                            ) as self.lora_param:
+                                    self.lora_alpha = gr.Number(
+                                        label='LoRA Alpha',
+                                        value=self.para_data.get(
+                                            'lora_alpha', 256),
+                                        interactive=True)
+                                    self.lora_rank = gr.Number(
+                                        label='LoRA Rank',
+                                        value=self.para_data.get(
+                                            'lora_rank', 256),
+                                        interactive=True)
+                                with gr.Row(visible=text_lora_visible
+                                            ) as self.text_lora_param:
+                                    self.text_lora_alpha = gr.Number(
+                                        label='Text LoRA Alpha',
+                                        value=self.para_data.get(
+                                            'text_lora_alpha', 256),
+                                        interactive=True)
+                                    self.text_lora_rank = gr.Number(
+                                        label='Text LoRA Rank',
+                                        value=self.para_data.get(
+                                            'text_lora_rank', 256),
+                                        interactive=True)
+                                with gr.Row(
+                                        visible=sce_visible) as self.sce_param:
+                                    self.sce_ratio = gr.Slider(
+                                        label='SCE Ratio',
+                                        minimum=0.2,
+                                        maximum=2.0,
+                                        step=0.1,
+                                        value=self.para_data.get(
+                                            'sce_ratio', 1.0),
+                                        interactive=True)
+
+                        with gr.Row():
+                            with gr.Column(scale=2, min_width=0):
                                 self.resolution_height = gr.Dropdown(
                                     choices=list(self.h_level_dict.keys()),
                                     value=self.para_data.get(
                                         'RESOLUTION', 1024)[0],
                                     label=self.component_names.
                                     resolution_height,
                                     allow_custom_value=True,
                                     interactive=True)
 
-                            with gr.Column(scale=1, min_width=0):
+                            with gr.Column(scale=2, min_width=0):
                                 self.resolution_width = gr.Dropdown(
                                     choices=self.h_level_dict[
                                         self.resolution_height.value],
                                     value=self.para_data.get(
                                         'RESOLUTION', 1024)[1],
                                     label=self.component_names.
                                     resolution_width,
                                     allow_custom_value=True,
                                     interactive=True)
 
                         with gr.Row():
+                            with gr.Accordion(label=self.component_names.
+                                              resolution_param,
+                                              open=False):
+                                self.enable_resolution_bucket = gr.Checkbox(
+                                    value=False,
+                                    container=True,
+                                    interactive=True,
+                                    label=self.component_names.
+                                    enable_resolution_bucket)
+                                with gr.Column(
+                                        visible=self.enable_resolution_bucket.
+                                        value) as self.resolution_bucket_param:
+                                    with gr.Row():
+                                        self.min_bucket_resolution = gr.Number(
+                                            label=self.component_names.
+                                            min_bucket_resolution,
+                                            value=self.para_data.get(
+                                                'min_bucket_resolution', 256),
+                                            interactive=True)
+                                        self.max_bucket_resolution = gr.Number(
+                                            label=self.component_names.
+                                            max_bucket_resolution,
+                                            value=self.para_data.get(
+                                                'max_bucket_resolution', 1024),
+                                            interactive=True)
+                                    with gr.Row():
+                                        self.bucket_resolution_steps = gr.Number(
+                                            label=self.component_names.
+                                            bucket_resolution_steps,
+                                            value=self.para_data.get(
+                                                'bucket_resolution_steps', 64),
+                                            interactive=True)
+                                        self.bucket_no_upscale = gr.Checkbox(
+                                            value=False,
+                                            container=True,
+                                            interactive=True,
+                                            label=self.component_names.
+                                            bucket_no_upscale)
+
+                        with gr.Row():
                             with gr.Column(scale=1, min_width=0):
                                 self.train_epoch = gr.Number(
                                     label=self.component_names.train_epoch,
                                     value=self.para_data.get('EPOCHS', 10),
                                     precision=0,
                                     interactive=True)
                             with gr.Column(scale=1, min_width=0):
@@ -228,15 +343,15 @@
 
             with gr.Row(variant='panel', equal_height=True):
                 self.examples = gr.Examples(
                     examples=[
                         [
                             self.component_names.data_type_choices[0],
                             '',
-                            'https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip',  # noqa
+                            'https://www.modelscope.cn/api/v1/models/iic/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip',  # noqa
                             ''
                         ],
                         [
                             self.component_names.data_type_choices[1], 'damo',
                             'style_custom_dataset', '3D'
                         ]
                     ],
@@ -369,37 +484,43 @@
                 Supported Fine-tuning Methods:
                 Supported Fine-tuning Methods:
                 Prefix for Saved Model:
             '''
             ret_data = get_values_by_model_version_tuner(
                 self.BASE_CFG_VALUE, base_model, base_model_revision,
                 tuner_name)
+            lora_visible, text_lora_visible, sce_visible = judge_tuner_visible(
+                tuner_name)
             return ret_data.get('EPOCHS', 10), \
                 ret_data.get('LEARNING_RATE', 0.0001), \
                 ret_data.get('SAVE_INTERVAL', 10), \
                 ret_data.get('TRAIN_BATCH_SIZE', 4), \
                 ret_data.get('TRAIN_PREFIX', ''), \
                 gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[0],
                             choices=list(self.h_level_dict.keys()),
                             interactive=True), \
                 gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[1],
                             choices=self.h_level_dict[ret_data.get('RESOLUTION', 1024)[0]],
-                            interactive=True)
+                            interactive=True), \
+                gr.Row.update(visible=lora_visible), \
+                gr.Row.update(visible=text_lora_visible), \
+                gr.Row.update(visible=sce_visible)
 
         #
         self.tuner_name.change(fn=change_train_value_by_model_version_tuner,
                                inputs=[
                                    self.base_model, self.base_model_revision,
                                    self.tuner_name
                                ],
                                outputs=[
                                    self.train_epoch, self.learning_rate,
                                    self.save_interval, self.train_batch_size,
                                    self.prompt_prefix, self.resolution_height,
-                                   self.resolution_width
+                                   self.resolution_width, self.lora_param,
+                                   self.text_lora_param, self.sce_param
                                ],
                                queue=False)
 
         def change_resolution(h):
             if h not in self.h_level_dict:
                 return gr.Dropdown()
             all_choices = self.h_level_dict[h]
@@ -407,21 +528,44 @@
             return gr.Dropdown(choices=all_choices, value=default)
 
         self.resolution_height.change(change_resolution,
                                       inputs=[self.resolution_height],
                                       outputs=[self.resolution_width],
                                       queue=False)
 
-        def run_train(work_name, data_type, ms_data_space, ms_data_name,
-                      ms_data_subname, base_model, base_model_revision,
-                      tuner_name, resolution_height, resolution_width,
-                      train_epoch, learning_rate, save_interval,
-                      train_batch_size, prompt_prefix, replace_keywords,
-                      push_to_hub, eval_prompts):
-
+        def change_resolution_bucket(evt: gr.SelectData):
+            is_selected = evt.selected
+            return (
+                gr.update(
+                    label=self.component_names.resolution_height_max if
+                    is_selected else self.component_names.resolution_height),
+                gr.update(
+                    label=self.component_names.resolution_width_max
+                    if is_selected else self.component_names.resolution_width),
+                gr.update(visible=is_selected))
+
+        self.enable_resolution_bucket.select(change_resolution_bucket,
+                                             inputs=[],
+                                             outputs=[
+                                                 self.resolution_height,
+                                                 self.resolution_width,
+                                                 self.resolution_bucket_param
+                                             ],
+                                             queue=False)
+
+        def run_train(work_name, data_type, ori_data_name, ms_data_space,
+                      ms_data_name, ms_data_subname, base_model,
+                      base_model_revision, tuner_name, resolution_height,
+                      resolution_width, train_epoch, learning_rate,
+                      save_interval, train_batch_size, prompt_prefix,
+                      replace_keywords, push_to_hub, eval_prompts, lora_alpha,
+                      lora_rank, text_lora_alpha, text_lora_rank, sce_ratio,
+                      enable_resolution_bucket, min_bucket_resolution,
+                      max_bucket_resolution, bucket_resolution_steps,
+                      bucket_no_upscale):
             # Check Cuda
             if not torch.cuda.is_available() and not self.is_debug:
                 raise gr.Error(self.component_names.training_err1)
 
             if work_name == 'custom' or work_name is None or work_name == '':
                 raise gr.Error(self.component_names.training_err4)
             work_dir = os.path.join(self.work_dir_pre, work_name)
@@ -525,14 +669,57 @@
                             data_cfg['MS_REMAP_KEYS'] = {'Text': 'Prompt'}
                         else:
                             data_cfg['MS_REMAP_KEYS'] = None
                     data_cfg['OUTPUT_SIZE'] = [
                         int(resolution_height),
                         int(resolution_width)
                     ]
+                    if enable_resolution_bucket:
+                        local_data_dir = data_cfg['MS_DATASET_NAME']
+                        local_file_list = os.path.join(local_data_dir,
+                                                       'file.txt')
+                        data_num = sum(1 for line in open(local_file_list))
+                        if os.path.exists(local_data_dir) and os.path.exists(
+                                local_file_list):
+                            data_cfg.update({
+                                'NAME': 'ImageTextPairDataset',
+                                'SAMPLER': {
+                                    'NAME':
+                                    'ResolutionBatchSampler',
+                                    'DATA_FILE':
+                                    local_file_list,
+                                    'FIELDS':
+                                    ['img_path', 'width', 'height', 'prompt'],
+                                    'DELIMITER':
+                                    '#;#',
+                                    'MAX_RESO': [
+                                        int(resolution_width),
+                                        int(resolution_height)
+                                    ],
+                                    'MIN_BUCKET_RESO':
+                                    int(min_bucket_resolution),
+                                    'MAX_BUCKET_RESO':
+                                    int(max_bucket_resolution),
+                                    'BUCKET_RESO_STEPS':
+                                    int(bucket_resolution_steps),
+                                    'BUCKET_NO_UPSCALE':
+                                    bucket_no_upscale
+                                },
+                                'DATA_NUM': data_num
+                            })
+                            for trans in data_cfg['TRANSFORMS']:
+                                if trans['NAME'] == 'Select':
+                                    trans['META_KEYS'] = [
+                                        'img_path', 'image_size'
+                                    ]
+                        else:
+                            raise Exception(
+                                'Cannot find right data format for resolution_bucket'
+                            )
+
                 return data_cfg
 
             def prepare_eval_data(data_cfg):
                 data_cfg['PROMPT_PREFIX'] = prompt_prefix
                 data_cfg['IMAGE_SIZE'] = [
                     int(resolution_height),
                     int(resolution_width)
@@ -561,36 +748,50 @@
                                         copy.deepcopy(cache_value[idx][c_k]))
                                 current_val = copy.deepcopy(val)
                                 for c_k, v in zip(c_k_list[::-1],
                                                   cache_value[:-1][::-1]):
                                     v[c_k] = current_val
                                     current_val = v
                                 cfg = current_val
-
                 # update config
                 cfg['SOLVER']['WORK_DIR'] = work_dir
                 cfg['SOLVER']['OPTIMIZER']['LEARNING_RATE'] = float(
                     learning_rate * 640 / int(train_batch_size))
                 cfg['SOLVER']['MAX_EPOCHS'] = int(train_epoch)
                 cfg['SOLVER']['TRAIN_DATA']['BATCH_SIZE'] = int(
                     train_batch_size)
                 if 'TUNER' in cfg['SOLVER']:
-                    cfg['SOLVER']['TUNER'] = current_model_info['tuner_para'][
+                    tuner_cfg_list = current_model_info['tuner_para'][
                         tuner_name] if isinstance(
                             current_model_info['tuner_para'],
                             dict) and tuner_name in current_model_info[
                                 'tuner_para'] else None
+                    if tuner_cfg_list is not None:
+                        tuner_params = dict(
+                            lora_alpha=int(lora_alpha),
+                            lora_rank=int(lora_rank),
+                            text_lora_alpha=int(text_lora_alpha),
+                            text_lora_rank=int(text_lora_rank),
+                            sce_ratio=sce_ratio)
+                        tuner_cfg_list = [
+                            update_tuner_cfg(tuner_name, tuner_cfg,
+                                             **tuner_params)
+                            for tuner_cfg in tuner_cfg_list
+                        ]
+                    cfg['SOLVER']['TUNER'] = tuner_cfg_list
+
                 cfg['SOLVER']['TRAIN_DATA'] = prepare_train_data(
                     cfg['SOLVER']['TRAIN_DATA'])
                 if eval_prompts is not None and len(eval_prompts) > 0:
                     cfg['SOLVER']['EVAL_DATA'] = prepare_eval_data(
                         cfg['SOLVER']['EVAL_DATA'])
                 else:
                     cfg['SOLVER'].pop('EVAL_DATA')
-                if 'SAMPLE_ARGS' in cfg['SOLVER']:
+                if 'SAMPLE_ARGS' in cfg[
+                        'SOLVER'] and not enable_resolution_bucket:
                     cfg['SOLVER']['SAMPLE_ARGS']['IMAGE_SIZE'] = [
                         int(resolution_height),
                         int(resolution_width)
                     ]
                 for hook in cfg['SOLVER']['TRAIN_HOOKS']:
                     if hook['NAME'] == 'CheckpointHook':
                         hook['INTERVAL'] = save_interval
@@ -606,18 +807,25 @@
                               f_out,
                               encoding='utf-8',
                               allow_unicode=True,
                               default_flow_style=False)
                 return cfg_file
 
             before_kill_inference = self.trainer_ins.check_memory()
-            for k, v in manager.inference.pipe_manager.pipeline_level_modules.items(
-            ):
-                if hasattr(v, 'dynamic_unload'):
-                    v.dynamic_unload(name='all')
+            if hasattr(manager, 'inference'):
+                for k, v in manager.inference.pipe_manager.pipeline_level_modules.items(
+                ):
+                    if hasattr(v, 'dynamic_unload'):
+                        v.dynamic_unload(name='all')
+            if (hasattr(manager, 'preprocess') and hasattr(
+                    manager.preprocess.dataset_gallery.processors_manager,
+                    'dynamic_unload')):
+                manager.preprocess.dataset_gallery.processors_manager.dynamic_unload(
+                )
+
             after_kill_inference = self.trainer_ins.check_memory()
             message = f'GPU info: {before_kill_inference}. \n\n'
             message += f'After unloading inference models, the GPU info: {after_kill_inference}. \n\n'
             _ = prepare_train_config()
             self.trainer_ins.start_task(work_name)
             message += self.trainer_ins.get_log(work_name)
             if work_name not in inference_ui.model_list:
@@ -625,17 +833,22 @@
             gr.Info('Start Training!' + message)
             return gr.Dropdown.update(choices=inference_ui.model_list,
                                       value=work_name)
 
         self.training_button.click(
             run_train,
             inputs=[
-                self.work_name, self.data_type, self.ms_data_space,
-                self.ms_data_name, self.ms_data_subname, self.base_model,
-                self.base_model_revision, self.tuner_name,
+                self.work_name, self.data_type, self.ori_data_name,
+                self.ms_data_space, self.ms_data_name, self.ms_data_subname,
+                self.base_model, self.base_model_revision, self.tuner_name,
                 self.resolution_height, self.resolution_width,
                 self.train_epoch, self.learning_rate, self.save_interval,
                 self.train_batch_size, self.prompt_prefix,
-                self.replace_keywords, self.push_to_hub, self.eval_prompts
+                self.replace_keywords, self.push_to_hub, self.eval_prompts,
+                self.lora_alpha, self.lora_rank, self.text_lora_alpha,
+                self.text_lora_rank, self.sce_ratio,
+                self.enable_resolution_bucket, self.min_bucket_resolution,
+                self.max_bucket_resolution, self.bucket_resolution_steps,
+                self.bucket_no_upscale
             ],
             outputs=[inference_ui.output_model_name],
             queue=True)
```

## scepter/studio/self_train/utils/config_parser.py

```diff
@@ -92,15 +92,16 @@
         tuner_type, tuner_para = build_meta_index(meta_cfg, config_file)
         config_dict[base_model_name][version] = {
             'config_file': config_file,
             'config_value': cfg,
             'inference_para': inference_paras,
             'tuner_type': tuner_type,
             'tuner_para': tuner_para,
-            'modify_para': modify_para
+            'modify_para': modify_para,
+            'is_share': 'IS_SHARE' in meta_cfg and meta_cfg['IS_SHARE']
         }
         if 'CONTROL_PARAS' in meta_cfg:
             control_type, control_para = build_meta_index_control(
                 meta_cfg, config_file)
             config_dict[base_model_name][version].update({
                 'control_type':
                 control_type,
```

## scepter/studio/tuner_manager/tuner_manager.py

```diff
@@ -14,14 +14,16 @@
                  cfg_general_file,
                  is_debug=False,
                  language='en',
                  root_work_dir='./'):
         cfg_general = Config(cfg_file=cfg_general_file)
         cfg_general.WORK_DIR = os.path.join(root_work_dir,
                                             cfg_general.WORK_DIR)
+        cfg_general.SELF_TRAIN_DIR = os.path.join(root_work_dir,
+                                                  cfg_general.SELF_TRAIN_DIR)
         if not FS.exists(cfg_general.WORK_DIR):
             FS.make_dir(cfg_general.WORK_DIR)
 
         cfg_general = init_env(cfg_general)
         self.info_ui = InfoUI(cfg_general, language=language)
         self.browser_ui = BrowserUI(cfg_general, language=language)
```

## scepter/studio/tuner_manager/manager_ui/browser_ui.py

```diff
@@ -1,13 +1,14 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 from collections import OrderedDict
 
 import gradio as gr
+from swift import push_to_hub
 
 from scepter.modules.utils.config import Config
 from scepter.modules.utils.file_system import FS
 from scepter.studio.tuner_manager.manager_ui.component_names import \
     TunerManagerNames
 from scepter.studio.tuner_manager.utils.dict import (delete_2level_dict,
                                                      update_2level_dict)
@@ -15,26 +16,30 @@
 from scepter.studio.tuner_manager.utils.yaml import save_yaml
 from scepter.studio.utils.uibase import UIBase
 
 
 class BrowserUI(UIBase):
     def __init__(self, cfg, language='en'):
         self.work_dir = cfg.WORK_DIR
+        self.train_dir = cfg.SELF_TRAIN_DIR
+        self.base_model_tuner_methods = cfg.BASE_MODEL_VERSION
         self.yaml = os.path.join(self.work_dir, cfg.TUNER_LIST_YAML)
         if not FS.exists(self.yaml):
             self.saved_tuners = []
             with FS.put_to(self.yaml) as local_path:
                 save_yaml({'TUNERS': self.saved_tuners}, local_path)
         else:
             with FS.get_from(self.yaml) as local_path:
                 self.saved_tuners = Config.get_plain_cfg(
                     Config(cfg_file=local_path).TUNERS)
         self.saved_tuners_to_category()
         self.component_names = TunerManagerNames(language)
         self.language = language
+        self.export_folder = os.path.join(self.work_dir, cfg.EXPORT_DIR)
+        self.readme_file = cfg.README_EN if self.language == 'en' else cfg.README_ZH
 
     def saved_tuners_to_category(self):
         self.saved_tuners_category = OrderedDict()
         for tuner in self.saved_tuners:
             first_level = f"{tuner['BASE_MODEL']}-{tuner['TUNER_TYPE']}"
             second_level = f"{tuner['NAME']}"
             update_2level_dict(self.saved_tuners_category,
@@ -68,22 +73,22 @@
             with gr.Box():
                 gr.Markdown(self.component_names.browser_block_name)
                 with gr.Row(variant='panel', equal_height=True):
                     with gr.Column(scale=4, min_width=0):
                         self.diffusion_models = gr.Dropdown(
                             label=self.component_names.base_models,
                             choices=diffusion_models_choice,
-                            value=diffusion_model,
+                            value=None,
                             multiselect=False,
                             interactive=True)
                     with gr.Column(scale=4, min_width=0):
                         self.tuner_models = gr.Dropdown(
                             label=self.component_names.tuner_name,
                             choices=tuner_models_choice,
-                            value=tuner_model,
+                            value=None,
                             multiselect=False,
                             interactive=True)
                     with gr.Column(scale=1, min_width=0):
                         self.save_button = gr.Button(
                             label='Save',
                             value=self.component_names.save_symbol,
                             elem_classes='type_row',
@@ -91,48 +96,243 @@
                             visible=True)
                         self.delete_button = gr.Button(
                             label='Delete',
                             value=self.component_names.delete_symbol,
                             elem_classes='type_row',
                             elem_id='delete_button',
                             visible=False)
+                        self.model_upload = gr.Button(
+                            label='ModelScope Upload',
+                            value=self.component_names.upload,
+                            elem_classes='type_row',
+                            elem_id='save_button',
+                            visible=True)
                     with gr.Column(scale=1, min_width=0):
                         self.refresh_button = gr.Button(
-                            label='Delete',
+                            label='Refresh',
                             value=self.component_names.refresh_symbol,
                             elem_classes='type_row',
                             elem_id='refresh_button',
                             visible=True)
+                        self.model_download = gr.Button(
+                            label='ModelScope Download',
+                            value=self.component_names.download,
+                            elem_classes='type_row',
+                            elem_id='save_button',
+                            visible=True)
+
+                with gr.Box(visible=False) as self.upload_setting:
+                    gr.Markdown(self.component_names.export_desc)
+                    with gr.Column(variant='panel'):
+                        with gr.Row(equal_height=True):
+                            with gr.Column(scale=9, min_width=0):
+                                self.import_src = gr.Dropdown(
+                                    choices=['modelscope'],
+                                    value='modelscope',
+                                    label=None,
+                                    show_label=False)
+                            with gr.Column(scale=1, min_width=0):
+                                self.ms_upload_close = gr.Button(
+                                    label='Close MS Upload',
+                                    value=self.component_names.close,
+                                    elem_classes='type_row',
+                                    elem_id='save_button')
+                        with gr.Row(equal_height=True):
+                            with gr.Column(scale=3, min_width=0):
+                                self.ms_sdk = gr.Text(
+                                    label=self.component_names.ms_sdk,
+                                    show_label=False,
+                                    container=False,
+                                    placeholder='ModelScope SDK Token',
+                                    value='')
+                            with gr.Column(scale=3, min_width=0):
+                                self.ms_upload_username = gr.Text(
+                                    label=self.component_names.ms_username,
+                                    show_label=False,
+                                    container=False,
+                                    placeholder='ModelScope UserName',
+                                    value='')
+                            with gr.Column(scale=3, min_width=0):
+                                self.model_private = gr.Checkbox(
+                                    label=self.component_names.model_private,
+                                    value=False)
+                            with gr.Column(scale=1, min_width=0):
+                                self.ms_upload_submit = gr.Button(
+                                    label='Submit MS',
+                                    value=self.component_names.ms_submit,
+                                    elem_classes='type_row',
+                                    elem_id='save_button')
+
+                with gr.Box(visible=False) as self.download_setting:
+                    gr.Markdown(self.component_names.import_desc)
+                    with gr.Column(variant='panel'):
+                        with gr.Row(equal_height=True):
+                            with gr.Column(scale=9, min_width=0):
+                                self.import_src = gr.Dropdown(
+                                    choices=['modelscope', 'local'],
+                                    value='modelscope',
+                                    label=None,
+                                    show_label=False)
+                            with gr.Column(scale=1, min_width=0):
+                                self.ms_download_close = gr.Button(
+                                    label='Close MS Download',
+                                    value=self.component_names.close,
+                                    elem_classes='type_row',
+                                    elem_id='save_button')
+                        with gr.Row(
+                                equal_height=True) as self.ms_import_setting:
+                            with gr.Column(scale=4.5, min_width=0):
+                                self.ms_modelid = gr.Text(
+                                    label=self.component_names.ms_modelid,
+                                    show_label=False,
+                                    container=False,
+                                    placeholder='ModelScope Model Path',
+                                    value='')
+                            with gr.Column(scale=4.5, min_width=0):
+                                self.ms_download_username = gr.Text(
+                                    label=self.component_names.ms_username,
+                                    show_label=False,
+                                    container=False,
+                                    placeholder='ModelScope UserName',
+                                    value='')
+                            with gr.Column(scale=1, min_width=0):
+                                self.ms_download_submit = gr.Button(
+                                    label='Submit MS',
+                                    value=self.component_names.ms_submit,
+                                    elem_classes='type_row',
+                                    elem_id='save_button')
+                        with gr.Row(visible=False, equal_height=True
+                                    ) as self.local_import_setting:
+                            with gr.Column(scale=2, min_width=0):
+                                self.file_path = gr.File(
+                                    label=self.component_names.zip_file,
+                                    min_width=0,
+                                    file_types=['.zip'],
+                                    elem_classes='upload_zone')
+                            with gr.Column(scale=1, min_width=0):
+                                self.upload_base_models = gr.Dropdown(
+                                    label=self.component_names.ubase_model,
+                                    choices=[
+                                        base_model_version.BASE_MODEL
+                                        for base_model_version in
+                                        self.base_model_tuner_methods
+                                    ],
+                                    value=self.base_model_tuner_methods[0].
+                                    BASE_MODEL,
+                                    multiselect=False,
+                                    interactive=True)
+                            with gr.Column(scale=1, min_width=0):
+                                self.upload_tuner_type = gr.Dropdown(
+                                    label=self.component_names.utuner_type,
+                                    choices=self.base_model_tuner_methods[0].
+                                    TUNER_TYPE,
+                                    value=self.base_model_tuner_methods[0].
+                                    TUNER_TYPE[0],
+                                    multiselect=False,
+                                    interactive=True)
+                            with gr.Column(scale=1, min_width=0):
+                                self.upload_tuner_name = gr.Text(
+                                    label=self.component_names.utuner_name,
+                                    show_label=False,
+                                    container=False,
+                                    placeholder='Upload Tuner Name',
+                                    value='')
+                                self.local_upload_bt = gr.Button(
+                                    label='Submit Local Model',
+                                    value=self.component_names.ms_submit,
+                                    elem_classes='type_row',
+                                    elem_id='upload_button')
 
     def check_new_name(self, tuner_name):
         if tuner_name.strip() == '':
             return False, f"'{tuner_name}' is whitespace! Only support 'a-z', 'A-Z', '0-9' and '_'."
         if not is_valid_filename(tuner_name):
             return False, f"'{tuner_name}' is not a valid tuner name! Only support 'a-z', 'A-Z', '0-9' and '_'."
         for tuner in self.saved_tuners:
             if tuner_name == tuner['NAME']:
                 return False, f"Tuner name '{tuner_name}' has been taken!"
         return True, 'legal'
 
-    def save_tuner(self, src_path, sub_dir, tuner_name, tuner_example):
+    def save_tuner(self, src_path, sub_dir, tuner_name, tuner_desc,
+                   tuner_example, tuner_prompt_example):
         tar_path = os.path.join(self.work_dir, sub_dir)
         if not FS.exists(tar_path):
             FS.make_dir(tar_path)
         tar_path = os.path.join(tar_path, tuner_name)
 
         FS.put_dir_from_local_dir(src_path, tar_path)
 
+        # save image
         tuner_example_path = None
         if tuner_example is not None:
             from PIL import Image
-            tuner_example_path = os.path.join(tar_path, f'{tuner_name}.jpg')
-            tuner_example = Image.fromarray(tuner_example)
-            with FS.put_to(tuner_example_path) as local_path:
-                tuner_example.save(local_path)
-        return tar_path, tuner_example_path
+            tuner_example_path = os.path.join(tar_path, 'image.jpg')
+            if not os.path.exists(tuner_example_path):
+                tuner_example = Image.fromarray(tuner_example)
+                with FS.put_to(tuner_example_path) as local_path:
+                    tuner_example.save(local_path)
+
+        # save param
+        enable_share = True
+        split_path = src_path.split('/')
+        if split_path[-2] == 'checkpoints':
+            src_dir = '/'.join(split_path[:-2])
+            ckpt_name = split_path[-1]
+            meta_read = f'{src_dir}/meta_{ckpt_name}.yaml'
+            meta_save = f'{tar_path}/params.yaml'
+        else:
+            meta_read = f'{src_path}/params.yaml'
+            meta_save = f'{tar_path}/params.yaml'
+
+        if os.path.exists(meta_read):
+            meta = Config(cfg_file=meta_read)
+            with FS.put_to(meta_save) as local_path:
+                enable_share = Config.get_plain_cfg(meta.get('IS_SHARE', True))
+                params = Config.get_plain_cfg(meta.get('PARAMS', {}))
+                params['work_dir'] = ''
+                params['work_name'] = ''
+                save_yaml({'PARAMS': params}, local_path)
+
+            # rewrite readme
+            with open(self.readme_file, 'r') as f:
+                rc = f.read()
+
+            rc = rc.replace(r'{MODEL_NAME}', tuner_name)
+            rc = rc.replace(r'{MODEL_DESCRIPTION}',
+                            tuner_desc if len(tuner_desc) > 0 else tuner_name)
+            rc = rc.replace(r'{EVAL_PROMPT}', tuner_prompt_example)
+            rc = rc.replace(r'{IMAGE_PATH}', './image.jpg')
+            rc = rc.replace(r'{BASE_MODEL}',
+                            meta['PARAMS'].get('base_model_revision', ''))
+            rc = rc.replace(r'{TUNER_TYPE}',
+                            meta['PARAMS'].get('tuner_name', ''))
+            rc = rc.replace(r'{TRAIN_BATCH_SIZE}',
+                            str(meta['PARAMS'].get('train_batch_size', '')))
+            rc = rc.replace(r'{TRAIN_EPOCH}',
+                            str(meta['PARAMS'].get('train_epoch', '')))
+            rc = rc.replace(r'{LEARNING_RATE}',
+                            str(meta['PARAMS'].get('learning_rate', '')))
+            rc = rc.replace(r'{HEIGHT}',
+                            str(meta['PARAMS'].get('resolution_height', '')))
+            rc = rc.replace(r'{WIDTH}',
+                            str(meta['PARAMS'].get('resolution_width', '')))
+            rc = rc.replace(r'{DATA_TYPE}',
+                            meta['PARAMS'].get('data_type', ''))
+            rc = rc.replace(r'{MS_DATA_SPACE}',
+                            meta['PARAMS'].get('ms_data_space', ''))
+            rc = rc.replace(r'{MS_DATA_NAME}',
+                            meta['PARAMS'].get('ms_data_name', ''))
+            rc = rc.replace(r'{MS_DATA_SUBNAME}',
+                            meta['PARAMS'].get('ms_data_subname', ''))
+
+            with FS.put_to(os.path.join(tar_path, 'README.md')) as local_path:
+                with open(local_path, 'w') as f:
+                    f.write(rc)
+
+        return tar_path, tuner_example_path, enable_share
 
     def add_tuner(self, new_tuner, manager, now_diffusion_model):
         self.saved_tuners.append(new_tuner)
         self.saved_tuners_to_category()
         with FS.put_to(self.yaml) as local_path:
             save_yaml({'TUNERS': self.saved_tuners}, local_path)
 
@@ -192,14 +392,20 @@
             if self.language == 'zh':
                 del name_level_tuners[del_tuner.BASE_MODEL][del_tuner.NAME_ZH]
             else:
                 del name_level_tuners[del_tuner.BASE_MODEL][del_tuner.NAME]
 
         return custom_tuner_choices
 
+    def update_tuner_info(self, base_model, tuner_name, update_items):
+        self.saved_tuners_category[base_model][tuner_name].update(update_items)
+        self.category_to_saved_tuners()
+        with FS.put_to(self.yaml) as local_path:
+            save_yaml({'TUNERS': self.saved_tuners}, local_path)
+
     def set_callbacks(self, manager, info_ui):
         def refresh_browser():
             diffusion_models_choice, diffusion_model, tuner_models_choice, tuner_model = self.get_choices_and_values(
             )
             return (gr.Dropdown(choices=diffusion_models_choice,
                                 value=diffusion_model),
                     gr.Dropdown(choices=tuner_models_choice,
@@ -218,84 +424,105 @@
 
         self.diffusion_models.change(diffusion_model_change,
                                      inputs=[self.diffusion_models],
                                      outputs=[self.tuner_models],
                                      queue=True)
 
         def tuner_model_change(tuner_model, diffusion_model):
-            tuner_info = {}
-            if tuner_model is not None:
-                tuner_info = self.saved_tuners_category[diffusion_model][
-                    tuner_model]
+            if tuner_model is None:
+                # fix refresh bug
+                return (gr.Text(), gr.Text(), gr.Text(), gr.Text(), gr.Text(),
+                        gr.Image(), gr.Text(), gr.Text())
+
+            tuner_info = self.saved_tuners_category[diffusion_model][
+                tuner_model]
             image_path = tuner_info.get('IMAGE_PATH', None)
             if image_path is not None:
                 image_path = FS.get_from(image_path)
             return (gr.Text(value=tuner_info.get('NAME', '')),
                     gr.Text(value=tuner_info.get('NAME', ''),
                             interactive=True),
                     gr.Text(value=tuner_info.get('TUNER_TYPE', '')),
                     gr.Text(value=tuner_info.get('BASE_MODEL', '')),
                     gr.Text(value=tuner_info.get('DESCRIPTION', ''),
                             interactive=True), gr.Image(value=image_path),
                     gr.Text(value=tuner_info.get('PROMPT_EXAMPLE', ''),
-                            interactive=True))
+                            interactive=True),
+                    gr.Text(value=tuner_info.get('MODELSCOPE_URL', ''),
+                            interactive=False))
 
         self.tuner_models.change(
             tuner_model_change,
             inputs=[self.tuner_models, self.diffusion_models],
             outputs=[
                 info_ui.tuner_name, info_ui.new_name, info_ui.tuner_type,
                 info_ui.base_model, info_ui.tuner_desc, info_ui.tuner_example,
-                info_ui.tuner_prompt_example
+                info_ui.tuner_prompt_example, info_ui.ms_url
             ],
             queue=False)
 
         def save_tuner(tuner_name, new_name, tuner_desc, tuner_example,
-                       tuner_prompt_example, now_diffusion_model, info_path):
+                       tuner_prompt_example, base_model, tuner_type):
             is_legal, msg = self.check_new_name(new_name)
             if not is_legal:
                 gr.Info('Save failed because ' + msg)
-                return (gr.Dropdown(), gr.Text(), gr.Dropdown())
-
-            info = Config(cfg_file=info_path)
-            model_dir = info.MODEL_PATH
-            sub_dir = f'{info.BASE_MODEL}-{info.TUNER_TYPE}'
-            model_dir, tuner_example = self.save_tuner(model_dir, sub_dir,
-                                                       new_name, tuner_example)
+                return (gr.Dropdown(), gr.Dropdown(), gr.Text(), gr.Dropdown())
 
+            sub_dir = f'{base_model}-{tuner_type}'
+            if os.path.exists(os.path.join(self.train_dir, '@'.join(tuner_name.split('@')[:-1]))) \
+                    and len(tuner_name.split('@')[:-1]) > 0:
+                steps = tuner_name.split('@')[-1]
+                model_dir = os.path.join(self.train_dir,
+                                         '@'.join(tuner_name.split('@')[:-1]),
+                                         'checkpoints', steps)
+            else:
+                model_dir = self.saved_tuners_category.get(sub_dir, {}).get(
+                    tuner_name, {}).get('MODEL_PATH', '')
+                if model_dir == '':
+                    gr.Error(self.component_names.model_err4 + tuner_name)
+
+            model_dir, tuner_example, enable_share = self.save_tuner(
+                model_dir, sub_dir, new_name, tuner_desc, tuner_example,
+                tuner_prompt_example)
             # config info update
             new_tuner = {
                 'NAME': new_name,
                 'NAME_ZH': new_name,
                 'SOURCE': 'self_train',
                 'DESCRIPTION': tuner_desc,
-                'BASE_MODEL': info.BASE_MODEL,
+                'BASE_MODEL': base_model,
                 'MODEL_PATH': model_dir,
                 'IMAGE_PATH': tuner_example,
-                'TUNER_TYPE': info.TUNER_TYPE,
-                'PROMPT_EXAMPLE': tuner_prompt_example
+                'TUNER_TYPE': tuner_type,
+                'PROMPT_EXAMPLE': tuner_prompt_example,
+                'ENABLE_SHARE': enable_share,
             }
+            pipeline_level_modules = manager.inference.model_manage_ui.pipe_manager.pipeline_level_modules
+            if new_tuner['BASE_MODEL'] not in pipeline_level_modules:
+                gr.Error(self.component_names.model_err3 +
+                         new_tuner['BASE_MODEL'])
+            pipeline_ins = pipeline_level_modules[new_tuner['BASE_MODEL']]
+            now_diffusion_model = f"{new_tuner['BASE_MODEL']}_{pipeline_ins.diffusion_model['name']}"
+
             custom_tuner_choices = self.add_tuner(new_tuner, manager,
                                                   now_diffusion_model)
 
-            return (gr.Dropdown(choices=list(
-                self.saved_tuners_category.keys()),
-                                value=sub_dir),
-                    gr.Dropdown(choices=list(
+            return (gr.update(choices=list(self.saved_tuners_category.keys()),
+                              value=sub_dir),
+                    gr.update(choices=list(
                         self.saved_tuners_category.get(sub_dir, {}).keys()),
-                                value=new_name), gr.Text(value=new_name),
+                              value=new_name), gr.Text(value=new_name),
                     gr.Dropdown(choices=custom_tuner_choices))
 
         self.save_button.click(
             save_tuner,
             inputs=[
                 info_ui.tuner_name, info_ui.new_name, info_ui.tuner_desc,
                 info_ui.tuner_example, info_ui.tuner_prompt_example,
-                manager.inference.model_manage_ui.diffusion_model,
-                manager.inference.infer_info
+                info_ui.base_model, info_ui.tuner_type
             ],
             outputs=[
                 self.diffusion_models, self.tuner_models, info_ui.tuner_name,
                 manager.inference.tuner_ui.custom_tuner_model
             ],
             queue=True)
 
@@ -317,7 +544,267 @@
                 manager.inference.model_manage_ui.diffusion_model
             ],
             outputs=[
                 self.diffusion_models,
                 manager.inference.tuner_ui.custom_tuner_model
             ],
             queue=True)
+
+        def change_visible():
+            return gr.update(visible=True), gr.update(visible=False)
+
+        self.model_upload.click(
+            fn=change_visible,
+            inputs=[],
+            outputs=[self.upload_setting, self.download_setting],
+            queue=False)
+        self.model_download.click(
+            fn=change_visible,
+            inputs=[],
+            outputs=[self.download_setting, self.upload_setting],
+            queue=False)
+
+        def change_invisible():
+            return gr.update(visible=False)
+
+        self.ms_upload_close.click(fn=change_invisible,
+                                   inputs=[],
+                                   outputs=[self.upload_setting],
+                                   queue=False)
+        self.ms_download_close.click(fn=change_invisible,
+                                     inputs=[],
+                                     outputs=[self.download_setting],
+                                     queue=False)
+
+        def change_import_source(import_src):
+            if import_src == 'modelscope':
+                ms_visible = True
+                local_visible = False
+            elif import_src == 'local':
+                ms_visible = False
+                local_visible = True
+            return gr.update(visible=ms_visible), gr.update(
+                visible=local_visible)
+
+        self.import_src.change(
+            fn=change_import_source,
+            inputs=[self.import_src],
+            outputs=[self.ms_import_setting, self.local_import_setting],
+            queue=False)
+
+        def push_to_modelscope(ms_sdk, username, private, base_model_name,
+                               tuner_model_name):
+            tuner = self.saved_tuners_category[base_model_name][
+                tuner_model_name]
+
+            enable_share = tuner.get('ENABLE_SHARE', True)
+            if enable_share:
+                repo_name = f'{username}/{tuner_model_name}'
+                ckpt_path = tuner['MODEL_PATH']
+                ms_url = f'https://www.modelscope.cn/models/{repo_name}'
+
+                with open(os.path.join(ckpt_path, 'README.md'), 'r') as f:
+                    rc = f.read()
+                    rc = rc.replace(r'{MODEL_URL}', ms_url)
+                    rc = rc.replace(r'{USER_NAME}', username)
+                with open(os.path.join(ckpt_path, 'README.md'), 'w') as f:
+                    f.write(rc)
+
+                push_status = push_to_hub(repo_name,
+                                          ckpt_path,
+                                          token=ms_sdk,
+                                          private=private)
+                if push_status:
+                    update_items = {'MODELSCOPE_URL': ms_url}
+                    self.update_tuner_info(base_model_name, tuner_model_name,
+                                           update_items)
+                    gr.Info(
+                        'The tuner model has been uploaded to ModelScope Successfully!'
+                    )
+                    return update_items['MODELSCOPE_URL']
+                else:
+                    gr.Info(
+                        'Error: The model failed to be uploaded to ModelScope!'
+                    )
+                    return ''
+            else:
+                gr.Info(
+                    'Error: The model is not allowed to be shared to ModelScope!'
+                )
+                return ''
+
+        self.ms_upload_submit.click(fn=push_to_modelscope,
+                                    inputs=[
+                                        self.ms_sdk, self.ms_upload_username,
+                                        self.model_private,
+                                        self.diffusion_models,
+                                        self.tuner_models
+                                    ],
+                                    outputs=[info_ui.ms_url],
+                                    queue=True)
+
+        def pull_from_modelscope(modelid, username):
+            tar_path = os.path.join(self.work_dir, 'modelscope')
+            src_path = f'ms://{username}/{modelid}'
+            FS.get_dir_to_local_dir(src_path, tar_path)
+            gr.Info(
+                'The tuner model has been downloaded from ModelScope Successfully!'
+            )
+
+            tar_path = f'{tar_path}/{username}/{modelid}'
+            meta_file = f'{tar_path}/params.yaml'
+            meta = Config(cfg_file=meta_file)
+            base_model = meta['PARAMS']['base_model_revision']
+            tuner_type = meta['PARAMS']['tuner_name']
+            tuner_prompt_example = meta['PARAMS']['eval_prompts'][0]
+            tuner_category = f'{base_model}-{tuner_type}'
+            new_name = f'modelscope@{username}@{modelid}'
+            new_tuner = {
+                'NAME': new_name,
+                'NAME_ZH': new_name,
+                'SOURCE': 'modelscope',
+                'DESCRIPTION': '',
+                'BASE_MODEL': base_model,
+                'MODEL_PATH': tar_path,
+                'IMAGE_PATH': f'{tar_path}/image.jpg',
+                'TUNER_TYPE': tuner_type,
+                'PROMPT_EXAMPLE': tuner_prompt_example
+            }
+            pipeline_level_modules = manager.inference.model_manage_ui.pipe_manager.pipeline_level_modules
+            if new_tuner['BASE_MODEL'] not in pipeline_level_modules:
+                gr.Error(self.component_names.model_err3 +
+                         new_tuner['BASE_MODEL'])
+            pipeline_ins = pipeline_level_modules[new_tuner['BASE_MODEL']]
+            now_diffusion_model = f"{new_tuner['BASE_MODEL']}_{pipeline_ins.diffusion_model['name']}"
+
+            custom_tuner_choices = self.add_tuner(new_tuner, manager,
+                                                  now_diffusion_model)
+
+            update_items = {
+                'MODELSCOPE_URL':
+                f'https://www.modelscope.cn/models/{username}/{modelid}'
+            }
+            self.update_tuner_info(tuner_category,
+                                   new_name,
+                                   update_items=update_items)
+
+            return (gr.update(choices=list(self.saved_tuners_category.keys()),
+                              value=tuner_category),
+                    gr.update(choices=list(
+                        self.saved_tuners_category.get(tuner_category,
+                                                       {}).keys()),
+                              value=new_name), gr.Text(value=new_name),
+                    gr.Dropdown(choices=custom_tuner_choices))
+
+        self.ms_download_submit.click(
+            fn=pull_from_modelscope,
+            inputs=[
+                self.ms_modelid,
+                self.ms_download_username,
+            ],
+            outputs=[
+                self.diffusion_models, self.tuner_models, info_ui.tuner_name,
+                manager.inference.tuner_ui.custom_tuner_model
+            ],
+            queue=True)
+
+        def change_tuner_type_by_model_version(base_model_revision):
+            for base_model_tuner in self.base_model_tuner_methods:
+                if base_model_tuner.BASE_MODEL == base_model_revision:
+                    return gr.Dropdown(value=base_model_tuner.TUNER_TYPE[0],
+                                       choices=base_model_tuner.TUNER_TYPE,
+                                       interactive=True)
+            return gr.Dropdown(value='', choices=[], interactive=True)
+
+        self.upload_base_models.change(fn=change_tuner_type_by_model_version,
+                                       inputs=[self.upload_base_models],
+                                       outputs=[self.upload_tuner_type],
+                                       queue=False)
+
+        def upload_zip(file_path, tuner_name, base_model, tuner_type):
+            sub_dir = f'{base_model}-{tuner_type}'
+            save_file = os.path.join(self.work_dir, sub_dir,
+                                     f'{tuner_name}.zip')
+            model_dir = os.path.join(self.work_dir, sub_dir, tuner_name)
+            with FS.put_to(save_file) as local_zip:
+                res = os.popen(f"cp '{file_path.name}' '{local_zip}'")
+                res = res.readlines()
+            with FS.get_from(save_file) as local_path:
+                res = os.popen(f"unzip -o '{local_path}' -d '{model_dir}'")
+                res = res.readlines()
+            if not os.path.exists(model_dir):
+                raise gr.Error(f'解压{save_file}失败{str(res)}')
+            # find meta.yaml
+            if os.path.exists(os.path.join(model_dir, 'meta.yaml')):
+                meta = Config(cfg_file=os.path.join(model_dir, 'meta.yaml'))
+                tuner_desc = meta.get('DESCRIPTION', '')
+                tuner_example = meta.get('IMAGE_PATH', None)
+                if tuner_example is not None:
+                    tuner_example = os.path.join(
+                        model_dir, os.path.basename(tuner_example))
+                    if os.path.exists(tuner_example):
+                        from PIL import Image
+                        tuner_example_path = os.path.join(
+                            model_dir, 'image.jpg')
+                        if not os.path.exists(tuner_example_path):
+                            tuner_example = Image.open(tuner_example)
+                            with FS.put_to(tuner_example_path) as local_path:
+                                tuner_example.save(local_path)
+                        tuner_example = tuner_example_path
+                    else:
+                        tuner_example = None
+                tuner_prompt_example = meta.get('PROMPT_EXAMPLE', '')
+            else:
+                tuner_desc = ''
+                tuner_example = None
+                tuner_prompt_example = ''
+
+            if not FS.exists(model_dir):
+                raise gr.Error(
+                    f'{self.component_names.illegal_data_err1}{str(res)}')
+            # config info update
+            new_tuner = {
+                'NAME': tuner_name,
+                'NAME_ZH': tuner_name,
+                'SOURCE': 'self_train',
+                'DESCRIPTION': tuner_desc,
+                'BASE_MODEL': base_model,
+                'MODEL_PATH': model_dir,
+                'IMAGE_PATH': tuner_example,
+                'TUNER_TYPE': tuner_type,
+                'PROMPT_EXAMPLE': tuner_prompt_example
+            }
+            pipeline_level_modules = manager.inference.model_manage_ui.pipe_manager.pipeline_level_modules
+            if new_tuner['BASE_MODEL'] not in pipeline_level_modules:
+                gr.Error(self.component_names.model_err3 +
+                         new_tuner['BASE_MODEL'])
+            pipeline_ins = pipeline_level_modules[new_tuner['BASE_MODEL']]
+            now_diffusion_model = f"{new_tuner['BASE_MODEL']}_{pipeline_ins.diffusion_model['name']}"
+
+            custom_tuner_choices = self.add_tuner(new_tuner, manager,
+                                                  now_diffusion_model)
+
+            return (gr.Dropdown(choices=list(
+                self.saved_tuners_category.keys()),
+                                value=sub_dir),
+                    gr.Dropdown(choices=list(
+                        self.saved_tuners_category.get(sub_dir, {}).keys()),
+                                value=tuner_name), gr.Text(value=tuner_name),
+                    gr.Text(value=tuner_type), gr.Text(value=base_model),
+                    gr.Text(value=tuner_desc),
+                    gr.Text(value=tuner_prompt_example),
+                    gr.Image(value=tuner_example),
+                    gr.Dropdown(choices=custom_tuner_choices))
+
+        self.local_upload_bt.click(
+            upload_zip,
+            inputs=[
+                self.file_path, self.upload_tuner_name,
+                self.upload_base_models, self.upload_tuner_type
+            ],
+            outputs=[
+                self.diffusion_models, self.tuner_models, info_ui.tuner_name,
+                info_ui.tuner_type, info_ui.base_model, info_ui.tuner_desc,
+                info_ui.tuner_prompt_example, info_ui.tuner_example,
+                manager.inference.tuner_ui.custom_tuner_model
+            ],
+            queue=False)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## scepter/studio/tuner_manager/manager_ui/component_names.py

```diff
@@ -4,35 +4,82 @@
 
 
 class TunerManagerNames():
     def __init__(self, language='en'):
         self.save_symbol = '\U0001F4BE'  # 💾
         self.delete_symbol = '\U0001f5d1'  # 🗑️
         self.refresh_symbol = '\U0001f504'  # 🔄
+        self.upload = '\U0001F517'  # 🔗
+        self.download = '\U00002795'  # ➕
+        self.ms_submit = '\U00002714'  # ✔️
+        self.close = '\U00002716'  # ✖️
+
         if language == 'en':
-            self.browser_block_name = 'Tuner Browser'
+            self.browser_block_name = 'Tuner Browser  ' \
+                                      '(\U0001F4BE: Save; \U0001f504: Refresh; \U0001F517: Export; \U00002795: Import)'
             self.base_models = 'Base Model-Tuner Type'
             self.tuner_models = 'Tuner Name'
             self.info_block_name = 'Tuner Info'
             self.tuner_name = 'Tuner Name'
             self.rename = 'Rename'
             self.tuner_type = 'Tuner Type'
             self.base_model_name = 'Base Model Name'
             self.tuner_desc = 'Tuner Description'
             self.tuner_example = 'Results Example'
             self.tuner_prompt_example = 'Prompt Example'
+            self.model_err3 = "Doesn't surpport this base model"
+            self.model_err4 = \
+                "This model maybe not finish training, because model doesn't exist. Please save model first."
+            self.model_err5 = 'Model name not registered locally.'
+            self.go_to_inference = 'Go To Inference'
             self.save = 'save changes'
             self.delete = 'Delete'
+            self.ms_sdk = 'ModelScope API Token'
+            self.ms_username = 'ModelScope User Name'
+            self.model_private = 'Model Private'
+            self.ms_modelid = 'ModelScope Model ID'
+            self.ms_url = 'ModelScope Model Url'
+            self.ms_model_path = 'Hub Model ID'
+            self.export_file = 'Download Model'
+            self.export_zip_err1 = 'export model failure'
+            self.zip_file = 'upload model'
+            self.utuner_name = 'Upload Tuner Name'
+            self.ubase_model = 'Upload Base Model'
+            self.utuner_type = 'Upload Tuner Type'
+            self.illegal_data_err1 = 'Upload File Format Error(not .zip)'
+            self.download_to_local = 'Download to Local'
+            self.export_desc = 'Model Export To ModelScope  (\U00002714: Submit; \U00002716: Close)'
+            self.import_desc = 'Model Import From **ModelScope/Local**  (\U00002714: Submit; \U00002716: Close)'
         elif language == 'zh':
-            self.browser_block_name = '微调模型查找'
+            self.browser_block_name = '微调模型查找  (\U0001F4BE: 保存; \U0001f504: 刷新; \U0001F517: 导出; \U00002795: 导入)'
             self.base_models = '基模型-微调类型'
             self.tuner_models = '微调模型名称'
             self.info_block_name = '微调模型详情'
             self.tuner_name = '微调模型名称'
             self.rename = '重命名'
             self.tuner_type = '微调模型类型'
             self.base_model_name = '基模型名称'
             self.tuner_desc = '微调模型描述'
             self.tuner_example = '示例结果'
             self.tuner_prompt_example = '示例提示词'
+            self.model_err3 = '不支持的基础模型'
+            self.model_err4 = '模型可能没有训练完成或者模型不存在，请先保存模型'
+            self.model_err5 = '模型名未本地注册'
+            self.go_to_inference = '使用模型'
             self.save = '保存修改'
             self.delete = '删除'
+            self.ms_sdk = 'ModelScope API Token'
+            self.ms_username = 'ModelScope用户名'
+            self.model_private = '模型不公开'
+            self.ms_modelid = 'ModelScope模型ID'
+            self.ms_url = 'ModelScope模型地址'
+            self.ms_model_path = 'MS模型地址'
+            self.export_file = '下载数据'
+            self.export_zip_err1 = '导出模型失败'
+            self.zip_file = '上传模型'
+            self.utuner_name = '上传微调模型名称'
+            self.ubase_model = '上传基模型类型'
+            self.utuner_type = '上传微调模型类型'
+            self.illegal_data_err1 = '上传文件格式错误(not .zip)'
+            self.download_to_local = '下载至本地'
+            self.export_desc = '模型导出至modelscope  (\U00002714: 提交; \U00002716: 关闭)'
+            self.import_desc = '模型从 **modelscope/本地** 导入   (\U00002714: 提交; \U00002716: 关闭)'
```

## scepter/studio/tuner_manager/manager_ui/info_ui.py

```diff
@@ -1,20 +1,28 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
+import copy
+import os
 
 import gradio as gr
+import yaml
 
+from scepter.modules.utils.config import Config
+from scepter.modules.utils.file_system import FS
 from scepter.studio.tuner_manager.manager_ui.component_names import \
     TunerManagerNames
 from scepter.studio.utils.uibase import UIBase
 
 
 class InfoUI(UIBase):
     def __init__(self, cfg, language='en'):
         self.component_names = TunerManagerNames(language)
+        self.work_dir = cfg.WORK_DIR
+        self.export_folder = os.path.join(self.work_dir, cfg.EXPORT_DIR)
+        self.language = language
 
     def create_ui(self, *args, **kwargs):
         with gr.Column():
             with gr.Box():
                 gr.Markdown(self.component_names.info_block_name)
                 with gr.Row(variant='panel', equal_height=True):
                     with gr.Column(variant='panel', scale=1, min_width=0):
@@ -56,10 +64,159 @@
                                     interactive=True)
                             with gr.Row(equal_height=True):
                                 self.tuner_prompt_example = gr.Text(
                                     value='',
                                     label=self.component_names.
                                     tuner_prompt_example,
                                     lines=2)
+                            with gr.Row(equal_height=True):
+                                self.ms_url = gr.Text(
+                                    value='',
+                                    label=self.component_names.ms_url,
+                                    lines=2)
+                            with gr.Row(equal_height=True):
+                                with gr.Column(scale=1, min_width=0):
+                                    self.go_to_inferece_btn = gr.Button(
+                                        self.component_names.go_to_inference)
+                                with gr.Column(scale=1, min_width=0):
+                                    self.local_download_bt = gr.Button(
+                                        label='Download to Local Dir',
+                                        value=self.component_names.
+                                        download_to_local,
+                                        # elem_classes='type_row',
+                                        elem_id='save_button')
+                                    self.export_url = gr.File(
+                                        label=self.component_names.export_file,
+                                        visible=False,
+                                        value=None,
+                                        interactive=False,
+                                        show_label=True)
 
     def set_callbacks(self, manager):
-        pass
+        def go_to_inferece(new_name, tuner_desc, tuner_prompt_example,
+                           tuner_type, base_model):
+            sub_dir = f'{base_model}-{tuner_type}'
+            tar_path = os.path.join(self.work_dir, sub_dir)
+            model_dir = os.path.join(tar_path, new_name)
+            if not os.path.exists(model_dir):
+                tuner_list = Config(
+                    cfg_file=os.path.join(self.work_dir, 'tuner_list.yaml'))
+                for tuner_item in tuner_list.get('TUNERS', []):
+                    if tuner_item.NAME == new_name:
+                        model_dir = tuner_item.MODEL_PATH
+            tuner_example = os.path.join(model_dir, 'image.jpg')
+            if not os.path.exists(tuner_example):
+                tuner_example = None
+            tuner_dict = {
+                'NAME': new_name,
+                'NAME_ZH': new_name,
+                'SOURCE': 'self_train',
+                'DESCRIPTION': tuner_desc,
+                'BASE_MODEL': base_model,
+                'MODEL_PATH': model_dir,
+                'IMAGE_PATH': tuner_example,
+                'TUNER_TYPE': tuner_type,
+                'PROMPT_EXAMPLE': tuner_prompt_example
+            }
+            tuner_cfg = Config(cfg_dict=tuner_dict, load=False)
+            cfg_file = os.path.join(model_dir, 'meta.yaml')
+            if not os.path.exists(model_dir):
+                gr.Error(self.component_names.model_err4)
+
+            pipeline_level_modules = manager.inference.model_manage_ui.pipe_manager.pipeline_level_modules
+            if tuner_cfg.BASE_MODEL not in pipeline_level_modules:
+                gr.Error(self.component_names.model_err3 +
+                         tuner_cfg.BASE_MODEL)
+            pipeline_ins = pipeline_level_modules[tuner_cfg.BASE_MODEL]
+            diffusion_model = f"{tuner_cfg.BASE_MODEL}_{pipeline_ins.diffusion_model['name']}"
+
+            default_choices = manager.inference.model_manage_ui.pipe_manager.module_level_choices
+            if 'customized_tuners' in default_choices:
+                if tuner_cfg.BASE_MODEL not in default_choices[
+                        'customized_tuners']:
+                    default_choices['customized_tuners'] = {}
+                tunner_choices = default_choices['customized_tuners'][
+                    tuner_cfg.BASE_MODEL]['choices']
+                tunner_default = tuner_cfg.NAME if self.language == 'en' else tuner_cfg.NAME_ZH
+                if tunner_default not in tunner_choices:
+                    if self.language == 'zh':
+                        gr.Error(self.component_names.model_err5 +
+                                 tuner_cfg.NAME_ZH)
+                    else:
+                        gr.Error(self.component_names.model_err5 +
+                                 tuner_cfg.NAME)
+                if not isinstance(tunner_default, list):
+                    tunner_default = [tunner_default]
+            else:
+                tunner_choices = []
+                tunner_default = []
+
+            with open(cfg_file, 'w') as f_out:
+                yaml.dump(copy.deepcopy(tuner_cfg.cfg_dict),
+                          f_out,
+                          encoding='utf-8',
+                          allow_unicode=True,
+                          default_flow_style=False)
+
+            base_model = tuner_cfg.get('BASE_MODEL', '')
+
+            if not base_model == '':
+                if base_model not in manager.inference.tuner_ui.name_level_tuners:
+                    manager.inference.tuner_ui.name_level_tuners[
+                        base_model] = {}
+                manager.inference.tuner_ui.name_level_tuners[base_model][
+                    tuner_cfg.NAME] = tuner_cfg
+
+            return (
+                gr.Tabs(selected='inference'), cfg_file,
+                gr.Tabs(selected='tuner_ui'),
+                gr.CheckboxGroup(
+                    value='使用微调' if self.language == 'zh' else 'Use Tuners'),
+                gr.Dropdown(value=diffusion_model),
+                gr.Dropdown(choices=tunner_choices, value=tunner_default))
+
+        self.go_to_inferece_btn.click(
+            go_to_inferece,
+            inputs=[
+                self.new_name, self.tuner_desc, self.tuner_prompt_example,
+                self.tuner_type, self.base_model
+            ],
+            outputs=[
+                manager.tabs, manager.inference.infer_info,
+                manager.inference.setting_tab,
+                manager.inference.check_box_for_setting,
+                manager.inference.model_manage_ui.diffusion_model,
+                manager.inference.tuner_ui.custom_tuner_model
+            ],
+            queue=True)
+
+        def export_zip(tuner_name, base_model, tuner_type):
+            sub_dir = f'{base_model}-{tuner_type}'
+            if os.path.exists(os.path.join(self.work_dir, sub_dir,
+                                           tuner_name)):
+                model_dir = os.path.join(self.work_dir, sub_dir, tuner_name)
+            else:
+                model_dir = ''
+                tuner_list = Config(
+                    cfg_file=os.path.join(self.work_dir, 'tuner_list.yaml'))
+                for tuner_item in tuner_list.get('TUNERS', []):
+                    if tuner_item.NAME == tuner_name:
+                        model_dir = tuner_item.MODEL_PATH
+                        break
+                if not os.path.exists(model_dir) or model_dir == '':
+                    raise gr.Error(self.component_names.model_err4)
+            zip_path = os.path.join(self.export_folder, f'{tuner_name}.zip')
+            with FS.put_to(zip_path) as local_zip:
+                res = os.popen(
+                    f"cd '{model_dir}' "
+                    f"&& zip -r '{os.path.abspath(local_zip)}' ./* ")
+                print(res.readlines())
+            if not FS.exists(zip_path):
+                raise gr.Error(self.component_names.export_zip_err1)
+            local_zip = FS.get_from(zip_path)
+            return gr.File(value=local_zip, visible=True)
+
+        self.local_download_bt.click(
+            export_zip,
+            inputs=[self.tuner_name, self.base_model, self.tuner_type],
+            outputs=[self.export_url],
+            queue=False)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## scepter/studio/utils/env.py

```diff
@@ -11,7 +11,13 @@
             for file_sys in file_system:
                 _prefix = FS.init_fs_client(file_sys)
         elif file_system is not None:
             _prefix = FS.init_fs_client(file_system)  # noqa
     is_flag = FS.make_dir(work_dir)
     assert is_flag
     return cfg_general
+
+
+def get_available_memory():
+    import psutil
+    mem = psutil.virtual_memory()
+    return {'total': mem.total, 'available': mem.available}
```

## scepter/tools/webui.py

```diff
@@ -124,23 +124,27 @@
                                     root_work_dir=config.WORK_DIR)
         if ifid == '':
             pass  # TODO: Add New Features
         if interface:
             interfaces.append((interface, name, ifid))
             setattr(tab_manager, ifid, interface)
 
-    with gr.Blocks() as demo:
+    css = """
+    .upload_zone { height: 100px; }
+    """
+
+    with gr.Blocks(css=css) as demo:
         if 'BANNER' in config:
             gr.HTML(config.BANNER)
         else:
             gr.Markdown(
                 f"<h2><center>{config.get('TITLE', 'scepter studio')}</center></h2>"
             )
         setattr(tab_manager, 'user_name',
-                gr.Text(value='', visible=False, show_label=False))
+                gr.Text(value='admin', visible=False, show_label=False))
         with gr.Tabs(elem_id='tabs') as tabs:
             setattr(tab_manager, 'tabs', tabs)
             for interface, label, ifid in interfaces:
                 with gr.TabItem(label, id=ifid, elem_id=f'tab_{ifid}'):
                     interface.create_ui()
             for interface, label, ifid in interfaces:
                 interface.set_callbacks(tab_manager)
```

## Comparing `scepter-0.0.4.dist-info/LICENSE` & `scepter-0.0.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `scepter-0.0.4.dist-info/RECORD` & `scepter-0.0.5.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,48 +1,53 @@
 scepter/__init__.py,sha256=EBJ3e1-bjeH1OYlPb-zCg60axF4cC5YDZX5xODEAsSc,602
-scepter/version.py,sha256=0jkVIqgLD4WczOM-4OFKYa85sX0YIBjAHnyMkDyKKls,205
+scepter/version.py,sha256=sJwOQyK5Cb3NsliTS6U7maswPTUMF9lOdbYrcJJvRK4,205
 scepter/methods/examples/classification/example.yaml,sha256=jyB6_Qh5wvht__v0AWLe6oAoNo_-KMOSTZi6amRpRiQ,12618
 scepter/methods/examples/generation/stable_diffusion_1.5_512.yaml,sha256=mKFddVuHkEF3Ae4MIpuPxij6_FdzQeXbfx7yOnOKHU0,5383
 scepter/methods/examples/generation/stable_diffusion_1.5_512_lora.yaml,sha256=RVteGNuYR2NEDnKsRsa2BX0i7HNa-3wlU29yqC-BylM,5568
+scepter/methods/examples/generation/stable_diffusion_1.5_512_textlora.yaml,sha256=PjkB84UA0YlLrIwJM8HdN-Lt_JZgcoeVYe119PHBNv8,5790
 scepter/methods/examples/generation/stable_diffusion_2.1_512.yaml,sha256=yOJIZWIozb24_dPTIyizhm1gO21H_2GOsROvrT_VrLs,4792
 scepter/methods/examples/generation/stable_diffusion_2.1_512_lora.yaml,sha256=PWozRgfdLvA1aYowOSf71O-TOAaJUeEnfTUMDwX_3IA,4982
 scepter/methods/examples/generation/stable_diffusion_2.1_768.yaml,sha256=zdRrwKVbG2FEMg3ZlBNfo5FaGUh2EX55wNNl-TDOrZQ,4789
 scepter/methods/examples/generation/stable_diffusion_2.1_768_lora.yaml,sha256=g81cJDDgjxhNpfzVLuXcdMyTmpCPnn2dzQRzmYgqfNc,4978
 scepter/methods/examples/generation/stable_diffusion_xl_1024.yaml,sha256=UT-c-iKkuBxmztZB-6mpNxIH5Dp_DcrF1sQZDFwfxrQ,9088
 scepter/methods/examples/generation/stable_diffusion_xl_1024_lora.yaml,sha256=RUwN-gmO2b5DFgJfTyGxIyaUsiP0fr6P6Gep09FS_MM,9277
+scepter/methods/examples/generation/stable_diffusion_xl_1024_textlora.yaml,sha256=9_TZP7PmH3jpXoeSxUWSOn-hKEdDFH8JpKhERjVFuuQ,9511
 scepter/methods/scedit/ctr/sd15_512_sce_ctr_hed.yaml,sha256=FCYe6deoCzTFjLg9KsAP-ZQkiFJPTRJ-2e5B1BkUuRs,6440
 scepter/methods/scedit/ctr/sd21_768_sce_ctr_canny.yaml,sha256=TnINnEsyxx-PMZChcy5hhaa_p28fJSg-gyM63pDSb4c,6315
 scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml,sha256=zGFZd8VL0fjg98VeYhH6DlpaM3hU0PNs4zk3kjsO5S8,6434
 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_canny.yaml,sha256=uU5i7KDn-7y2vAPE9RZbc1j6_8H0LKP5zneGYd4K7Ds,10042
 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color.yaml,sha256=M9AbuE2V25fJY1lwSUPzL_snoQgjOZTdmefBKaaFh5c,10058
 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color_datatxt.yaml,sha256=OgRE0GHJxu8_XT_PAQII08sXNiAzVZFekZaULNmSyoU,10420
 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_depth.yaml,sha256=TlvXYfTt1YoHjv-gHPpqqCdUn-sO00eXwdJdMpmF5Qw,10135
 scepter/methods/scedit/t2i/sd15_512_sce_t2i.yaml,sha256=xgvV9wAAVCBvDXkrIh-EK2yNmZcrcArZlqbA3jQGfXA,5639
 scepter/methods/scedit/t2i/sd15_512_sce_t2i_swift.yaml,sha256=nKYtbEvQjXBVwHonmap_ScLomUpI4aIzdytjF0x5uXg,5611
+scepter/methods/scedit/t2i/sd15_512_textsce_t2i_swift.yaml,sha256=cHyoiwnZ9RaiAL9rsM1W2uBE6Cij_M83IrCHHfAxUwQ,5831
 scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml,sha256=Q5iS01J5FxvNEVMRiXQvJum4gl2nIUt04rUCIVZAsvk,5045
 scepter/methods/scedit/t2i/sd21_768_sce_t2i_swift.yaml,sha256=YoIlyaBhr35wdtX0ocOrwot1nAdaUAcV5Dau-j33vZ8,5017
 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml,sha256=v5BUpf8-NVI4gIcuYnjTxCJC-wWYbR52Ojb9dOeHdn0,9370
 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_datatxt.yaml,sha256=jwdaR-0k2RMavb9EVKaSVEoyolxgcNnE1bswhCB7xWI,9514
 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_swift.yaml,sha256=x16N0e4xYdcNYrw8bS-8yNBqgyDhlT5F7nsf1QyCpGk,9324
+scepter/methods/scedit/t2i/sdxl_1024_textsce_t2i_swift.yaml,sha256=bRtdlOhVl2_qm3aW0gUaB1utklQPLP1TSutJfEf6oX4,9556
 scepter/methods/studio/scepter_ui.yaml,sha256=DiPvYhGdYwk8I-zwLCH5sZqNiv90Gr97s-cXyF0_3E4,2539
 scepter/methods/studio/extensions/controllers/official_controllers.yaml,sha256=oWSC2Iua5qFvIpk9tIjLEEZJ8kCOCIhv88BqMc1obXQ,1707
 scepter/methods/studio/extensions/mantra_book/mantra_book.yaml,sha256=Jt4NQYlVeIksIJePEpu5CQhV3xsOy4Zr_pRIYWLAAKs,294732
-scepter/methods/studio/extensions/tuners/official_tuners.yaml,sha256=OZFERJyO35D511n_9jj-iYpP5hVabl5qaaKp6vaJ6oM,29045
+scepter/methods/studio/extensions/tuners/official_tuners.yaml,sha256=7K-UqOrMSYbLnwz3ZsCoU2-es-30v97gfhvP6AZr3uw,29069
 scepter/methods/studio/home/home.yaml,sha256=iVD6Hb2VlNzLEJ9OrH4GJnW70w5IPNmeBxm7_Pmw_c0,2833
 scepter/methods/studio/inference/inference.yaml,sha256=d5c7dFZySYDJvOh-bassKz2lBwCsLcWCQTdAKB7BgRQ,3317
+scepter/methods/studio/inference/edit/stylebooth_tb_pro.yaml,sha256=HEEMczPVXxKDOgDa9Bd8AzkRWIPPuZGVP9LCiC3jcos,2913
 scepter/methods/studio/inference/largen/largen_pro.yaml,sha256=RcrjhcAF6441Tgi3MHcBS66B_HddkoMqr-Q78IfuPpM,10894
 scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml,sha256=J-y_qtPqLUULf1mvoylXR4hcpZDH7tGWrAc4Fyr4CFE,10136
 scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml,sha256=vTbtprC8Dv1UmV4QEempr11orWIxUAauPZaAM5Mu7Kk,2880
 scepter/methods/studio/inference/stable_diffusion/sd21_pro.yaml,sha256=VCd36LyIF2Ye5wEWei9m8OH1z1IRmcR9VMnva7NfQsY,2788
-scepter/methods/studio/preprocess/preprocess.yaml,sha256=N4FewmrR3YR7Q5p79lV9BkmJm_0pf8DMYc5k-78uCCU,148
+scepter/methods/studio/preprocess/preprocess.yaml,sha256=KA0tUi8n1zd3Kw2Twjp3itRyTGXIdjBnVZnMO90giWM,3605
 scepter/methods/studio/self_train/self_train.yaml,sha256=gOPb1ExaykTDAbR9-X0wpsSAErwX-xx-_moP20mg4qg,519
-scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml,sha256=uXjLvQ8qidbU74LqIlncxdXo19YCimqbUREgZGfI_LY,28594
-scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml,sha256=FqQpv0P0wQrTQZupRaMm8RYCjHlV-fAc0hOUDFqFR-E,8290
-scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml,sha256=wujsHz7hHLgB7PDeziTD6PjOE6mTn4aQ4G7pw_--FfA,6781
-scepter/methods/studio/tuner_manager/tuner_manager.yaml,sha256=vH9Q9wPNo7RcgiNf05D1S358u-kxrTfZOCo-R04FpRw,61
+scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml,sha256=dd1iGQ6_6Ng6b72JskAmvc1ezPxtpQlMWcjGDCgd_vQ,28610
+scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml,sha256=fjmDY-FvSHzGbWJpHluMVf6ynmulr3vQnnMdmtIJ7QY,8343
+scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml,sha256=hexrVi3cguQUlbHcZ5Rl8buvxFYKJjqgxaLp22VG7iQ,6834
+scepter/methods/studio/tuner_manager/tuner_manager.yaml,sha256=A746hqlA0olsdn_gq2ICydgmTPC9cyYyJQhA2mNJOOs,515
 scepter/modules/__init__.py,sha256=pY0acKRZ2Q-2mGTta5uW7TtslR4ANmhennHqB1TXJQM,187
 scepter/modules/annotator/__init__.py,sha256=l5Yu-hIW4b9KJrYzbCyzliWmePxwl4_5T6-onLv7qyA,628
 scepter/modules/annotator/base_annotator.py,sha256=kWBQR7w0st9xWO_m6ud-C7ia-YxcFdA8GWSgAGXASb4,2095
 scepter/modules/annotator/canny.py,sha256=jYBdWJl4xuasm367jRgQQDATITxg_8IydxSavMDWcDw,2474
 scepter/modules/annotator/color.py,sha256=snZcvvrBt78qfYVdx18wdXp8toNBo33rdtU5v6BEq-E,2159
 scepter/modules/annotator/hed.py,sha256=1HnnsLrTv1yVkmYCUutLurX2W_uj6EatvHEAXxxEh60,6346
 scepter/modules/annotator/identity.py,sha256=jytDJMq40ELMc3BHb-FXHOc6HKzbEoLWdKzAylTmeRs,767
@@ -65,26 +70,29 @@
 scepter/modules/annotator/mlsd/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scepter/modules/annotator/mlsd/mbv2_mlsd_large.py,sha256=SVZBTEORkbuoPO8H-NyJPnwZMuSc-IUdjtNSg9ROwMU,10166
 scepter/modules/annotator/mlsd/mbv2_mlsd_tiny.py,sha256=8ngw8s7ByEVpgli4YooQg4W3EQMGc5km1v0riGmQ8K8,9706
 scepter/modules/annotator/mlsd/utils.py,sha256=QSaGxOh269CjXM_iy3NjE9BKuc8Cf8CgdPDj_a3VAMc,25395
 scepter/modules/data/__init__.py,sha256=ybb08d8eVgj5mm7qp8Obf-wEhJrrWg6CNuYngrijO5A,125
 scepter/modules/data/dataset/__init__.py,sha256=eD4yKWUbrJdYAVGbH0r-qngQxHmm1ZrN2F4KqL6ZltQ,599
 scepter/modules/data/dataset/base_dataset.py,sha256=tFLNRpXDVW65MXGZGEJXaYoD0kRcSoXzoo6Ovow-aTA,3941
-scepter/modules/data/dataset/dataset.py,sha256=_apyp8NJaU7OYdfDoxwnCEGpZC0uMUPOo9GFsizOOIc,9148
+scepter/modules/data/dataset/dataset.py,sha256=VDtBij3ghkB4EfjFdHi1HbRRkVkCcAuZziPxn2bgvYo,9290
 scepter/modules/data/dataset/ms_dataset.py,sha256=fVTLHUtB2Tr0pF60qsZhmzn2E1bMAODfx32VTVvnFFw,11650
-scepter/modules/data/dataset/registry.py,sha256=0_Gp2zQnKwWqOimFuHNJ-REelQZPuB7U5smEXczK5Ew,15112
+scepter/modules/data/dataset/registry.py,sha256=7PoZHFYLhprYpFp9U5kw1B1hDHHFjK1CTa36Z1frtek,15274
 scepter/modules/data/dataset/utils.py,sha256=Od_qzQVQc-5u4DuVqVqx8tub0p2nweMZe_gQOJoVvPQ,771
-scepter/modules/data/sampler/__init__.py,sha256=KPHaW0uTTANTAJV3bZeIjdx7EaXFhDc-zKXs85cmn0M,407
+scepter/modules/data/sampler/__init__.py,sha256=MtAxuWyiJyjaG7cqbHkjfUdbGThTAyfTDlhliZyAgxY,431
 scepter/modules/data/sampler/base_sampler.py,sha256=R-1vdxA7JxdbisXkPMU5WOh6-uEInVhTe1yN7L8GtcA,1002
 scepter/modules/data/sampler/registry.py,sha256=JvfZnyiJ8HVtKL-PcSZDimWoGTYqoHU6yFRxAE3ObyE,2013
-scepter/modules/data/sampler/sampler.py,sha256=5uMNHRU7qR0g8EZGakwvREigJLA3NB7X1k_41ruci14,20503
+scepter/modules/data/sampler/sampler.py,sha256=hCKDTgEEznsZr8AaOrcXL-7ZhxLdUsn5XoTRTkCx57c,25333
+scepter/modules/data/utils/__init__.py,sha256=b-LxIXP3uIgprSoYfCXE2obVR09bDIixX4mBOevM-xw,140
+scepter/modules/data/utils/data_bucket.py,sha256=17GDGfvGaB9b06ZGsT-Tq68ECm9Um1JhiRz06Z_I6Ls,8453
 scepter/modules/inference/__init__.py,sha256=rm0_DKvx5QCjU9R62mbhj3k4WZk-vEXyFtfG--3KVLo,151
-scepter/modules/inference/control_inference.py,sha256=Wk-COmU961u9qWfZLOzzOTIrbN27M9aNj9Jr_CHDFEw,5117
-scepter/modules/inference/diffusion_inference.py,sha256=Lm53QdZwtEhgGbltpNb8AaClSgbgvIX4ziIih3j8rG8,31050
-scepter/modules/inference/largen_inference.py,sha256=_FmTYScNpJcrkUstEwXx668eZFr8PaUxu_uHoe4CGlg,25815
+scepter/modules/inference/control_inference.py,sha256=VJ1mot7gRZKiAi0Tzd4LeKAJ0sRSqousZtnczUMokH8,5141
+scepter/modules/inference/diffusion_inference.py,sha256=ZTJhaU9J2nVXcT__HxG4AhJRrE6pqSrltRD-velCHHg,31593
+scepter/modules/inference/largen_inference.py,sha256=jsXoub-7CTsTe0ogfsWiqNaIQp3kHob_qV50NzvlVpg,13923
+scepter/modules/inference/stylebooth_inference.py,sha256=q-y5YowkDkn2SxAPTCOQfVoODZXGINNGytQPZQ84pnE,14708
 scepter/modules/inference/tuner_inference.py,sha256=MTj3qBuX__tmrhsFDXXFbRJQiSZ0aVcmTrZ-QRzd-EM,11183
 scepter/modules/model/__init__.py,sha256=miIofw9iOkmpcOdPw_cROKJxEIgz5F4VWFelYRKuO9w,218
 scepter/modules/model/base_model.py,sha256=7W-F96cDBa-pZJw5148KsiC6QP6XaeLkZ2O-Q4A9HOo,4022
 scepter/modules/model/registry.py,sha256=SyTUwIR5nbvuwSlmGa37TiSLKgB40ZaPFl2lbfsRc7Y,1653
 scepter/modules/model/backbone/__init__.py,sha256=bDhKv-uYJ90OOA-85MBvby7BKMU_8-saoFrkiM5qpFw,202
 scepter/modules/model/backbone/autoencoder/__init__.py,sha256=V90piBn8-E3IuD4mKgLz2FZTP1itxAo7dDnKI36Ltr4,300
 scepter/modules/model/backbone/autoencoder/ae_module.py,sha256=9_DF90kdBYKTecoTUg8ZajSPrHPtPDsm1_aSzKZPu20,12971
@@ -97,15 +105,15 @@
 scepter/modules/model/backbone/image/vit_modify.py,sha256=9Blqsl2B2zelvso65IaR_s227-QGVVxvAxxcUmVNmko,8783
 scepter/modules/model/backbone/image/utils/__init__.py,sha256=HNrujjlO25vLPbER-hL1pWMqTMUadDQPLI4vPQo7Cb8,133
 scepter/modules/model/backbone/image/utils/clip.py,sha256=eJ0Xf0tScIpHepOBMGnzD_JwS-cfULQ6TC_2tXurvxk,21405
 scepter/modules/model/backbone/image/utils/simple_tokenizer.py,sha256=QtOuwCsM9a1m7bE_chV0byHtIKFxs9taBfEJyWeaBYQ,5109
 scepter/modules/model/backbone/image/utils/vit.py,sha256=JIy-121urJaLEOJEmVCd1w-L6z2x0QdPc6l0gCOH4jk,21302
 scepter/modules/model/backbone/unet/__init__.py,sha256=v0lr_6oJuxRkmiBlnHVA4smzYBGNVK8w5HDNClGwI2I,303
 scepter/modules/model/backbone/unet/unet_module.py,sha256=I6E7Q8QoiIF74gALAibzDFqmTFNQleZsVrsrQJLBoNo,56311
-scepter/modules/model/backbone/unet/unet_utils.py,sha256=DvsCVO-Yt5Beu8sPyGOcD8Hv0nywDwKyTgwmdFGm85E,46227
+scepter/modules/model/backbone/unet/unet_utils.py,sha256=dHk7kx-47VSpRBX5dj7TY9A9Pc2W4osYpAlk5LD2sLQ,46328
 scepter/modules/model/backbone/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/modules/model/backbone/utils/transformer.py,sha256=w4-z1JTREitVjb5gdsbEoSvSVl-E6rMf3l98ziJvYWo,2031
 scepter/modules/model/backbone/video/__init__.py,sha256=yqIWhv2LrwdSyZF3Y_NsSNM04S8QgkeJPd6TmVtoybI,318
 scepter/modules/model/backbone/video/init_helper.py,sha256=P-ljDhyItxfqhkI8elnyzR--ji2Z9tYLFbhjEhNs2jc,7081
 scepter/modules/model/backbone/video/resnet_3d.py,sha256=yRthN57nR029FhM3CHGsCeZWAO-6VDlHTdJUGGXrELo,30685
 scepter/modules/model/backbone/video/video_transformer.py,sha256=fOjbbSp4YkYWpkMfD-VeMkVfg4VnXsFVTaz-AVL8JfU,13371
 scepter/modules/model/backbone/video/bricks/__init__.py,sha256=RpROQPDeyFTSs4B69mn0bDpSPLiNk8MgzYiyMRoi4c4,683
@@ -119,19 +127,19 @@
 scepter/modules/model/backbone/video/bricks/visualize_3d_module.py,sha256=ipdePnDQlewntQMq-oBsZNuKXHGowVSBSwuN4qOXfc0,2530
 scepter/modules/model/backbone/video/bricks/stems/__init__.py,sha256=HI05tzKiYJ8jQ6zYBkUREbZuH8fGjt6uzUYKWy-xV0c,575
 scepter/modules/model/backbone/video/bricks/stems/base_2d_stem.py,sha256=XYppnQh0ZKPTnMd2_hk-qbc-Yyrpz3nfp_fi2_PFM-w,3006
 scepter/modules/model/backbone/video/bricks/stems/base_3d_stem.py,sha256=BM6V3Id7os4eWHWlN3Gm2905EQARPQyU29oFNWzyYkQ,3160
 scepter/modules/model/backbone/video/bricks/stems/down_sample_stem.py,sha256=dEt3Wd9bZZkt_TiMDgXGDciBdb5Ytu18Dru8agS0q48,1240
 scepter/modules/model/backbone/video/bricks/stems/embedding_stem.py,sha256=N7bxSxRUJthvJ7mG6giyBWyamFpsX7Ex94Ts7Vkg_mI,5534
 scepter/modules/model/backbone/video/bricks/stems/r2plus1d_stem.py,sha256=5V2OiEiC12HPGafQI0RRPoaAhfW6lO4kJiwzt4pjUI0,2576
-scepter/modules/model/embedder/__init__.py,sha256=SSFXSwHX__Ke1HSDN5rwu5MQvONlUKSgWGu8DmDsO-A,602
+scepter/modules/model/embedder/__init__.py,sha256=RKfz7eW2BgcnNFtxK5VOPdJNTDGExJk6oO3AnuCOhRQ,297
 scepter/modules/model/embedder/base_embedder.py,sha256=PVtV2kwI0lsl8B3VmEZ1PSNm_szORM-perui6obI0CI,915
 scepter/modules/model/embedder/embedder.py,sha256=5Pn8bUn_4hdDIe0nh6yKZ_qnLCUKauuQWTVOL5pyIr0,28394
 scepter/modules/model/embedder/resampler.py,sha256=BOODSJOFuRpdNfiGEBWLCpfQf3Um6x-9q5eFyK2vZL8,5133
-scepter/modules/model/head/__init__.py,sha256=kmwV46k7Oi57osOnaumbHI5u-sM1q0NIrmA3T8GbCTw,524
+scepter/modules/model/head/__init__.py,sha256=Gnm7S9Ta0d-GkrEeOOVL44o35HIuBwhN2nO7NPTHYw8,253
 scepter/modules/model/head/classifier_head.py,sha256=6BzgQxO4JAwwSbYEWl3lG5-tMLSmPNSEGcjo5YnCWhk,11687
 scepter/modules/model/loss/__init__.py,sha256=MYaQNATlIOS-5oSRfutAW_vuSRnawABz__paEDzwS0Y,214
 scepter/modules/model/loss/base_losses.py,sha256=SKKeVVnfJHEdQZh3sgTI2f4I_3jLMa3PUtUoyPicO_I,3955
 scepter/modules/model/loss/rec_loss.py,sha256=clrLSbQCt6kh33uaA8kF9T3L-xO-4VOWmcu8TFiYvxo,2178
 scepter/modules/model/metric/__init__.py,sha256=EqZ8qdJG4aMfSdojh7VMaBTfUls4I8dxpnZESusaUOs,286
 scepter/modules/model/metric/base_metric.py,sha256=-uiCGCqnKojvXA9nAz8uYcyzvOnK1HjTnCDelHCnozo,783
 scepter/modules/model/metric/classification.py,sha256=bQ_gYPgaeQgByRrMGVn0qNxdj69UbjS0fnPPZqHvfQI,5076
@@ -139,21 +147,21 @@
 scepter/modules/model/neck/__init__.py,sha256=xtZV4C7EqNWZlftpbZLDeozvWDSLxXCaHFqS3enMDtg,220
 scepter/modules/model/neck/global_average_pooling.py,sha256=kzv8TVtUG5Gw4VWUvtA3FpsB35LfT0ZC9VEUQ3tA29o,1870
 scepter/modules/model/neck/identity.py,sha256=jewod2iymjX1s9ePxaoUR6kl-sVh9uJrsjT9CY7xSvM,918
 scepter/modules/model/network/__init__.py,sha256=TeOJRmt9mYI6GOliYLdVZSBiKaN1V3QS2rKbpOEs1Dc,402
 scepter/modules/model/network/classifier.py,sha256=4T5T4WJkHdkXuqwsjPpafOGAFhm5kmBeqkPiDme36H4,7188
 scepter/modules/model/network/train_module.py,sha256=tJpmsprgYvmncrZBDqNWJWVW5KdxWn4dXX_Bfoh0nb4,1189
 scepter/modules/model/network/autoencoder/__init__.py,sha256=5ZnQ7sjnpRI1KXaM-E8KeyeQl449_e9Kd4SfjzAGsWc,148
-scepter/modules/model/network/autoencoder/ae_kl.py,sha256=JAWJpK5aaZwGqdTRBMy09tIcmnukPg9UD7ExbS58W4I,9601
+scepter/modules/model/network/autoencoder/ae_kl.py,sha256=R9s5aiTckgPHYfYnffl-5wuiomA5ht7GmUsbrv5QQ3M,9534
 scepter/modules/model/network/diffusion/__init__.py,sha256=hPnNnhWF0r-YgU5KLanOLCFPwcGZO25YOHF6V-E2_wg,211
-scepter/modules/model/network/diffusion/diffusion.py,sha256=mytZcp2p86IvSEbaYX1B2X1FfZWuEmoQ59caSLg2Saw,25265
+scepter/modules/model/network/diffusion/diffusion.py,sha256=0bZ4s8HvOIqBh5Ni6ap5eJXmbN3AYCuk6Qhp1SweVqs,25395
 scepter/modules/model/network/diffusion/schedules.py,sha256=O6Ivs1biFNBliFhscc9dcuRHoU15j5s1i9xmH8jEZeI,6226
 scepter/modules/model/network/diffusion/solvers.py,sha256=pur7SDg2-4cO9ssGergjUl686oC1g2Vu6zdBmOJ2tzI,22561
 scepter/modules/model/network/ldm/__init__.py,sha256=DYmZcVOGKcBUDfCmxbwo9oNUy5KduetaCd8HEN6L25U,385
-scepter/modules/model/network/ldm/ldm.py,sha256=tlB5m_HtmRcHLxa7oAxEhCRQAMYYpL7HU44amhY6_zo,19522
+scepter/modules/model/network/ldm/ldm.py,sha256=ANZaRT8nG9xfj2LtrbdGkmfQHImNxbQ4CvGoDw5WT68,19559
 scepter/modules/model/network/ldm/ldm_sce.py,sha256=NcvEsml6feN7ImPLPk_7ZA1o0H08lbxEMc3k90tDnRs,5860
 scepter/modules/model/network/ldm/ldm_xl.py,sha256=12pGU4UkykHCzodjaBxdvOxuHimBAgRA_tCAgjYq8po,21293
 scepter/modules/model/tokenizer/__init__.py,sha256=Gs2lmyHvhLJREg5iVfuHMWL4JgeF1dUepCyvNY7rHBc,368
 scepter/modules/model/tokenizer/base_tokenizer.py,sha256=VuvIHYBdTyL--mMIwEohddtp2Lg56EjqmKW4XSlKBZA,770
 scepter/modules/model/tokenizer/tokenizer.py,sha256=DHmEm_ZZMNN4mqvxcH0y8teYzibS6Lpop425ubw5DCU,5670
 scepter/modules/model/tokenizer/tokenizer_component.py,sha256=EgBazrpxpTx-cF0mTXv_Qqtk_Z2vO4xzikBXspzEGU8,1757
 scepter/modules/model/tuner/__init__.py,sha256=sKQLyvis6V92vZkOrfd_x6tMg0hQXx06hvHXUKCKzaY,260
@@ -161,138 +169,146 @@
 scepter/modules/model/tuner/swift_tuner.py,sha256=YPKjGw04ljMdf4k4l5amDGHOFj7Vy-ol6NCLrtESnlI,4381
 scepter/modules/model/tuner/tuner_component.py,sha256=jvjLji8kjbr3HdL35SaP30svxgZKG_gKhnYKWjNH-RE,6852
 scepter/modules/model/tuner/tuner_utils.py,sha256=8k9qh3wI1_7T5HnBY5IxzJ6Gderv7qbhyyj36s3A6n4,965
 scepter/modules/model/tuner/sce/__init__.py,sha256=38HST4QX_DcrYl9N519RnmrLjHac2UFp84zsiIFUpCQ,222
 scepter/modules/model/tuner/sce/scetuning.py,sha256=t86P6CO2wanpOxnl9sJhsum8q0q6q4RYu7NHU0xQF_E,6716
 scepter/modules/model/tuner/sce/scetuning_component.py,sha256=XPeH5hBfna2QX-_IkUHoioaB4NfGXawMQeXdV6Vjqsk,2485
 scepter/modules/model/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/modules/model/utils/basic_utils.py,sha256=qZrCY5abuKIEg_5_qvqZOu5_pPlKch-QJCvt9efQ6R0,3344
+scepter/modules/model/utils/basic_utils.py,sha256=x0lBr1z2bsCTttcq2DOp1oV2oJLNPBWGSDPzvPAseQg,1176
 scepter/modules/model/utils/data_utils.py,sha256=JfNj_BnhxnIbRFH845oCb9AKN9GeV2YQoAPqa1zfHW0,3776
 scepter/modules/opt/__init__.py,sha256=SDOnU7zqJSWGu1CyYK7dRYku2KB36hsNQjBuJHE1LSo,133
 scepter/modules/opt/lr_schedulers/__init__.py,sha256=nDSd4Fc8gHX_Xdwflu-ACF0dpnaVc8PLaSFtzfCpCGg,298
 scepter/modules/opt/lr_schedulers/base_scheduler.py,sha256=9o6SHelilMdxvHBEpWbnU6MH6sYJUJghUy6gzF1UMgY,227
 scepter/modules/opt/lr_schedulers/define_schedulers.py,sha256=a5W5TYLWG56toXR1nBn-rNNdaWG0kj-x3ORyXiICXRc,3005
 scepter/modules/opt/lr_schedulers/official_schedulers.py,sha256=r81T2IUHGCrpOiw7Vjzcu2YUEwZWsGXK6Rul44WwNsQ,14312
 scepter/modules/opt/lr_schedulers/registry.py,sha256=YD0Z7U7eIHsSvy9d9V_bOGMhbQEJRgN4KDZ0siPldYc,1405
 scepter/modules/opt/lr_schedulers/warmup.py,sha256=Y_sh8yzQpJ4MJY4JZ2Yvhn_55psMDLd-Yt3yugemRbQ,5420
-scepter/modules/opt/optimizers/__init__.py,sha256=wACa7WxPl6nbyeyHRsSRzN0GkubKZ_XAZ5WgXYfnZB4,608
+scepter/modules/opt/optimizers/__init__.py,sha256=nrBWNU5VyePkMSjOKlLvESIMu57YY-RDfkqsiAsYU4o,297
 scepter/modules/opt/optimizers/base_optimizer.py,sha256=BvI5QIossVYBt_4-tJUOEVyNz6GGvkd6z4WnLtXcaYE,226
 scepter/modules/opt/optimizers/official_optimizers.py,sha256=3lBTEaL7iPeg1Hcktyyd7XaoYtcM_hIJsHTVzR4gx0Q,19032
 scepter/modules/opt/optimizers/registry.py,sha256=h4PAtZGKNVWki07Rq58JrQBbDz9lfmSJb1F3_ssC2e4,1388
 scepter/modules/solver/__init__.py,sha256=E9F7wakzeszP9JXqmZbwRCThTPOFfR_HxbkJXVmOEb8,314
 scepter/modules/solver/base_solver.py,sha256=I4zzsAug0fv0aYP9aBLiJ4WYd9QfceghbkFf5xwX8pY,35587
 scepter/modules/solver/diffusion_solver.py,sha256=aNUWI6HdvDptfzP6LeBxl2vnJDYF8sQGctkTamvOfhM,31755
 scepter/modules/solver/registry.py,sha256=py-UMNwR3MHa-Cf0B_jh3hUIZ7VSoPZGXPnVcgPcbMk,1351
 scepter/modules/solver/train_val_solver.py,sha256=_fUhbuaEJetga5R-3rJjuRVeWVrqEQNPmy1FK0MZPm0,7538
 scepter/modules/solver/hooks/__init__.py,sha256=sKIjOVIO3eOXQxRlkpQ_xFRbSDANcOaC3rcijSOJOoI,1654
 scepter/modules/solver/hooks/backward.py,sha256=vmdgigEzLUhu15lyLli3Gh816s2w25Rr-D8-1kEQcaM,3617
-scepter/modules/solver/hooks/checkpoint.py,sha256=VEuQ5roI7blP5b0d0Jc-C2OrKV3KaL_Ih3Kvbijfpf8,11993
+scepter/modules/solver/hooks/checkpoint.py,sha256=5qrU3EaXuJLH79DY6o0tioT2hMbwJnIM8nbaCJ-rmRI,12356
 scepter/modules/solver/hooks/data_probe.py,sha256=uGRdlpR37Z-Xap4ABN4ymM5X2FODn2pptYbsUsmtpv8,4944
 scepter/modules/solver/hooks/ema.py,sha256=McqGd__JCp69gOPGsuYFyJhbUBZU4uQV_yBzIbmXhc4,2090
 scepter/modules/solver/hooks/hook.py,sha256=4CKZzAKobSbydX2It5wg5wK-Qnu0j0UVnNeV7dvMyoc,611
 scepter/modules/solver/hooks/log.py,sha256=RbwWgv0JkWLbJ3dj-lS5RehdrmzX3B8KQTqvZn_4ueQ,10821
 scepter/modules/solver/hooks/lr.py,sha256=yie_1DmDv0-LKlX1DaaG3vVMn_gDiHQFnLsnAoaVLSc,5202
 scepter/modules/solver/hooks/registry.py,sha256=S_4CniuqNs4rmt-qQ6F4ijEo8c-UVV01njflEw_EEd4,154
 scepter/modules/solver/hooks/safetensors.py,sha256=SveRWQ5cYyXe_6uGEowsojoAkEXesOhz-qXa9EuSC94,2192
 scepter/modules/solver/hooks/sampler.py,sha256=LQ2_7UWCw_JPsbKwhALoR5BEHcRQq8S074L4yj6RIH8,1544
 scepter/modules/transform/__init__.py,sha256=cO5D439eas7hOS__Pc09xaa-k4MZYwy8IGLplLbgEYE,1718
 scepter/modules/transform/augmention.py,sha256=ANrNmx7ERVipdkfkvHRRufbzLP1tktOHgY7fHZ2r8s0,19927
 scepter/modules/transform/compose.py,sha256=2kkiY8_wopn6fPWVZErGs-KcO9TJohWaFt9dFOVDGpU,1011
 scepter/modules/transform/identity.py,sha256=kAb8sfmGjtYsXA9Gp6RfF-MoFDMCfT1ODBHMQZ56DYA,725
-scepter/modules/transform/image.py,sha256=9qA5TsUDWIsVFksossBp5luJWOKklBviMBB0R7Jzft0,22592
+scepter/modules/transform/image.py,sha256=Wwot-nRehYMazOx_ikLOwJtofECjmT1NjdogLdTxiLE,22204
 scepter/modules/transform/io.py,sha256=gkBomg3Kw7yWH3Mgk3_rSsjaEM5QXD06YDle6mep9L8,12249
 scepter/modules/transform/io_video.py,sha256=JGF5sxgQ6qJk-b6TPBCa5ji0HhAavNG7c6NTbNM3lO0,18995
 scepter/modules/transform/registry.py,sha256=CSfKJqS5ZjGwD-3PKu_tr3dnI9x3Pom1zugUdi6Vesk,1984
-scepter/modules/transform/tensor.py,sha256=8veL-D1xoWOJm3yLEX34Ed6md702AIItiRF_GaNdGZk,9951
+scepter/modules/transform/tensor.py,sha256=8EluUL5SfWKSO5J2A_hH4VauR62CipPO1SGuqLeDRcU,10216
 scepter/modules/transform/transform_xl.py,sha256=m6Q99CwF5YUIwuAgYK1D-ps4aXhXoSPHp96nitE3xdA,3201
 scepter/modules/transform/utils.py,sha256=D719oKO9Mjoe_zKPcpYqoiXRBC20x0A9WY7IrCmvMco,1865
 scepter/modules/transform/video.py,sha256=pwEjgopnUxxRSyyx-YjR_HWo-QMR_3khy-9rd6V5Td4,18831
 scepter/modules/utils/__init__.py,sha256=LZn9TqvmCnPtUHH-XP2xLt7T4xDSLepF98vdolKslfQ,154
-scepter/modules/utils/config.py,sha256=TNChnt8AU_xEFP14A-gt-V2l_J-reEvsD3rhGTrKX-A,24691
+scepter/modules/utils/config.py,sha256=m-23TTflO1kcd8jXo5jxcs9X_D2m0t0nxi_mUKaOOQ8,25847
 scepter/modules/utils/data.py,sha256=5JpG9RFUPxC4lPZw_nTEX6AkgQYHmwCH0kgSQonb0cs,2865
 scepter/modules/utils/directory.py,sha256=0L7k0fKb5l2RGKexM2YC-E2Y3Rniho4ApAP2ioGtCUM,482
 scepter/modules/utils/distribute.py,sha256=mb8poXOyaY5dWX4R3D4GCJfFMNnXfR-XO_5SZ7UZeiU,15775
 scepter/modules/utils/export_model.py,sha256=pbKAVxxA72jVnM_meRp5mAmzwOETjaYM0jW3ygKUBGo,3755
 scepter/modules/utils/file_system.py,sha256=tO__SZJjecSdQ7C6S4pj193MKcqUtjTXQna8rWLPPWo,17112
 scepter/modules/utils/index.py,sha256=T1MLOd1WwFoCKT7jsKOaEtk-bo_e8W6wE_ebso4CkaA,1684
 scepter/modules/utils/logger.py,sha256=7C1aQKKPTJe0unMtrxNszZYvgDxUpcEJRyD7qNg_EUY,5554
 scepter/modules/utils/math_plot.py,sha256=hbT75inJuSqQRezsx5IvDduZGXmWc-dqXyrHIu-YiVE,2735
 scepter/modules/utils/model.py,sha256=_QbbyunhuiNQnf5PpKrW3sv3YkJtKc4pXvfmHHvLX0o,5548
 scepter/modules/utils/probe.py,sha256=Dng3k9wtWILrOlgJ-ObC9f4TejS-44RTFbh8IZxquK0,17482
 scepter/modules/utils/registry.py,sha256=yz77lqnJ33MDNULo4uVKFKXABLFiaaQeHx0ytdGxx-k,7995
 scepter/modules/utils/file_clients/__init__.py,sha256=v_KZEJsenhG8MNrRXickswfnEXbUjUo_0X2ljBWZx2c,423
 scepter/modules/utils/file_clients/aliyun_oss_fs.py,sha256=-e4AMnMt3az11WQ8XMW5iQv4iXxy8K2GtgOI7TjY7jo,44598
 scepter/modules/utils/file_clients/base_fs.py,sha256=Jbu0Mzjfgpz1bZ6k7Vy5Ak7ILxroqJeHAT9kKZ8dB_g,11136
-scepter/modules/utils/file_clients/http_fs.py,sha256=Gc6Ynfzod23nylmbEJAL4RhOe701GwujcSQYV09sjvQ,4610
-scepter/modules/utils/file_clients/huggingface_fs.py,sha256=-l37F2s3XeRby2BOcG3qpTFLqpcGgF94KmP2d2VHI60,6249
+scepter/modules/utils/file_clients/http_fs.py,sha256=-AtGXFOlgZJDp0KjFH__npjFwgEDRYO5_kpcJGCqkoc,4628
+scepter/modules/utils/file_clients/huggingface_fs.py,sha256=AUn3K_6-wvqBDHUaux72L3OC-tiPMYmFI0rT_ZxonpE,6267
 scepter/modules/utils/file_clients/local_fs.py,sha256=cPV_Zod31Y10lJ3RjsbLh88TZcOTw1X_NmatQrspefY,13560
-scepter/modules/utils/file_clients/modelscope_fs.py,sha256=yY6-66wfK7zY9Z2KwTFV8qNYdy-zBv5pFEkoErmTcnA,7207
+scepter/modules/utils/file_clients/modelscope_fs.py,sha256=tbEoLf4wydcshiFZVvMph-I61KXXO4ueguwLMt49pb0,7225
 scepter/modules/utils/file_clients/registry.py,sha256=j6qvcbSpVJO82KC34my2Wrwv0ndWrBfdQ_41oM-W_BI,168
 scepter/modules/utils/file_clients/utils.py,sha256=utAoEjwCpf2qdXn5_zSmk4SHCTzE3cKS2nAaergJoRM,1067
 scepter/modules/utils/video_reader/__init__.py,sha256=s3YbM1LNrDuDRdgMPhP_qR0Zw-IOIJkopuSTqDZMCec,324
 scepter/modules/utils/video_reader/frame_sampler.py,sha256=K8zsuTrTYxsjXXHRqUsYkDHtOwL-qrqLKU--UHNAOpw,6201
 scepter/modules/utils/video_reader/video_reader.py,sha256=LUlZp3zslBwsyT1iip-8USkEyuOGcrGuE6uGocXz9gA,5133
 scepter/studio/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/home/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/home/home.py,sha256=vciZwJtQe3p-6I_-xpc3h2IBOJ2wn0Y2o9WlwzFRXcU,1751
 scepter/studio/home/home_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/home/home_ui/component_names.py,sha256=KkKa_SOrg61N8x293n-_6jrKrQ0bXLVdmglXec6RNjk,387
 scepter/studio/home/home_ui/desc_ui.py,sha256=vJM7hgfUaAmbuUHiBgRDoJjZSMpuP0-uMsmVrKJ8NRM,719
 scepter/studio/home/home_ui/guide_ui.py,sha256=DHsW0aXzXvR15cCfs62hjSscKMoZ5Eh8ZSruKB3XK3M,728
 scepter/studio/inference/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/inference/inference.py,sha256=ZcC53TizC1-j236od7EHgSjuGKozG49h7F5d3jlTYto,10711
+scepter/studio/inference/inference.py,sha256=J-W2k7J_xdHnO6Xtty_vjuzPQIcQ8rnonqIZ3JxtXcs,11904
 scepter/studio/inference/inference_manager/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/inference/inference_manager/infer_runer.py,sha256=ymwnqx7S9dKOB9wOv9BIT0DEEGQECaQFLGVzAabAFjU,7003
+scepter/studio/inference/inference_manager/infer_runer.py,sha256=2rILmwYfvLgMN0VzmPwosYPH6QVCjjSJLhdSI6gF-p0,7179
 scepter/studio/inference/inference_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/inference/inference_ui/component_names.py,sha256=5QBS9cboaha-A5Jo2VK6-_0jBlaQjEnA81DOtx3kPtM,21707
-scepter/studio/inference/inference_ui/control_ui.py,sha256=q5GwqvStDjDemyjCMrJnc-dCx2cWLbuobxMBUbbR-ZQ,8185
-scepter/studio/inference/inference_ui/diffusion_ui.py,sha256=Z2HBT-nPgqAROntzg8UH-iCCAprVfggsHrTePanfKAg,8631
-scepter/studio/inference/inference_ui/gallery_ui.py,sha256=3_6pm5zSTedXdVeRfP4516IX0B_Brml1OTZCXn2FKiE,13683
-scepter/studio/inference/inference_ui/largen_ui.py,sha256=ez-DrovhW9g6qs8Kc1bwD0BckNHFnBs3U20QG2i8GcE,22326
-scepter/studio/inference/inference_ui/mantra_ui.py,sha256=2gxI3DVZG9Bczde9O2OjCZFn5dhbTllXIHIB5zhrcOQ,8373
-scepter/studio/inference/inference_ui/model_manage_ui.py,sha256=6rF6Vzk3bXvXV0v87aJ05yYkoM0ItV5XlK7VUQX6mXM,11945
+scepter/studio/inference/inference_ui/component_names.py,sha256=cGwuQ0b_mP4KRxq9_aINKoumys7PM477nBtaUiyOKEg,26742
+scepter/studio/inference/inference_ui/control_ui.py,sha256=j0MzAkG3-quYwav1N6tL0qr0wYIRdQ4wRTCXHzGsj_4,8520
+scepter/studio/inference/inference_ui/diffusion_ui.py,sha256=Sd2GwrReNKZ_Fbs7XEJzWUxUpdkD5E5YKJxVXPvYG4g,9251
+scepter/studio/inference/inference_ui/gallery_ui.py,sha256=WQKe8W3KVG1RQ7279IuAKuyUPg1D9bIIo4ox-7jF394,12423
+scepter/studio/inference/inference_ui/largen_ui.py,sha256=RAeB5PVFr-J2XbMdUur0PCMmTrNQ6W8UPhfUkAWDFco,23616
+scepter/studio/inference/inference_ui/mantra_ui.py,sha256=wCDaBlAz1EVjn1OPgRi8Ll8VXB3KYIW9oqpkneEeyAY,8674
+scepter/studio/inference/inference_ui/model_manage_ui.py,sha256=5N75VI9vCyvQNuqvxffXfxHG4OAe5YhOQ5zNgNePtgA,12250
 scepter/studio/inference/inference_ui/refiner_ui.py,sha256=3FbbzQr8duMO-8lyTcNf9-FoZWWPXBE017pzXoSLruc,4281
-scepter/studio/inference/inference_ui/tuner_ui.py,sha256=PqEOa6wCkDaosA0kZlE5cYjGQX6T_7kpQ0f4YBCtgE4,9619
+scepter/studio/inference/inference_ui/stylebooth_ui.py,sha256=L8KdPbVJOcswYRLgbsgPnZukoq86P5OXM4Oojng400E,8675
+scepter/studio/inference/inference_ui/tuner_ui.py,sha256=HZyTDNF5BfoLMSE6OAdD5cS1zzipJ78j7w6px96YTy4,9959
 scepter/studio/preprocess/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/preprocess/preprocess.py,sha256=61v9BYjo8CjeyoRCXeTVZZIvaqIE_mwHcWQ6zkqvLAc,2815
+scepter/studio/preprocess/preprocess.py,sha256=iJyGwYFt9XB_-xB47cbgWEX7fr5upntWV8th14dtpFk,2748
 scepter/studio/preprocess/caption_editor_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/preprocess/caption_editor_ui/component_names.py,sha256=0UybSU5PBi68Xc6g0trdLgCwgq9f0zSoi2UJzRKD6XY,7823
-scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py,sha256=cwS3M6ekcBOsggoXdwZABCtER1sZFtrIQLAALzc4K_4,32565
-scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py,sha256=v3rikH1FebdCUDf9K3MPIavQtedBCutAckDNAlhVPy8,13405
-scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py,sha256=lk4k-wjaXNeqcbjEpwTrXniEZ_0WXyEQ1CeF_9dUKaE,6875
+scepter/studio/preprocess/caption_editor_ui/component_names.py,sha256=NgLEmQCjl0WGewercxMFj03io6f6f12A9aNX0rzw_lM,19501
+scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py,sha256=8CBgClxjt5J0UudLelwkGrD_Rx612IBU7K1AGA0iAac,27907
+scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py,sha256=e8_BlswXzPCIj17lXiyecBykV4OqT51o2QA57mjyF3A,70517
+scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py,sha256=kKErzjb8j4JMA4GeySPE3btQ5ZhU1_zWl5Xs1yk-GgQ,3013
+scepter/studio/preprocess/processors/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
+scepter/studio/preprocess/processors/base_processor.py,sha256=RvVEOjAu4PKTby6vWtc4ltP6SS2C_m8YNnk4m77WCbc,5014
+scepter/studio/preprocess/processors/caption_processors.py,sha256=7t1CYERnQlnUQEGywaffT86pqKOt2h9tCOUnxHiV6Qc,10504
+scepter/studio/preprocess/processors/image_processors.py,sha256=EKp0IvdlDrjDgmEbglqoaNyZX_GswGgD7YWMApvQvQ8,1351
+scepter/studio/preprocess/processors/processor_manager.py,sha256=_AxiyvAL7IYXa72pyOwxiMPf7Staioo1vVdrnpZRVcs,2475
+scepter/studio/preprocess/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
+scepter/studio/preprocess/utils/data_card.py,sha256=yfqtZUIeyqVYhcu0ftBdCiUsQTvWkh7mPqm8OQLsjQ4,48911
 scepter/studio/self_train/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/self_train/self_train.py,sha256=haPlS2bNOVMoEDCF7iLnfbMKdzfn3jsH6xBhWMY74ho,2575
 scepter/studio/self_train/scripts/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/self_train/scripts/run_task.py,sha256=JF85l8-C2D0uFTUrWp_pw7eUHxs49kgR6bs7X43WKoE,7438
+scepter/studio/self_train/scripts/run_task.py,sha256=RpHgiKeuU2CRFldrzBMWjfxEIVh6mRI3ZUg5-NOavT8,7452
 scepter/studio/self_train/scripts/sleep.py,sha256=pNhV7zhrQVuzsB3keTVRVDeNYBLwtp0tAewIKme8I2M,148
-scepter/studio/self_train/scripts/trainer.py,sha256=q_7adcA4yLNhkiBVqQ6v-cZmxrHWi4RisPNbPb-isMo,14891
+scepter/studio/self_train/scripts/trainer.py,sha256=GMrf9GW0Lp288HrRp95-dtNuokKSZ2OmtGIuV2Wyrdc,15170
 scepter/studio/self_train/self_train_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/self_train/self_train_ui/component_names.py,sha256=BexlTz4MfqKyuaiRjFioLUvwcNPxj4SmPPDFgeJnQO8,8907
-scepter/studio/self_train/self_train_ui/model_ui.py,sha256=34RL89KV5ZuP_tl7ZH3PWtMY2Pk_4YfJLIcHQBoGV34,22179
-scepter/studio/self_train/self_train_ui/trainer_ui.py,sha256=cM3I8mAMhaWQqNRT6uDPfByG58ONPl9L2rX0539H8Mo,32081
+scepter/studio/self_train/self_train_ui/component_names.py,sha256=5QUNbK4deti4pJmfiAc1D3mn2CGHc-1vuVFG95vbp4E,10123
+scepter/studio/self_train/self_train_ui/model_ui.py,sha256=0I7I0aRs7gvEwtDGKiebFC0gSJxHafm8v4y0K8lBIxk,22717
+scepter/studio/self_train/self_train_ui/trainer_ui.py,sha256=9P21EY_wHlQNNDEoJQwJXAS3WMOMTxa8nXH5tR93eyM,44050
 scepter/studio/self_train/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/self_train/utils/config_parser.py,sha256=BgUt5BuW9D98ASu6PfEwUTcujUt3fqi4Od3sgBS9feE,12071
+scepter/studio/self_train/utils/config_parser.py,sha256=GAOJfqX8JvudgFPOiTbhBjagW59-b5hpG-q5532kNx0,12144
 scepter/studio/tuner_manager/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/tuner_manager/tuner_manager.py,sha256=5WEg4ofLMHHdeJVhQ7hoznHCljJTUigiIS3DKFgFoNA,1249
+scepter/studio/tuner_manager/tuner_manager.py,sha256=RJJRebKJlU40jp7r067ZFBuM12BJANCSJTfWDYLFaMA,1392
 scepter/studio/tuner_manager/manager_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/tuner_manager/manager_ui/browser_ui.py,sha256=s5vQRsUgcXfY-hEu6a0-91gTpOX65L4tk3XIZkxKYow,14704
-scepter/studio/tuner_manager/manager_ui/component_names.py,sha256=2hxVqvEXjvbsUeN4BdJiLFlAVxiNt3MvQtSPwurfqqo,1632
-scepter/studio/tuner_manager/manager_ui/info_ui.py,sha256=qj50S8CoZFVWAsu4YX9AzBrMCt7p41NVeQEMrBV1D4A,3226
+scepter/studio/tuner_manager/manager_ui/browser_ui.py,sha256=0piBVrLDQGCx7vZThXRCIc6F90r0LYPQCF1DqF90pR0,39679
+scepter/studio/tuner_manager/manager_ui/component_names.py,sha256=EknKKrTOrwd7JIeDU1-7aARvRGF87OA5YWvHT4SbX30,4528
+scepter/studio/tuner_manager/manager_ui/info_ui.py,sha256=kwfxdJycoG08A8avTnJ5_JE8Fd7r2QouvOiRZaYoees,11052
 scepter/studio/tuner_manager/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/tuner_manager/utils/dict.py,sha256=mzVQXIbIKLSmyHL2aGbeV7cYmiX2rdhFEV062cQH1jI,452
 scepter/studio/tuner_manager/utils/path.py,sha256=EbYBAkgzq4g1HhAJ7H7_0rtMj8cNRpUMx0qDqwUk7Hk,217
 scepter/studio/tuner_manager/utils/yaml.py,sha256=x9JnYzgiIW9rvw3GvuqGIk0TIyeLDHJM4xhOU4OsJWg,327
 scepter/studio/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/utils/env.py,sha256=x-7Qu2eJodeZO7qjDBxS0IzK3cumpl-JoRDpi2Xxx0Y,589
+scepter/studio/utils/env.py,sha256=AzyKY-727jbhwwL4ovaAUf2w7_53NL_o-P0Q1rpKYxk,731
 scepter/studio/utils/file.py,sha256=fQ__QerL3rESJ-0qzE39cbfJciYAArP0FACXfgacoxI,567
 scepter/studio/utils/singleton.py,sha256=G9BVZxPb-NBvNxXJkxTUof6txosAaE6Pl24P3VLJwxU,276
 scepter/studio/utils/uibase.py,sha256=A9onH8lWu6LUYcBWiPxtq6mtvAKexHUvqOdoNj8wbGg,545
 scepter/tools/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/tools/helper.py,sha256=amLZ2C6KJjess9zf5uNPTc_zn59o5iDHHcbpM0H4zF4,3371
 scepter/tools/run_inference.py,sha256=Gnh6GJ4Mo0A6IwUDu1BH7UN4x_XryMoc7QSNhk-Dczk,9409
 scepter/tools/run_train.py,sha256=TGaGesDmnRyLdboXT1SvlDs_BSR4D2iEttoATRyB3Ts,2114
-scepter/tools/webui.py,sha256=Xd5rzURqT_bFFYnMfKLm4WAzM_hEkYHYQ1FR-ZYHPUs,6671
-scepter-0.0.4.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-scepter-0.0.4.dist-info/METADATA,sha256=yjIHVBpHlM3CmCELavSdPho3c_FstCo9k991qfDNIiQ,20162
-scepter-0.0.4.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-scepter-0.0.4.dist-info/top_level.txt,sha256=McPcsMl7sOYhmjnZsk8LNKmlk5jEH-78WdVVrRy1pqQ,8
-scepter-0.0.4.dist-info/RECORD,,
+scepter/tools/webui.py,sha256=b4qHgB8NgbU1w5TDziCqDIE5GQpuvR88n8B_UY30TPk,6742
+scepter-0.0.5.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+scepter-0.0.5.dist-info/METADATA,sha256=oB1q8GJdE6rpgv8XPj6zlZy0AGATzDxz-jDTpOSIEe0,14203
+scepter-0.0.5.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+scepter-0.0.5.dist-info/top_level.txt,sha256=McPcsMl7sOYhmjnZsk8LNKmlk5jEH-78WdVVrRy1pqQ,8
+scepter-0.0.5.dist-info/RECORD,,
```

