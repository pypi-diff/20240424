# Comparing `tmp/RagCLI-0.1.0-py3-none-any.whl.zip` & `tmp/RagCLI-0.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,34 @@
-Zip file size: 25982 bytes, number of entries: 27
--rw-rw-rw-  2.0 fat     3301 b- defN 24-Apr-04 07:49 RagAdhocQueryDoc.py
--rw-rw-rw-  2.0 fat     2716 b- defN 24-Apr-04 07:17 RagChunkText.py
--rw-rw-rw-  2.0 fat     2355 b- defN 24-Apr-03 16:51 RagEmbeddings.py
--rw-rw-rw-  2.0 fat     3869 b- defN 24-Apr-04 07:51 RagFaiss.py
--rw-rw-rw-  2.0 fat     1415 b- defN 24-Apr-04 07:14 RagLLM.py
--rw-rw-rw-  2.0 fat     1603 b- defN 24-Apr-03 16:51 RagPdf2Text.py
--rw-rw-rw-  2.0 fat     1434 b- defN 24-Apr-03 16:51 RagPrompt.py
+Zip file size: 31211 bytes, number of entries: 32
+-rw-rw-rw-  2.0 fat     3140 b- defN 24-Apr-23 08:48 RagAdhocQueryDoc.py
+-rw-rw-rw-  2.0 fat     3629 b- defN 24-Apr-23 08:48 RagChromaDB.py
+-rw-rw-rw-  2.0 fat     2833 b- defN 24-Apr-23 16:45 RagChunkText.py
+-rw-rw-rw-  2.0 fat     2204 b- defN 24-Apr-23 16:38 RagEmbeddings.py
+-rw-rw-rw-  2.0 fat     3854 b- defN 24-Apr-23 16:30 RagFaiss.py
+-rw-rw-rw-  2.0 fat     1439 b- defN 24-Apr-23 08:48 RagLLM.py
+-rw-rw-rw-  2.0 fat     1653 b- defN 24-Apr-23 08:48 RagPdf2Text.py
+-rw-rw-rw-  2.0 fat     1484 b- defN 24-Apr-23 08:48 RagPrompt.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 __init__.py
--rw-rw-rw-  2.0 fat     6581 b- defN 24-Apr-04 07:12 rag.py
--rw-rw-rw-  2.0 fat     2375 b- defN 24-Apr-03 16:51 backup/RagQueryFaiss.py
--rw-rw-rw-  2.0 fat     1704 b- defN 24-Apr-03 16:51 backup/RagStoreFaiss.py
--rw-rw-rw-  2.0 fat     4190 b- defN 24-Apr-01 13:32 elements/FAISSWrapper.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 elements/__init__.py
--rw-rw-rw-  2.0 fat     6937 b- defN 24-Apr-04 07:10 elements/document.py
--rw-rw-rw-  2.0 fat     3011 b- defN 24-Apr-01 13:32 elements/embeddingsFactory.py
+-rw-rw-rw-  2.0 fat     8331 b- defN 24-Apr-23 16:58 elements/document.py
 -rw-rw-rw-  2.0 fat     1252 b- defN 24-Apr-01 13:32 elements/ollamaWrapper.py
--rw-rw-rw-  2.0 fat     1139 b- defN 24-Apr-01 13:32 elements/prompt.py
--rw-rw-rw-  2.0 fat     3316 b- defN 24-Apr-04 07:22 utils/CONST.py
+-rw-rw-rw-  2.0 fat     1181 b- defN 24-Apr-15 06:53 elements/prompt.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 elements/chromadb/__init__.py
+-rw-rw-rw-  2.0 fat     3306 b- defN 24-Apr-22 13:12 elements/chromadb/cdbWrapper.py
+-rw-rw-rw-  2.0 fat     3027 b- defN 24-Apr-15 06:58 elements/embeddings/sentTransEmbsFactory.py
+-rw-rw-rw-  2.0 fat     4220 b- defN 24-Apr-24 06:41 elements/faiss/FAISSWrapper.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 elements/faiss/__init__.py
+-rw-rw-rw-  2.0 fat     9450 b- defN 24-Apr-23 08:48 rag/ragCL.py
+-rw-rw-rw-  2.0 fat     2250 b- defN 24-Apr-24 13:39 rag/ragCLChromadb.py
+-rw-rw-rw-  2.0 fat     3165 b- defN 24-Apr-24 06:38 rag/ragCLFaiss.py
+-rw-rw-rw-  2.0 fat     4046 b- defN 24-Apr-24 07:30 utils/CONST.py
+-rw-rw-rw-  2.0 fat      867 b- defN 24-Apr-23 08:40 utils/FUNCTIONS.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 utils/__init__.py
--rw-rw-rw-  2.0 fat     1839 b- defN 24-Apr-01 13:45 utils/log.py
--rw-rw-rw-  2.0 fat     1942 b- defN 24-Apr-01 13:32 utils/traceOut.py
--rw-rw-rw-  2.0 fat     1091 b- defN 24-Apr-04 07:54 RagCLI-0.1.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     8418 b- defN 24-Apr-04 07:54 RagCLI-0.1.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-04 07:54 RagCLI-0.1.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat      231 b- defN 24-Apr-04 07:54 RagCLI-0.1.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat      117 b- defN 24-Apr-04 07:54 RagCLI-0.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2020 b- defN 24-Apr-04 07:54 RagCLI-0.1.0.dist-info/RECORD
-27 files, 62948 bytes uncompressed, 22812 bytes compressed:  63.8%
+-rw-rw-rw-  2.0 fat     1857 b- defN 24-Apr-24 07:29 utils/log.py
+-rw-rw-rw-  2.0 fat     1907 b- defN 24-Apr-22 13:53 utils/milestone.py
+-rw-rw-rw-  2.0 fat     1091 b- defN 24-Apr-24 13:45 RagCLI-0.1.1.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     8530 b- defN 24-Apr-24 13:45 RagCLI-0.1.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-24 13:45 RagCLI-0.1.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat      262 b- defN 24-Apr-24 13:45 RagCLI-0.1.1.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat      122 b- defN 24-Apr-24 13:45 RagCLI-0.1.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2434 b- defN 24-Apr-24 13:45 RagCLI-0.1.1.dist-info/RECORD
+32 files, 77626 bytes uncompressed, 27389 bytes compressed:  64.7%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: RagAdhocQueryDoc.py
 Comment: 
 
+Filename: RagChromaDB.py
+Comment: 
+
 Filename: RagChunkText.py
 Comment: 
 
 Filename: RagEmbeddings.py
 Comment: 
 
 Filename: RagFaiss.py
@@ -18,65 +21,77 @@
 
 Filename: RagPrompt.py
 Comment: 
 
 Filename: __init__.py
 Comment: 
 
-Filename: rag.py
+Filename: elements/__init__.py
 Comment: 
 
-Filename: backup/RagQueryFaiss.py
+Filename: elements/document.py
 Comment: 
 
-Filename: backup/RagStoreFaiss.py
+Filename: elements/ollamaWrapper.py
 Comment: 
 
-Filename: elements/FAISSWrapper.py
+Filename: elements/prompt.py
 Comment: 
 
-Filename: elements/__init__.py
+Filename: elements/chromadb/__init__.py
 Comment: 
 
-Filename: elements/document.py
+Filename: elements/chromadb/cdbWrapper.py
 Comment: 
 
-Filename: elements/embeddingsFactory.py
+Filename: elements/embeddings/sentTransEmbsFactory.py
 Comment: 
 
-Filename: elements/ollamaWrapper.py
+Filename: elements/faiss/FAISSWrapper.py
 Comment: 
 
-Filename: elements/prompt.py
+Filename: elements/faiss/__init__.py
+Comment: 
+
+Filename: rag/ragCL.py
+Comment: 
+
+Filename: rag/ragCLChromadb.py
+Comment: 
+
+Filename: rag/ragCLFaiss.py
 Comment: 
 
 Filename: utils/CONST.py
 Comment: 
 
+Filename: utils/FUNCTIONS.py
+Comment: 
+
 Filename: utils/__init__.py
 Comment: 
 
 Filename: utils/log.py
 Comment: 
 
-Filename: utils/traceOut.py
+Filename: utils/milestone.py
 Comment: 
 
-Filename: RagCLI-0.1.0.dist-info/LICENSE
+Filename: RagCLI-0.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: RagCLI-0.1.0.dist-info/METADATA
+Filename: RagCLI-0.1.1.dist-info/METADATA
 Comment: 
 
-Filename: RagCLI-0.1.0.dist-info/WHEEL
+Filename: RagCLI-0.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: RagCLI-0.1.0.dist-info/entry_points.txt
+Filename: RagCLI-0.1.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: RagCLI-0.1.0.dist-info/top_level.txt
+Filename: RagCLI-0.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: RagCLI-0.1.0.dist-info/RECORD
+Filename: RagCLI-0.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## RagAdhocQueryDoc.py

```diff
@@ -1,15 +1,13 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import argparse
-from elements.embeddingsFactory import embeddingsFactory
-from rag import rag
-from elements.FAISSWrapper import FAISSWrapper
+from rag.ragCLFaiss import ragCLFaiss
 import utils.CONST as C
 
 """
     usage: RagAdhocQueryDoc [-h] 
                             -prompt {question to the LLM} 
                             -pdf {PDF file and path} 
                             [-temperature {LLM temperature}] 
@@ -19,48 +17,46 @@
                             [-nearest {Number of nearest chunks}] 
                             [-model {Ollama LLM Model}]
                             [-urlbase {Ollama URL}]
 """
 
 def main():
     parser = argparse.ArgumentParser()
-    myRag = rag()
+    myRag = ragCLFaiss()
     try:
         parser.add_argument("-" + C.ARG_PROMPT[0], help=C.ARG_PROMPT[1], required=True)
         parser.add_argument("-" + C.ARG_PDFFILE[0], help=C.ARG_PDFFILE[1], required=True)
         parser.add_argument("-" + C.ARG_TEMP[0], help=C.ARG_TEMP[1], required=False, type=float, default=C.LLM_DEFAULT_TEMPERATURE) # float(self.temperature.replace(",", "."))
         parser.add_argument("-" + C.ARG_CHUNKSIZE[0], help=C.ARG_CHUNKSIZE[1], required=False, type=int, default=C.CHKS_DEFAULT_SIZE)
         parser.add_argument("-" + C.ARG_CHUNKOVAP[0], help=C.ARG_CHUNKOVAP[1], required=False, type=int, default=C.CHKS_DEFAULT_OVERLAP)
         parser.add_argument("-" + C.ARG_SEP[0], help=C.ARG_SEP[1], required=False, default=C.CHKS_DEFAULT_SEP)
         parser.add_argument("-" + C.ARG_NEAREST[0], help=C.ARG_NEAREST[1], required=False, type=int, default=C.SM_DEFAULT_NEAREST)
         parser.add_argument("-" + C.ARG_MODEL[0], help=C.ARG_MODEL[1], required=False, default=C.OLLAMA_DEFAULT_LLM)
         parser.add_argument("-" + C.ARG_URL[0], help=C.ARG_URL[1], required=False, default=C.OLLAMA_LOCAL_URL)
         args = vars(parser.parse_args())
-        myRag.init(args)
+        myRag.setCLIArgs(args)
 
         # 1 - Read the pdf content
-        pdf = myRag.readPDF(args[C.ARG_PDFFILE[0]])
+        pdf = myRag.readPDF(args[C.ARG_PDFFILE[0]], C.ARG_READER_VALPYPDF)
         # 2 - Chunk document
-        nb, chunks = myRag.characterchunking(pdf, args[C.ARG_SEP[0]], args[C.ARG_CHUNKSIZE[0]], args[C.ARG_CHUNKOVAP[0]])
+        nb, chunks = myRag.chunk(pdf, C.CHUNK_METHOD.CHARACTER, args[C.ARG_SEP[0]], args[C.ARG_CHUNKSIZE[0]], args[C.ARG_CHUNKOVAP[0]])
         # 3 - Text embeddings
-        embFactory = embeddingsFactory()
-        vPrompt = myRag.textEmbeddings(embFactory, args[C.ARG_PROMPT[0]])
+        vPrompt = myRag.textEmbeddings(args[C.ARG_PROMPT[0]])
         # 4 - Chunks embeddings
-        vChunks = myRag.chunkEmbeddings(embFactory, chunks)
+        vChunks = myRag.chunkEmbeddings(chunks)
         # 5 - Index the chunks
-        myfaiss = FAISSWrapper()
-        myRag.FAISSaddToIndex(myfaiss, vChunks)
+        myRag.add(vChunks)
         # 6 - Similarity Search
-        similars = myRag.FAISSSearch(myfaiss, args[C.ARG_NEAREST[0]], vPrompt)
+        similars = myRag.search(args[C.ARG_NEAREST[0]], vPrompt)
         # 7 - Build prompt
-        customPrompt = myRag.buildPrompt(args[C.ARG_PROMPT[0]], similars["text"])
+        customPrompt = myRag.buildPrompt(args[C.ARG_PROMPT[0]], similars[C.JST_TEXT])
         # 8 - Ask to the LLM ...
         resp = myRag.promptLLM(customPrompt, args[C.ARG_URL[0]], args[C.ARG_MODEL[0]], args[C.ARG_TEMP[0]])
     
-        myRag.output(resp)
+        myRag.CLI_output(resp)
 
     except Exception as e:
         parser.print_help()
-        myRag.output(C.OUT_ERROR, True, str(e))
+        myRag.CLI_output(C.OUT_ERROR, True, str(e))
         
 if __name__ == "__main__":
     main()
```

## RagChunkText.py

```diff
@@ -1,15 +1,16 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import argparse
-from rag import rag
+from rag.ragCL import ragCL
 import utils.CONST as C
 from elements.document import document
+import utils.FUNCTIONS as F 
 
 """
     Chunks a text into several parts.
     output Format is JSON -> {'chunks': ['Transcript of ...', ...] }
     2 techniques can be used (option chunktype):
         1) Semantic chunking
         2) character chunking
@@ -20,43 +21,44 @@
                         [-chunk_size {Chunk size for char chunking, def 500}]
                         [-chunk_overlap {Chunk overlap for char chunking, def 50}] 
                         [-sep {Chunk separator for char chunking, def .}]
 """
 
 def main():
     parser = argparse.ArgumentParser()
-    myRag = rag()
+    myRag = ragCL()
     try:
         parser.add_argument("-" + C.ARG_CHUNKTYPE[0], help=C.ARG_CHUNKTYPE[1], required=False, default=C.ARG_CHUNKTYPE_VALCHAR,
                             choices = [C.ARG_CHUNKTYPE_VALCHAR, C.ARG_CHUNKTYPE_VALSEM])
         parser.add_argument("-" + C.ARG_TXTFILE[0], help=C.ARG_TXTFILE[1], required=True)
         parser.add_argument("-" + C.ARG_CHUNKS[0], help=C.ARG_CHUNKS[1], required=True)
         parser.add_argument("-" + C.ARG_CHUNKSIZE[0], help=C.ARG_CHUNKSIZE[1], required=False, type=int, default=C.CHKS_DEFAULT_SIZE)
         parser.add_argument("-" + C.ARG_CHUNKOVAP[0], help=C.ARG_CHUNKOVAP[1], required=False, type=int, default=C.CHKS_DEFAULT_OVERLAP)
         parser.add_argument("-" + C.ARG_SEP[0], help=C.ARG_SEP[1], required=False, default=C.CHKS_DEFAULT_SEP)
         args = vars(parser.parse_args())
-        myRag.init(args)
+        myRag.setCLIArgs(args)
 
         # Read the Text file first
         nb = -1
         doc = document(args[C.ARG_TXTFILE[0]])
         if not(doc.readTextFile()):
             raise Exception("Impossible to read the document content")
 
         # Document chunking
         if (args[C.ARG_CHUNKTYPE[0]] == C.ARG_CHUNKTYPE_VALCHAR):
-            nb, chunks = myRag.characterchunking(doc, args[C.ARG_SEP[0]], args[C.ARG_CHUNKSIZE[0]], args[C.ARG_CHUNKOVAP[0]])
+            nb, chunks = myRag.chunk(doc, C.CHUNK_METHOD.CHARACTER, 
+                                     args[C.ARG_SEP[0]], args[C.ARG_CHUNKSIZE[0]], args[C.ARG_CHUNKOVAP[0]])
         elif (args[C.ARG_CHUNKTYPE[0]] == C.ARG_CHUNKTYPE_VALSEM):
-            nb, chunks = myRag.semanticChunking(doc)
+            nb, chunks = myRag.chunk(doc, C.CHUNK_METHOD.SEMANTIC)
 
         # Write the json in a file 
-        if (not myRag.writeJsonToFile(args[C.ARG_CHUNKS[0]], chunks)):
+        if (not F.writeJsonToFile(args[C.ARG_CHUNKS[0]], chunks)):
             raise Exception("Impossible to write the chunks in a file")
 
-        myRag.output(str(nb))
+        myRag.CLI_output(str(nb))
 
     except Exception as e:
         parser.print_help()
-        myRag.output(C.OUT_ERROR, True, str(e))
+        myRag.CLI_output(C.OUT_ERROR, True, str(e))
         
 if __name__ == "__main__":
     main()
```

## RagEmbeddings.py

```diff
@@ -1,63 +1,58 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import argparse
-from rag import rag
+from rag.ragCLFaiss import ragCL
 import utils.CONST as C
-from elements.embeddingsFactory import embeddingsFactory
+import utils.FUNCTIONS as F 
 
 """
     Create embeddings:
         1) from a single string (prompt)
         2) from a list of chunks (JSON)
             Format -> {'chunks': ['Transcript of ...', ...] }
     usage: RagEmbeddings [-h] 
                          -embeddings {File and path for the embeddings / JSON}
                          [-chunks {List of chunks in a JSON format}] 
                          [-prompt {prompt}] 
 """
 
-def getArg(arg, name):
-    try:
-        return arg[name]
-    except:
-        return C.NULLSTRING
+
 
 def main():
     parser = argparse.ArgumentParser()
-    myRag = rag()
+    myRag = ragCL()
     try:
         parser.add_argument("-" + C.ARG_CHUNKS[0], help=C.ARG_CHUNKS[1], required=False, default=C.NULLSTRING)
         parser.add_argument("-" + C.ARG_PROMPT[0], help=C.ARG_PROMPT[1], required=False, default=C.NULLSTRING)
         parser.add_argument("-" + C.ARG_EMBEDDINGS[0], help=C.ARG_EMBEDDINGS[1], required=True)
         args = vars(parser.parse_args())
-        myRag.init(args)
+        myRag.setCLIArgs(args)
 
-        # We must have a lit of chunks or a prompt, otherwise -> Exception
-        chunks = getArg(args, C.ARG_CHUNKS[0])
-        prompt = getArg(args, C.ARG_PROMPT[0])
+        # We must have a bit of chunks or a prompt, otherwise -> Exception
+        chunks = F.getCLIArgurment(args, C.ARG_CHUNKS[0])
+        prompt = F.getCLIArgurment(args, C.ARG_PROMPT[0])
         if (chunks == C.NULLSTRING and prompt == C.NULLSTRING or 
             chunks != C.NULLSTRING and prompt != C.NULLSTRING):
             raise Exception("A prompt or a list of chunks must be provided, but not both!")
 
-        embFactory = embeddingsFactory()
         if (prompt != C.NULLSTRING):
-            embeddings = myRag.textEmbeddings(embFactory, args[C.ARG_PROMPT[0]])
+            embeddings = myRag.textEmbeddings(args[C.ARG_PROMPT[0]])
         else:
             # Get the chunks first as list
-            chunks = myRag.readJsonFromFile(args[C.ARG_CHUNKS[0]])
-            embeddings = myRag.chunkEmbeddings(embFactory, chunks)
+            chunks = F.readJsonFromFile(args[C.ARG_CHUNKS[0]])
+            embeddings = myRag.chunkEmbeddings(chunks)
 
         # Write the json in a file 
-        if (not myRag.writeJsonToFile(args[C.ARG_EMBEDDINGS[0]],  embeddings)):
+        if (not F.writeJsonToFile(args[C.ARG_EMBEDDINGS[0]],  embeddings)):
             raise Exception("Impossible to write the embeddings in a file")
         
-        myRag.output(C.OUT_SUCCESS)
+        myRag.CLI_output(C.OUT_SUCCESS)
 
     except Exception as e:
         parser.print_help()
-        myRag.output(C.OUT_ERROR, True, str(e))
+        myRag.CLI_output(C.OUT_ERROR, True, str(e))
         
 if __name__ == "__main__":
     main()
```

## RagFaiss.py

```diff
@@ -1,15 +1,15 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import argparse
-from rag import rag
+from src.rag.ragCLFaiss import ragCLFaiss
 import utils.CONST as C
-from elements.FAISSWrapper import FAISSWrapper
+import utils.FUNCTIONS as F 
 
 """
     Manages Meta FAISS Interactions (search & store)
     usage: RagFaiss [-h] 
                     [-action {memsearch,indexsearch,store}] 
                     [-embprompt {JSON with the prompt embeddings}] 
                     [-embchunks {JSON with the chunks embeddings}]
@@ -21,53 +21,54 @@
     3 Usages :
         * store -> Index/Store a PDF content : Mandatory parameters: -embchunks / -faissname / -faisspath
         * indexsearch -> Search on an existing index: Mandatory parameters: -embprompt / -faissname / -faisspath
         * memsearch -> Similarity search with PDF content (no index): -embchunks / -embchunks
 """
 
 def main():
-    myRag = rag()
+    myRag = ragCLFaiss()
     parser = argparse.ArgumentParser()
     try:
         parser.add_argument("-" + C.ARG_FAISSACTION[0], help=C.ARG_FAISSACTION[1], required=False, 
                             choices=[C.ARG_FAISSACTION_VALMSEARCH, C.ARG_FAISSACTION_VALISEARCH, C.ARG_FAISSACTION_VALSTORE])
         parser.add_argument("-" + C.ARG_EMBEDDINGS_PT[0], help=C.ARG_EMBEDDINGS_PT[1], required=False)
         parser.add_argument("-" + C.ARG_EMBEDDINGS_CK[0], help=C.ARG_EMBEDDINGS_CK[1], required=False)
         parser.add_argument("-" + C.ARG_NEAREST[0], help=C.ARG_NEAREST[1], required=False, type=int, default=C.SM_DEFAULT_NEAREST)
         parser.add_argument("-" + C.ARG_FAISSNAME[0], help=C.ARG_FAISSNAME[1], required=False, default=C.FAISS_DEFAULT_NAME)
         parser.add_argument("-" + C.ARG_FAISSPATH[0], help=C.ARG_FAISSPATH[1], required=False, default=C.FAISS_DEFAULT_STORE)
         parser.add_argument("-" + C.ARG_NEARESTFILE[0], help=C.ARG_NEARESTFILE[1], required=False)
         args = vars(parser.parse_args())
-        myRag.init(args)
+        myRag.setCLIArgs(args)
 
-        myfaiss = FAISSWrapper()
         if (args[C.ARG_FAISSACTION[0]] == C.ARG_FAISSACTION_VALMSEARCH):
             # Memory search / need -> ARG_EMBEDDINGS_CK / ARG_EMBEDDINGS_PT
-            vChunks = myRag.readJsonFromFile(args[C.ARG_EMBEDDINGS_CK[0]])
-            vPrompt = myRag.readJsonFromFile(args[C.ARG_EMBEDDINGS_PT[0]])
-            myRag.FAISSaddToIndex(myfaiss, vChunks)
-            similars = myRag.FAISSSearch(myfaiss, args[C.ARG_NEAREST[0]], vPrompt)
-            myRag.writeToFile(args[C.ARG_NEARESTFILE[0]], similars["text"].to_json())
+            vChunks = F.readJsonFromFile(args[C.ARG_EMBEDDINGS_CK[0]])
+            vPrompt = F.readJsonFromFile(args[C.ARG_EMBEDDINGS_PT[0]])
+            myRag.add(vChunks)
+            similars = myRag.search(args[C.ARG_NEAREST[0]], vPrompt)
+            F.writeToFile(args[C.ARG_NEARESTFILE[0]], similars[C.JST_TEXT].to_json())
 
         elif (args[C.ARG_FAISSACTION[0]] == C.ARG_FAISSACTION_VALISEARCH):
             # Index search / need -> ARG_EMBEDDINGS_PT / ARG_FAISSNAME / ARG_FAISSPATH
-            vPrompt = myRag.readJsonFromFile(args[C.ARG_EMBEDDINGS_PT[0]])
-            myRag.FAISSLoad(myfaiss, args[C.ARG_FAISSPATH[0]], args[C.ARG_FAISSNAME[0]])
-            similars = myRag.FAISSSearch(myfaiss, args[C.ARG_NEAREST[0]], vPrompt)
-            myRag.writeToFile(args[C.ARG_NEARESTFILE[0]], similars["text"].to_json())
+            vPrompt = F.readJsonFromFile(args[C.ARG_EMBEDDINGS_PT[0]])
+            myRag.indexName = args[C.ARG_FAISSNAME[0]]
+            myRag.load(args[C.ARG_FAISSPATH[0]])
+            similars = myRag.search(args[C.ARG_NEAREST[0]], vPrompt)
+            F.writeToFile(args[C.ARG_NEARESTFILE[0]], similars[C.JST_TEXT].to_json())
 
         elif (args[C.ARG_FAISSACTION[0]] == C.ARG_FAISSACTION_VALSTORE):
             # Calculate and Store index / need -> ARG_EMBEDDINGS_CK / ARG_FAISSNAME / ARG_FAISSPATH
-            vChunks = myRag.readJsonFromFile(args[C.ARG_EMBEDDINGS_CK[0]])
-            myRag.FAISSStore(vChunks,  args[C.ARG_FAISSPATH[0]],  args[C.ARG_FAISSNAME[0]])
-
+            myRag.indexName = args[C.ARG_FAISSNAME[0]]
+            vChunks = F.readJsonFromFile(args[C.ARG_EMBEDDINGS_CK[0]])
+            myRag.add(vChunks)
+            myRag.save(args[C.ARG_FAISSPATH[0]])
         else:
             raise Exception("No action selected, please select search or store")
 
-        myRag.output(C.OUT_SUCCESS)
+        myRag.CLI_output(C.OUT_SUCCESS)
 
     except Exception as e:
         parser.print_help()
-        myRag.output(C.OUT_ERROR, True, str(e))
+        myRag.CLI_output(C.OUT_ERROR, True, str(e))
         
 if __name__ == "__main__":
     main()
```

## RagLLM.py

```diff
@@ -1,37 +1,37 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import argparse
-from rag import rag
+from rag.ragCL import ragCL
 import utils.CONST as C
 
 """ 
     Prompt a LLM managed by Ollama and returns the response.
     usage: RagLLM [-h] 
                   -prompt {question to the LLM} 
                   [-model {Ollama Model installed}] 
                   [-urlbase {Ollama URL base, def http://localhost:11434/api}] 
                   [-temperature {Model temperature, def 0.9}]
 """
 
 def main():
     parser = argparse.ArgumentParser()
-    myRag = rag()
+    myRag = ragCL()
     try:
         parser.add_argument("-" + C.ARG_PROMPT[0], help=C.ARG_PROMPT[1], required=True)
         parser.add_argument("-" + C.ARG_MODEL[0], help=C.ARG_MODEL[1], required=False, default=C.OLLAMA_DEFAULT_LLM)
         parser.add_argument("-" + C.ARG_URL[0], help=C.ARG_URL[1], required=False, default=C.OLLAMA_LOCAL_URL)
         parser.add_argument("-" + C.ARG_TEMP[0], help=C.ARG_TEMP[1], required=False, type=float, default=C.LLM_DEFAULT_TEMPERATURE)
         args = vars(parser.parse_args())
         
-        myRag.init(args)
+        myRag.setCLIArgs(args)
         resp = myRag.promptLLM(args[C.ARG_PROMPT[0]], args[C.ARG_URL[0]], args[C.ARG_MODEL[0]], args[C.ARG_TEMP[0]])
-        myRag.output(resp)
+        myRag.CLI_output(resp)
 
     except Exception as e:
         parser.print_help()
-        myRag.output(C.OUT_ERROR, True, str(e))
+        myRag.CLI_output(C.OUT_ERROR, True, str(e))
         
 if __name__ == "__main__":
     main()
```

## RagPdf2Text.py

```diff
@@ -1,14 +1,15 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import argparse
-from rag import rag
+from rag.ragCL import ragCL
 import utils.CONST as C
+import utils.FUNCTIONS as F 
 
 """
     Read a PDF file and converts it into text.
     2 readers available:
         1) Using pymupdf
         2) using llamaparse
            Note: the env. variable LLAMAINDEX_API_KEY must be set with the llamaindex key
@@ -16,29 +17,29 @@
                        -pdf {PDF file and path} 
                        -txt {Text file and path}  
                        [-reader {pymupdf,llamaparse}]
 """
 
 def main():
     parser = argparse.ArgumentParser()
-    myRag = rag()
+    myRag = ragCL()
     try:
         parser.add_argument("-" + C.ARG_PDFFILE[0], help=C.ARG_PDFFILE[1], required=True)
         parser.add_argument("-" + C.ARG_TXTFILE[0], help=C.ARG_TXTFILE[1], required=True)
         parser.add_argument("-" + C.ARG_READER[0], help=C.ARG_TXTFILE[1], required=False, 
                             choices=[C.ARG_READER_VALPYPDF, C.ARG_READER_VALLLAMAPARSE],
                             default=C.ARG_READER_VALPYPDF)
         args = vars(parser.parse_args())
-        myRag.init(args)
+        myRag.setCLIArgs(args)
 
         # 1 - Read the pdf content
         pdf = myRag.readPDF(args[C.ARG_PDFFILE[0]], args[C.ARG_READER[0]])
-        if not myRag.writeToFile(args[C.ARG_TXTFILE[0]], pdf.content):
+        if not F.writeToFile(args[C.ARG_TXTFILE[0]], pdf.content):
             raise Exception("Impossible to write the content into the file")
-        myRag.output(C.OUT_SUCCESS)
+        myRag.CLI_output(C.OUT_SUCCESS)
 
     except Exception as e:
         parser.print_help()
-        myRag.output(C.OUT_ERROR, True, str(e))
+        myRag.CLI_output(C.OUT_ERROR, True, str(e))
         
 if __name__ == "__main__":
     main()
```

## RagPrompt.py

```diff
@@ -1,38 +1,39 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import argparse
-from rag import rag
+from rag.ragCL import ragCL
 import utils.CONST as C
+import utils.FUNCTIONS as F 
 
 """
     Build a prompt based on a template. The template must contain the {context} and {prompt} tags inside.
     By default: "Question: {prompt}\n Please answer the question based on the informations listed below: {context}"
 
     usage: RagPrompt [-h] 
                      -prompt {question to ask to the LLM} 
                      -nfile {list of the nearest chunks / json}
                      [-template {template string}] 
 """
 def main():
     parser = argparse.ArgumentParser()
-    myRag = rag()
+    myRag = ragCL()
     try:
         parser.add_argument("-" + C.ARG_PROMPT[0], help=C.ARG_PROMPT[1], required=True)
         parser.add_argument("-" + C.ARG_PROMPT_TEMPLATE[0], help=C.ARG_PROMPT_TEMPLATE[1], required=False, 
                             default = C.PROMPT_RAG_TEMPLATE)
         parser.add_argument("-" + C.ARG_NEARESTFILE[0], help=C.ARG_NEARESTFILE[1], required=True)
         args = vars(parser.parse_args())
         
-        myRag.init(args)
-        nearest = myRag.readJsonFromFile(args[C.ARG_NEARESTFILE[0]])
+        myRag.setCLIArgs(args)
+        nearest = F.readJsonFromFile(args[C.ARG_NEARESTFILE[0]])
         resp = myRag.buildPrompt(args[C.ARG_PROMPT[0]], nearest)
-        myRag.output(resp)
+        myRag.CLI_output(resp)
 
     except Exception as e:
         parser.print_help()
-        myRag.output(C.OUT_ERROR, True, str(e))
+        myRag.CLI_output(C.OUT_ERROR, True, str(e))
         
 if __name__ == "__main__":
     main()
```

## elements/document.py

```diff
@@ -8,14 +8,15 @@
 from langchain_community.embeddings import HuggingFaceEmbeddings
 import utils.CONST as C
 import os
 import mimetypes
 import requests
 import time
 
+
 class document:
     def __init__(self, __filepath):
         self.__filepath = __filepath
         self.__content = ""
         
     @property
     def filepath(self) -> str: 
@@ -29,14 +30,33 @@
         try:
             with open(self.filepath, "r", encoding=C.ENCODING) as f:
                 self.__content = f.read()
             return True
         except Exception as e:
             return False
 
+    def chunk(self, method, *args):
+        """ Document chunking main method call
+            If method == C.CHUNK_METHOD.SEMANTIC -> *args = {}
+            If method == C.CHUNK_METHOD.CHARACTER -> *args = {separator, chunk_size, chunk_overlap}
+        Args:
+            method (CHUNK_METHOD): SEMANTIC or CHARACTER chunk
+        Returns:
+            _type_: -1, {} if error, Nb of chunks and list of chunks else
+        """
+        try:
+            if (method == C.CHUNK_METHOD.SEMANTIC):
+                return self.__semanticChunking()
+            elif (method == C.CHUNK_METHOD.CHARACTER):
+                return self.__characterChunking(args[0], args[1], args[2])
+            else:
+                return -1
+        except Exception as e:
+            return -1, {}
+
     def pyMuPDFParseDocument(self, fromPage=0, toPage=0, heightToRemove=0) -> bool:
         """ Read a pdf file and add the content as text by using PyMuPDF
 
         Args:
             fromPage (int, optional): Starts from page Number. Defaults to 0.
             toPage (int, optional): Ends at page Number. Defaults to 0.
             heightToRemove (int, optional): Height in pixel to remove (header and footer). Defaults to 0.
@@ -53,23 +73,84 @@
                     rect = fitz.Rect(pageBox[0], 
                                     pageBox[1] + heightToRemove, 
                                     pageBox[2], 
                                     pageBox[3] - heightToRemove)
                     self.__content = self.__content + page.get_textbox(rect) # get plain text encoded as UTF-8
             return True
         except Exception as e:
+            self.__content = ""
             return False
+       
+    def llamaParseDocument(self, extractType="markdown"):
+        """ Read a pdf file and add the content as text by using llamaparse
+            the LLAMAINDEX_API_KEY environment variable must be set to the API Key
+            Cf.
+                Login : https://cloud.llamaindex.ai/login
+                Docs : https://docs.llamaindex.ai/
+                Post : https://medium.com/llamaindex-blog/introducing-llamacloud-and-llamaparse-af8cedf9006b
+                Example : https://github.com/allthingsllm/llama_parse/blob/main/examples/demo_api.ipynb
+        Args:
+            extractType (str): Extraction type text or markdown (default)
+            
+        Returns:
+            bool: True if no errors
+        """
+        # Get the LLamaIndex Key from the LLAMAINDEX_API_KEY environment variable
+        try:
+            try:
+                llamaIndexKey = os.environ[C.LLAMAINDEX_API_KEY]
+            except:
+                raise Exception ("The {} environment variable needs to be defined to use llamaparse.".format(C.LLAMAINDEX_API_KEY))
+            # Upload the file
+            headers = {"Authorization": f"Bearer {llamaIndexKey}", "accept": "application/json"}
+            
+            with open(self.filepath, "rb") as f:
+                mime_type = mimetypes.guess_type(self.filepath)[0]
+                files = {"file": (f.name, f, mime_type)}
+                # send the request, upload the file
+                url_upload = f"{C.LLAMAPARSE_API_URL}/upload"
+                response = requests.post(url_upload, headers=headers, files=files) 
+                
+            response.raise_for_status()
+            # get the job id for the result_url
+            job_id = response.json()["id"]
+            url_result = f"{C.LLAMAPARSE_API_URL}/job/{job_id}/result/{extractType}"
+            # check for the result until its ready
+            iteration = 1
+            while True:
+                response = requests.get(url_result, headers=headers)
+                if response.status_code == 200:
+                    break
+                time.sleep(C.LLAMAPARSE_API_WAITSEC)
+                if (iteration >= C.LLAMAPARSE_ITERATION_MAX):
+                    raise Exception ("Llamaindex seems not responsive or not responsive enough, please retry again.")
+                iteration += 1
+            # download the result
+            result = response.json()
+            self.__content = result[extractType]
+            return True
         
+        except Exception as e:
+            self.__content = ""
+            return False
+
     def __wrapChunks(self, docs):
+        """ Wrap the chunks into a JSON format
+        Args:
+            docs (document): _description_
+        Returns:
+            int: Number of chunks
+            str: json chunks -> {'chunks': ['Transcript of ...', ...] }
+        """
         nbChunks = len(docs)
         jsonInputs = {}
-        jsonInputs["chunks"] = [ x.page_content for x in docs ] 
+        jsonInputs[C.JST_CHUNKS] = [ x.page_content for x in docs ] 
         return nbChunks, jsonInputs
 
-    def characterChunking(self, separator, chunk_size, chunk_overlap):
+    def __characterChunking(self, separator, chunk_size, chunk_overlap):
         """ Chunks the document content into several pieces/chunks and returns a json text with the chunks
             format : {'chunks': ['Transcript of ...', ...] }
             Note: Leverage character langchain to manage the chunks
 
         Args:
             separator (str): Chunks separator
             chunk_size (str): chunk size
@@ -85,15 +166,15 @@
                                                 length_function = len, 
                                                 is_separator_regex = False)
             docs = text_splitter.create_documents([self.__content])
             return self.__wrapChunks(docs) 
         except Exception as e:
             return -1, {}
     
-    def semanticChunking(self):
+    def __semanticChunking(self):
         """Chunks the document content into several pieces/chunks and returns a json text with the chunks
             format : {'chunks': ['Transcript of ...', ...] }
             Note: Leverage semantic langchain to manage the chunks
 
         Returns:
             str: A JSON text which looks like this: {'chunks': ['Transcript of ...', ...] }
         """
@@ -108,56 +189,8 @@
                     encode_kwargs=encode_kwargs,
                     )
             text_splitter = SemanticChunker(hf_embeddings)
             docs = text_splitter.create_documents([self.__content])
             return self.__wrapChunks(docs) 
         except Exception as e:
             return -1, {}
-        
-    def llamaParseDocument(self, extractType="markdown"):
-        """ Read a pdf file and add the content as text by using llamaparse
-            the LLAMAINDEX_API_KEY environment variable must be set to the API Key
-            Cf.
-                Login : https://cloud.llamaindex.ai/login
-                Docs : https://docs.llamaindex.ai/
-                Post : https://medium.com/llamaindex-blog/introducing-llamacloud-and-llamaparse-af8cedf9006b
-                Example : https://github.com/allthingsllm/llama_parse/blob/main/examples/demo_api.ipynb
-        Args:
-            extractType (str): Extraction type text or markdown (default)
-            
-        Returns:
-            bool: True if no errors
-        """
-        # Get the LLamaIndex Key from the LLAMAINDEX_API_KEY environment variable
-
-        try:
-            try:
-                llamaIndexKey = os.environ[C.LLAMAINDEX_API_KEY]
-            except:
-                raise Exception ("The {} environment variable needs to be defined to use llamaparse.".format(C.LLAMAINDEX_API_KEY))
-            # Upload the file
-            headers = {"Authorization": f"Bearer {llamaIndexKey}"}
-            
-            with open(self.filepath, "rb") as f:
-                mime_type = mimetypes.guess_type(self.filepath)[0]
-                files = {"file": (f.name, f, mime_type)}
-                # send the request, upload the file
-                url_upload = f"{C.LLAMAPARSE_API_URL}/upload"
-                response = requests.post(url_upload, headers=headers, files=files) 
-                
-            response.raise_for_status()
-            # get the job id for the result_url
-            job_id = response.json()["id"]
-            url_result = f"{C.LLAMAPARSE_API_URL}/job/{job_id}/result/{extractType}"
-            # check for the result until its ready
-            while True:
-                response = requests.get(url_result, headers=headers)
-                if response.status_code == 200:
-                    break
-                time.sleep(C.LLAMAPARSE_API_WAITSEC)
-
-            # download the result
-            result = response.json()
-            self.__content = result[extractType]
-            
-        except Exception as e:
-            return -1, {}
+
```

## elements/prompt.py

```diff
@@ -32,10 +32,11 @@
         self.__context = q
     
     def build(self):
         try: 
             itemContext = ""
             for i, item in self.context.items():
                 itemContext = itemContext + C.ITEM_CONTEXT_TEMPLATE_LINE.format(i=i, contextItem=item) + "\n"
-            return self.template.format(prompt=self.question, context=itemContext)
+            return self.template.format(prompt=self.question, 
+                                        context=itemContext)
         except:
             return ""
```

## utils/CONST.py

```diff
@@ -1,23 +1,28 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import logging
+from enum import IntEnum
 
+class CHUNK_METHOD(IntEnum):
+    SEMANTIC = 1
+    CHARACTER = 0
+    
 # Diverse Consts
 NULLSTRING = ""
 ENCODING = "utf-8"
 
 # Logger configuration
 TRACE_DEFAULT_LEVEL = logging.DEBUG
 TRACE_DEFAULT_FORMAT = "%(asctime)s|%(name)s|%(levelname)s|%(message)s"
 TRACE_FILENAME = "ragcli.log"
 TRACE_MAXBYTES = 10000
-TRACE_LOGGER = "FULLRAG"
+TRACE_LOGGER = "RAGCLI"
 TRACE_MSG_LENGTH = 200
 RAGCLI_LOGFILE_ENV = "RAGCLI_LOGFILE"
 FAISS_DEFAULT_NAME = "faiss"
 FAISS_DEFAULT_STORE = "./vstore"
 
 # LLM stuff
 SEMCHUNK_EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"
@@ -26,14 +31,19 @@
 OLLAMA_DEFAULT_LLM = "tinydolphin"
 LLM_DEFAULT_TEMPERATURE = 0.9
 SM_DEFAULT_NEAREST = 3
 CHKS_DEFAULT_SIZE = 500
 CHKS_DEFAULT_OVERLAP = 50
 CHKS_DEFAULT_SEP = "."
 
+# JSON "tags" for chunks & embeddings
+JST_CHUNKS = "chunks"
+JST_TEXT = "text"
+JST_EMBEDDINGS = "embedding"
+
 # Prompts
 PROMPT_RAG_TEMPLATE = "Question: {prompt}\n Please answer the question based on the informations listed below: {context}"
 ITEM_CONTEXT_TEMPLATE_LINE = "Context {i}: {contextItem}"
 
 # Output status
 OUT_ERROR = "ERROR"
 OUT_SUCCESS = "SUCCESS"
@@ -74,12 +84,25 @@
 ARG_EMBEDDINGS_PT = ["embprompt", "JSON file path which contains the data and embeddings for the prompt"]
 ARG_EMBEDDINGS_CK = ["embchunks", "JSON file path which contains the data and embeddings for the chunks"]
 ARG_FAISSACTION = ["action", "Action to execute on the FAISS Engine (store/indexsearch/memsearch)"]
 ARG_FAISSACTION_VALMSEARCH = "memsearch"
 ARG_FAISSACTION_VALISEARCH = "indexsearch"
 ARG_FAISSACTION_VALSTORE = "store"
 ARG_NEARESTFILE = ["nfile", "JSON file path which contains the nearest chunks/texts"]
+ARG_CDB_COLLECTION = ["collection", "Chroma DB collection name"]
+ARG_CDB_VALSEARCH = "search"
+ARG_CDB_VALSTORE = "store"
+ARG_CDB_ACTION = ["action", "Action to execute on the Chroma DB Engine (store/search)"]
+ARG_CDB_HOST = ["host", "Chroma DB host name (or IP address)"]
+ARG_CDB_PORT = ["port", "Chroma DB port number"]
 
 # Llamaparse
 LLAMAPARSE_API_URL = "https://api.cloud.llamaindex.ai/api/parsing"
 LLAMAPARSE_API_WAITSEC = 2
-LLAMAINDEX_API_KEY = "LLAMAINDEX_API_KEY"
+LLAMAPARSE_ITERATION_MAX = 20
+LLAMAINDEX_API_KEY = "LLAMAINDEX_API_KEY"
+
+# Chroma DB
+CDB_DEFAULT_HOST = "localhost"
+CDB_DEFAULT_PORT = 8000
+CDB_DEFAULT_COLLECTION = "default"
+CDB_DEFAULT_EMBEDDINGSMODEL_ST = "all-MiniLM-L6-v2"
```

## utils/log.py

```diff
@@ -29,24 +29,24 @@
         final_message = ""
         for msg in _msg:
             final_message += str(msg)
         return final_message
     
     def info(self, *message):
         final_message = self.buildMessage(message)
-        self.display("I> " + final_message)
+        self.display("Info: " + final_message)
         self.__logger.info(final_message)
 
     def error(self, *message):
         final_message = self.buildMessage(message)
-        self.display("E> " + final_message)
+        self.display("Error: " + final_message)
         self.__logger.error(final_message)
 
     def debug(self, *message):
         final_message = self.buildMessage(message)
-        self.display("D> " + final_message)
+        self.display("Debug:> " + final_message)
         self.__logger.debug(final_message)
 
     def warning(self, *message):
         final_message = self.buildMessage(message)
-        self.display("W> " + final_message)
+        self.display("Warning: " + final_message)
         self.__logger.warning(final_message)
```

## Comparing `rag.py` & `rag/ragCL.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,165 +1,224 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 from elements.document import document
-from elements.FAISSWrapper import FAISSWrapper
 from elements.ollamaWrapper import ollamaWrapper
 from elements.prompt import prompt
-import json
-from utils.traceOut import traceOut
+from utils.milestone import milestone
 import utils.CONST as C
-from numpyencoder import NumpyEncoder
 from utils.log import log
 import os
+from elements.embeddings.sentTransEmbsFactory import sentTransEmbsFactory
 
-class rag():
+class ragCL():
     def __init__(self):
-        self.myTrace = traceOut()
+        self.__milestones = milestone()
+        self.__embFactory = sentTransEmbsFactory()
         try:
             ragLogFileName = os.environ[C.RAGCLI_LOGFILE_ENV]
         except:
             ragLogFileName = C.TRACE_FILENAME
         self.__myLog = log(C.TRACE_LOGGER, ragLogFileName)
-
-    def init(self, args):
-        self.myTrace.initialize(args)
-        self.myTrace.start()
+        self.__milestones.start()
         self.log.info("** START **")
+        
+    def setCLIArgs(self, args):
+        self.__milestones.initialize(args)
 
     @property
-    def trace(self):
-        return self.myTrace
+    def milestones(self):
+        return self.__milestones
     @property
     def log(self):
         return self.__myLog
-    
-    def addTrace(self, name, description, *others):
-        self.trace.add(name, description, others)
-        self.log.info("Step {} -> {}".format(name, self.__fmtMsgForLog(description)))
-    
-    def __fmtMsgForLog(self, message, limit = C.TRACE_MSG_LENGTH):
+
+    def addMilestone(self, name, description, *others):
+        self.__milestones.add(name, description, others)
+        self.log.info("Step {} -> {}".format(name, self.__CLI_fmtMsgForLog(description)))
+
+    def __CLI_fmtMsgForLog(self, message, limit = C.TRACE_MSG_LENGTH):
+        """ Format a message for logging
+        Args:
+            message (str): log message
+            limit (int, optional): message max length. Defaults to C.TRACE_MSG_LENGTH.
+        Returns:
+            formatted message: _description_
+        """
         logMsg = message.replace("\n", " ")
         dots = ""
         if (len(message) > limit):
             dots = " ..."
         logMsg = logMsg[:limit] + dots
         return logMsg
     
-    # Standard Output printing via XML tags
-    def output(self, response, error = False, errorMsg = C.NULLSTRING):
-        self.myTrace.stop()
-        print(C.TAG_O_LOG + self.trace.getFullJSON() + C.TAG_C_LOG)
-        if (error):
-            self.log.error("Output: Response> {} | Error> {}".format(self.__fmtMsgForLog(response), errorMsg))
-            print(C.TAG_O_STATUS + C.OUT_ERROR + C.TAG_C_STATUS)
-            print(C.TAG_O_RESPONSE + errorMsg + C.TAG_C_RESPONSE)
-        else:
-            self.log.info("Output: Response> {}".format(self.__fmtMsgForLog(response)))
-            print(C.TAG_O_RESPONSE + response + C.TAG_C_RESPONSE)
-            print(C.TAG_O_STATUS + C.OUT_SUCCESS + C.TAG_C_STATUS)
-        self.log.info("** STOP **")
-        
-    def readPDF(self, pdffile, method = C.ARG_READER_VALPYPDF):
-        # Read the pdf content
-        self.log.info("Read PDF file {}".format(pdffile))
-        pdf = document(pdffile)
-        if (method == C.ARG_READER_VALPYPDF):
-            pdf.pyMuPDFParseDocument()
-        else:
-            pdf.llamaParseDocument()
-        if (len(pdf.content) <= 0):
-            raise Exception("Error while converting the PDF document to text")
-        self.addTrace("PDF2TXT", "PDF converted to TEXT successfully. Text length : {}".format(len(pdf.content)))
-        return pdf
-
-    def characterchunking(self, doc, separator, size, overlap):
-        # Chunk document
-        nb, chunks = doc.characterChunking(separator, size, overlap)
-        if (nb<=0):
-            raise Exception("Error while chunking the document")
-        self.addTrace("CHUNKING","Document (character) chunked successfully, Number of chunks : {}".format(nb), nb)
-        return nb, chunks
-    
-    def semanticChunking(self, doc):
-        # Chunk document
-        nb, chunks = doc.semanticChunking()
-        if (nb<=0):
-            raise Exception("Error while chunking the document")
-        self.addTrace("CHUNKING","Document (semantic) chunked successfully, Number of chunks : {}".format(nb), nb)
-        return nb, chunks
-    
-    def textEmbeddings(self, embFactory, prompt):
-        vPrompt = embFactory.createEmbeddingsFromTXT(prompt)
-        if (vPrompt == {}):
-            raise Exception("Error while creating the prompt embeddings")
-        self.addTrace("PTEMBEDDGS", "Embeddings created from prompt successfully")
-        return vPrompt
-
-    def chunkEmbeddings(self, embFactory, chunks):
-        vChunks = embFactory.createEmbeddingsFromList(chunks)
-        if (vChunks == {}):
-            raise Exception("Error while creating the chunks embeddings")
-        self.addTrace("DOCEMBEDDGS", "Embeddings created from chunks successfully")
-        return vChunks
-
-    def FAISSaddToIndex(self, myfaiss, vChunks):
-        myfaiss.addToIndex(vChunks)
-        self.addTrace("ADDTOINDEX", "Add chunks to the FAISS Index")
-
-    def FAISSSearch(self, myfaiss, k, vPrompt):
-        similars = myfaiss.getNearest(vPrompt, k)
-        self.addTrace("SIMILARSEARCH", "Similarity Search executed successfully")
-        return similars
-
-    def FAISSStore(self, vChunks, path, name):
-        myfaiss = FAISSWrapper()
-        myfaiss.addToIndex(vChunks)
-        self.addTrace("FAISSSTORE", "Chunks embeddings indexed and stored successfully")
-        myfaiss.save(path, name)
-        return myfaiss
-
-    def FAISSLoad(self, myfaiss, path, name):
-        self.addTrace("FAISSSTORE", "Chunks embeddings indexed and stored successfully")
-        myfaiss.load(path, name)
-
-    def buildPrompt(self, question, similarText):
-        myPrompt = prompt(question, similarText)
-        customPrompt = myPrompt.build()
-        if (len(customPrompt) == 0):
-            raise Exception("Error while creating the prompt")
-        self.addTrace("PROMPT", "Prompt built successfully", customPrompt)
-        return customPrompt
+    def __CLI_buildOutput(self, response, error = False, errorMsg = C.NULLSTRING):
+        """ Build the final output display of the process
+        Args:
+            response (str): final response
+            error (bool, optional): has error ? Defaults to False.
+            errorMsg (str, optional): error message. Defaults to C.NULLSTRING.
+        Returns:
+            str: JSON output with main milestones
+            str: status
+            str: final LLM response
+        """
+        try:
+            outJson, outStatus, outResponse = "", "", ""
+            self.__milestones.stop()
+            outJson = self.milestones.getFullJSON()
+            if (error):
+                self.log.error("Output: Response> {} | Error> {}".format(self.__CLI_fmtMsgForLog(response), errorMsg))
+                outStatus = C.OUT_ERROR
+                outResponse = errorMsg
+            else:
+                self.log.info("Output: Response> {}".format(self.__CLI_fmtMsgForLog(response)))
+                outStatus = C.OUT_SUCCESS
+                outResponse = response
+            self.log.info("** STOP **")
+            return outJson, outStatus, outResponse
+        except Exception as e:
+            self.log.error(str(e))
+            return outJson, outStatus, outResponse
+            
+    def CLI_output(self, response, error = False, errorMsg = C.NULLSTRING):
+        """ Build the final output of the process for the CLI (Standard Output (CLI) printing via XML tags)
+        Args:
+            response (str): final response
+            error (bool, optional): has error ? Defaults to False.
+            errorMsg (str, optional): error message. Defaults to C.NULLSTRING.
+        Returns:
+            str: JSON output with main milestones
+            str: status
+            str: final LLM response
+        """
+        outJson, outStatus, outResponse = self.__CLI_buildOutput(response, error, errorMsg)
+        print(C.TAG_O_LOG + outJson + C.TAG_C_LOG)
+        print(C.TAG_O_STATUS + outStatus + C.TAG_C_STATUS)
+        print(C.TAG_O_RESPONSE + outResponse + C.TAG_C_RESPONSE)
+            
+    def readPDF(self, pdffile, method = C.ARG_READER_VALPYPDF) -> str:
+        """ Reads a pdf file and converts it to Text
+        Args:
+            pdffile (str): pdf file path
+            method (str, optional): Type of conversion. Defaults to C.ARG_READER_VALPYPDF.
+        Returns:
+            str: text converted
+        """
+        try:
+            # Read and parse a pdf file
+            self.log.info("Read PDF file {} ...".format(pdffile))
+            pdf = document(pdffile)
+            if (method == C.ARG_READER_VALPYPDF):
+                pdf.pyMuPDFParseDocument()
+            else:
+                pdf.llamaParseDocument()
+            if (len(pdf.content) <= 0):
+                raise Exception("Error while converting the PDF document to text")
+            self.addMilestone("PDF2TXT", "PDF converted to TEXT successfully. Text length : {}".format(len(pdf.content)))
+            self.log.info("PDF file opened successfully")
+            return pdf
+        except Exception as e:
+            self.log.error("Error while reading the PDF file: {}".format(str(e)))
+            return ""
+            
+    def chunk(self, doc, method, *args):
+        """ Document chunking process
+        Args:
+            doc (elements.document): Text / document to chunk
+            method (CHUNK_METHOD): Type of chunking method required
+        Returns:
+            int: number of chunks
+            list: List of chunks / JSON format -> {'chunks': ['Transcript of ...', ...] }
+        """
+        try:
+            self.log.info("Chunking document with Method {} ...".format(method))
+            nb, chunks =  doc.chunk(method, *args)
+            if (nb<=0):
+                raise Exception("Error while chunking the document")
+            self.addMilestone("CHUNKING","Document (character) chunked successfully, Number of chunks : {}".format(nb), nb)
+            self.log.info("Document chunked successfully with {} chunks".format(nb))
+            return nb, chunks
+        except Exception as e:
+            self.log.error("Error while chunking the document: {}".format(str(e)))
+            return -1, {}
+            
+    def buildPrompt(self, question, similarText) -> str:
+        """ Build smart prompt (for RAG)
+        Args:
+            question (str): initial question
+            similarText (list): list of the nearest / most similar chunks
+        Returns:
+            str: new prompt
+        """
+        try:
+            self.log.info("Building RAG prompt ...")
+            myPrompt = prompt(question, similarText)
+            customPrompt = myPrompt.build()
+            if (len(customPrompt) == 0):
+                raise Exception("Error while creating the prompt")
+            self.addMilestone("PROMPT", "Prompt built successfully", customPrompt)
+            self.log.info("RAG Prompt created successfully")
+            return customPrompt
+        except Exception as e:
+            self.log.error("Error while building the LLM prompt {}".format(str(e)))
+            return ""
 
     def promptLLM(self, question, urlOllama, model, temperature):
-        myllm = ollamaWrapper(urlOllama, model, temperature)
-        resp = myllm.prompt(question)
-        self.addTrace("LLMPT", "LLM Reponse\n {}\n".format(resp))
-        return resp
-
-    def writeToFile(self, filename, content):
+        """ send a prompt to the LLM
+        Args:
+            question (str): prompt
+            urlOllama (str): Ollama URL
+            model (str): Ollama Model
+            temperature (str): Ollama Model LLM Temperature
+        Returns:
+            str: LLM response
+        """
         try:
-            self.log.info("Write to file {}".format(filename))
-            with open(filename, "w", encoding=C.ENCODING) as f:
-                f.write(content)
-            return True
+            self.log.info("Send the prompt to the LLM ...")
+            myllm = ollamaWrapper(urlOllama, model, temperature)
+            resp = myllm.prompt(question)
+            self.addMilestone("LLMPT", "LLM Reponse\n {}\n".format(resp))
+            self.log.info("Prompt managed successfully by the LLM.")
+            return resp
         except Exception as e:
-            return False
+            self.log.error("Error while prompting the LLM {}".format(str(e)))
+            return ""
 
-    def writeJsonToFile(self, filename, jsonContent):
+    def textEmbeddings(self, prompt):
+        """ create embeddings for a text (prompt)
+        Args:
+            prompt (str): text
+        Returns:
+            json: data and embeddings
+        """
         try:
-            self.log.info("Write to JSON file {}".format(filename))
-            with open(filename, "w", encoding=C.ENCODING) as f:
-                f.write(json.dumps(jsonContent, cls=NumpyEncoder))
-            return True
+            self.log.info("Create embeddings for text {} ...".format(prompt))
+            vPrompt = self.__embFactory.createFromText(prompt)
+            if (vPrompt == {}):
+                raise Exception("Error while creating the text embeddings")
+            self.addMilestone("PTEMBEDDGS", "Embeddings created from text successfully")
+            self.log.info("Text Embeddings created successfully")
+            return vPrompt
         except Exception as e:
-            return False
-        
-    def readJsonFromFile(self, filename):
+            self.log.error("Error while creating the text embeddings {}".format(str(e)))
+            return {}
+    
+    def chunkEmbeddings(self, chunks):
+        """ create embeddings for a list of chunks
+        Args:
+            prompt (str/json): chunks
+        Returns:
+            json: data and embeddings
+        """
         try:
-            self.log.info("Read from JSON file {}".format(filename))
-            with open(filename, "r", encoding=C.ENCODING) as f:
-                data = json.load(f)
-                return data
+            self.log.info("Create embeddings for list of texts/chunks ...")
+            vChunks = self.__embFactory.createFromList(chunks)
+            if (vChunks == {}):
+                raise Exception("Error while creating the chunks embeddings")
+            self.addMilestone("DOCEMBEDDGS", "Embeddings created from chunks successfully")
+            self.log.info("Chunks Embeddings created successfully")
+            return vChunks
         except Exception as e:
+            self.log.error("Error while creating the list of texts/chunks embeddings {}".format(str(e)))
             return {}
```

## Comparing `elements/FAISSWrapper.py` & `elements/faiss/FAISSWrapper.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,22 +3,23 @@
 __license__ = "MIT"
 
 import pandas as pd
 import numpy as np
 import faiss # pip install faiss-cpu (https://pypi.org/project/faiss-cpu/)
 import pickle
 import os
+import utils.CONST as C
 
 """ 
     Leverage Meta FAISS
 """
 class FAISSWrapper:
     def __init__(self):
         self.index = None   # FAISS Index
-        self.dfContent = pd.DataFrame(columns = ["text", "embedding"]) # real data that are indexed
+        self.dfContent = pd.DataFrame(columns = [C.JST_TEXT, C.JST_EMBEDDINGS]) # real data that are indexed
 
     def save(self, filepath="./backup/", name="faissbackup"):
         """ Save the FAISS index and the data (chunks)
         Args:
             filepath (str, optional): _description_. Defaults to "./backup/".
             name (str, optional): _description_. Defaults to "faissbackup".
         """
@@ -47,23 +48,23 @@
         Returns:
             bool: True if index ready
         """
         try:
             return self.index.is_trained #and not self.dfContent.empty
         except:
             return False
-        
-    def addToIndex(self, item):
+
+    def add(self, item):
         """ Index a new item
         Args:
             item (DataFrame): single embeddings to index
         """
         # Get source data and JSON -> DF
         dfNewContent = pd.DataFrame(item).T
-        embeddings = [ np.asarray(v) for v in dfNewContent["embedding"] ]
+        embeddings = [ np.asarray(v) for v in dfNewContent[C.JST_EMBEDDINGS] ]
         self.__addToIndexFlatL2(embeddings)
         # Concat the content with the existing DF
         self.dfContent = pd.concat([self.dfContent, dfNewContent])
 
     def __addToIndexFlatL2(self, embeddings):
         """
             Build a Flat L2 index
@@ -80,28 +81,28 @@
             array/embedding: vector prepared
         """
         vout =  np.asarray(vects)
         vout = vout.astype(np.float32) # Only support ndarray in 32 bits !
         faiss.normalize_L2(vout)
         return vout
 
-    def getNearest(self, prompt, max):
+    def getNearest(self, prompt, k):
         """ Process the similarity search on the existing FAISS index (and the given prompt)
                 --> k is set to the total number of vectors within the index
                 --> ann is the approximate nearest neighbour corresponding to those distances
         Args:
             prompt (json): Prompt's embeddings
-            max (_type_): Nb of nearest to return
+            k (int): Nb of nearest to return
         Returns:
             DataFrame: List of the most nearest neighbors
         """
         # Get prompt vector only and normalize it
         idx = "0" if (str(type(list(prompt.keys())[0])) == "<class 'str'>") else 0
-        vector = self.__prepareEmbeddings([ prompt[idx]["embedding"] ])
+        vector = self.__prepareEmbeddings([ prompt[idx][C.JST_EMBEDDINGS] ])
         # process the Similarity search
-        k = self.index.ntotal
-        distances, ann = self.index.search(vector, k=k)
+        ktotal = self.index.ntotal
+        distances, ann = self.index.search(vector, k=ktotal)
         # Sort search results and return a DataFrame
         results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})
         self.dfContent.index = self.dfContent.index.astype(int)
         merge = pd.merge(results, self.dfContent, left_on='ann', right_index=True)
-        return merge[:max]
+        return merge[:k]
```

## Comparing `utils/traceOut.py` & `utils/milestone.py`

 * *Files 22% similar despite different names*

```diff
@@ -2,56 +2,56 @@
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import time
 from datetime import timedelta, datetime
 import json
 
-class traceOut:
+class milestone:
     def __init__(self):
         self.perfCounter = None
         self.startTime = None
         self.stopTime = None
         self.traceSteps = []
-        self.traceHeader = {}
+        self.msHeader = {}
         self.stepIdx = 1
-        self.traceHeader = {}
+        self.msHeader = {}
 
     def initialize(self, args):
         for arg in args.keys():
-            self.traceHeader[arg] = args[arg]
+            self.msHeader[arg] = args[arg]
 
     def start(self):
         if (self.perfCounter == None):
             self.perfCounter = time.perf_counter()
             self.startTime = datetime.now()
 
     def add(self, name, description, *others) -> bool:
         try:
             if (self.perfCounter == None):
                 self.start()
-            curTrace = {}
-            curTrace["step"] = self.stepIdx
-            curTrace["name"] = name
-            curTrace["description"] = description
-            curTrace["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
-            curTrace["stepduration"] = str(timedelta(seconds=time.perf_counter() - self.perfCounter))
+            curms = {}
+            curms["step"] = self.stepIdx
+            curms["name"] = name
+            curms["description"] = description
+            curms["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
+            curms["stepduration"] = str(timedelta(seconds=time.perf_counter() - self.perfCounter))
             if (len(others) > 0):
-                curTrace["details"] = others
-            self.traceSteps.append(curTrace)
+                curms["details"] = others
+            self.traceSteps.append(curms)
             self.stepIdx = self.stepIdx + 1
             return True
         except Exception as e:
             return False
         
     def stop(self):
         self.stopTime = datetime.now()
 
     def getFullJSON(self):
         fullJson = {}
-        fullJson["parameters"] = self.traceHeader
+        fullJson["parameters"] = self.msHeader
         fullJson["steps"] = self.traceSteps
         fullJson["start"] = str(self.startTime)
         self.stopTime = datetime.now() if self.stopTime == None else self.stopTime
         fullJson["stop"] = str(self.stopTime)
         fullJson["duration"] = str(self.stopTime - self.startTime)
         return json.dumps(fullJson)
```

## Comparing `RagCLI-0.1.0.dist-info/LICENSE` & `RagCLI-0.1.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `RagCLI-0.1.0.dist-info/METADATA` & `RagCLI-0.1.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: RagCLI
-Version: 0.1.0
+Version: 0.1.1
 Summary: This solution manages a full RAG by using FAISS, Ollama and sentence_transformers for embeddings.
 Author-email: Benoit Cayla <benoit@datacorner.fr>
 License: MIT License
         
         Copyright (c) 2023 Benoît Cayla
         
         Permission is hereby granted, free of charge, to any person obtaining a copy
@@ -35,14 +35,15 @@
 Requires-Dist: pandas ==2.2.1
 Requires-Dist: PyMuPDF ==1.24.0
 Requires-Dist: langchain ==0.1.13
 Requires-Dist: sentence-transformers ==2.6.0
 Requires-Dist: faiss-cpu ==1.8.0
 Requires-Dist: numpyencoder ==0.3.0
 Requires-Dist: langchain-experimental ==0.0.55
+Requires-Dist: chromadb-client ==0.4.25
 
 # Description
 Intrigued by RAG's potential but lost in the labyrinth of code? Fear not, aspiring linguist! Buckle up for a playful Python playground where you can tinker with RAG concepts right on your local machine. Let's ditch the deep learning deployment and dive into an educational exploration of RAG, one command line at a time!
 
 This project aims to run locally (i mean on your laptop without GPUs) and leverages:
 * **Ollama** (https://ollama.com/) for running locally LLMs
 * **Sentence-transformers** (https://pypi.org/project/sentence-transformers/) for the embeddings management
@@ -64,16 +65,16 @@
 3) install ragcli by using pip:
 ```
 pip install [--force-reinstall] wheel file
 ```
 4) use the command lines provided (see in the reference below)
 
 **Note:** Some environment variables may need to be set:  
-* If you plan to use llamaParse, the llamaindex key (generated on the web site: https://cloud.llamaindex.ai/login) must be filled out to LLAMAINDEX_API_KEY 
-* If a specific log file must be specified (by default the programs create the ragcli.log file) with the environment variable RAGCLI_LOGFILE
+* If you plan to use llamaParse, the llamaindex key (generated on the web site: https://cloud.llamaindex.ai/login) must be filled out to **LLAMAINDEX_API_KEY** 
+* If a specific log file must be specified (by default the programs create the ragcli.log file in the working directory) create the environment variable **RAGCLI_LOGFILE** with the file and path accordingly.
 
 # CLI Reference
 ## RagChunkText
 Chunks a text into several parts.  
 output Format is JSON: 
 ```
 {'chunks': ['Transcript of ...', ...] }
```

