# Comparing `tmp/pami-2024.4.24.1.tar.gz` & `tmp/pami-2024.4.9.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pami-2024.4.24.1.tar", last modified: Wed Apr 24 13:33:41 2024, max compression
+gzip compressed data, was "pami-2024.4.9.1.tar", last modified: Tue Apr  9 02:13:28 2024, max compression
```

## Comparing `pami-2024.4.24.1.tar` & `pami-2024.4.9.1.tar`

### file list

```diff
@@ -1,509 +1,505 @@
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.412954 pami-2024.4.24.1/
--rw-r--r--   0 uday       (501) staff       (20)    35149 2024-03-07 22:55:35.000000 pami-2024.4.24.1/LICENSE
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.260948 pami-2024.4.24.1/PAMI/
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.261180 pami-2024.4.24.1/PAMI/AssociationRules/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/AssociationRules/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.263812 pami-2024.4.24.1/PAMI/AssociationRules/basic/
--rw-r--r--   0 uday       (501) staff       (20)    13724 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/AssociationRules/basic/ARWithConfidence.py
--rw-r--r--   0 uday       (501) staff       (20)    14061 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/AssociationRules/basic/ARWithLeverage.py
--rw-r--r--   0 uday       (501) staff       (20)    14030 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/AssociationRules/basic/ARWithLift.py
--rw-r--r--   0 uday       (501) staff       (20)    19416 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/AssociationRules/basic/RuleMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/AssociationRules/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6594 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/AssociationRules/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)      139 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.264068 pami-2024.4.24.1/PAMI/correlatedPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/correlatedPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.266353 pami-2024.4.24.1/PAMI/correlatedPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    25689 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/correlatedPattern/basic/CoMine.py
--rw-r--r--   0 uday       (501) staff       (20)    27462 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/correlatedPattern/basic/CoMinePlus.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/correlatedPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6208 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/correlatedPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.266592 pami-2024.4.24.1/PAMI/coveragePattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/coveragePattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.268736 pami-2024.4.24.1/PAMI/coveragePattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    14643 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/coveragePattern/basic/CMine.py
--rw-r--r--   0 uday       (501) staff       (20)    17200 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/coveragePattern/basic/CPPG.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/coveragePattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7155 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/coveragePattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.271192 pami-2024.4.24.1/PAMI/extras/
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.275041 pami-2024.4.24.1/PAMI/extras/DF2DB/
--rw-r--r--   0 uday       (501) staff       (20)     4360 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/DF2DB.py
--rw-r--r--   0 uday       (501) staff       (20)     4287 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/DF2DBPlus.py
--rw-r--r--   0 uday       (501) staff       (20)    10331 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/DenseFormatDF.py
--rw-r--r--   0 uday       (501) staff       (20)     5413 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/SparseFormatDF.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     3103 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/createTDB.py
--rw-r--r--   0 uday       (501) staff       (20)     6948 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/denseDF2DBPlus.py
--rw-r--r--   0 uday       (501) staff       (20)    11940 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/denseDF2DB_dump.py
--rw-r--r--   0 uday       (501) staff       (20)     5336 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.276103 pami-2024.4.24.1/PAMI/extras/calculateMISValues/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/calculateMISValues/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6468 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/calculateMISValues/usingBeta.py
--rw-r--r--   0 uday       (501) staff       (20)     6499 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/calculateMISValues/usingSD.py
--rw-r--r--   0 uday       (501) staff       (20)     7345 2024-04-17 13:45:09.000000 pami-2024.4.24.1/PAMI/extras/convertMultiTSIntoFuzzy.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.279848 pami-2024.4.24.1/PAMI/extras/dbStats/
--rw-r--r--   0 uday       (501) staff       (20)    14951 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/dbStats/FuzzyDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    13796 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
--rw-r--r--   0 uday       (501) staff       (20)    16034 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/dbStats/SequentialDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    16883 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/dbStats/TemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    12839 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/dbStats/TransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    15120 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    11953 2024-03-30 04:20:04.000000 pami-2024.4.24.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    12679 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/dbStats/UtilityDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/dbStats/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.280970 pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5238 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)     8594 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
--rw-r--r--   0 uday       (501) staff       (20)     8792 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
--rw-r--r--   0 uday       (501) staff       (20)     8313 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.283906 pami-2024.4.24.1/PAMI/extras/generateDatabase/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/generateDatabase/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5685 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     9558 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     5971 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     5157 2024-04-11 02:56:57.000000 pami-2024.4.24.1/PAMI/extras/generateLatexGraphFile.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.286403 pami-2024.4.24.1/PAMI/extras/graph/
--rw-r--r--   0 uday       (501) staff       (20)     3223 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/graph/DF2Fig.py
--rw-r--r--   0 uday       (501) staff       (20)     3577 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/graph/DF2Tex.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/graph/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     2750 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/graph/plotLineGraphFromDictionary.py
--rw-r--r--   0 uday       (501) staff       (20)     3599 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
--rw-r--r--   0 uday       (501) staff       (20)     4465 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/graph/visualizeFuzzyPatterns.py
--rw-r--r--   0 uday       (501) staff       (20)     4240 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/graph/visualizePatterns.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.286666 pami-2024.4.24.1/PAMI/extras/image2Database/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/image2Database/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.287382 pami-2024.4.24.1/PAMI/extras/imageProcessing/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/imageProcessing/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6488 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/imageProcessing/imagery2Databases.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.288359 pami-2024.4.24.1/PAMI/extras/messaging/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-09 02:24:23.000000 pami-2024.4.24.1/PAMI/extras/messaging/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)      533 2024-03-09 07:14:07.000000 pami-2024.4.24.1/PAMI/extras/messaging/discord.py
--rw-r--r--   0 uday       (501) staff       (20)     1575 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/messaging/gmail.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.289670 pami-2024.4.24.1/PAMI/extras/neighbours/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/neighbours/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4784 2024-04-17 13:45:09.000000 pami-2024.4.24.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
--rw-r--r--   0 uday       (501) staff       (20)     4410 2024-04-17 13:45:09.000000 pami-2024.4.24.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
--rw-r--r--   0 uday       (501) staff       (20)     4305 2024-04-17 13:45:09.000000 pami-2024.4.24.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
--rw-r--r--   0 uday       (501) staff       (20)     5013 2024-04-11 02:56:57.000000 pami-2024.4.24.1/PAMI/extras/plotPointOnMap.py
--rw-r--r--   0 uday       (501) staff       (20)     5183 2024-04-11 02:56:57.000000 pami-2024.4.24.1/PAMI/extras/plotPointOnMap_dump.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.289844 pami-2024.4.24.1/PAMI/extras/sampleDatasets/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/sampleDatasets/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4024 2024-04-11 02:56:57.000000 pami-2024.4.24.1/PAMI/extras/scatterPlotSpatialPoints.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.292352 pami-2024.4.24.1/PAMI/extras/stats/
--rw-r--r--   0 uday       (501) staff       (20)    12724 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/stats/TransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-09 15:21:42.000000 pami-2024.4.24.1/PAMI/extras/stats/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4144 2024-03-11 10:03:40.000000 pami-2024.4.24.1/PAMI/extras/stats/graphDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    15998 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/stats/sequentialDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    16926 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/stats/temporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)    12692 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/extras/stats/utilityDatabase.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.299737 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/
--rw-r--r--   0 uday       (501) staff       (20)     7276 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     6380 2024-04-24 13:32:30.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     2325 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     2254 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
--rw-r--r--   0 uday       (501) staff       (20)     2539 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
--rw-r--r--   0 uday       (501) staff       (20)     1880 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     1843 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
--rw-r--r--   0 uday       (501) staff       (20)     2117 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     2066 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
--rw-r--r--   0 uday       (501) staff       (20)     2262 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     1121 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     1111 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py
--rw-r--r--   0 uday       (501) staff       (20)     1625 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     1610 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
--rw-r--r--   0 uday       (501) staff       (20)     3613 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
--rw-r--r--   0 uday       (501) staff       (20)     3603 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     4324 2024-03-11 10:03:40.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     3283 2024-03-11 10:03:40.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
--rw-r--r--   0 uday       (501) staff       (20)     4842 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
--rw-r--r--   0 uday       (501) staff       (20)     3240 2024-04-11 02:56:57.000000 pami-2024.4.24.1/PAMI/extras/topKPatterns.py
--rw-r--r--   0 uday       (501) staff       (20)     2322 2024-04-11 02:56:57.000000 pami-2024.4.24.1/PAMI/extras/uncertaindb_convert.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.300114 pami-2024.4.24.1/PAMI/extras/visualize/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-09 16:06:30.000000 pami-2024.4.24.1/PAMI/extras/visualize/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     1897 2024-03-13 15:35:41.000000 pami-2024.4.24.1/PAMI/extras/visualize/graphs.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.300427 pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.301560 pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    14510 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
--rw-r--r--   0 uday       (501) staff       (20)    23166 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6856 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.301789 pami-2024.4.24.1/PAMI/frequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.305699 pami-2024.4.24.1/PAMI/frequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    13313 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/Apriori.py
--rw-r--r--   0 uday       (501) staff       (20)    13493 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/ECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)    13827 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/ECLATDiffset.py
--rw-r--r--   0 uday       (501) staff       (20)    13267 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/ECLATbitset.py
--rw-r--r--   0 uday       (501) staff       (20)    19151 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/FPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    15219 2024-04-18 12:21:21.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/_Apriori.py
--rw-r--r--   0 uday       (501) staff       (20)    22703 2024-04-18 12:21:21.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/_FPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7867 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.306380 pami-2024.4.24.1/PAMI/frequentPattern/closed/
--rw-r--r--   0 uday       (501) staff       (20)    20479 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/closed/CHARM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6580 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/closed/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.309455 pami-2024.4.24.1/PAMI/frequentPattern/cuda/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5980 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    13664 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuApriori.py
--rw-r--r--   0 uday       (501) staff       (20)    14418 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuAprioriBit.py
--rw-r--r--   0 uday       (501) staff       (20)    13015 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuEclat.py
--rw-r--r--   0 uday       (501) staff       (20)    14583 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuEclatBit.py
--rw-r--r--   0 uday       (501) staff       (20)    14499 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
--rw-r--r--   0 uday       (501) staff       (20)    17118 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py
--rw-r--r--   0 uday       (501) staff       (20)    14178 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.310301 pami-2024.4.24.1/PAMI/frequentPattern/maximal/
--rw-r--r--   0 uday       (501) staff       (20)    26092 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/maximal/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6561 2024-03-09 15:50:50.000000 pami-2024.4.24.1/PAMI/frequentPattern/maximal/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.311553 pami-2024.4.24.1/PAMI/frequentPattern/pyspark/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/pyspark/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5573 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/pyspark/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    15452 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/pyspark/parallelApriori.py
--rw-r--r--   0 uday       (501) staff       (20)    12947 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/pyspark/parallelECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)    17438 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.313149 pami-2024.4.24.1/PAMI/frequentPattern/topk/
--rw-r--r--   0 uday       (501) staff       (20)    15426 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/frequentPattern/topk/FAE.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/topk/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4575 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/frequentPattern/topk/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.314094 pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.315770 pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    28229 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6652 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.316035 pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.320411 pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    22973 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    28621 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6442 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.320860 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.322049 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    26388 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    28303 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6730 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.323291 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.324486 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    28672 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    33585 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6623 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.325205 pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.326987 pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/basic/
--rw-r--r--   0 uday       (501) staff       (20)    22562 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6469 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.327941 pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.329729 pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    25786 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    27472 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6683 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.330205 pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.332560 pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    22107 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6791 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.333373 pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.335160 pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    22965 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    21152 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6697 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.335484 pami-2024.4.24.1/PAMI/georeferencedFrequentSequencePattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/georeferencedFrequentSequencePattern/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6696 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/georeferencedFrequentSequencePattern/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.335840 pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.336680 pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    21718 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6178 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.336920 pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.337785 pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    38524 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6179 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.338013 pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.338688 pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    42772 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6307 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.338914 pami-2024.4.24.1/PAMI/highUtilityPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.340988 pami-2024.4.24.1/PAMI/highUtilityPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    35224 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/basic/EFIM.py
--rw-r--r--   0 uday       (501) staff       (20)    26511 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/basic/HMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    28921 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/basic/UPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5166 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    19909 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/basic/efimParallel.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.341842 pami-2024.4.24.1/PAMI/highUtilityPattern/parallel/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/parallel/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5166 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/parallel/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    17831 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityPattern/parallel/efimparallel.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.342569 pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/
--rw-r--r--   0 uday       (501) staff       (20)    30022 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/HUPMS.py
--rw-r--r--   0 uday       (501) staff       (20)    32848 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5193 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.343318 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6716 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.344034 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    29850 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)    37379 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5934 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.345270 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/topk/
--rw-r--r--   0 uday       (501) staff       (20)    37519 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/topk/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6618 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/topk/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.346094 pami-2024.4.24.1/PAMI/localPeriodicPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/localPeriodicPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.348979 pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    35370 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    24681 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
--rw-r--r--   0 uday       (501) staff       (20)    23709 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     8385 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.349971 pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.350805 pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    24308 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    22008 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5921 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.351497 pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.352297 pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    28420 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    22330 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5398 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.353293 pami-2024.4.24.1/PAMI/partialPeriodicPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.355885 pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    26672 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)     4329 2024-03-09 01:54:15.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/Gabstract.py
--rw-r--r--   0 uday       (501) staff       (20)    25525 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19577 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5520 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.358219 pami-2024.4.24.1/PAMI/partialPeriodicPattern/closed/
--rw-r--r--   0 uday       (501) staff       (20)    22070 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/closed/PPPClose.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5605 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/closed/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.358991 pami-2024.4.24.1/PAMI/partialPeriodicPattern/maximal/
--rw-r--r--   0 uday       (501) staff       (20)    29371 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/maximal/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4278 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/maximal/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.359727 pami-2024.4.24.1/PAMI/partialPeriodicPattern/pyspark/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/pyspark/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5765 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/pyspark/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    28554 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.361338 pami-2024.4.24.1/PAMI/partialPeriodicPattern/topk/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/topk/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6441 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/topk/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    19588 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.362520 pami-2024.4.24.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/
--rw-r--r--   0 uday       (501) staff       (20)    28160 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6350 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.362779 pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.363313 pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27559 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6691 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.363536 pami-2024.4.24.1/PAMI/periodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.367073 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    17272 2024-04-18 12:21:21.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)    21302 2024-04-18 12:21:21.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    26383 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
--rw-r--r--   0 uday       (501) staff       (20)    18106 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFPMC.py
--rw-r--r--   0 uday       (501) staff       (20)    36300 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    17904 2024-04-18 12:21:21.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)    28583 2024-04-18 12:21:21.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      726 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6549 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    26643 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.368785 pami-2024.4.24.1/PAMI/periodicFrequentPattern/closed/
--rw-r--r--   0 uday       (501) staff       (20)    24417 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6539 2024-04-23 00:46:42.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/closed/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.370519 pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6568 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    23867 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
--rw-r--r--   0 uday       (501) staff       (20)    18982 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.371398 pami-2024.4.24.1/PAMI/periodicFrequentPattern/maximal/
--rw-r--r--   0 uday       (501) staff       (20)    31832 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/maximal/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7869 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/maximal/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.372828 pami-2024.4.24.1/PAMI/periodicFrequentPattern/pyspark/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/pyspark/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     5219 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/pyspark/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    26749 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.373600 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.375939 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/TopkPFP/
--rw-r--r--   0 uday       (501) staff       (20)    19951 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6862 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.376652 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4589 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    17514 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.376990 pami-2024.4.24.1/PAMI/recurringPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/recurringPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.377747 pami-2024.4.24.1/PAMI/recurringPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    29135 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/recurringPattern/basic/RPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/recurringPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6637 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/recurringPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.378008 pami-2024.4.24.1/PAMI/relativeFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/relativeFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.378827 pami-2024.4.24.1/PAMI/relativeFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    30349 2024-03-13 19:35:46.000000 pami-2024.4.24.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/relativeFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4261 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/relativeFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.379067 pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.380279 pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    35406 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6052 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.380526 pami-2024.4.24.1/PAMI/sequence/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/sequence/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.380633 pami-2024.4.24.1/PAMI/sequentialPatternMining/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.382805 pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/
--rw-r--r--   0 uday       (501) staff       (20)    42265 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/SPADE.py
--rw-r--r--   0 uday       (501) staff       (20)    19986 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/SPAM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6569 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    24786 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/prefixSpan.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.383782 pami-2024.4.24.1/PAMI/sequentialPatternMining/closed/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/closed/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6285 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/closed/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/sequentialPatternMining/closed/bide.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.383895 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.388411 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    18431 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
--rw-r--r--   0 uday       (501) staff       (20)    26753 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19807 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7271 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.390705 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/topK/
--rw-r--r--   0 uday       (501) staff       (20)    27859 2024-03-09 01:54:15.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7173 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.391865 pami-2024.4.24.1/PAMI/subgraphMining/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.395046 pami-2024.4.24.1/PAMI/subgraphMining/basic/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     1241 2024-03-09 01:54:15.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)     2396 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/dfsCode.py
--rw-r--r--   0 uday       (501) staff       (20)      772 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/edge.py
--rw-r--r--   0 uday       (501) staff       (20)     2616 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/extendedEdge.py
--rw-r--r--   0 uday       (501) staff       (20)      670 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/frequentSubgraph.py
--rw-r--r--   0 uday       (501) staff       (20)     4943 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/graph.py
--rw-r--r--   0 uday       (501) staff       (20)    28244 2024-03-13 19:52:03.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/gspan.py
--rw-r--r--   0 uday       (501) staff       (20)     1748 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
--rw-r--r--   0 uday       (501) staff       (20)      826 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/subgraphMining/basic/vertex.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.398479 pami-2024.4.24.1/PAMI/subgraphMining/topK/
--rw-r--r--   0 uday       (501) staff       (20)     1949 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/DFSCode.py
--rw-r--r--   0 uday       (501) staff       (20)      593 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/DFSThread.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     1316 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)      772 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/edge.py
--rw-r--r--   0 uday       (501) staff       (20)     2613 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/extendedEdge.py
--rw-r--r--   0 uday       (501) staff       (20)      674 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/frequentSubgraph.py
--rw-r--r--   0 uday       (501) staff       (20)     4295 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/graph.py
--rw-r--r--   0 uday       (501) staff       (20)     1486 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py
--rw-r--r--   0 uday       (501) staff       (20)    20979 2024-03-13 17:24:33.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/tkg.py
--rw-r--r--   0 uday       (501) staff       (20)      818 2024-03-13 15:38:51.000000 pami-2024.4.24.1/PAMI/subgraphMining/topK/vertex.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.399048 pami-2024.4.24.1/PAMI/uncertainFaultTolerantFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)    15959 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6756 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.399285 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.402950 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    27774 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
--rw-r--r--   0 uday       (501) staff       (20)    26760 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19947 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/TUFP.py
--rw-r--r--   0 uday       (501) staff       (20)    19818 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/TubeP.py
--rw-r--r--   0 uday       (501) staff       (20)    28152 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/TubeS.py
--rw-r--r--   0 uday       (501) staff       (20)    26028 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    19883 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4945 2024-04-08 07:45:50.000000 pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.403204 pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.404962 pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    29949 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4986 2024-03-09 15:50:50.000000 pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.405207 pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)      727 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.406950 pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    32289 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)    32890 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6536 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.407203 pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.407886 pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    29141 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6603 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.408152 pami-2024.4.24.1/PAMI/weightedFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.408939 pami-2024.4.24.1/PAMI/weightedFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    25734 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/weightedFrequentPattern/basic/WFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     6659 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.409182 pami-2024.4.24.1/PAMI/weightedFrequentRegularPattern/
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentRegularPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.409616 pami-2024.4.24.1/PAMI/weightedFrequentRegularPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    28917 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentRegularPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     7495 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.409873 pami-2024.4.24.1/PAMI/weightedUncertainFrequentPattern/
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedUncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.410400 pami-2024.4.24.1/PAMI/weightedUncertainFrequentPattern/basic/
--rw-r--r--   0 uday       (501) staff       (20)    30905 2024-04-19 10:58:10.000000 pami-2024.4.24.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
--rw-r--r--   0 uday       (501) staff       (20)        0 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 uday       (501) staff       (20)     4771 2024-03-07 22:55:35.000000 pami-2024.4.24.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
--rw-r--r--   0 uday       (501) staff       (20)    68992 2024-04-24 13:33:41.412667 pami-2024.4.24.1/PKG-INFO
--rw-r--r--   0 uday       (501) staff       (20)    67500 2024-04-17 13:45:09.000000 pami-2024.4.24.1/README.md
-drwxr-xr-x   0 uday       (501) staff       (20)        0 2024-04-24 13:33:41.411353 pami-2024.4.24.1/pami.egg-info/
--rw-r--r--   0 uday       (501) staff       (20)    68992 2024-04-24 13:33:41.000000 pami-2024.4.24.1/pami.egg-info/PKG-INFO
--rw-r--r--   0 uday       (501) staff       (20)    18233 2024-04-24 13:33:41.000000 pami-2024.4.24.1/pami.egg-info/SOURCES.txt
--rw-r--r--   0 uday       (501) staff       (20)        1 2024-04-24 13:33:41.000000 pami-2024.4.24.1/pami.egg-info/dependency_links.txt
--rw-r--r--   0 uday       (501) staff       (20)      255 2024-04-24 13:33:41.000000 pami-2024.4.24.1/pami.egg-info/requires.txt
--rw-r--r--   0 uday       (501) staff       (20)        5 2024-04-24 13:33:41.000000 pami-2024.4.24.1/pami.egg-info/top_level.txt
--rw-r--r--   0 uday       (501) staff       (20)       38 2024-04-24 13:33:41.413091 pami-2024.4.24.1/setup.cfg
--rw-r--r--   0 uday       (501) staff       (20)     1536 2024-04-24 13:33:26.000000 pami-2024.4.24.1/setup.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.602975 pami-2024.4.9.1/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35149 2024-03-12 04:33:29.000000 pami-2024.4.9.1/LICENSE
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.793712 pami-2024.4.9.1/PAMI/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.795296 pami-2024.4.9.1/PAMI/AssociationRules/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.809616 pami-2024.4.9.1/PAMI/AssociationRules/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    14314 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithConfidence.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14661 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLeverage.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14622 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLift.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20378 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/RuleMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6594 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      139 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.811156 pami-2024.4.9.1/PAMI/correlatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.817103 pami-2024.4.9.1/PAMI/correlatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27142 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    29081 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMinePlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6208 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.818346 pami-2024.4.9.1/PAMI/coveragePattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.826282 pami-2024.4.9.1/PAMI/coveragePattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    15616 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/CMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18923 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/CPPG.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7155 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.838845 pami-2024.4.9.1/PAMI/extras/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.853335 pami-2024.4.9.1/PAMI/extras/DF2DB/
+-rw-r--r--   0 vanithak   (502) staff       (20)     4360 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DB.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4287 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    10331 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DenseFormatDF.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5413 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/SparseFormatDF.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3103 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/createTDB.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6948 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    11940 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DB_dump.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5336 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.858573 pami-2024.4.9.1/PAMI/extras/calculateMISValues/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6468 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingBeta.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6499 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingSD.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6964 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/convertMultiTSIntoFuzzy.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.876165 pami-2024.4.9.1/PAMI/extras/dbStats/
+-rw-r--r--   0 vanithak   (502) staff       (20)    14951 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/FuzzyDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13796 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16034 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/dbStats/SequentialDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16883 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/TemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12839 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15120 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    11953 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12679 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UtilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.880811 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5238 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8594 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8792 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8313 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.889684 pami-2024.4.9.1/PAMI/extras/generateDatabase/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5685 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     9558 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5971 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5156 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/generateLatexGraphFile.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.901708 pami-2024.4.9.1/PAMI/extras/graph/
+-rw-r--r--   0 vanithak   (502) staff       (20)     3223 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/DF2Fig.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3577 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/DF2Tex.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/graph/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2750 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphFromDictionary.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3599 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4465 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/visualizeFuzzyPatterns.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4240 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/visualizePatterns.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.904228 pami-2024.4.9.1/PAMI/extras/image2Database/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/image2Database/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.906815 pami-2024.4.9.1/PAMI/extras/imageProcessing/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/imageProcessing/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6488 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/imageProcessing/imagery2Databases.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.911056 pami-2024.4.9.1/PAMI/extras/messaging/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/messaging/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      533 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/messaging/discord.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1575 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/messaging/gmail.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.917378 pami-2024.4.9.1/PAMI/extras/neighbours/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/neighbours/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4789 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4415 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4310 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5011 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/plotPointOnMap.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5182 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/plotPointOnMap_dump.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.922472 pami-2024.4.9.1/PAMI/extras/sampleDatasets/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/sampleDatasets/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4023 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/scatterPlotSpatialPoints.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.932869 pami-2024.4.9.1/PAMI/extras/stats/
+-rw-r--r--   0 vanithak   (502) staff       (20)    12724 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/stats/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4144 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/stats/graphDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15998 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/sequentialDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16926 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/temporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12692 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/utilityDatabase.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.964837 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/
+-rw-r--r--   0 vanithak   (502) staff       (20)     8471 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5543 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2325 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2254 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2539 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1880 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1843 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2117 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2066 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2262 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1121 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1111 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1625 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1610 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3613 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3603 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4324 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3283 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4842 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3238 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/topKPatterns.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2321 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/uncertaindb_convert.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.969666 pami-2024.4.9.1/PAMI/extras/visualize/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/visualize/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1897 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/visualize/graphs.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.971091 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.976128 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    15253 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    24431 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6856 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.977409 pami-2024.4.9.1/PAMI/frequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.988154 pami-2024.4.9.1/PAMI/frequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    15220 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/Apriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14362 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14885 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATDiffset.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15362 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATbitset.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22703 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/FPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7867 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.991640 pami-2024.4.9.1/PAMI/frequentPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    22294 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/CHARM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6580 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.052468 pami-2024.4.9.1/PAMI/frequentPattern/cuda/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5980 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15188 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16228 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuAprioriBit.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14636 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16604 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclatBit.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16419 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    21681 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15055 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.056713 pami-2024.4.9.1/PAMI/frequentPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27531 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6561 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.064878 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5573 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16543 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14856 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19026 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.073549 pami-2024.4.9.1/PAMI/frequentPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)    16687 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/FAE.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4575 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.075074 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.080427 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    32988 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6645 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.085119 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.092414 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25855 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32901 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6428 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.094061 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.101426 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29496 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32795 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6724 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.103125 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.110674 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    33628 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    39030 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.114640 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.121669 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25016 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6463 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.122749 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.128995 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29275 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32773 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6678 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.130182 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.138617 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    23633 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6782 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.139762 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.145864 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    23719 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22366 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6689 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.148990 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6690 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.150420 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.155070 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    23257 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6178 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.156998 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.160629 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    41228 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6179 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.162526 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.169244 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    46718 2024-04-09 02:02:27.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6307 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.170240 pami-2024.4.9.1/PAMI/highUtilityPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.178668 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    37128 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/EFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    30770 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/HMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32709 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/UPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20285 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/efimParallel.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.185414 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18207 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/efimparallel.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.191526 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/
+-rw-r--r--   0 vanithak   (502) staff       (20)    32873 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/HUPMS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    35784 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5193 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.194856 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6716 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.201955 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35564 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    40140 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5934 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.205999 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)    40254 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.207268 pami-2024.4.9.1/PAMI/localPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.214488 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    34480 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23965 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22949 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8385 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.217320 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.222491 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25470 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23153 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5921 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.224344 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.234442 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28279 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22083 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5398 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.235758 pami-2024.4.9.1/PAMI/partialPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.251077 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    26417 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4329 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/Gabstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26878 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20736 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5520 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.257160 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    24133 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/PPPClose.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5605 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.274878 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29141 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4278 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.286087 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5765 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    30910 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.293743 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6441 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20928 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.303465 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28158 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5556 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.309577 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.319198 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27514 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6640 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.320539 pami-2024.4.9.1/PAMI/periodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.340954 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    17905 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28583 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26383 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18106 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPMC.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    36300 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      726 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6545 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26643 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.347343 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    24312 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6539 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.356329 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6568 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23867 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18982 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.363446 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    31832 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7869 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.373650 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5219 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26749 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.376040 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.383726 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/
+-rw-r--r--   0 vanithak   (502) staff       (20)    19951 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6862 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.387608 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4589 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17514 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.389862 pami-2024.4.9.1/PAMI/recurringPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.394287 pami-2024.4.9.1/PAMI/recurringPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29135 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/RPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6637 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.396535 pami-2024.4.9.1/PAMI/relativeFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.403016 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30349 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4261 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.404650 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.407969 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35406 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6052 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.410534 pami-2024.4.9.1/PAMI/sequence/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequence/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.411261 pami-2024.4.9.1/PAMI/sequentialPatternMining/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.421832 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    42265 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPADE.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19986 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPAM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6569 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    24786 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/prefixSpan.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.426003 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6285 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/bide.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.427496 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.436338 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    19114 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26504 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    21391 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7271 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.440665 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27859 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7173 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.441985 pami-2024.4.9.1/PAMI/subgraphMining/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.456287 pami-2024.4.9.1/PAMI/subgraphMining/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1241 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2396 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/dfsCode.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/edge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2616 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/extendedEdge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      670 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/frequentSubgraph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4943 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/graph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28244 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/gspan.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1748 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      826 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/vertex.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.478210 pami-2024.4.9.1/PAMI/subgraphMining/topK/
+-rw-r--r--   0 vanithak   (502) staff       (20)     1949 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSCode.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      593 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSThread.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1316 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/edge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2613 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/extendedEdge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      674 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/frequentSubgraph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4295 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/graph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1486 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20979 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/tkg.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      818 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/vertex.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.482255 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)    17358 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6756 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.483938 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.506622 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28610 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26391 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19572 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TUFP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19454 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    27790 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    25664 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19522 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4945 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.508127 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.520288 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30868 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4986 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.524799 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.537346 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    33395 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    33912 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6536 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.538277 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.546173 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30821 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6603 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.549586 pami-2024.4.9.1/PAMI/weightedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.558446 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27246 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/WFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6659 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.563418 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.574205 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30320 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7495 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.575583 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.579690 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    31958 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4771 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    67204 2024-04-09 02:13:28.599349 pami-2024.4.9.1/PKG-INFO
+-rw-r--r--   0 vanithak   (502) staff       (20)    65760 2024-03-29 21:11:29.000000 pami-2024.4.9.1/README.md
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.589956 pami-2024.4.9.1/pami.egg-info/
+-rw-r--r--   0 vanithak   (502) staff       (20)    67204 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/PKG-INFO
+-rw-r--r--   0 vanithak   (502) staff       (20)    18058 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/SOURCES.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/dependency_links.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)      237 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/requires.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)        5 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/top_level.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)       38 2024-04-09 02:13:28.603191 pami-2024.4.9.1/setup.cfg
+-rw-r--r--   0 vanithak   (502) staff       (20)     1494 2024-04-09 02:13:23.000000 pami-2024.4.9.1/setup.py
```

### Comparing `pami-2024.4.24.1/LICENSE` & `pami-2024.4.9.1/LICENSE`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/AssociationRules/basic/ARWithConfidence.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithConfidence.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# This code uses "confidence" metric to extract the association rules from given frequent patterns.
+#  This code uses "confidence" metric to extract the association rules from given frequent patterns.
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
 #
 #             import PAMI.AssociationRules.basic import ARWithConfidence as alg
 #
@@ -85,15 +85,14 @@
         self._singleItems = singleItems
         self._minConf = minConf
         self._finalPatterns = {}
 
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
-
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             conf = self._generateWithConfidence(prefix, suffix[0])
@@ -104,15 +103,14 @@
                 self._generateWithConfidence(prefix + ' ' + suffix[i], suffix[j])
                 # self._generation(prefix+ ' ' +suffix[i], suffix[i+1:])
             self._generation(prefix1, suffix1)
 
     def _generateWithConfidence(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
-
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -137,17 +135,14 @@
             for j in range(i + 1, len(self._singleItems)):
                 self._generateWithConfidence(self._singleItems[i], self._singleItems[j])
             self._generation(prefix, suffix)
 
 
 class ARWithConfidence:
     """
-    About this algorithm
-    ====================
-
     :Description: Association Rules are derived from frequent patterns using "confidence" metric.
 
     :Reference:
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of association rules
     :param  oFile: str :
@@ -157,14 +152,15 @@
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
         
     :Attributes:
 
+
         startTime : float
             To record the start time of the mining process
 
         endTime : float
             To record the completion time of the mining process
 
         finalPatterns : dict
@@ -173,34 +169,32 @@
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
-    Execution methods
-    =================
-
-    **Terminal command**
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 ARWithConfidence.py <inputFile> <outputFile> <minConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 ARWithConfidence.py sampleDB.txt patterns.txt 0.5 ' '
 
-    .. note:: minConf can be specified in a value between 0 and 1.
+    .. note:: minConf will be considered only in 0 to 1.
     
     
-    **Calling from a python program**
-
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import ARWithConfidence as alg
 
             obj = alg.ARWithConfidence(iFile, minConf)
 
             obj.mine()
@@ -222,16 +216,16 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    Credits
-    =======
+    **Credits:**
+    -------------
 
             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
 
     _minConf = float()
     _startTime = float()
     _endTime = float()
@@ -301,15 +295,26 @@
         return k
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Association rule mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        a = _Confidence(self._frequentPatterns, k, self._minConf)
+        a.run()
+        self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
 
 
 
     def mine(self):
         """
         Association rule mining process will start from here
         """
@@ -325,45 +330,41 @@
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -371,28 +372,26 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the outputfile
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
```

### Comparing `pami-2024.4.24.1/PAMI/AssociationRules/basic/ARWithLeverage.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLift.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# This code uses "leverage" metric to extract the association rules from given frequent patterns.
+#  This code uses "lift" metric to extract the association rules from given frequent patterns.
+#
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
+#             import PAMI.AssociationRules.basic import ARWithLift as alg
 #
-#             import PAMI.AssociationRules.basic import ARWithLeverage as alg
-#
-#             obj = alg.ARWithLeverage(iFile, minConf)
+#             obj = alg.ARWithLift(iFile, minConf)
 #
 #             obj.mine()
 #
 #             associationRules = obj.getPatterns()
 #
 #             print("Total number of Association Rules:", len(associationRules))
 #
@@ -49,30 +49,30 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
+
 from PAMI.AssociationRules.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
-
-class _Leverage:
+class Lift:
 
     """
-    :param patterns: Dictionary containing patterns and its support value.
-    :type patterns: dict
+    :param  patterns: Dictionary containing patterns and its support value.
+    :type  patterns: dict
     :param  singleItems: List containing all the single frequent items.
-    :type singleItems: list
+    :type  singleItems: list
     :param  minConf: Minimum confidence to mine all the satisfying association rules.
-    :type minConf: int
+    :type  minConf: int
     """
-
+    
     def __init__(self, patterns, singleItems, minConf) -> None:
         """
         :param patterns: given frequent patterns
         :type patterns: dict
         :param singleItems: one-length frequent patterns
         :type singleItems: list
         :param minConf: minimum confidence
@@ -83,50 +83,48 @@
         self._singleItems = singleItems
         self._minConf = minConf
         self._finalPatterns = {}
 
     def _generation(self, prefix, suffix) -> None:
         """
         To generate the combinations all association rules.
-
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
+        :return: None
         """
-
-
         if len(suffix) == 1:
-            conf = self._generateWithLeverage(prefix, suffix[0])
+            self._generateWithLift(prefix, suffix[0])
         for i in range(len(suffix)):
             suffix1 = suffix[:i] + suffix[i + 1:]
             prefix1 = prefix + ' ' + suffix[i]
             for j in range(i + 1, len(suffix)):
-                self._generateWithLeverage(prefix + ' ' + suffix[i], suffix[j])
+                self._generateWithLift(prefix + ' ' + suffix[i], suffix[j])
+                # self._generation(prefix+ ' ' +suffix[i], suffix[i+1:])
             self._generation(prefix1, suffix1)
 
-    def _generateWithLeverage(self, lhs, rhs) -> float:
+    def _generateWithLift(self, lhs, rhs)  -> float:
         """
         To find association rules satisfying user-specified minConf
-
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         :return: the association rule
         :rtype: float
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
             return 0
         minimum = self._frequentPatterns[s]
         conf_lhs = minimum / self._frequentPatterns[lhs]
         conf_rhs = minimum / self._frequentPatterns[rhs]
-        lift_lhs = conf_lhs - self._frequentPatterns[rhs] * self._frequentPatterns[lhs]
-        right_rhs = conf_rhs - self._frequentPatterns[lhs] * self._frequentPatterns[rhs]
+        lift_lhs = conf_lhs / self._frequentPatterns[rhs] * self._frequentPatterns[lhs]
+        right_rhs = conf_rhs / self._frequentPatterns[lhs] * self._frequentPatterns[rhs]
         if lift_lhs >= self._minConf:
             s1 = lhs + '->' + rhs
             self._finalPatterns[s1] = conf_lhs
         if right_rhs >= self._minConf:
             s1 = rhs + '->' + lhs
             self._finalPatterns[s1] = conf_rhs
 
@@ -134,36 +132,33 @@
         """
         To generate the combinations all association rules.
         """
         for i in range(len(self._singleItems)):
             suffix = self._singleItems[:i] + self._singleItems[i + 1:]
             prefix = self._singleItems[i]
             for j in range(i + 1, len(self._singleItems)):
-                conf = self._generateWithLeverage(self._singleItems[i], self._singleItems[j])
+                self._generateWithLift(self._singleItems[i], self._singleItems[j])
             self._generation(prefix, suffix)
 
 
-class ARWithLeverage:
+class ARWithLift:
     """
-    About this algorithm
-    ====================
-
-    :Description: Association Rules are derived from frequent patterns using "leverage" metric.
+    :Description: Association Rules are derived from frequent patterns using "lift" metric.
 
     :Reference:
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of association rules
     :param  oFile: str :
                    Name of the output file to store complete set of association rules
     :param  minConf: float :
                    The user can specify the minConf in float between the range of 0 to 1.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-        
+
         
     :Attributes:
 
 
         startTime : float
             To record the start time of the mining process
 
@@ -176,39 +171,37 @@
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
-    Execution methods
-    =================
-
-    **Terminal command**
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ARWithLeverage.py <inputFile> <outputFile> <minConf> <sep>
+      (.venv) $ python3 ARWithLift.py <inputFile> <outputFile> <minConf> <sep>
 
       Example Usage:
 
-      (.venv) $ python3 ARWithLeverage.py sampleDB.txt patterns.txt 10.0 ' '
-
-    .. note:: minConf can be specified in a value between 0 and 1.
+      (.venv) $ python3 ARWithLift.py sampleDB.txt patterns.txt 0.5 ' '
 
+    .. note:: minConf will be considered only in 0 to 1.
     
-    **Calling from a python program**
-
+    
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
     .. code-block:: python
 
-            import PAMI.AssociationRules.basic import ARWithLeverage as alg
+            import PAMI.AssociationRules.basic import ARWithLift as alg
 
-            obj = alg.ARWithLeverage(iFile, minConf)
+            obj = alg.ARWithLift(iFile, minConf)
 
             obj.mine()
 
             associationRules = obj.getPatterns()
 
             print("Total number of Association Rules:", len(associationRules))
 
@@ -224,41 +217,40 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
             
-    Credits
-    =======
+    **Credits:**
+    -----------------------
 
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     def __init__(self, iFile, minConf, sep) -> None:
         """
         :param iFile: input file name or path
         :type iFile: str
-        :param minConf: The user can specify the minConf in float between the range of 0 to 1.
+        :param minConf: minimum confidence
         :type minConf: float
-        :param sep: This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+        :param sep: Delimiter of input file
         :type sep: str
         :return: None
         """
         self._iFile = iFile
         self._minConf = minConf
         self._finalPatterns = {}
         self._sep = sep
 
     def _readPatterns(self) -> list:
         """
-        To read patterns  of leverage
-
-        :return: List of patterns
+        Reading the input file and storing all the frequent patterns and their support respectively in a frequentPatterns variable.
+        :return: list of frequent patterns and their support respectively in a frequentPatterns
         :rtype: list
         """
         self._frequentPatterns = {}
         k = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             pattern, sup = [], []
             if self._iFile.empty:
@@ -298,67 +290,74 @@
         return k
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Association rule mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        a = Lift(self._frequentPatterns, k, self._minConf)
+        a.run()
+        self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
 
     def mine(self) -> None:
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
         k = self._readPatterns()
-        a = _Leverage(self._frequentPatterns, k, self._minConf)
+        a = Lift(self._frequentPatterns, k, self._minConf)
         a.run()
         self._finalPatterns = a._finalPatterns
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -366,29 +365,27 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the outputfile
         :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
@@ -400,17 +397,17 @@
         print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ARWithLeverage(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
+            _ap = ARWithLift(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ARWithLeverage(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = ARWithLift(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
```

### Comparing `pami-2024.4.24.1/PAMI/AssociationRules/basic/ARWithLift.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLeverage.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# This code uses "lift" metric to extract the association rules from given frequent patterns.
-#
+# This code uses "leverage" metric to extract the association rules from given frequent patterns.
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
-#             import PAMI.AssociationRules.basic import ARWithLift as alg
 #
-#             obj = alg.ARWithLift(iFile, minConf)
+#             import PAMI.AssociationRules.basic import ARWithLeverage as alg
+#
+#             obj = alg.ARWithLeverage(iFile, minConf)
 #
 #             obj.mine()
 #
 #             associationRules = obj.getPatterns()
 #
 #             print("Total number of Association Rules:", len(associationRules))
 #
@@ -49,30 +49,35 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
-
 from PAMI.AssociationRules.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
-class Lift:
+
+class _Leverage:
 
     """
-    :param  patterns: Dictionary containing patterns and its support value.
-    :type  patterns: dict
+    :param patterns: Dictionary containing patterns and its support value.
+
+    :type patterns: dict
+
     :param  singleItems: List containing all the single frequent items.
-    :type  singleItems: list
+
+    :type singleItems: list
+
     :param  minConf: Minimum confidence to mine all the satisfying association rules.
-    :type  minConf: int
+
+    :type minConf: int
     """
-    
+
     def __init__(self, patterns, singleItems, minConf) -> None:
         """
         :param patterns: given frequent patterns
         :type patterns: dict
         :param singleItems: one-length frequent patterns
         :type singleItems: list
         :param minConf: minimum confidence
@@ -83,50 +88,48 @@
         self._singleItems = singleItems
         self._minConf = minConf
         self._finalPatterns = {}
 
     def _generation(self, prefix, suffix) -> None:
         """
         To generate the combinations all association rules.
-
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
-        :return: None
         """
+
+
         if len(suffix) == 1:
-            self._generateWithLift(prefix, suffix[0])
+            conf = self._generateWithLeverage(prefix, suffix[0])
         for i in range(len(suffix)):
             suffix1 = suffix[:i] + suffix[i + 1:]
             prefix1 = prefix + ' ' + suffix[i]
             for j in range(i + 1, len(suffix)):
-                self._generateWithLift(prefix + ' ' + suffix[i], suffix[j])
-                # self._generation(prefix+ ' ' +suffix[i], suffix[i+1:])
+                self._generateWithLeverage(prefix + ' ' + suffix[i], suffix[j])
             self._generation(prefix1, suffix1)
 
-    def _generateWithLift(self, lhs, rhs)  -> float:
+    def _generateWithLeverage(self, lhs, rhs) -> float:
         """
         To find association rules satisfying user-specified minConf
-
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         :return: the association rule
         :rtype: float
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
             return 0
         minimum = self._frequentPatterns[s]
         conf_lhs = minimum / self._frequentPatterns[lhs]
         conf_rhs = minimum / self._frequentPatterns[rhs]
-        lift_lhs = conf_lhs / self._frequentPatterns[rhs] * self._frequentPatterns[lhs]
-        right_rhs = conf_rhs / self._frequentPatterns[lhs] * self._frequentPatterns[rhs]
+        lift_lhs = conf_lhs - self._frequentPatterns[rhs] * self._frequentPatterns[lhs]
+        right_rhs = conf_rhs - self._frequentPatterns[lhs] * self._frequentPatterns[rhs]
         if lift_lhs >= self._minConf:
             s1 = lhs + '->' + rhs
             self._finalPatterns[s1] = conf_lhs
         if right_rhs >= self._minConf:
             s1 = rhs + '->' + lhs
             self._finalPatterns[s1] = conf_rhs
 
@@ -134,39 +137,37 @@
         """
         To generate the combinations all association rules.
         """
         for i in range(len(self._singleItems)):
             suffix = self._singleItems[:i] + self._singleItems[i + 1:]
             prefix = self._singleItems[i]
             for j in range(i + 1, len(self._singleItems)):
-                self._generateWithLift(self._singleItems[i], self._singleItems[j])
+                conf = self._generateWithLeverage(self._singleItems[i], self._singleItems[j])
             self._generation(prefix, suffix)
 
 
-class ARWithLift:
+class ARWithLeverage:
     """
-    About this algorithm
-    ====================
-
-    :Description: Association Rules are derived from frequent patterns using "lift" metric.
+    :Description: Association Rules are derived from frequent patterns using "leverage" metric.
 
     :Reference:
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of association rules
     :param  oFile: str :
                    Name of the output file to store complete set of association rules
     :param  minConf: float :
                    The user can specify the minConf in float between the range of 0 to 1.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
+        
         
     :Attributes:
 
+
         startTime : float
             To record the start time of the mining process
 
         endTime : float
             To record the completion time of the mining process
 
         finalPatterns : dict
@@ -175,39 +176,37 @@
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
-    Execution methods
-    =================
-
-    **Terminal command**
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ARWithLift.py <inputFile> <outputFile> <minConf> <sep>
+      (.venv) $ python3 ARWithLeverage.py <inputFile> <outputFile> <minConf> <sep>
 
       Example Usage:
 
-      (.venv) $ python3 ARWithLift.py sampleDB.txt patterns.txt 0.5 ' '
+      (.venv) $ python3 ARWithLeverage.py sampleDB.txt patterns.txt 10.0 ' '
 
-    .. note:: minConf can be specified in a value between 0 and 1.
-    
-    
-    **Calling from a python program**
+    .. note:: minConf will be considered only in 0 to 1.
 
+    
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
     .. code-block:: python
 
-            import PAMI.AssociationRules.basic import ARWithLift as alg
+            import PAMI.AssociationRules.basic import ARWithLeverage as alg
 
-            obj = alg.ARWithLift(iFile, minConf)
+            obj = alg.ARWithLeverage(iFile, minConf)
 
             obj.mine()
 
             associationRules = obj.getPatterns()
 
             print("Total number of Association Rules:", len(associationRules))
 
@@ -223,41 +222,40 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
             
-    Credits
-    =======
+    **Credits:**
+    --------------------
 
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     def __init__(self, iFile, minConf, sep) -> None:
         """
         :param iFile: input file name or path
         :type iFile: str
-        :param minConf: minimum confidence
+        :param minConf: The user can specify the minConf in float between the range of 0 to 1.
         :type minConf: float
-        :param sep: Delimiter of input file
+        :param sep: This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
         :type sep: str
         :return: None
         """
         self._iFile = iFile
         self._minConf = minConf
         self._finalPatterns = {}
         self._sep = sep
 
     def _readPatterns(self) -> list:
         """
-        Reading the input file and storing all the frequent patterns and their support respectively in a frequentPatterns variable.
-
-        :return: list of frequent patterns and their support respectively in a frequentPatterns
+        To read patterns  of leverage.
+        :return: List of patterns
         :rtype: list
         """
         self._frequentPatterns = {}
         k = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             pattern, sup = [], []
             if self._iFile.empty:
@@ -297,67 +295,74 @@
         return k
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Association rule mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        a = _Leverage(self._frequentPatterns, k, self._minConf)
+        a.run()
+        self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
 
     def mine(self) -> None:
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
         k = self._readPatterns()
-        a = Lift(self._frequentPatterns, k, self._minConf)
+        a = _Leverage(self._frequentPatterns, k, self._minConf)
         a.run()
         self._finalPatterns = a._finalPatterns
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -365,29 +370,27 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the outputfile
         :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
@@ -399,19 +402,21 @@
         print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ARWithLift(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
+            _ap = ARWithLeverage(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ARWithLift(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = ARWithLeverage(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

### Comparing `pami-2024.4.24.1/PAMI/AssociationRules/basic/RuleMiner.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/RuleMiner.py`

 * *Files 4% similar despite different names*

```diff
@@ -64,15 +64,14 @@
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
-
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             conf = self._generaeWithConfidence(prefix, suffix[0])
@@ -83,15 +82,14 @@
                 self._generaeWithConfidence(prefix + ' ' + suffix[i], suffix[j])
                 #self._generation(prefix+ ' ' +suffix[i], suffix[i+1:]) 
             self._generation(prefix1, suffix1)
             
     def _generaeWithConfidence(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
-
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -137,15 +135,14 @@
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
-
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             self._generateWithLift(prefix, suffix[0])
@@ -156,15 +153,14 @@
                 self._generateWithLift(prefix + ' ' + suffix[i], suffix[j])
                 #self._generation(prefix+ ' ' +suffix[i], suffix[i+1:]) 
             self._generation(prefix1, suffix1)
             
     def _generateWithLift(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
-
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -212,15 +208,14 @@
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
-
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             conf = self._generateWithLeverage(prefix, suffix[0])
@@ -230,15 +225,14 @@
             for j in range(i+1, len(suffix)):
                 self._generateWithLeverage(prefix + ' ' + suffix[i], suffix[j])
             self._generation(prefix1, suffix1)
             
     def _generateWithLeverage(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
-
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -264,17 +258,14 @@
             prefix = self._singleItems[i]
             for j in range(i+1, len(self._singleItems)):
                 conf = self._generateWithLeverage(self._singleItems[i], self._singleItems[j])
             self._generation(prefix, suffix)
 
 class RuleMiner:
     """
-    About this algorithm
-    ====================
-
     :Description: RuleMiner code is used to extract the association rules from given frequent patterns
 
     :Reference:
 
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of association rules
@@ -306,34 +297,32 @@
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
-    Execution methods
-    =================
-
-    **Terminal command**
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 RuleMiner.py <inputFile> <outputFile> <minConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 RuleMiner.py sampleDB.txt patterns.txt 0.5 ' '
 
-    .. note:: minConf can be specified in a value between 0 and 1.
-
+    .. note:: minConf will be considered only in 0 to 1.
 
-    **Calling from a python program**
 
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import RuleMiner as alg
 
             obj = alg.RuleMiner(iFile, measure, o.5, "\t")
 
             obj.mine()
@@ -356,15 +345,15 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     :Methods:
 
-            mine()
+            startMine()
     """
 
     def __init__(self, iFile, measure, threshold, sep):
         """
         :param iFile: input file name or path
         :type iFile: str
         :param measure: measure
@@ -425,15 +414,35 @@
         return k
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Association rule mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        k = self._readPatterns()
+        if self._measure == 'confidence':
+            a = Confidence(self._frequentPatterns, k, self._threshold)
+            a.run()
+            self._finalPatterns = a._finalPatterns
+        if self._measure == 'lift':
+            a = Lift(self._frequentPatterns, k, self._threshold)
+            a.run()
+            self._finalPatterns = a._finalPatterns
+        if self._measure == 'leverage':
+            a = Leverage(self._frequentPatterns, k, self._threshold)
+            a.run()
+            self._finalPatterns = a._finalPatterns
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Association rules successfully  generated from frequent patterns ")
 
 
     def mine(self):
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
@@ -457,45 +466,41 @@
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
     
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -503,28 +508,26 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to a output file
-
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
@@ -540,15 +543,14 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = RuleMiner(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = RuleMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         _ap = RuleMiner('sensorOutput.txt', "lift", 0.5, '\t')
```

### Comparing `pami-2024.4.24.1/PAMI/AssociationRules/basic/abstract.py` & `pami-2024.4.9.1/PAMI/AssociationRules/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/correlatedPattern/__init__.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/correlatedPattern/basic/CoMine.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMine.py`

 * *Files 10% similar despite different names*

```diff
@@ -87,18 +87,16 @@
         self.counter = 1
         self.parent = None
         self.child = []
         self.nodeLink = None
 
     def getChild(self, id1) -> Union[None, '_Node']:
         """
-        :param id1: give item id as input
-        :type id1: int
-        :return: the node with same itemId
-        :rtype: _Node
+        Param id1: give item id as input
+        type id1:
         """
         for i in self.child:
             if i.itemId == id1:
                 return i
         return None
 
 
@@ -136,16 +134,15 @@
         self.headerList = []
         self.mapItemNodes = {}
         self.mapItemLastNodes = {}
         self.root = _Node()
 
     def addTransaction(self, transaction: List[int]) -> None:
         """
-        Adding transaction into tree
-
+        adding transaction into tree
         :param transaction : it represents a single transaction in a database
         :type transaction : list
         :return: None
         """
 
         current = self.root
         for i in transaction:
@@ -160,32 +157,31 @@
             else:
                 child.counter += 1
                 current = child
 
     def fixNodeLinks(self, item: int, newNode: '_Node') -> None:
         """
         Fixing node link for the newNode that inserted into correlatedPatternTree
-
         :param item: it represents the item of newNode
         :type item : int
         :param newNode : it represents the newNode that inserted in correlatedPatternTree
         :type newNode : Node
         :return: None
+
         """
         if item in self.mapItemLastNodes.keys():
             lastNode = self.mapItemLastNodes[item]
             lastNode.nodeLink = newNode
         self.mapItemLastNodes[item] = newNode
         if item not in self.mapItemNodes.keys():
             self.mapItemNodes[item] = newNode
 
     def printTree(self, root: '_Node') -> None:
         """
         This method is to find the details of parent, children, and support of a Node
-
         :param root: it represents the Node in correlatedPatternTree
         :type root: Node
         :return: None
         """
 
         if root.child is None:
             return
@@ -193,15 +189,14 @@
             for i in root.child:
                 print(i.itemId, i.counter, i.parent.itemId)
                 self.printTree(i)
 
     def createHeaderList(self, mapSupport: Dict[int, int], minSup: int) -> None:
         """
         To create the headerList
-
         :param mapSupport : it represents the items with their supports
         :type mapSupport : dictionary
         :param minSup : it represents the minSup
         :param minSup : float
         :return: None
         """
         
@@ -211,15 +206,14 @@
                 t1.append(x)
         itemSetBuffer = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
         self.headerList = [i for i in t1 if i in itemSetBuffer]
 
     def addPrefixPath(self, prefix: List['_Node'], mapSupportBeta, minSup) -> None:
         """
         To construct the conditional tree with prefix paths of a node in correlatedPatternTree
-
         :param prefix : it represents the prefix items of a Node
         :type prefix : list
         :param mapSupportBeta : it represents the items with their supports
         :param mapSupportBeta : dictionary
         :param minSup : to check the item meets with minSup
         :param minSup : float
         :return: None
@@ -242,29 +236,27 @@
                 else:
                     child.counter += pathCount
                     current = child
 
 
 class CoMine(_ab._correlatedPatterns):
     """
-    About this algorithm
-    ====================
-
     :Description: CoMine is one of the fundamental algorithm to discover correlated  patterns in a transactional database. It is based on the traditional FP-Growth algorithm. This algorithm uses depth-first search technique to find all correlated patterns in a transactional database.
 
     :Reference: Lee, Y.K., Kim, W.Y., Cao, D., Han, J. (2003). CoMine: efficient mining of correlated patterns. In ICDM (pp. 581584).
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of correlated patterns
     :param  oFile: str :
                    Name of the output file to store complete set of correlated patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
     :param minAllConf: float :
                     The user can specify minAllConf values within the range (0, 1).
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
@@ -291,33 +283,31 @@
         finalPatterns : dict
             it represents to store the patterns
         itemSetBuffer : list
             it represents the store the items in mining
         maxPatternLength : int
            it represents the constraint for pattern length
 
-    Execution methods
-    =================
-
-    **Terminal command**
+    **Methods to execute code on terminal**
+    ------------------------------------------
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 CoMine.py <inputFile> <outputFile> <minSup> <minAllConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 CoMine.py sampleTDB.txt output.txt 0.25 0.2
 
-    .. note:: minSup can be specified in support count or a value between 0 and 1.
-
-    **Calling from a python program**
+    .. note:: minSup will be considered in percentage of database transactions
 
+    **Importing this algorithm into a python program**
+    --------------------------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.correlatedPattern.basic import CoMine as alg
 
             obj = alg.CoMine(iFile, minSup, minAllConf,sep)
 
             obj.mine()
@@ -338,17 +328,16 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    Credits
-    =======
-
+    **Credits:**
+    ----------------------------------------
              The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _minSup = float()
@@ -411,15 +400,14 @@
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _getRatio(self, prefix: List[int], prefixLength: int, s: int) -> float:
         """
         A Function to get itemSet Ratio
-
         :param prefix:the path
         :type prefix: list
         :param prefixLength: length
         :type prefixLength:int
         :s :current ratio
         :type s:float
         :return: minAllConf of prefix
@@ -443,38 +431,35 @@
                     self._mapSupport[j] = 1
                 else:
                     self._mapSupport[j] += 1
 
     def _saveItemSet(self, prefix, prefixLength, support) -> None:
         """
         To save the correlated patterns mined form correlatedPatternTree
-
         :param prefix: the correlated pattern
         :type prefix: list
         :param prefixLength : the length of a correlated pattern
         :type prefixLength : int
         :param support: the support of a pattern
         :type support :  int
         :return: None
-
-        The correlated patterns were stored in a global variable finalPatterns
+        :The correlated patterns were stored in a global variable finalPatterns
         """
         all_conf = self._getRatio(prefix, prefixLength, support)
         if all_conf < self._minAllConf:
             return
         l = []
         for i in range(prefixLength):
             l.append(prefix[i])
         self._itemSetCount += 1
         self._finalPatterns[tuple(l)] = [support, all_conf]
     
     def _convert(self, value: Union[int, float, str]) -> None:
         """
         To convert the type of user specified minSup value
-
         :param value: user specified minSup value
         :return: None
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
@@ -485,40 +470,39 @@
             else:
                 value = int(value)
         return value
 
     def _saveAllCombinations(self, tempBuffer, s, position, prefix, prefixLength) -> None:
         """
         Generating all the combinations for items in single branch in correlatedPatternTree
-
         :param tempBuffer: items in a single branch
         :type tempBuffer: list
         :param s : support at leaf node of a branch
         :param position : the length of a tempBuffer
         :type position : int
         :param prefix : it represents the list of leaf node
         :type prefix : list
         :param prefixLength : the length of prefix
         :type prefixLength :int
         :return: None
+        
         """
         max1 = 1 << position
         for i in range(1, max1):
             newPrefixLength = prefixLength
             for j in range(position):
                 isSet = i & (1 << j)
                 if isSet > 0:
                     prefix.insert(newPrefixLength, tempBuffer[j].itemId)
                     newPrefixLength += 1
             self._saveItemSet(prefix, newPrefixLength, s)
 
     def _correlatedPatternGrowthGenerate(self, correlatedPatternTree, prefix, prefixLength, mapSupport) -> None:
         """
         Mining the fp tree
-
         :param correlatedPatternTree: it represents the correlatedPatternTree
         :type correlatedPatternTree: class Tree
         :param prefix : it represents an empty list and store the patterns that are mined
         :type prefix : list
         :param prefixLength : the length of prefix
         :type prefixLength :int
         :param mapSupport : it represents the support of item
@@ -579,15 +563,42 @@
                         self._correlatedPatternGrowthGenerate(treeBeta, prefix, prefixLength + 1, mapSupportBeta)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main method to start
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._tree = _Tree()
+        self._finalPatterns = {}
+        self._correlatedOneItem()
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
+        _itemSetBuffer = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        for i in self._Database:
+            _transaction = []
+            for j in i:
+                if j in _itemSetBuffer:
+                    _transaction.append(j)
+            _transaction.sort(key=lambda val: self._mapSupport[val], reverse=True)
+            self._tree.addTransaction(_transaction)
+        self._tree.createHeaderList(self._mapSupport, self._minSup)
+        if len(self._tree.headerList) > 0:
+            self._itemSetBuffer = []
+            self._correlatedPatternGrowthGenerate(self._tree, self._itemSetBuffer, 0, self._mapSupport)
+        print("Correlated patterns were generated successfully using CoMine algorithm")
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
         main method to start
         """
         self._startTime = _ab._time.time()
         if self._iFile is None:
@@ -617,45 +628,41 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _pd.DataFrame:
         """
         Storing final correlated patterns in a dataframe
-
         :return: returning correlated patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -665,15 +672,14 @@
             data.append([pat, b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Confidence'])
         return dataframe
 
     def save(self, outFile) -> None:
         """
         Complete set of correlated patterns will be saved into an output file
-
         :param outFile: name of the outputfile
         :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
@@ -682,24 +688,22 @@
                 pat += str(i) + "\t"
             patternsAndSupport = pat.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self) -> Dict[Tuple[int], List[Union[int, float]]]:
         """
         Function to send the set of correlated patterns after completion of the mining process
-
         :return: returning correlated patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         function to print the result after completing the process
-
         :return: None
         """
         print("Total number of Correlated Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.4.24.1/PAMI/correlatedPattern/basic/CoMinePlus.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMinePlus.py`

 * *Files 6% similar despite different names*

```diff
@@ -88,15 +88,14 @@
         self.parent = None
         self.child = []
         self.nodeLink = None
 
     def getChild(self, itemName: int) -> Union['_Node', None]:
         """
         Retrieving the child from the tree
-
         :param itemName: name of the child
         :type itemName: list
         :return: returns the node with same itemName from correlatedPatternTree
         :rtype: list
         """
         for i in self.child:
             if i.itemId == itemName:
@@ -138,15 +137,14 @@
         self.mapItemNodes = {}
         self.mapItemLastNodes = {}
         self.root = _Node()
 
     def addTransaction(self, transaction: List[int]) -> None:
         """
         Adding a transaction into a tree
-
         :param transaction: it represents a transaction in a database
         :type transaction: list
         :return: None
         """
 
         # This method taken a transaction as input and returns the tree
         current = self.root
@@ -162,73 +160,73 @@
             else:
                 child.counter += 1
                 current = child
 
     def fixNodeLinks(self, item: int, newNode: _Node) -> None:
         """
         Fixing node link for the newNode that inserted into correlatedPatternTree
-
         :param item: it represents the item of newNode
         :type item: int
         :param newNode: it represents the newNode that inserted in correlatedPatternTree
         :type newNode: Node
         :return: None
+
         """
         if item in self.mapItemLastNodes.keys():
             lastNode = self.mapItemLastNodes[item]
             lastNode.nodeLink = newNode
         self.mapItemLastNodes[item] = newNode
         if item not in self.mapItemNodes.keys():
             self.mapItemNodes[item] = newNode
 
     def printTree(self, root: _Node) -> None:
         """
         Print the details of Node in correlatedPatternTree
-
         :param root: it represents the Node in correlatedPatternTree
         :type root: Node
         :return: None
+
         """
         # this method is used print the details of tree
         if not root.child:
             return
         else:
             for i in root.child:
                 print(i.itemId, i.counter, i.parent.itemId)
                 self.printTree(i)
 
     def createHeaderList(self, mapSupport: Dict[int, int], minSup: int) -> None:
         """
         To create the headerList
-
         :param mapSupport: it represents the items with their supports
         :type mapSupport: dictionary
         :param minSup: it represents the minSup
         :param minSup: float
         :return: None
+
         """
         # the correlatedPatternTree always maintains the header table to start the mining from leaf nodes
         t1 = []
         for x, y in mapSupport.items():
             if y >= minSup:
                 t1.append(x)
         itemSetBuffer = [k for k, v in sorted(mapSupport.items(), key=lambda val: val[1], reverse=True)]
         self.headerList = [i for i in t1 if i in itemSetBuffer]
 
     def addPrefixPath(self, prefix: List[_Node], mapSupportBeta: Dict[int, int], minSup: int) -> None:
         """
         To construct the conditional tree with prefix paths of a node in correlatedPatternTree
-
         :param prefix: it represents the prefix items of a Node
         :type prefix: list
         :param mapSupportBeta: it represents the items with their supports
         :param mapSupportBeta: dictionary
         :param minSup: to check the item meets with minSup
         :param minSup: float
         :return: None
+
         """
         # this method is used to add prefix paths in conditional trees of correlatedPatternTree
         pathCount = prefix[0].counter
         current = self.root
         prefix.reverse()
         for i in range(0, len(prefix) - 1):
             pathItem = prefix[i]
@@ -244,18 +242,15 @@
                     self.fixNodeLinks(pathItem.itemId, newNode)
                 else:
                     child.counter += pathCount
                     current = child
 
 
 class CoMinePlus(_ab._correlatedPatterns):
-    """
-    About this algorithm
-    ====================
-
+    """ 
     :Description: CoMinePlus is one of the efficient algorithm to discover correlated patterns in a transactional database. Using Item Support Intervals technique which is generating correlated patterns of higher order by combining only with items that have support within specified interval.
 
     :Reference:
         Uday Kiran R., Kitsuregawa M. (2012) Efficient Discovery of Correlated Patterns in Transactional Databases Using Items Support Intervals.
         In: Liddle S.W., Schewe KD., Tjoa A.M., Zhou X. (eds) Database and Expert Systems Applications. DEXA 2012. Lecture Notes in Computer Science, vol 7446. Springer, Berlin, Heidelberg.
         https://doi.org/10.1007/978-3-642-32600-4_18
 
@@ -299,33 +294,32 @@
         finalPatterns : dict
             it represents to store the patterns
         itemSetBuffer : list
             it represents the store the items in mining
         maxPatternLength : int
            it represents the constraint for pattern length
 
-    Execution methods
-    =================
-
-    **Terminal command**
+    **Methods to execute code on terminal**
+    ----------------------------------------------
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 CoMinePlus.py <inputFile> <outputFile> <minSup> <minAllConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 CoMinePlus.py sampleTDB.txt patterns.txt 0.4 0.5 ','
 
-    .. note:: minSup can be specified in support count or a value between 0 and 1.
+    .. note:: minSup will be considered in percentage of database transactions
 
-    **Calling from a python program**
 
+    **Importing this algorithm into a python program**
+    -----------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.correlatedPattern.basic import CoMinePlus as alg
 
             obj = alg.CoMinePlus(iFile, minSup, minAllConf, sep)
 
             obj.mine()
@@ -347,16 +341,16 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    Credits
-    =======
+    **Credits:**
+    -------------
 
              The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
         """
 
     _startTime = float()
     _endTime = float()
@@ -376,15 +370,14 @@
     _itemSetCount = 0
     _maxPatternLength = 1000
     _sep = "\t"
 
     def __init__(self, iFile: Union[str, _pd.DataFrame], minSup: Union[int, float, str], minAllConf: str, sep: str="\t") -> None:
         """
         param iFile: input file name
-
         type iFile: str or DataFrame or url
         param minSup: user-specified minimum support
         type minSup: int or float
         param minAllConf: user-specified minimum all confidence
         type minAllConf: float
         param sep: delimiter of input file
         type sep : str
@@ -434,15 +427,14 @@
                     self._mapSupport[j] = 1
                 else:
                     self._mapSupport[j] += 1
 
     def _saveItemSet(self, prefix: List[_Node], prefixLength: int, support: int, ratio: float) -> None:
         """
         To save the correlated patterns mined form correlatedPatternTree
-
         :param prefix: the correlated pattern
         :type prefix: list
         :param prefixLength: the length of a correlated pattern
         :type prefixLength: int
         :param support: the support of a pattern
         :type support:  int
         :param ratio: float
@@ -454,15 +446,14 @@
             sample.append(prefix[i])
         self._itemSetCount += 1
         self._finalPatterns[tuple(sample)] = [support, ratio]
 
     def _saveAllCombinations(self, tempBuffer: List[_Node], s: int, position: int, prefix: List[_Node], prefixLength: int) -> None:
         """
         Generating all the combinations for items in single branch in correlatedPatternTree
-
         :param tempBuffer: items in a single branch
         :type tempBuffer: list
         :param s: support at leaf node of a branch
         :param position: the length of a tempBuffer
         :type position: int
         :param prefix: it represents the list of leaf node
         :type prefix: list
@@ -481,15 +472,14 @@
             ratio = s/self._mapSupport[self._getMaxItem(prefix, newPrefixLength)]
             if ratio >= self._minAllConf:
                 self._saveItemSet(prefix, newPrefixLength, s, ratio)
 
     def _correlatedPatternGrowthGenerate(self, correlatedPatternTree: _Tree, prefix: List[_Node], prefixLength: int, mapSupport: Dict[int, int], minConf: float)  -> None:
         """
         Mining the fp tree
-
         :param correlatedPatternTree: it represents the correlatedPatternTree
         :type correlatedPatternTree: class Tree
         :param prefix: it represents an empty list and store the patterns that are mined
         :type prefix: list
         :param param prefixLength: the length of prefix
         :type prefixLength: int
         :param mapSupport : it represents the support of item
@@ -560,15 +550,14 @@
                     if len(treeBeta.root.child) > 0:
                         treeBeta.createHeaderList(mapSupportBeta, self._minSup)
                         self._correlatedPatternGrowthGenerate(treeBeta, prefix, prefixLength + 1, mapSupportBeta, minConf)
 
     def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
         To convert the type of user specified minSup value
-
         :param value: user specified minSup value
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
@@ -580,15 +569,45 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main program to start the operation
         """
-        self.mine()
+
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._tree = _Tree()
+        self._minSup = self._convert(self._minSup)
+        self._correlatedOneItem()
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
+        _itemSetBuffer = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        for i in self._Database:
+            _transaction = []
+            for j in i:
+                if j in _itemSetBuffer:
+                    _transaction.append(j)
+            _transaction.sort(key=lambda val: self._mapSupport[val], reverse=True)
+            self._tree.addTransaction(_transaction)
+        self._tree.createHeaderList(self._mapSupport, self._minSup)
+        if len(self._tree.headerList) > 0:
+            self._itemSetBuffer = []
+            self._correlatedPatternGrowthGenerate(self._tree, self._itemSetBuffer, 0, self._mapSupport, self._minAllConf)
+        print("Correlated Frequent patterns were generated successfully using CorrelatedPatternGrowth algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
         Main program to start the operation
         """
 
         self._startTime = _ab._time.time()
@@ -621,25 +640,23 @@
         self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def _getMaxItem(self, prefix: List[_Node], prefixLength: int) -> int:
@@ -648,25 +665,23 @@
             if self._mapSupport[maxItem] < self._mapSupport[prefix[i]]:
                 maxItem = prefix[i]
         return maxItem
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _pd.DataFrame:
         """
         Storing final correlated patterns in a dataframe
-
         :return: returning correlated patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -676,32 +691,30 @@
             data.append([pat, b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Confidence'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
         Complete set of correlated patterns will be loaded in to an output file
-
-        :param outFile: name of the output file
-        :type outFile: csv file
+        :param outFile: name of the outputfile
+        :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             pattern = str()
             for i in x:
                 pattern = pattern + i + "\t"
             s1 = str(pattern.strip()) + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> Dict[Tuple[str], List[Union[int, float]]]:
         """
         Function to send the set of correlated patterns after completion of the mining process
-
         :return: returning correlated patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
```

### Comparing `pami-2024.4.24.1/PAMI/correlatedPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/correlatedPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/correlatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/coveragePattern/basic/CMine.py` & `pami-2024.4.9.1/PAMI/coveragePattern/basic/CMine.py`

 * *Files 4% similar despite different names*

```diff
@@ -58,16 +58,14 @@
 
 from PAMI.coveragePattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
 class CMine(_ab._coveragePatterns):
     """
-    About this algorithm
-    ====================
 
     :Description:  CMine algorithms aims to discover the coverage patterns in transactional databases.
 
     :Reference:    Bhargav Sripada, Polepalli Krishna Reddy, Rage Uday Kiran:
                    Coverage patterns for efficient banner advertisement placement. WWW (Companion Volume) 2011: 131-132
                    __https://dl.acm.org/doi/10.1145/1963192.1963259
 
@@ -102,33 +100,32 @@
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
         Database : list
             To store the transactions of a database in list
 
 
-    Execution methods
-    =================
-
-    **Terminal command**
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 CMine.py <inputFile> <outputFile> <minRF> <minCS> <maxOR> <'\t'>
 
       Example Usage:
 
       (.venv) $ python3 CMine.py sampleTDB.txt patterns.txt 0.4 0.7 0.5 '\t'
 
     .. note: At the fixed minCS value, it can also be observed that the number of patterns increases as maxOR value increases.
 
-    **Calling from a python program**
 
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.coveragePattern.basic import CMine as alg
 
             obj = alg.CMine(iFile, minRF, minCS, maxOR, seperator)
 
             obj.mine()
@@ -150,17 +147,16 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    Credits
-    =======
-
+    **Credits:**
+    --------------------------
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
@@ -176,15 +172,14 @@
     _mapSupport = {}
     _lno = 0
 
 
     def _convert(self, value) -> Union[int, float]:
         """
         To convert the user specified minSup value
-
         :param value: user specified minSup value
         :return: converted type
         :rtype: Union[int, float]
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
@@ -194,15 +189,15 @@
                 value = float(value)
             else:
                 value = int(value)
         return value
 
     def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+            Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
         self._mapSupport = {}
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -228,15 +223,14 @@
                             self._Database.append(splitter)
                 except IOError:
                     print("File Not Found")
 
     def creatingCoverageItems(self) -> Dict[str, List[str]]:
         """
         This function creates coverage items from _database.
-
         :return: coverageTidData that stores coverage items and their tid list.
         :rtype: dict
         """
         tidData = {}
         self._lno = 0
         for transaction in self._Database:
             self._lno = self._lno + 1
@@ -248,15 +242,14 @@
         coverageTidData = {k: v for k, v in tidData.items() if len(v) / len(self._Database) >= self._minRF}
         coverageTidData = dict(sorted(coverageTidData.items(), reverse=True, key=lambda x: len(x[1])))
         return coverageTidData
 
     def tidToBitset(self,item_set: Dict[str, int]) -> Dict[str, int]:
         """
         This function converts tid list to bitset.
-
         :param item_set:
         :return: Dictionary
         :rtype: dict
         """
         bitset = {}
 
         for k,v in item_set.items():
@@ -267,15 +260,14 @@
                 bitset[k] = (bitset[k] << diff) | 0b1
             bitset[k] = (bitset[k] << (self._lno - int(v[i])))
         return bitset
 
     def genPatterns(self,prefix: Tuple[str, int],tidData: List[Tuple[str, int]]) -> None:
         """
         This function generate coverage pattern about prefix.
-
         :param prefix: String
         :param tidData: list
         :return: None
         """
         # variables to store coverage item set and
         item_set = prefix[0]
 
@@ -291,29 +283,46 @@
                 if orCount / len(self._Database) >= self._minRF:
                     self._finalPatterns[coverageItem_set] = andCount
                 self.genPatterns((coverageItem_set,tid),tidData[i+1:length])
 
     def generateAllPatterns(self,coverageItems: Dict[str, int]) -> None:
         """
         This function generates all coverage patterns.
-
         :param coverageItems: coverage items
         :return: None
         """
         tidData = list(coverageItems.items())
         length = len(tidData)
         for i in range(length):
             #print(i,tidData[i][0])
             self.genPatterns(tidData[i],tidData[i+1:length])
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ Main method to start """
 
-        self.mine()
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minCS = self._convert(self._minCS)
+        self._minRF =  self._convert(self._minRF)
+        self._maxOR = self._convert(self._maxOR)
+        coverageItems = self.creatingCoverageItems()
+        self._finalPatterns = {k: len(v) for k, v in coverageItems.items()}
+        coverageItemsBitset = self.tidToBitset(coverageItems)
+        self.generateAllPatterns(coverageItemsBitset)
+        self.save('output.txt')
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Coverage patterns were generated successfully using CMine  algorithm")
 
     def mine(self) -> None:
         """ Main method to start """
 
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
@@ -341,66 +350,56 @@
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
-
+        """Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
-        """
-        Storing final coverage patterns in a dataframe
-
+        """Storing final coverage patterns in a dataframe
         :return: returning coverage patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
-        """
-        Complete set of coverage patterns will be loaded in to an output file
-
+        """Complete set of coverage patterns will be loaded in to an output file
         :param outFile: name of the outputfile
         :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self) -> Dict[str, int]:
-        """
-        Function to send the set of coverage patterns after completion of the mining process
-
+        """ Function to send the set of coverage patterns after completion of the mining process
         :return: returning coverage patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
```

### Comparing `pami-2024.4.24.1/PAMI/coveragePattern/basic/CPPG.py` & `pami-2024.4.9.1/PAMI/coveragePattern/basic/CPPG.py`

 * *Files 4% similar despite different names*

```diff
@@ -352,15 +352,49 @@
                 value = int(value)
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ Mining process will start from this function
         """
-        self.mine()
+
+        #global _minSup, _maxPer, _lno
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minRF is None:
+            raise Exception("Please enter the Relative Frequency")
+        if self._maxOR is None:
+            raise Exception("Please enter the Overlap Ratio")
+        if self._minCS is None:
+            raise Exception("Please enter the Coverage Ratio")
+        self._creatingItemSets()
+        self._minRF = self._convert(self._minRF)
+        self._maxOR = self._convert(self._maxOR)
+        self._minCS = self._convert(self._minCS)
+        if self._minRF > len(self._Database) or self._minCS > len(self._Database) or self._maxOR > len(self._Database):
+            raise Exception("Please enter the constraints in range between 0 to 1")
+        generatedItems, pfList = self._coverageOneItem()
+        self._finalPatterns = {k: v for k, v in generatedItems.items()}
+        updatedDatabases = self._updateDatabases(pfList)
+        proData = self._buildProjectedDatabase(updatedDatabases, pfList)
+        for x, y in proData.items():
+            uniqueItems = [x]
+            for i in y:
+                for j in i:
+                    if j not in uniqueItems:
+                        uniqueItems.append(j)
+            self._generateFrequentPatterns(uniqueItems)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Coverage patterns were generated successfully using CPPG algorithm ")
 
     def mine(self) -> None:
         """ Mining process will start from this function
         """
 
         #global _minSup, _maxPer, _lno
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/coveragePattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/coveragePattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/DF2DB.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DB.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/DF2DBPlus.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/DenseFormatDF.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/DenseFormatDF.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/SparseFormatDF.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/SparseFormatDF.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/createTDB.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/createTDB.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/denseDF2DBPlus.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/denseDF2DB_dump.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DB_dump.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py` & `pami-2024.4.9.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/calculateMISValues/usingBeta.py` & `pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingBeta.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/calculateMISValues/usingSD.py` & `pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingSD.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/convertMultiTSIntoFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/convertMultiTSIntoFuzzy.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,22 +5,19 @@
 #
 #     from PAMI.extras.syntheticDataGenerator import convertMultiTSIntoFuzzy as fuz
 #
 #     obj = fuz.convertMultiTSIntoFuzzy(iFile, FuzFile)
 #
 #     obj.save()
 #
-#     obj.mine()
+#     obj.startMine()
 #
 
-
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -52,15 +49,16 @@
 
                 from PAMI.extras.syntheticDataGenerator import convertMultiTSIntoFuzzy as fuz
 
                 obj = fuz.convertMultiTSIntoFuzzy(iFile, FuzFile)
 
                 obj.save()
 
-                obj.mine()
+                obj.startMine()
+
 
     """
 
 
     def __init__(self, iFile: str,  FuzFile: str) -> None:
         #super().__init__(iFile, nFile, FuzFile, minSup, maxPer, sep)
         self._iFile = iFile
@@ -138,15 +136,15 @@
                                 (self._RegionsCal[i][1] - quantity) / base)
                         else:
                             self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
                                 (quantity - self._RegionsCal[i][0]) / base)
             return
        
     def save(self, outputFile: str) -> None:
-        self.mine()
+        self.startMine()
         writer = open(outputFile, 'w+')
         for line in range(len(self._transactionsDB)):
             item_list = self._transactionsDB[line]
             fuzzyValues_list = self._fuzzyValuesDB[line]
             times = self._timeEvents[line]
             s = str()
             s2 = str()
@@ -165,32 +163,22 @@
                     s = s +  item + '.' + self._LabelKeyOne[k]  + '\t'
                     ss = ss + str(round(self.list[k], 2))+ '\t'
             s2 = s1 + ':' + s + ':' + ss
             # print(s2)
             # break
             writer.write("%s\n" %s2)
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+
     def startMine(self) -> None:
-        """
-        Frequent pattern mining process will start from here
+        """ Frequent pattern mining process will start from here
         """
         
         self._creatingItemSets()
         self._fuzzyMembershipFunc()
         self._finalPatterns = {}
-
-    def mine(self) -> None:
-        """
-        Frequent pattern mining process will start from here
-        """
-
-        self._creatingItemSets()
-        self._fuzzyMembershipFunc()
-        self._finalPatterns = {}
         
 if __name__ == "__main__":
     convertMultipleTSIntoFuzzy(sys.argv[1], sys.argv[2])
```

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/FuzzyDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/FuzzyDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/SequentialDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/SequentialDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/TemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/TemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/TransactionalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/TransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/dbStats/UtilityDatabase.py` & `pami-2024.4.9.1/PAMI/extras/dbStats/UtilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/abstract.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py` & `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/generateLatexGraphFile.py` & `pami-2024.4.9.1/PAMI/extras/generateLatexGraphFile.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,18 +7,16 @@
 #
 #     obj = fuz.generateLatexGraphFile(idf)
 #
 #     obj.save()
 #
 
 
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.24.1/PAMI/extras/graph/DF2Fig.py` & `pami-2024.4.9.1/PAMI/extras/graph/DF2Fig.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/graph/DF2Tex.py` & `pami-2024.4.9.1/PAMI/extras/graph/DF2Tex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/graph/plotLineGraphFromDictionary.py` & `pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphFromDictionary.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py` & `pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/graph/visualizeFuzzyPatterns.py` & `pami-2024.4.9.1/PAMI/extras/graph/visualizeFuzzyPatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/graph/visualizePatterns.py` & `pami-2024.4.9.1/PAMI/extras/graph/visualizePatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/imageProcessing/imagery2Databases.py` & `pami-2024.4.9.1/PAMI/extras/imageProcessing/imagery2Databases.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/messaging/discord.py` & `pami-2024.4.9.1/PAMI/extras/messaging/discord.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/messaging/gmail.py` & `pami-2024.4.9.1/PAMI/extras/messaging/gmail.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py` & `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,15 +50,15 @@
             This program find pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace
             and store the pairs.
         :param  seperator: str :
                     This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Methods:
 
-        mine()
+        startMine()
             find and store the pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace.
         getFileName()
             This function returns output file name.
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
```

### Comparing `pami-2024.4.24.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py` & `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py`

 * *Files 2% similar despite different names*

```diff
@@ -49,15 +49,15 @@
             This program find pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace
             and store the pairs.
         :param  seperator: str :
                     This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Methods:
 
-        mine()
+        startMine()
             find and store the pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace.
         getFileName()
             This function returns output file name.
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
```

### Comparing `pami-2024.4.24.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py` & `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py`

 * *Files 3% similar despite different names*

```diff
@@ -51,15 +51,15 @@
             and store the pairs.
         :param  seperator: str :
                     This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Methods:
 
-        mine()
+        startMine()
             find and store the pairs of values whose Geodesic distance is less than or equal to maxDistace.
         getFileName()
             This function returns output file name.
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
```

### Comparing `pami-2024.4.24.1/PAMI/extras/plotPointOnMap.py` & `pami-2024.4.9.1/PAMI/extras/plotPointOnMap.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,19 +6,16 @@
 #     from PAMI.extras.syntheticDataGenerator import plotPointOnMap as plt
 #
 #     obj = plt.plotPointOnMap(" ", 10, "\t")
 #
 #     obj.save()
 #
 
-
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.24.1/PAMI/extras/plotPointOnMap_dump.py` & `pami-2024.4.9.1/PAMI/extras/plotPointOnMap_dump.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,20 +5,17 @@
 #
 #     from PAMI.extras.syntheticDataGenerator import plotPointOnMap_dump as plt
 #
 #     obj = plt.plotPointOnMap_dump(" ", 10, "\t")
 #
 #     obj.save()
 #
-
-
-
-
+#
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.24.1/PAMI/extras/scatterPlotSpatialPoints.py` & `pami-2024.4.9.1/PAMI/extras/scatterPlotSpatialPoints.py`

 * *Files 5% similar despite different names*

```diff
@@ -6,19 +6,16 @@
 #   from PAMI.extras.syntheticDataGenerator import scatterPlotSpatialPoints as plt
 #
 #   obj = plt.scatterPlotSpatialPoints(iFile, "\t")
 #
 #   obj.save()
 #
 
-
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -50,14 +47,15 @@
 
                 from PAMI.extras.syntheticDataGenerator import scatterPlotSpatialPoints as plt
 
                 obj = plt.scatterPlotSpatialPoints(iFile, "\t" )
 
                 obj.save(oFile)
 
+
     """
 
     def __init__(self, iFile: str, sep: str = '\t') ->None:
 
         self._iFile = iFile
         self._sep = sep
```

### Comparing `pami-2024.4.24.1/PAMI/extras/stats/TransactionalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/stats/TransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/stats/graphDatabase.py` & `pami-2024.4.9.1/PAMI/extras/stats/graphDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/stats/sequentialDatabase.py` & `pami-2024.4.9.1/PAMI/extras/stats/sequentialDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/stats/temporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/stats/temporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/stats/utilityDatabase.py` & `pami-2024.4.9.1/PAMI/extras/stats/utilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,23 @@
-# TemporalDatabase is a collection of timestamps and along with data at particular time.
+# TemporalDatabase is a code used to create a synthetic temporal database.
 #
 #  **Importing this algorithm into a python program**
 #  --------------------------------------------------------
 #
 #             from PAMI.extras.syntheticDataGenerator import TemporalDatabase as db
 #
-#             temporalDB = db(numOfTransactions, avgTransactionLength, numItems, outFileName)
+#             obj = db.TemporalDatabase(100, 10, 6, oFile, %, "\t")
 #
-#             temporalDB.create(percentage)
+#             obj.save()
 #
+#             obj.getFileName("outputFileName") # to create a file
 #
-
+#             obj.getDatabaseAsDataFrame("outputFileName") # to convert database into dataframe
+#
+#             obj.createTemporalFile("outputFileName") # to get outputfile
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -25,98 +28,112 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
+from typing import Tuple, List, Union
 import pandas as pd
 import numpy as np
+import random
 import sys
-
+import os
 
 class TemporalDatabase:
     """
-    :Description: - creates a temporal database with required parameter (e.g.,numOfTransactions, avgLenOfTransactions, numItems and outputFile).
-                  - output can be printed in two ways either in text file or dataframe depending on the input type.
+    :Description:   generateTemporalDatabase creates a temporal database and outputs a database or a frame depending on input
 
     :Attributes:
-
         :param numOfTransactions: int
             number of transactions
-
         :param avgLenOfTransactions: int
             average length of transactions
-
         :param numItems: int
             number of items
-
         :param outputFile: str
-            the name the output file
-
+            output file name
         :param percentage: int
             percentage of coinToss for TID of temporalDatabase
-
         :param sep: str
             seperator for database output file
-
         :param typeOfFile: str
             specify database or dataframe to get corresponding output
 
     :Methods:
         getFileName():
             returns filename
-
         createTemporalFile():
             creates temporal database file or dataframe
-
         getDatabaseAsDataFrame:
             returns dataframe
-
         performCoinFlip():
             Perform a coin flip with the given probability
-
         tuning():
             Tune the arrayLength to match avgLenOfTransactions
-
         createTemporalFile():
             create Temporal database or dataframe depending on input
 
-    **Methods to execute code on terminal**
-    ---------------------------------------------
-
-    .. code-block:: console
-
-      Format:
-
-      (.venv) $ python3 TemporalDatabase.py <numOfTransactions> <avgLenOfTransactions> <numItems> <outputFile>
-
-      Example Usage:
-
-      (.venv) $ python3 TemporalDatabase.py 50.0 10.0 100 temporal.txt
-
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
 
-            from PAMI.extras.syntheticDataGenerator import TemporalDatabase as db
-
-            temporalDB = db(numOfTransactions, avgTransactionLength, numItems, outFileName)
-
-            temporalDB.create(percentage)
+            from PAMI.extras.generateDatabase import generateTemporalDatabase as db
 
+            numOfTransactions = 100
+            numItems = 15
+            avgTransactionLength = 6
+            outFileName = 'temporal_ot.txt'
+            sep = '\t'
+            percent = 75
+            frameOrBase = "dataframe" # if you want to get dataframe as output
+            frameOrBase = "database" # if you want to get database/csv/file as output
+
+            temporalDB = db.generateTemporalDatabase(numOfTransactions, avgTransactionLength, numItems, outFileName, percent, sep, frameOrBase )
+            temporalDB.createTemporalFile()
+            print(temporalDB.getDatabaseAsDataFrame())
 
     """
     def __init__(self, numOfTransactions: int, avgLenOfTransactions: int, 
                  numItems: int, outputFile: str, percentage: int=50,
                  sep: str='\t', typeOfFile: str="Database") -> None:
         
         """
-        Initialize the generateTemporalDatabase class with required parameters.
+        :Description:   Initialize the generateTemporalDatabase class
+
+        :Attributes:
+            :param numOfTransactions: int
+                number of transactions
+            :param avgLenOfTransactions: int
+                average length of transactions
+            :param numItems: int
+                number of items
+            :param outputFile: str
+                output file name
+            :param percentage: int
+                percentage of coinToss for TID of temporalDatabase
+            :param sep: str
+                seperator for database output file
+            :param typeOfFile: str
+                specify database or dataframe to get corresponding output
+
+        :Methods:
+            getFileName():
+                returns filename
+            createTemporalFile():
+                creates temporal database file or dataframe
+            getDatabaseAsDataFrame:
+                returns dataframe
+            performCoinFlip():
+                Perform a coin flip with the given probability
+            tuning():
+                Tune the arrayLength to match avgLenOfTransactions
+            createTemporalFile():
+                create Temporal database or dataframe depending on input
+        
         """
 
         self.numOfTransactions = numOfTransactions
         self.avgLenOfTransactions = avgLenOfTransactions
         self.numItems = numItems
         self.outputFile = outputFile
         if percentage > 1:
@@ -124,23 +141,22 @@
         else:
             self.percentage = percentage
         self.sep = sep
         self.typeOfFile = typeOfFile.lower()
 
     def getFileName(self) -> str:
         """
-        This function take the name of the outputfile.
-        :return: outputFile.
+        return filename
+        :return:
         """
         return self.outputFile
 
     def getDatabaseAsDataFrame(self) -> pd.DataFrame:
         """
-        This function return the database in dataframe format.
-
+        return dataframe
         return: pd.dataframe
         """
         return self.df
     
     def performCoinFlip(self, probability: float) -> bool:
         """Perform a coin flip with the given probability."""
         result = np.random.choice([0, 1], p=[1 - probability, probability])
@@ -148,22 +164,21 @@
 
 
     def tuning(self, array, sumRes) -> list:
         """
         Tune the array so that the sum of the values is equal to sumRes
 
         Parameters:
-        :param array: list of values randomly generated.
-        :type array: list
-        :param sumRes: target sum
-        :type sumRes: int
+        array: list - list of values
+        sumRes: int - target sum
 
         Returns:
         array: list - tuned array
         """
+
         # first generate a random array of length n whose values average to m
         values = np.random.randint(1, self.numItems, len(array))
 
         while np.sum(values) != sumRes:
             # get index of largest value
             # if sum is too large, decrease the largest value
             if np.sum(values) > sumRes:
@@ -179,16 +194,16 @@
         for i in range(len(array)):
             array[i][1] = values[i]
 
         return array
 
     def create(self) -> None:
         """
-        create Temporal database or dataframe depending on type of file specified.
-        :return: None
+        create Temporal database or dataframe depending on input
+        :return:
         """
 
         db = []
         lineSize = []
         for i in range(self.numOfTransactions):
             db.append([i])
             if self.performCoinFlip(self.percentage):
@@ -215,11 +230,26 @@
                 'timestamp': [line[0] for line in db],
                 'transactions': pd.Series([line[1:] for line in db])
             }
             self.df = pd.DataFrame(data)
 
         print("Temporal database created successfully")
 
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 if __name__ == '__main__':
 
     obj = TemporalDatabase(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
     obj.create(sys.argv[5])
```

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py` & `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/extras/topKPatterns.py` & `pami-2024.4.9.1/PAMI/extras/topKPatterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,19 +6,16 @@
 #     from PAMI.extras.syntheticDataGenerator import topKPatterns as tK
 #
 #     obj = tK.topKPatterns(" ", 10, "\t")
 #
 #     obj.save()
 #
 
-
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.24.1/PAMI/extras/uncertaindb_convert.py` & `pami-2024.4.9.1/PAMI/extras/uncertaindb_convert.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,18 +7,16 @@
 #
 #     obj = un.predictedClass2Transaction(predicted_classes, 0.8)
 #
 #     obj.save()
 #
 
 
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.24.1/PAMI/extras/visualize/graphs.py` & `pami-2024.4.9.1/PAMI/extras/visualize/graphs.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py` & `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py`

 * *Files 3% similar despite different names*

```diff
@@ -297,16 +297,31 @@
                     self._finalPatterns[tuple(j)] = res
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Fault-tolerant frequent pattern mining process will start from here
         """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._itemSup = self._convert(self._itemSup)
+        self._minLength = int(self._minLength)
+        self._faultTolerance = int(self._faultTolerance)
+        self._oneLengthFrequentItems()
 
-        self.mine()
+        self._getFaultPatterns()
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Fault-Tolerant Frequent patterns were generated successfully using FTApriori algorithm ")
 
     def mine(self) -> None:
         """
         Fault-tolerant frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py` & `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -305,15 +305,15 @@
         tree : class
             it represents the Tree class
         finalPatterns : dict
             it represents to store the patterns
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -551,15 +551,41 @@
         return temp
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main program to start the operation
         """
-        self.mine()
+        global _minSup
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        _minSup = self._minSup
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
         Main program to start the operation
         """
         global _minSup
         self.__startTime = _fp._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/frequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/Apriori.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATDiffset.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-# Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
-#
+# ECLATDiffest uses diffset to extract the frequent patterns in a transactional database.
+
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# ---------------------------------------------------------
 #
-#             import PAMI.frequentPattern.basic.Apriori as alg
+#             import PAMI.frequentPattern.basic.ECLATDiffset as alg
 #
-#             obj = alg.Apriori(iFile, minSup)
+#             obj = alg.ECLATDiffset(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save(oFile)
+#             obj.savePatterns(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -27,14 +27,18 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
+
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -44,91 +48,89 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+
+# from abstract import *
+
 from PAMI.frequentPattern.basic import abstract as _ab
-from typing import Dict, Union
 from deprecated import deprecated
 
 
-class Apriori(_ab._frequentPatterns):
+class ECLATDiffset(_ab._frequentPatterns):
     """
-    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
-
-    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-            In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+    :Description:   ECLATDiffset uses diffset to extract the frequent patterns in a transactional database.
 
+    :Reference:  KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
+            August 2003 Pages 326335 https://doi.org/10.1145/956750.956788
+            
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-
-
+    
     :Attributes:
-
+    
         startTime : float
           To record the start time of the mining process
 
         endTime : float
           To record the completion time of the mining process
 
         finalPatterns : dict
           Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
           To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
-
+        
         Database : list
           To store the transactions of a database in list
-
-
-
+          
+        
     **Methods to execute code on terminal**
-    ----------------------------------------------------
+    ------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 Apriori.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 ECLATDiffset.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 Apriori.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 ECLATDiffset.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
-
+    
     **Importing this algorithm into a python program**
-    -----------------------------------------------------
-
+    ---------------------------------------------------------
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.Apriori as alg
+            import PAMI.frequentPattern.basic.ECLATDiffset as alg
 
-            obj = alg.Apriori(iFile, minSup)
+            obj = alg.ECLATDiffset(iFile, minSup)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save(oFile)
+            obj.savePatterns(oFile)
 
             Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -138,266 +140,283 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    -------------
+    -------------------
 
-             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+               The complete program was written by Kundai under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _diffSets = {}
+    _trans_set = set()
 
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
-
-            for k in temp:
-                self._Database.append(set(k))
+                self._Database = self._iFile['Transactions'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    def _convert(self, value):
         """
         To convert the user specified minSup value
-
         :param value: user specified minSup value
-
-        :type value: int or float or str
-
         :return: converted type
-
-        :rtype: int or float
-
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated(
-        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
-        """
-        Frequent pattern mining process will start from here
+    def _getUniqueItemList(self):
+
+        # tidSets will store all the initial tids
+        tidSets = {}
+        # uniqueItem will store all frequent 1 items
+        uniqueItem = []
+        for line in self._Database:
+                transNum = 0
+                # Database = [set([i.rstrip() for i in transaction.split('\t')]) for transaction in f]
+                for transaction in self._Database:
+                    transNum += 1
+                    self._trans_set.add(transNum)
+                    for item in transaction:
+                        if item in tidSets:
+                            tidSets[item].add(transNum)
+                        else:
+                            tidSets[item] = {transNum}
+        for key, value in tidSets.items():
+            supp = len(value)
+            if supp >= self._minSup:
+                self._diffSets[key] = [supp, self._trans_set.difference(value)]
+                uniqueItem.append(key)
+        # for x, y in self._diffSets.items():
+        #     print(x, y)
+        uniqueItem.sort()
+        # print()
+        return uniqueItem
+
+    def _runDeclat(self, candidateList):
+        """
+        It will generate the combinations of frequent items
+        :param candidateList :it represents the items with their respective transaction identifiers
+        :type candidateList: list
+        :return: returning transaction dictionary
+        :rtype: dict
         """
-        self.mine()
 
-    def mine(self) -> None:
+        newList = []
+        for i in range(0, len(candidateList)):
+            item1 = candidateList[i]
+            iList = item1.split()
+            for j in range(i + 1, len(candidateList)):
+                item2 = candidateList[j]
+                jList = item2.split()
+                if iList[:-1] == jList[:-1]:
+                    unionDiffSet = self._diffSets[item2][1].difference(self._diffSets[item1][1])
+                    unionSup = self._diffSets[item1][0] - len(unionDiffSet)
+                    if unionSup >= self._minSup:
+                        newKey = item1 + "\t" + jList[-1]
+                        self._diffSets[newKey] = [unionSup, unionDiffSet]
+                        newList.append(newKey)
+                    else: 
+                        break
+
+        if len(newList) > 0:
+            self._runDeclat(newList)
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
 
+        self._startTime = _ab._time.time()
+        self._Database = []
+        self._finalPatterns = {}
+        self._diffSets = {}
+        self._trans_set = set()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-
+        #print(len(self._Database))
         self._minSup = self._convert(self._minSup)
+        uniqueItemList = []
+        uniqueItemList = self._getUniqueItemList()
+        self._runDeclat(uniqueItemList)
+        self._finalPatterns = self._diffSets
+        #print(len(self._finalPatterns), len(uniqueItemList))
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
 
-        items = {}
-        index = 0
-        for line in self._Database:
-            for item in line:
-                if tuple([item]) in items:
-                    items[tuple([item])].append(index)
-                else:
-                    items[tuple([item])] = [index]
-            index += 1
-
-        # sort by length in descending order
-        items = dict(sorted(items.items(), key=lambda x: len(x[1]), reverse=True))
-
-        cands = []
-        fileData = {}
-        for key in items:
-            if len(items[key]) >= self._minSup:
-                cands.append(key)
-                self._finalPatterns["\t".join(key)] = len(items[key])
-                fileData[key] = set(items[key])
-            else:
-                break
-
-        while cands:
-            newKeys = []
-            for i in range(len(cands)):
-                for j in range(i + 1, len(cands)):
-                    if cands[i][:-1] == cands[j][:-1]:
-                        newCand = cands[i] + tuple([cands[j][-1]])
-                        intersection = fileData[tuple([newCand[0]])]
-                        for k in range(1, len(newCand)):
-                            intersection = intersection.intersection(fileData[tuple([newCand[k]])])
-                        if len(intersection) >= self._minSup:
-                            newKeys.append(newCand)
-                            newCand = "\t".join(newCand)
-                            self._finalPatterns[newCand] = len(intersection)
-            del cands
-            cands = newKeys
-            del newKeys
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
 
-        process = _ab._psutil.Process(_ab._os.getpid())
+        self._startTime = _ab._time.time()
+        self._Database = []
+        self._finalPatterns = {}
+        self._diffSets = {}
+        self._trans_set = set()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        #print(len(self._Database))
+        self._minSup = self._convert(self._minSup)
+        uniqueItemList = []
+        uniqueItemList = self._getUniqueItemList()
+        self._runDeclat(uniqueItemList)
+        self._finalPatterns = self._diffSets
+        #print(len(self._finalPatterns), len(uniqueItemList))
         self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Apriori algorithm ")
+        print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
-
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
-
         :rtype: float
-
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
-
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
-
         :rtype: float
-
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
-
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
-
         :rtype: float
-
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
-
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
-
         :rtype: pd.DataFrame
-
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
+            data.append([a.replace('\t', ' '), b[0]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
-    def save(self, outFile) -> None:
+    def save(self, outFile):
         """
-
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
-
         :type outFile: csvfile
-
-        :return: None
-
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+            patternsAndSupport = x.strip() + ":" + str(y[0])
+            writer.write("%s \n" % patternsAndSupport)
 
-    def getPatterns(self) -> Dict[str, int]:
+    def getPatterns(self):
         """
-
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
-
         :rtype: dict
-
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
-        This function is used to print the result
+        This function is used to print the results
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = Apriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = Apriori(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ap._sys.argv[2])
+        _ap.save(_ab._sys.argv[2])
+        print(_ap.getPatternsAsDataFrame())
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
+        print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/ECLAT.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLAT.py`

 * *Files 4% similar despite different names*

```diff
@@ -276,15 +276,32 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Frequent pattern mining process will start from here
         """
 
-        self.mine()
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        uniqueItemList = self._getUniqueItemList()
+        self._generateFrequentPatterns(uniqueItemList)
+        for x, y in self._finalPatterns.items():
+            self._finalPatterns[x] = len(y[0])
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using ECLAT algorithm")
 
     def mine(self) -> None:
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/ECLATDiffset.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuApriori.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,23 +1,24 @@
-# ECLATDiffest uses diffset to extract the frequent patterns in a transactional database.
-
+# cuApriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+#
+#
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+# ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.basic.ECLATDiffset as alg
+#             import PAMI.frequentPattern.cuda.cuApriori as alg
 #
-#             obj = alg.ECLATDiffset(iFile, minSup)
+#             obj = alg.cuApriori(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.savePatterns(oFile)
+#             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -29,16 +30,14 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
-
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,269 +47,297 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
-# from abstract import *
-
-from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
+from PAMI.frequentPattern.cuda import abstract as _ab
+# import abstract as _ab
 
-
-class ECLATDiffset(_ab._frequentPatterns):
+class cuApriori(_ab._frequentPatterns):
     """
-    :Description:   ECLATDiffset uses diffset to extract the frequent patterns in a transactional database.
+    :Description: cuApriori is one of the fundamental algorithm to discover frequent patterns using Cuda in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+
+    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
+            In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
 
-    :Reference:  KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
-            August 2003 Pages 326335 https://doi.org/10.1145/956750.956788
-            
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+    :param  minSup: int :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    
+
     :Attributes:
-    
+
         startTime : float
           To record the start time of the mining process
 
         endTime : float
           To record the completion time of the mining process
 
         finalPatterns : dict
           Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
           To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
-        
+
         Database : list
           To store the transactions of a database in list
-          
-        
+
+
+
     **Methods to execute code on terminal**
-    ------------------------------------------
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ECLATDiffset.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cuApriori.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ECLATDiffset.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cuApriori.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
-    
+
     **Importing this algorithm into a python program**
-    ---------------------------------------------------------
+    ----------------------------------------------------
+
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.ECLATDiffset as alg
+             import PAMI.frequentPattern.cuda.cuApriori as alg
 
-            obj = alg.ECLATDiffset(iFile, minSup)
+             obj = alg.cuApriori(iFile, minSup)
 
-            obj.mine()
+             obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+             frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.savePatterns(oFile)
+             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+             Df = obj.getPatternInDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+             memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+             print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+             memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+             print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+             run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    -------------------
+    -------------
 
-               The complete program was written by Kundai under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
+    _ab._cp.cuda.Device(0).use()
+
     _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _diffSets = {}
-    _trans_set = set()
+
+    _sumKernel = _ab._cp.RawKernel(r'''
+
+    #define uint32_t unsigned int
+
+    extern "C" __global__
+
+    void sumKernel(uint32_t *d_a, uint32_t *sum, uint32_t numElements)
+    {
+        uint32_t i = blockDim.x * blockIdx.x + threadIdx.x;
+        if (i < numElements)
+        {  
+            atomicAdd(&sum[0], __popc(d_a[i]));
+        }
+        return;    
+    }
+
+    ''', 'sumKernel')
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                temp = self._iFile['Transactions'].tolist()
+
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
+
         To convert the user specified minSup value
+
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _getUniqueItemList(self):
+    def arraysAndItems(self):
+        ArraysAndItems = {}
 
-        # tidSets will store all the initial tids
-        tidSets = {}
-        # uniqueItem will store all frequent 1 items
-        uniqueItem = []
-        for line in self._Database:
-                transNum = 0
-                # Database = [set([i.rstrip() for i in transaction.split('\t')]) for transaction in f]
-                for transaction in self._Database:
-                    transNum += 1
-                    self._trans_set.add(transNum)
-                    for item in transaction:
-                        if item in tidSets:
-                            tidSets[item].add(transNum)
-                        else:
-                            tidSets[item] = {transNum}
-        for key, value in tidSets.items():
-            supp = len(value)
-            if supp >= self._minSup:
-                self._diffSets[key] = [supp, self._trans_set.difference(value)]
-                uniqueItem.append(key)
-        # for x, y in self._diffSets.items():
-        #     print(x, y)
-        uniqueItem.sort()
-        # print()
-        return uniqueItem
-
-    def _runDeclat(self, candidateList):
-        """
-        It will generate the combinations of frequent items
-        :param candidateList :it represents the items with their respective transaction identifiers
-        :type candidateList: list
-        :return: returning transaction dictionary
-        :rtype: dict
-        """
+        for i in range(len(self._Database)):
+            for j in self._Database[i]:
+                j = tuple([j])
+                if j not in ArraysAndItems:
+                    ArraysAndItems[j] = [i]
+                else:
+                    ArraysAndItems[j].append(i)
+
+        newArraysAndItems = {}
+
+        for k, v in ArraysAndItems.items():
+            ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
+            if len(v) >= self._minSup:
+                self._finalPatterns[k] = len(v)
+                newArraysAndItems[k] = ArraysAndItems[k]
 
-        newList = []
-        for i in range(0, len(candidateList)):
-            item1 = candidateList[i]
-            iList = item1.split()
-            for j in range(i + 1, len(candidateList)):
-                item2 = candidateList[j]
-                jList = item2.split()
-                if iList[:-1] == jList[:-1]:
-                    unionDiffSet = self._diffSets[item2][1].difference(self._diffSets[item1][1])
-                    unionSup = self._diffSets[item1][0] - len(unionDiffSet)
-                    if unionSup >= self._minSup:
-                        newKey = item1 + "\t" + jList[-1]
-                        self._diffSets[newKey] = [unionSup, unionDiffSet]
-                        newList.append(newKey)
-                    else: 
-                        break
-
-        if len(newList) > 0:
-            self._runDeclat(newList)
+        return newArraysAndItems
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
 
-        self.mine()
+        ArraysAndItems = self.arraysAndItems()
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                iList = list(keys[i])
+                for j in range(i + 1, len(ArraysAndItems)):
+                    jList = list(keys[j])
+                    union = tuple(sorted(set(iList + jList)))
+                    intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]],
+                                                    assume_unique=True)
+                    if len(intersect) >= self._minSup and union not in self._finalPatterns:
+                        newArraysAndItems[union] = intersect
+                        self._finalPatterns[union] = len(intersect)
+            ArraysAndItems = newArraysAndItems
+            # print()
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuApriori algorithm ")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
-
-        self._startTime = _ab._time.time()
         self._Database = []
-        self._finalPatterns = {}
-        self._diffSets = {}
-        self._trans_set = set()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
+        self._startTime = _ab._time.time()
         self._creatingItemSets()
-        #print(len(self._Database))
         self._minSup = self._convert(self._minSup)
-        uniqueItemList = []
-        uniqueItemList = self._getUniqueItemList()
-        self._runDeclat(uniqueItemList)
-        self._finalPatterns = self._diffSets
-        #print(len(self._finalPatterns), len(uniqueItemList))
+
+        ArraysAndItems = self.arraysAndItems()
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                iList = list(keys[i])
+                for j in range(i + 1, len(ArraysAndItems)):
+                    jList = list(keys[j])
+                    union = tuple(sorted(set(iList + jList)))
+                    intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]],
+                                                    assume_unique=True)
+                    if len(intersect) >= self._minSup and union not in self._finalPatterns:
+                        newArraysAndItems[union] = intersect
+                        self._finalPatterns[union] = len(intersect)
+            ArraysAndItems = newArraysAndItems
+            # print()
+
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
+        print("Frequent patterns were generated successfully using cuApriori algorithm ")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -341,59 +368,61 @@
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0]])
+            data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y[0])
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
-        This function is used to print the results
+        This function is used to print results
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-
+        print("Total ExecutionTime in s:", self.getRuntime())
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cuApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cuApriori(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print(_ap.getPatternsAsDataFrame())
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        print("Total ExecutionTime in s:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
+
+
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/ECLATbitset.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclat.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-# ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+# cuECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+#
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+# ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.basic.ECLATbitset as alg
+#             import PAMI.frequentPattern.cuda.cuEclat as alg
 #
-#             obj = alg.ECLATbitset(iFile, minSup)
+#             obj = alg.cuEclat(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -27,14 +28,16 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -44,34 +47,37 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.frequentPattern.basic import abstract as _ab
-from deprecated import deprecated
 
+# from PAMI.frequentPattern.cuda import abstract as _ab
+import abstract as _ab
+from deprecated import deprecated
 
-class ECLATbitset(_ab._frequentPatterns):
+class cuEclat(_ab._frequentPatterns):
     """
-    :Description:  ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+    :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+    :param  minSup: int :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
+
+
     :Attributes:
 
         startTime : float
           To record the start time of the mining process
 
         endTime : float
           To record the completion time of the mining process
@@ -85,226 +91,254 @@
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
         Database : list
           To store the transactions of a database in list
 
 
+
     **Methods to execute code on terminal**
-    ------------------------------------------
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ECLATbitset.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cuEclat.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ECLATbitset.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cuEclat.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
-    ---------------------------------------------------------
+    ----------------------------------------------------
+
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.ECLATbitset as alg
+             import PAMI.frequentPattern.cuda.cuEclat as alg
 
-            obj = alg.ECLATbitset(iFile, minSup)
+             obj = alg.cuEclat(iFile, minSup)
 
-            obj.mine()
+             obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+             frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save(oFile)
+             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+             Df = obj.getPatternInDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+             memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+             print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+             memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+             print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+             run = obj.getRuntime()
+
+             print("Total ExecutionTime in seconds:", run)
 
-            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -------------------
+    -------------
 
-               The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
+    _ab._cp.cuda.Device(0).use()
+
+
+
+    _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _minSup = str()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _mapSupport = {}
-    _lno = 0
 
-    def _convert(self, value):
-        """
-        To convert the user specified minSup value
-
-        :param value: user specified minSup value
-
-        :type value: int
-
-        :return: converted type
-
-        :rtype: int or float or string
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
-        self._mapSupport = {}
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                temp = self._iFile['Transactions'].tolist()
 
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            self._lno += 1
-                            splitter = [i.rstrip() for i in line.split(self._sep)]
-                            splitter = [x for x in splitter if x]
-                            self._Database.append(splitter)
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
-        self._minSup = self._convert(self._minSup)
-
-    @deprecated(
-        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        We start with the scanning the itemSets and store the bitsets respectively.
-        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
-        """
-        self.mine()
+                    quit()
 
-    def _bitPacker(self, data, maxIndex):
+    def _convert(self, value):
         """
-        It takes the data and maxIndex as input and generates integer as output value.
 
-        :param data: it takes data as input.
+        To convert the user specified minSup value
 
-        :type data: int or float
+        :param value: user specified minSup value
 
-        :param maxIndex: It converts the data into bits By taking the maxIndex value as condition.
+        :type value: int or float or str
 
-        :type maxIndex: int
+        :return: converted type
 
         """
-        packed_bits = 0
-        for i in data:
-            packed_bits |= 1 << (maxIndex - i)
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+    
+    def _arraysAndItems(self):
+        """ 
+        Convert the items into arrays for cupy and store them in a dictionary variable
+        :return: dictionary variable
+        """
+        ArraysAndItems = {}
+
+        for i in range(len(self._Database)):
+            for j in self._Database[i]:
+                j = tuple([j])
+                if j not in ArraysAndItems:
+                    ArraysAndItems[j] = [i]
+                else:
+                    ArraysAndItems[j].append(i)
+
+        newArraysAndItems = {}
 
-        return packed_bits
+        for k,v in ArraysAndItems.items():
+            ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
+            if len(v) >= self._minSup:
+                self._finalPatterns[k] = len(v)
+                newArraysAndItems[k] = ArraysAndItems[k]
 
-    def mine(self) -> None:
+        return newArraysAndItems
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
         Frequent pattern mining process will start from here
-        # Bitset implementation
         """
+        self._Database = []
         self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
 
-        self._Database = []
+        ArraysAndItems = self._arraysAndItems()
 
-        self._creatingItemSets()
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                iList = list(keys[i])
+                for j in range(i+1, len(ArraysAndItems)):
+                    # print(i, "/", len(ArraysAndItems), end="\r")
+                    jList = list(keys[j])
+                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
+                        union = iList + [jList[-1]]
+                        union = tuple(union)
+                        intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]], assume_unique=True)
+                        if len(intersect) >= self._minSup:
+                            newArraysAndItems[union] = intersect
+                            self._finalPatterns[union] = len(intersect)
+                    else: 
+                        break
 
-        items = {}
-        index = 0
-        for line in self._Database:
-            for item in line:
-                if tuple([item]) in items:
-                    items[tuple([item])].append(index)
-                else:
-                    items[tuple([item])] = [index]
-            index += 1
+            ArraysAndItems = newArraysAndItems
+            # print()
 
-        # sort by length in descending order
-        items = dict(sorted(items.items(), key=lambda x: len(x[1]), reverse=True))
-        cands = []
-        for key in items:
-            if len(items[key]) >= self._minSup:
-                self._finalPatterns["\t".join(key)] = len(items[key])
-                cands.append(key)
-                items[key] = self._bitPacker(items[key], index)
-                # print(key, items[key])
-            else:
-                break
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuEclat algorithm ")
 
-        while cands:
-            newCands = []
-            for i in range(len(cands)):
-                for j in range(i + 1, len(cands)):
-                    if cands[i][:-1] == cands[j][:-1]:
-                        newCand = tuple(cands[i] + tuple([cands[j][-1]]))
-                        intersection = items[tuple([newCand[0]])]
-                        for k in range(1, len(newCand)):
-                            intersection &= items[tuple([newCand[k]])]
-                        count = int.bit_count(intersection)
-                        if count >= self._minSup:
-                            newCands.append(newCand)
-                            newCand = "\t".join(newCand)
-                            self._finalPatterns[newCand] = count
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self._arraysAndItems()
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                iList = list(keys[i])
+                for j in range(i+1, len(ArraysAndItems)):
+                    # print(i, "/", len(ArraysAndItems), end="\r")
+                    jList = list(keys[j])
+                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
+                        union = iList + [jList[-1]]
+                        union = tuple(union)
+                        intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]], assume_unique=True)
+                        if len(intersect) >= self._minSup:
+                            newArraysAndItems[union] = intersect
+                            self._finalPatterns[union] = len(intersect)
                     else:
                         break
 
-            cands = newCands
+            ArraysAndItems = newArraysAndItems
+            # print()
 
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Apriori algorithm ")
+        print("Frequent patterns were generated successfully using cuEclat algorithm ")
+            
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -337,55 +371,60 @@
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the outputfile
-        :type outFile: file
+        :param outFile: name of the output file
+        :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
-        This function is used to print the result
+        This function is used to print the results
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in s:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        print("Total ExecutionTime in s:", _ap.getRuntime())
     else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
+        print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
+
+
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/FPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,27 @@
-# FPGrowth is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
+# PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
+#
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.frequentPattern.basic import FPGrowth as alg
 #
-#             obj = alg.FPGrowth(iFile, minSup)
+#             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
+#
+#             obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#             obj.save(oFile)
+#             obj.save("patterns")
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -30,547 +32,440 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
 """
 
-from PAMI.frequentPattern.basic import abstract as _fp
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
-from itertools import combinations
-from collections import Counter
 
-_minSup = str()
-_fp._sys.setrecursionlimit(20000)
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 
 
-class _Node:
+class PFECLAT(_ab._periodicFrequentPatterns):
     """
-    A class used to represent the node of frequentPatternTree
-
-    :Attributes:
+    :Description:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
 
-        itemId: int
-            storing item of a node
-        counter: int
-            To maintain the support of node
-        parent: node
-            To maintain the parent of node
-        children: list
-            To maintain the children of node
-
-    :Methods:
-
-        addChild(node)
-            Updates the nodes children list and parent for the given node
-
-    """
-
-    def __init__(self, item, count, parent) -> None:
-        self.item = item
-        self.count = count
-        self.parent = parent
-        self.children = {}
-
-    def addChild(self, item, count = 1) -> Any:
-        """
-        Adds a child node to the current node with the specified item and count.
-
-        :param item: The item associated with the child node.
-        :type item: List
-
-        :param count: The count or support of the item. Default is 1.
-        :type count: int
-
-        :return: The child node added.
-        :rtype: List
-
-        """
-        if item not in self.children:
-            self.children[item] = _Node(item, count, self)
-        else:
-            self.children[item].count += count
-        return self.children[item]
-    
-    def traverse(self) -> Tuple[List[int], int]:
-        """
-        Traversing the tree to get the transaction
-
-        :return: transaction and count of each item in transaction
-
-        :rtype: Tuple, List and int
-        """
-        transaction = []
-        count = self.count
-        node = self.parent
-        while node.parent is not None:
-            transaction.append(node.item)
-            node = node.parent
-        return transaction[::-1], count
-
-
-class FPGrowth(_fp._frequentPatterns):
-    """
-
-    :Description:   FPGrowth is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
-
-    :Reference:  Han, J., Pei, J., Yin, Y. et al. Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern
-           Tree Approach. Data  Mining and Knowledge Discovery 8, 5387 (2004). https://doi.org/10.1023
+    :Reference:   P. Ravikumar, P.Likhitha, R. Uday kiran, Y. Watanobe, and Koji Zettsu, "Towards efficient discovery of
+                  periodic-frequent patterns in columnar temporal databases", 2021 IEA/AIE.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-
-
     :Attributes:
 
-        startTime : float
-          To record the start time of the mining process
-
-        endTime : float
-          To record the completion time of the mining process
-
-        finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
-
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
-
+            To store the total amount of USS memory consumed by the program
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
-
+            To store the total amount of RSS memory consumed by the program
+        startTime : float
+            To record the start time of the mining process
+        endTime : float
+            To record the completion time of the mining process
         Database : list
-          To store the transactions of a database in list
-
+            To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
             it represents the total no of transactions
         tree : class
             it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
         finalPatterns : dict
             it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
+        hashing : dict
+            stores the patterns with their support to check for the closed property
 
+    :Methods:
 
-    **Methods to execute code on terminal**
-    --------------------------------------------------------
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingOneItemSets()
+            Scan the database and store the items with their timestamps which are periodic frequent 
+        getPeriodAndSupport()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+            
 
+    **Methods to execute code on terminal**
+    ------------------------------------------
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 FPGrowth.py <inputFile> <outputFile> <minSup>
+       Format:
+
+       (.venv) $ python3 PFECLAT.py <inputFile> <outputFile> <minSup>
 
-      Example Usage:
+       Example usage:
 
-      (.venv) $ python3 FPGrowth.py sampleDB.txt patterns.txt 10.0
+       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
 
-    .. note:: minSup will be considered in percentage of database transactions
+
+
+               .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
 
-            from PAMI.frequentPattern.basic import FPGrowth as alg
+             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
 
-            obj = alg.FPGrowth(iFile, minSup)
+                obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
 
-            obj.mine()
+                obj.startMine()
 
-            frequentPatterns = obj.getPatterns()
+                periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
-            obj.savePatterns(oFile)
+                obj.save("patterns")
 
-            Df = obj.getPatternInDataFrame()
+                Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+                memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+                print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+                memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+                print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
-
-            print("Total ExecutionTime in seconds:", run)
+                run = obj.getRuntime()
 
+                print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ----------------------------
-               The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    --------------
+             The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
 
     """
-
-    __startTime = float()
-    __endTime = float()
-    _minSup = str()
-    __finalPatterns = {}
+    
     _iFile = " "
     _oFile = " "
     _sep = " "
-    __memoryUSS = float()
-    __memoryRSS = float()
-    __Database = []
-    __mapSupport = {}
-    __lno = 0
-    __rank = {}
-    __rankDup = {}
+    _dbSize = None
+    _Database = None
+    _minSup = str()
+    _maxPer = str()
+    _tidSet = set()
+    _finalPatterns = {}
+    _startTime = None
+    _endTime = None
+    _memoryUSS = float()
+    _memoryRSS = float()
+
+    def _getPeriodic(self, tids: set) -> int:
+        tidList = list(tids)
+        tidList.sort()
+        tidList.append(self._dbSize)
+        cur = 0
+        per = 0
+        for tid in tidList:
+            per = max(per, tid - cur)
+            if per > self._maxPer:  # early stopping
+                break
+            cur = tid
+        return per
+
+    def _convert(self, value) -> float:
+        """
+        To convert the given user specified value
 
-    def __init__(self, iFile, minSup, sep='\t') -> None:
-        super().__init__(iFile, minSup, sep)
+        :param value: user specified value
+        :return: converted value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._dbSize * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._dbSize * value)
+            else:
+                value = int(value)
+        return value
 
-    def __creatingItemSets(self) -> None:
+    def _creatingOneItemSets(self) -> list:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: list
         """
-        self.__Database = []
-        if isinstance(self._iFile, _fp._pd.DataFrame):
+        plist = []
+        Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
-
-            #print(self.Database)
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                Database.append(tr)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self.__Database.append(temp)
+                    Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self.__Database.append(temp)
+                            Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-
-    def __convert(self, value) -> float:
-        """
-
-        To convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-
-        :return: converted type
-
-        :rtype: float
-
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self.__Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self.__Database) * value)
-            else:
-                value = int(value)
-        return value
-    
-    def _construct(self, items, data, minSup):
-        """
-        Constructs the FP-tree from the given transactions.
-
-        :param items: A dictionary containing item frequencies.
-        :type items: Dict
-
-        :param data: A list of transactions.
-        :type data: List
-
-        :param minSup: The minimum support threshold.
-        :type minSup: int
-
-        :return: The root node of the constructed FP-tree and a dictionary containing information about nodes associated with each item.
-        :rtype: Tuple[_Node, Dict]
-        """
-
-        items = {k: v for k, v in items.items() if v >= minSup}
-
-        root = _Node([], 0, None)
-        itemNodes = {}
-        for line in data:
-            currNode = root
-            line = sorted([item for item in line if item in items], key = lambda x: items[x], reverse = True)
-            for item in line:
-                currNode = currNode.addChild(item)
-                if item in itemNodes:
-                    itemNodes[item][0].add(currNode)
-                    itemNodes[item][1] += 1
+        tid = 0
+        itemsets = {}  # {key: item, value: list of tids}
+        periodicHelper = {}  # {key: item, value: [period, last_tid]}
+        for line in Database:
+            tid = int(line[0])
+            self._tidSet.add(tid)
+            for item in line[1:]:
+                if item in itemsets:
+                    itemsets[item].add(tid)
+                    periodicHelper[item][0] = max(periodicHelper[item][0],
+                                                  abs(tid - periodicHelper[item][1]))  # update current max period
+                    periodicHelper[item][1] = tid  # update the last tid
                 else:
-                    itemNodes[item] = [set([currNode]), 1]
-
-        return root, itemNodes
-
-    def _all_combinations(self, arr):
-        """
-        Generates all possible combinations of items from a given transaction.
-
-        :param arr: A list of items in a transaction.
-        :type arr: List
-
-        :return: A list containing all possible combinations of items.
-        :rtype: List
-
-        """
+                    itemsets[item] = {tid}
+                    periodicHelper[item] = [abs(0 - tid), tid]  # initialize helper
 
-        all_combinations_list = []
-        for r in range(1, len(arr) + 1):
-            all_combinations_list.extend(combinations(arr, r))
-        return all_combinations_list
+        # finish all items' period
+        self._dbSize = len(Database)
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        del Database
+        for item, _ in periodicHelper.items():
+            periodicHelper[item][0] = max(periodicHelper[item][0],
+                                          abs(self._dbSize - periodicHelper[item][1]))  # tid of the last transaction
+        candidates = []
+        for item, tids in itemsets.items():
+            per = periodicHelper[item][0]
+            sup = len(tids)
+            if sup >= self._minSup and per <= self._maxPer:
+                candidates.append(item)
+                self._finalPatterns[item] = [sup, per, tids]
+        return candidates
     
-    def _recursive(self, root, itemNode, minSup, patterns):
-        """
-
-         Recursively explores the FP-tree to generate frequent patterns.
-
-         :param root: The root node of the current subtree.
-         :type root: _Node
-
-         :param itemNode: A dictionary containing information about the nodes associated with each item.
-         :type itemNode: Dict
+    def _generateEclat(self, candidates: list) -> None:
 
-         :param minSup: The minimum support threshold.
-         :type minSup: int
+        newCandidates = []
+        for i in range(0, len(candidates)):
+            prefixItem = candidates[i]
+            prefixItemSet = prefixItem.split()
+            for j in range(i + 1, len(candidates)):
+                item = candidates[j]
+                itemSet = item.split()
+                if prefixItemSet[:-1] == itemSet[:-1] and prefixItemSet[-1] != itemSet[-1]:
+                    _value = self._finalPatterns[item][2].intersection(self._finalPatterns[prefixItem][2])
+                    sup = len(_value)
+                    per = self._getPeriodic(_value)
+                    if sup >= self._minSup and per <= self._maxPer:
+                        newItem = prefixItem + "\t" + itemSet[-1]
+                        self._finalPatterns[newItem] = [sup, per, _value]
+                        newCandidates.append(newItem)
 
-         :param patterns: A dictionary to store the generated frequent patterns.
-         :type patterns: Dict
+        if len(newCandidates) > 0:
+            self._generateEclat(newCandidates)
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        itemNode = {k: v for k, v in sorted(itemNode.items(), key = lambda x: x[1][1])}
-
-        for item in itemNode:
-            if itemNode[item][1] < self._minSup:
-                break 
-
-            newRoot = _Node(root.item + [item], 0, None)
-            pat = "\t".join([str(i) for i in newRoot.item])
-            self.__finalPatterns[pat] = itemNode[item][1]
-            newItemNode = {}
-
-            if len(itemNode[item][0]) == 1:
-                transaction, count = itemNode[item][0].pop().traverse()
-                if len(transaction) == 0:
-                    continue
-                combination = self._all_combinations(transaction)
-                for comb in combination:
-                    pat = "\t".join([str(i) for i in comb])
-                    pat = pat + "\t" + "\t".join([str(i) for i in newRoot.item])
-                    self.__finalPatterns[pat] = count
-                    # self._finalPatterns[tuple(list(comb) + newRoot.item)] = count
-                pass
-
-
-            itemCount = {}
-            transactions = {}
-            for node in itemNode[item][0]:
-                transaction, count = node.traverse()
-                if len(transaction) == 0:
-                    continue
-                if tuple(transaction) in transactions:
-                    transactions[tuple(transaction)] += count
-                else:
-                    transactions[tuple(transaction)] = count
-
-
-                for item in transaction:
-                    if item in itemCount:
-                        itemCount[item] += count
-                    else:
-                        itemCount[item] = count
-
-
-            # remove items that are below minSup
-            itemCount = {k: v for k, v in itemCount.items() if v >= minSup}
-            if len(itemCount) == 0:
-                continue
-
-            for transaction, count in transactions.items():
-                transaction = sorted([item for item in transaction if item in itemCount], key = lambda x: itemCount[x], reverse = True)
-                currNode = newRoot
-                for item in transaction:
-                    currNode = currNode.addChild(item, count)
-                    if item in newItemNode:
-                        newItemNode[item][0].add(currNode)
-                        newItemNode[item][1] += count
-                    else:
-                        newItemNode[item] = [set([currNode]), count]
-
-            if len(newItemNode) < 1:
-                continue
-
-            # mine(newRoot, newItemNode, minSup, patterns)
-            self._recursive(newRoot, newItemNode, minSup, patterns)
-
-
-    def mine(self) -> None:
-        """
-        Main program to start the operation
-        """
-        global _minSup
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        _minSup = self._minSup
-
-        itemCount = Counter()
-        for line in self.__Database:
-            itemCount.update(line)
-
-        root, itemNode = self._construct(itemCount, self.__Database, self._minSup)
-        self._recursive(root, itemNode, self._minSup, self.__finalPatterns)
-        
-        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        Mining process will start from this function
+        :return: None
+        """
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        frequentSets = self._creatingOneItemSets()
+        self._generateEclat(frequentSets)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def Mine(self) -> None:
         """
-        Starting the mining process
+        Mining process will start from this function
+        :return: None
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        frequentSets = self._creatingOneItemSets()
+        self._generateEclat(frequentSets)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
 
     def getMemoryUSS(self) -> float:
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
-
         :rtype: float
         """
 
-        return self.__memoryUSS
+        return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memoryRSS
+        return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
 
-        :return: returning total amount of runtime taken by the mining process
 
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self.__endTime - self.__startTime
+        return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
-        Storing final frequent patterns in a dataframe
-
-        :return: returning frequent patterns in a dataframe
+        Storing final periodic-frequent patterns in a dataframe
 
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self.__finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        for a, b in self._finalPatterns.items():
+            data.append([a, b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of periodic-frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
-
-        :type outFile: csvfile
-
+        :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self.__finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+        for x, y in self._finalPatterns.items():
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, int]:
+    def getPatterns(self) -> dict:
         """
-        Function to send the set of frequent patterns after completion of the mining process
-        :return: returning frequent patterns
+        Function to send the set of periodic-frequent patterns after completion of the mining process
+
+        :return: returning periodic-frequent patterns
         :rtype: dict
         """
-        return self.__finalPatterns
-    
+        return self._finalPatterns
+
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+                    
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
-        if len(_fp._sys.argv) == 5:
-            _ap = FPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
-        if len(_fp._sys.argv) == 4:
-            _ap = FPGrowth(_fp._sys.argv[1], _fp._sys.argv[3])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Frequent Patterns:", len( _ap.getPatterns()))
-        _ap.save(_fp._sys.argv[2])
+        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-    
-
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/_Apriori.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/Apriori.py`

 * *Files 0% similar despite different names*

```diff
@@ -443,7 +443,8 @@
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ap._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/_FPGrowth.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/FPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/closed/CHARM.py` & `pami-2024.4.9.1/PAMI/frequentPattern/closed/CHARM.py`

 * *Files 4% similar despite different names*

```diff
@@ -416,15 +416,56 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Mining process will start from here by extracting the frequent patterns from the database. It performs prefix
         equivalence to generate the combinations and closed frequent patterns.
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        _plist = self._creatingItemsets()
+        self._finalPatterns = {}
+        self._hashing = {}
+        for i in range(len(_plist)):
+            itemX = _plist[i]
+            if itemX is None:
+                continue
+            tidSetx = self._tidList[itemX]
+            itemSetx = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(_plist)):
+                itemY = _plist[j]
+                if itemY is None:
+                    continue
+                tidSetY = self._tidList[itemY]
+                y1 = list(set(tidSetx).intersection(tidSetY))
+                if len(y1) < self._minSup:
+                    continue
+                if len(tidSetx) == len(tidSetY) and len(y1) == len(tidSetx):
+                    _plist.insert(j, None)
+                    itemSetx.append(itemY)
+                elif len(tidSetx) < len(tidSetY) and len(y1) == len(tidSetx):
+                    itemSetx.append(itemY)
+                elif len(tidSetx) > len(tidSetY) and len(y1) == len(tidSetY):
+                    _plist.insert(j, None)
+                    itemSets.append(itemY)
+                    tidSets.append(y1)
+                else:
+                    itemSets.append(itemY)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetx, itemSets, tidSets)
+            self._save(None, itemSetx, tidSetx)
+        print("Closed Frequent patterns were generated successfully using CHARM algorithm")
+        self._endTime = _ab._time.time()
+        _process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
 
     def mine(self):
         """
         Mining process will start from here by extracting the frequent patterns from the database. It performs prefix
         equivalence to generate the combinations and closed frequent patterns.
         """
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/closed/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuApriori.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuAprioriBit.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# cuApriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+# cuAprioriBit is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 #
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cuApriori as alg
+#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 #
-#             obj = alg.cuApriori(iFile, minSup)
+#             obj = alg.cuAprioriBit(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -29,15 +29,14 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,21 +46,22 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+# from PAMI.frequentPattern.cuda import abstract as _ab
+import abstract as _ab
 from deprecated import deprecated
-from PAMI.frequentPattern.cuda import abstract as _ab
-# import abstract as _ab
 
-class cuApriori(_ab._frequentPatterns):
+
+class cuAprioriBit(_ab._frequentPatterns):
     """
-    :Description: cuApriori is one of the fundamental algorithm to discover frequent patterns using Cuda in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: cuAprioriBit is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
             In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
@@ -96,64 +96,62 @@
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cuApriori.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cuAprioriBit.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 cuApriori.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cuAprioriBit.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
-             import PAMI.frequentPattern.cuda.cuApriori as alg
+            import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 
-             obj = alg.cuApriori(iFile, minSup)
+            obj = alg.cuAprioriBit(iFile, minSup)
 
-             obj.mine()
+            obj.mine()
 
-             frequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-             print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-             obj.save(oFile)
+            obj.save(oFile)
 
-             Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternInDataFrame()
 
-             memUSS = obj.getMemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
-             print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-             memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-             print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-             run = obj.getRuntime()
+            run = obj.getRuntime()
 
-             print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
     -------------
 
              The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _ab._cp.cuda.Device(0).use()
-
     _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
@@ -249,64 +247,119 @@
                     ArraysAndItems[j] = [i]
                 else:
                     ArraysAndItems[j].append(i)
 
         newArraysAndItems = {}
 
         for k, v in ArraysAndItems.items():
-            ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
+            ArraysAndItems[k] = _ab._np.array(v, dtype=_ab._np.uint32)
             if len(v) >= self._minSup:
                 self._finalPatterns[k] = len(v)
                 newArraysAndItems[k] = ArraysAndItems[k]
 
         return newArraysAndItems
 
+    def createBitRepresentation(self, ArraysAndItems):
+        bitRep = {}
+        arraySize = len(self._Database) // 32 + 1 if len(self._Database) % 32 != 0 else len(self._Database) // 32
+
+        for k, v in ArraysAndItems.items():
+            bitRep[k] = _ab._np.zeros(arraySize, dtype=_ab._np.uint32)
+            for i in v:
+                bitRep[k][i // 32] |= 1 << 31 - (i % 32)
+
+        for k, v in bitRep.items():
+            bitRep[k] = _ab._cp.array(v)
+
+        return bitRep
+
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self.arraysAndItems()
+        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                iList = list(keys[i])
+                for j in range(i + 1, len(ArraysAndItems)):
+                    unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
+                    sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
+                    self._sumKernel((len(unionData) // 32 + 1,), (32,),
+                                    (unionData, sum, _ab._cp.uint32(len(unionData))))
+                    sum = sum[0]
+                    jList = list(keys[j])
+                    union = tuple(sorted(set(iList + jList)))
+                    if sum >= self._minSup and union not in self._finalPatterns:
+                        newArraysAndItems[union] = unionData
+                        string = "\t".join(union)
+                        self._finalPatterns[string] = sum
+            ArraysAndItems = newArraysAndItems
+            # print()
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuAprioriBit algorithm ")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
 
         ArraysAndItems = self.arraysAndItems()
+        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
 
         while len(ArraysAndItems) > 0:
             # print("Total number of ArraysAndItems:", len(ArraysAndItems))
             newArraysAndItems = {}
             keys = list(ArraysAndItems.keys())
             for i in range(len(ArraysAndItems)):
                 # print(i, "/", len(ArraysAndItems), end="\r")
                 iList = list(keys[i])
                 for j in range(i + 1, len(ArraysAndItems)):
+                    unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
+                    sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
+                    self._sumKernel((len(unionData) // 32 + 1,), (32,),
+                                    (unionData, sum, _ab._cp.uint32(len(unionData))))
+                    sum = sum[0]
                     jList = list(keys[j])
                     union = tuple(sorted(set(iList + jList)))
-                    intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]],
-                                                    assume_unique=True)
-                    if len(intersect) >= self._minSup and union not in self._finalPatterns:
-                        newArraysAndItems[union] = intersect
-                        self._finalPatterns[union] = len(intersect)
+                    if sum >= self._minSup and union not in self._finalPatterns:
+                        newArraysAndItems[union] = unionData
+                        string = "\t".join(union)
+                        self._finalPatterns[string] = sum
             ArraysAndItems = newArraysAndItems
             # print()
 
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuApriori algorithm ")
+        print("Frequent patterns were generated successfully using cuAprioriBit algorithm ")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -364,34 +417,34 @@
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
-        This function is used to print results
+        This function is used to print the result
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in s:", self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
+
+
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cuApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cuAprioriBit(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cuApriori(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cuAprioriBit(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in s:", _ap.getRuntime())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
 
-
-
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuAprioriBit.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,16 +1,15 @@
-# cuAprioriBit is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
-#
+# cudaEclatGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# --------------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+#             from PAMI.frequentPattern.cuda.cudaEclatGCT as alg
 #
-#             obj = alg.cuAprioriBit(iFile, minSup)
+#             obj = alg.FPGrowth(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -29,14 +28,16 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
+
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -46,76 +47,85 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-# from PAMI.frequentPattern.cuda import abstract as _ab
-import abstract as _ab
 from deprecated import deprecated
+from PAMI.frequentPattern.basic import abstract as _ab
+
+minSup = str()
+_ab._sys.setrecursionlimit(20000)
 
+import os
+import csv
+import time
+import numpy as np
+import pycuda.gpuarray as _gpuarray
+import pycuda.autoinit
+import psutil
 
-class cuAprioriBit(_ab._frequentPatterns):
+
+class cudaEclatGCT:
     """
-    :Description: cuAprioriBit is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-            In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         startTime : float
-          To record the start time of the mining process
+            To record the start time of the mining process
 
         endTime : float
-          To record the completion time of the mining process
+            To record the completion time of the mining process
 
         finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
+            Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
+            To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
+            To store the total amount of RSS memory consumed by the program
 
         Database : list
-          To store the transactions of a database in list
+            To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cuAprioriBit.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cudaEclatGCT.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 cuAprioriBit.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cudaEclatGCT.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
-
     .. code-block:: python
 
             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 
             obj = alg.cuAprioriBit(iFile, minSup)
 
             obj.mine()
@@ -139,276 +149,273 @@
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
     -------------
-
              The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
-
     """
 
-    _minSup = float()
-    _startTime = float()
-    _endTime = float()
+    __time = 0
+    __memRSS = 0
+    __memUSS = 0
+    __GPU_MEM = 0
+    _minSup = 0
     _finalPatterns = {}
-    _iFile = " "
-    _oFile = " "
-    _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-
-    _sumKernel = _ab._cp.RawKernel(r'''
-
-    #define uint32_t unsigned int
-
-    extern "C" __global__
-
-    void sumKernel(uint32_t *d_a, uint32_t *sum, uint32_t numElements)
-    {
-        uint32_t i = blockDim.x * blockIdx.x + threadIdx.x;
-        if (i < numElements)
-        {  
-            atomicAdd(&sum[0], __popc(d_a[i]));
-        }
-        return;    
-    }
 
-    ''', 'sumKernel')
+    def __init__(self, filePath, minSup, sep):
+        self._iFile = filePath
+        self._sep = sep
+        self._minSup = minSup
+        self.__time = 0
+        self.__memRSS = 0
+        self.__memUSS = 0
+
+    """def read_data(self, data_path, sep):
+
 
-    def _creatingItemSets(self):
+        data = []
+        if not os.path.isfile(data_path):
+            raise ValueError('Invalid data path.' + data_path)
+        with open(data_path, 'r') as f:
+            file = csv.reader(f, delimiter=sep, quotechar='\r')
+            lineNo = 1
+            for row in file:
+                data.append([str(item) for item in row if item != ''])
+                lineNo += 1
+        return data, lineNo"""
+
+    def __creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._Database = []
+        self.__Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
+                self.__Database = self._iFile['Transactions'].tolist()
 
-            for k in temp:
-                self._Database.append(set(k))
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    self.__Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            line = line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value):
+    def __convert(self, value):
         """
 
-        To convert the user specified minSup value
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
 
         :type value: int or float or str
 
         :return: converted type
 
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    def arraysAndItems(self):
-        ArraysAndItems = {}
-
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                j = tuple([j])
-                if j not in ArraysAndItems:
-                    ArraysAndItems[j] = [i]
-                else:
-                    ArraysAndItems[j].append(i)
-
-        newArraysAndItems = {}
-
-        for k, v in ArraysAndItems.items():
-            ArraysAndItems[k] = _ab._np.array(v, dtype=_ab._np.uint32)
-            if len(v) >= self._minSup:
-                self._finalPatterns[k] = len(v)
-                newArraysAndItems[k] = ArraysAndItems[k]
-
-        return newArraysAndItems
-
-    def createBitRepresentation(self, ArraysAndItems):
-        bitRep = {}
-        arraySize = len(self._Database) // 32 + 1 if len(self._Database) % 32 != 0 else len(self._Database) // 32
-
-        for k, v in ArraysAndItems.items():
-            bitRep[k] = _ab._np.zeros(arraySize, dtype=_ab._np.uint32)
-            for i in v:
-                bitRep[k][i // 32] |= 1 << 31 - (i % 32)
-
-        for k, v in bitRep.items():
-            bitRep[k] = _ab._cp.array(v)
-
-        return bitRep
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def compute_vertical_bitvector_data(self):
         """
-        Frequent pattern mining process will start from here
+        Converting  database into bit vector
         """
-        self.mine()
+        # ---build item to idx mapping---#
+        idx = 0
+        item2idx = {}
+        for transaction in self.__Database:
+            for item in transaction:
+                if not item in item2idx:
+                    item2idx[item] = idx
+                    idx += 1
+        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
+        # ---build vertical data---#
+        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
+        for trans_id, transaction in enumerate(self.__Database):
+            for item in transaction:
+                vb_data[item2idx[item], trans_id] = 1
+        vb_data = _gpuarray.to_gpu(vb_data.astype(np.uint16))
+        return vb_data, idx2item
 
-    def mine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self.arraysAndItems()
-        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
-
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                # print(i, "/", len(ArraysAndItems), end="\r")
-                iList = list(keys[i])
-                for j in range(i + 1, len(ArraysAndItems)):
-                    unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
-                    sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
-                    self._sumKernel((len(unionData) // 32 + 1,), (32,),
-                                    (unionData, sum, _ab._cp.uint32(len(unionData))))
-                    sum = sum[0]
-                    jList = list(keys[j])
-                    union = tuple(sorted(set(iList + jList)))
-                    if sum >= self._minSup and union not in self._finalPatterns:
-                        newArraysAndItems[union] = unionData
-                        string = "\t".join(union)
-                        self._finalPatterns[string] = sum
-            ArraysAndItems = newArraysAndItems
-            # print()
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuAprioriBit algorithm ")
-
-    def getMemoryUSS(self):
+    def getRuntime(self):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of time taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
-        return self._memoryUSS
+        return self.__time
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memRSS
 
-        return self._memoryRSS
-
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-        Calculating the total amount of runtime taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memUSS
 
-        return self._endTime - self._startTime
-
-    def getPatternsAsDataFrame(self):
+    def getGPUMemory(self):
         """
-        Storing final frequent patterns in a dataframe
-        :return: returning frequent patterns in a dataframe
-        :rtype: pd.DataFrame
+        To calculate the total memory consumed by GPU
+        :return: return GPU memory
+        :rtype: int
         """
 
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
-        return dataFrame
-
-    def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the output file
-        :type outFile: csvfile
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+        return self.__GPU_MEM
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def get_numberOfPatterns(self):
+
+        return len(self._finalPatterns)
+
+    def eclat(self, basePattern, final, vb_data, idx2item, item2idx):
         """
-        This function is used to print the result
+        param basePattern: base pattern used for the mining process after completion of the mining process
+        type basePattern:
+        param final: final pattern used for the mining process after completion of the mining process
+        type final:
+        param vb_data: vb_data used for the mining process after completion of the mining process
+        type vb_data:
+        param idx2item: idx2item used for the mining process after completion of the mining process
+        type idx2item:
+        param item2idx: item2idx used for the mining process after completion of the mining process
+        type item2idx:
+        """
+        newBasePattern = []
+        for i in range(0, len(basePattern)):
+            item1 = basePattern[i]
+            i1_list = item1.split()
+            for j in range(i + 1, len(basePattern)):
+                item2 = basePattern[j]
+                i2_list = item2.split()
+                if i1_list[:-1] == i2_list[:-1]:
+                    unionOfKey = list(set(i1_list) | set(i2_list))
+                    unionOfKey.sort()
+                    valueList = []
+                    for key in unionOfKey:
+                        valueList.append(item2idx[key])
+                    total = vb_data[valueList[0]]
+                    for k in range(1, len(valueList)):
+                        total = total.__mul__(vb_data[valueList[k]])
+                    support = _gpuarray.sum(total).get()
+                    if support >= self._minSup:
+                        newBasePattern.append(" ".join(unionOfKey))
+                        final[" ".join(unionOfKey)] = support
+
+        if len(newBasePattern) > 0:
+            self.eclat(newBasePattern, final, vb_data, idx2item, item2idx)
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        startTime = time.time()
+        basePattern = []
+        final = {}
+
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern.append(idx2item[i])
+                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
+
+        # reverse idx2item
+        item2idx = {idx2item[i]: i for i in idx2item}
+        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
 
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        startTime = time.time()
+        basePattern = []
+        final = {}
+
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern.append(idx2item[i])
+                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
+
+        # reverse idx2item
+        item2idx = {idx2item[i]: i for i in idx2item}
+        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cuAprioriBit(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cuAprioriBit(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
+        _ap.save(_ap._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("GPU MEM: ", _ap.getGPUMemory())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuEclat.py` & `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATbitset.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,16 +1,15 @@
-# cuECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
-#
+# ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# ---------------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cuEclat as alg
+#             import PAMI.frequentPattern.basic.ECLATbitset as alg
 #
-#             obj = alg.cuEclat(iFile, minSup)
+#             obj = alg.ECLATbitset(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -30,14 +29,15 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,36 +48,33 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
-# from PAMI.frequentPattern.cuda import abstract as _ab
-import abstract as _ab
+from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
-class cuEclat(_ab._frequentPatterns):
+class ECLATbitset(_ab._frequentPatterns):
     """
-    :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+    :Description:  ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-
-
     :Attributes:
 
         startTime : float
           To record the start time of the mining process
 
         endTime : float
           To record the completion time of the mining process
@@ -91,219 +88,283 @@
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
         Database : list
           To store the transactions of a database in list
 
 
-
     **Methods to execute code on terminal**
-    ----------------------------------------------------
+    ------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cuEclat.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 ECLATbitset.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 cuEclat.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 ECLATbitset.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
-
+    ---------------------------------------------------------
     .. code-block:: python
 
-             import PAMI.frequentPattern.cuda.cuEclat as alg
+            import PAMI.frequentPattern.basic.ECLATbitset as alg
 
-             obj = alg.cuEclat(iFile, minSup)
+            obj = alg.ECLATbitset(iFile, minSup)
 
-             obj.mine()
+            obj.mine()
 
-             frequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-             print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-             obj.save(oFile)
+            obj.save(oFile)
 
-             Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternInDataFrame()
 
-             memUSS = obj.getMemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
-             print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-             memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-             print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-             run = obj.getRuntime()
-
-             print("Total ExecutionTime in seconds:", run)
+            run = obj.getRuntime()
 
+            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -------------
+    -------------------
 
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+               The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _ab._cp.cuda.Device(0).use()
-
-
-
-    _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
+    _minSup = str()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _mapSupport = {}
+    _lno = 0
+
+
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+
+        :param value: user specified minSup value
+
+        :type value: int
 
+        :return: converted type
+
+        :rtype: int or float or string
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
+        self._mapSupport = {}
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
 
-            for k in temp:
-                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            self._lno += 1
+                            splitter = [i.rstrip() for i in line.split(self._sep)]
+                            splitter = [x for x in splitter if x]
+                            self._Database.append(splitter)
                 except IOError:
                     print("File Not Found")
-                    quit()
+        self._minSup = self._convert(self._minSup)
 
-    def _convert(self, value):
+    def creatingFrequentItems(self):
         """
+        This function creates frequent items from _database.
 
-        To convert the user specified minSup value
+        :return: frequentTidData that stores frequent items and their tid list.
 
-        :param value: user specified minSup value
+        :rtype: Dict
 
-        :type value: int or float or str
+        """
+        tidData = {}
+        self._lno = 0
+        for transaction in self._Database:
+            self._lno = self._lno + 1
+            for item in transaction:
+                if item not in tidData:
+                    tidData[item] = [self._lno]
+                else:
+                    tidData[item].append(self._lno)
+        frequentTidData = {k: v for k, v in tidData.items() if len(v) >= self._minSup}
+        frequentTidData = dict(sorted(frequentTidData.items(), reverse=True, key=lambda x: len(x[1])))
+        return frequentTidData
 
-        :return: converted type
+    def tidToBitset(self,itemset):
+        """
+
+        This function converts tid list to bitset.
+
+        :param itemset: frequent itemset that generated
+
+        :type itemset: Dict
+
+        :return: patterns with original item names
+
+        :rtype: Dict
 
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
-    
-    def _arraysAndItems(self):
-        """ 
-        Convert the items into arrays for cupy and store them in a dictionary variable
-        :return: dictionary variable
-        """
-        ArraysAndItems = {}
-
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                j = tuple([j])
-                if j not in ArraysAndItems:
-                    ArraysAndItems[j] = [i]
-                else:
-                    ArraysAndItems[j].append(i)
+        bitset = {}
+
+        for k,v in itemset.items():
+            bitset[k] = 0b1
+            bitset[k] = (bitset[k] << int(v[0])) | 0b1
+            for i in range(1,len(v)):
+                diff = int(v[i]) - int(v[i-1])
+                bitset[k] = (bitset[k] << diff) | 0b1
+            bitset[k] = (bitset[k] << (self._lno - int(v[i])))
+        return bitset
+
+    def genPatterns(self,prefix,tidData):
+        """
 
-        newArraysAndItems = {}
+        This function generate frequent pattern about prefix.
 
-        for k,v in ArraysAndItems.items():
-            ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
-            if len(v) >= self._minSup:
-                self._finalPatterns[k] = len(v)
-                newArraysAndItems[k] = ArraysAndItems[k]
+        :param prefix: prefix of pattern to generate patterns
 
-        return newArraysAndItems
+        :type prefix: str
+
+        :param tidData: tidData for pattern generation
+
+        :type tidData: list
+
+        """
+        # variables to store frequent item set and
+        itemset = prefix[0]
+
+        # Get the length of tidData
+        length = len(tidData)
+
+        for i in range(length):
+            #tid = prefix[1].intersection(tidData[i][1])
+            tid = prefix[1] & tidData[i][1]
+            count = bin(tid).count("1") - 1
+            #tidLength = len(tid)
+            if count >= self._minSup:
+                frequentItemset = itemset + '\t' + tidData[i][0]
+                self._finalPatterns[frequentItemset] = count
+                self.genPatterns((frequentItemset,tid),tidData[i+1:length])
+
+    def genAllFrequentPatterns(self,frequentItems):
+        """
+        This function generates all frequent patterns.
+
+        :param frequentItems: frequent items
+
+        :type frequentItems: Dict
+
+        """
+        tidData = list(frequentItems.items())
+        length = len(tidData)
+        for i in range(length):
+            #print(i,tidData[i][0])
+            self.genPatterns(tidData[i],tidData[i+1:length])
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
+        We start with the scanning the itemSets and store the bitsets respectively.
+        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
         """
-        self.mine()
+
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+
+        self._creatingItemSets()
+        frequentItems = self.creatingFrequentItems()
+        self._finalPatterns = {k: len(v) for k, v in frequentItems.items()}
+        frequentItemsBitset = self.tidToBitset(frequentItems)
+        self.genAllFrequentPatterns(frequentItemsBitset)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Eclat_bitset algorithm")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
+        We start with the scanning the itemSets and store the bitsets respectively.
+        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self._arraysAndItems()
 
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                iList = list(keys[i])
-                for j in range(i+1, len(ArraysAndItems)):
-                    # print(i, "/", len(ArraysAndItems), end="\r")
-                    jList = list(keys[j])
-                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
-                        union = iList + [jList[-1]]
-                        union = tuple(union)
-                        intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]], assume_unique=True)
-                        if len(intersect) >= self._minSup:
-                            newArraysAndItems[union] = intersect
-                            self._finalPatterns[union] = len(intersect)
-                    else:
-                        break
-
-            ArraysAndItems = newArraysAndItems
-            # print()
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
 
+        self._creatingItemSets()
+        frequentItems = self.creatingFrequentItems()
+        self._finalPatterns = {k: len(v) for k, v in frequentItems.items()}
+        frequentItemsBitset = self.tidToBitset(frequentItems)
+        self.genAllFrequentPatterns(frequentItemsBitset)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuEclat algorithm ")
-            
+        print("Frequent patterns were generated successfully using Eclat_bitset algorithm")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -336,60 +397,54 @@
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the output file
-        :type outFile: csvfile
+        :param outFile: name of the outputfile
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
-        This function is used to print the results
+        This function is used to print the result
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in s:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
-
-if __name__ == "__main__":
+if __name__=="__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in s:", _ap.getRuntime())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
-
-
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/cuEclatBit.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclatBit.py`

 * *Files 4% similar despite different names*

```diff
@@ -280,15 +280,55 @@
         return bitRep
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        itemsList = sorted(list(set.union(*self._Database)))  # because Database is list
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self.arraysAndItems()
+
+        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                iList = list(keys[i])
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                for j in range(i+1, len(ArraysAndItems)):  
+                    jList = list(keys[j])
+                    union = []
+                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
+                        union = iList + [jList[-1]]
+                        union = tuple(union)
+                        unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
+                        sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
+                        self._sumKernel((len(unionData) // 32 + 1,), (32,), (unionData, sum, _ab._cp.uint32(len(unionData))))
+                        sum = sum[0]
+                        if sum >= self._minSup and union not in self._finalPatterns:
+                            newArraysAndItems[union] = unionData
+                            string = "\t".join(union)
+                            self._finalPatterns[string] = sum
+            ArraysAndItems = newArraysAndItems
+            # print()
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using cuEclatBit algorithm ")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
@@ -421,13 +461,12 @@
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
 """_ap = cuEclat("/home/tarun/PAMI/PAMI/frequentPattern/cuda/test.txt", 2, " ")
     _ap = cuEclat("/home/tarun/Transactional_T10I4D100K.csv", 450, "\t")
     _ap.startMine()
-    _ap.mine()
     print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
     print("Total Memory in USS:", _ap.getMemoryUSS())
     print("Total Memory in RSS", _ap.getMemoryRSS())
     print("Total ExecutionTime in s:", _ap.getRuntime())"""
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelECLAT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
-#
+# ParallelEclat is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+#  ----------------------------------------------------
+#
 #
-#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
+#             import PAMI.frequentPattern.pyspark.parallelECLAT as alg
 #
-#             obj = alg.cuAprioriGCT(iFile, minSup)
+#             obj = alg.parallelECLAT(iFile, minSup, numWorkers)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -30,14 +30,15 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,87 +48,85 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from deprecated import deprecated
-from PAMI.frequentPattern.basic import abstract as _ab
+from pyspark import SparkConf, SparkContext
 # import abstract as _ab
-
-import os
-import time
-import numpy as np
-import pycuda.gpuarray as gpuarray
-import psutil
+from PAMI.frequentPattern.pyspark import abstract as _ab
+from abc import ABC as _ABC, abstractmethod as _abstractmethod
+from deprecated import deprecated
 
 
-class cudaAprioriGCT(_ab._frequentPatterns):
+class parallelECLAT(_ab._frequentPatterns):
     """
-    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: ParallelEclat is an algorithm to discover frequent patterns in a transactional database.
+     This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 
-    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+    :Reference:
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
-                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  numPartitions: int :
+                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
+
 
     :Attributes:
 
         startTime : float
-              To record the start time of the mining process
+            To record the start time of the mining process
 
         endTime : float
-              To record the completion time of the mining process
+            To record the completion time of the mining process
 
         finalPatterns : dict
-              Storing the complete set of patterns in a dictionary variable
+            Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-              To store the total amount of USS memory consumed by the program
+            To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-              To store the total amount of RSS memory consumed by the program
-
-        Database : list
-              To store the transactions of a database in list
+            To store the total amount of RSS memory consumed by the program
 
+        lno : int
+            the number of transactions
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 parallelECLAT.py <inputFile> <outputFile> <minSup> <numWorkers>
 
       Example Usage:
 
-      (.venv) $ python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 parallelECLAT.py sampleDB.txt patterns.txt 10.0 3
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
+
     **Importing this algorithm into a python program**
     ----------------------------------------------------
-
     .. code-block:: python
 
-            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
+            import PAMI.frequentPattern.pyspark.parallelECLAT as alg
 
-            obj = alg.cuAprioriGCT(iFile, minSup)
+            obj = alg.parallelECLAT(iFile, minSup, numWorkers)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
@@ -145,158 +144,60 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    -------------
-
-                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    ----------------------------------------------------
+             The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    __time = 0
-    __memRSS = 0
-    __memUSS = 0
-    __GPU_MEM = 0
-    _minSup = 0
+    _minSup = float()
+    _numPartitions = int()
+    _startTime = float()
+    _endTime = float()
     _finalPatterns = {}
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _lno = int()
 
-    def __init__(self, filePath, minSup, sep):
-        self._iFile = filePath
-        self._sep = sep
-        self._minSup = minSup
-        self.__time = 0
-        self.__memRSS = 0
-        self.__memUSS = 0
-
-    def __creatingItemSets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        self.__Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
-        if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self.__Database.append(temp)
-            else:
-                try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self.__Database.append(temp)
-                except IOError:
-                    print("File Not Found")
-                    quit()
+    def __init__(self, iFile, minSup, numWorkers, sep="\t"):
+        super().__init__(iFile, minSup, int(numWorkers), sep)
 
-    def __convert(self, value):
-        """
-
-        To convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-
-        :type value: int or float or str
-
-        :return: converted type
-
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self.__Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self.__Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def compute_vertical_bitvector_data(self):
-        """
-        Converting database into bit vector
-        """
-        # ---build item to idx mapping---#
-        idx = 0
-        item2idx = {}
-        for transaction in self.__Database:
-            for item in transaction:
-                if not item in item2idx:
-                    item2idx[item] = idx
-                    idx += 1
-        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
-        # ---build vertical data---#
-        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
-        for trans_id, transaction in enumerate(self.__Database):
-            for item in transaction:
-                vb_data[item2idx[item], trans_id] = 1
-        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
-        return vb_data, idx2item
-
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-        Calculating the total amount of time taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self.__time
+
+        return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memRSS
+        return self._memoryRSS
 
-    def getMemoryUSS(self):
+    def getRuntime(self):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of runtime taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self.__memUSS
-
-    def getGPUMemory(self):
-        """
-        To calculate the total memory consumed by GPU
-        :return: return GPU memory
-        :rtype: int
-        """
-
-        return self.__GPU_MEM
-
-    def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
-        :return: returning frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
 
-    def get_numberOfPatterns(self):
-        return len(self._finalPatterns)
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
@@ -313,102 +214,206 @@
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            if type(x) == tuple:
-                pattern = ""
-                for item in x:
-                    pattern = pattern + str(item) + " "
-                s1 = pattern + ":" + str(y)
-            else:
-                s1 = str(x) + ":" + str(y)
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
+            
+    def printResults(self):
+        """
+        This method prints all the stats
+        """
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def getPatterns(self):
         """
-        Frequent pattern mining process will start from here
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
+        :rtype: dict
         """
-        self.mine()
+        return self._finalPatterns
 
-    def mine(self):
+    def _genPatterns(self, suffix, pattern, data):
         """
-        Frequent pattern mining process will start from here
+        This function is used to generate patterns
+        :param suffix: the suffix of the generated ptterns
+
+        :type suffix: str
+
+        :param pattern: the pattern of the generated ptterns
+
+        :type pattern: str
+
+        :param data: the data of the generated ptterns after completion of the mining process
+
+        :type data: str
         """
-        startTime = time.time()
-        basePattern = {}
-        final = {}
-
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
-
-        for i in range(len(vb_data)):
-            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern[idx2item[i]] = [i]
-                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
-
-        while len(basePattern) > 0:
-            temp = {}
-            keysList = list(basePattern.keys())
-            valuesList = list(basePattern.values())
-            for i in range(len(basePattern) - 1):
-                keyI = keysList[i].split(" ")
-                keyI = [int(x) for x in keyI]
-
-                for j in range(i + 1, len(basePattern)):
-                    keyJ = keysList[j].split(" ")
-                    keyJ = [int(x) for x in keyJ]
-                    values = set(valuesList[i])
-                    for val in valuesList[j]:
-                        values.add(val)
-                    values = list(sorted(values))
-                    totalArray = vb_data[values[0]]
-                    for k in range(1, len(values)):
-                        totalArray = totalArray.__mul__(vb_data[values[k]])
-                    support = gpuarray.sum(totalArray).get()
-                    if support >= self._minSup:
-                        combinedKey = " ".join(
-                            str(x) for x in sorted(set(keyI) | set(keyJ)))
-                        temp[combinedKey] = values
-                        final[str(combinedKey)] = support
-            basePattern = temp
-
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
+        freqPatterns = {}
+        index = data.index(suffix)
+        for i in range(index + 1, len(data)):
+            tid = pattern[1].intersection(data[i][1])
+            if len(tid) >= self._minSup:
+                freqPattern = pattern[0] + ' ' + data[i][0]
+                freqPatterns[freqPattern] = len(tid)
+                freqPatterns.update(self._genPatterns(data[i], (freqPattern, tid), data))
+        return freqPatterns
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Coverage Patterns:", len(self.getPatterns()))
-        print("GPU MEM: ", _ap.getGPUMemory())
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+        :param value: user specified minSup value
+        :type value: int or float or str
+        :return: converted type
+        """
+        print(value, type(value))
+        if type(value) is int:
+            value = int(value)
+        elif type(value) is float:
+            value = (self._lno * value)
+        elif type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._lno * value)
+            else:
+                value = int(value)
+        else:
+            print("None")
+        print(type(value), value)
+        return value
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
+        sc = SparkContext(conf=conf)
+
+        data = sc.textFile(self._iFile, self._numPartitions) \
+            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
+        self._lno = data.count()
+        self._minSup = self._convert(self._minSup)
+
+        frequentItems = None
+        frequentItems = data.zipWithIndex() \
+            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
+            .groupByKey() \
+            .filter(lambda x: len(x[1]) >= self._minSup) \
+            .sortBy(lambda x: len(x[1])) \
+            .mapValues(set) \
+            .persist()
+        data.unpersist()
+        # elif 'temporal' in self._iFile:
+        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
+        #         .groupByKey() \
+        #         .filter(lambda x: len(x[1]) >= self._minSup) \
+        #         .mapValues(set) \
+        #         .persist()
+        #     data.unpersist()
+        # else:
+        #     pass
+        #     # print("may be not able to process the input file")
+
+        freqItems = dict(frequentItems.collect())
+        # print(len(freqItems))
+        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
+
+        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
+                            .filter(lambda x: len(x) != 0).collect())
+        for value in freqPatterns:
+            self._finalPatterns.update(value)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
+        sc.stop()
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
+        sc = SparkContext(conf=conf)
+
+        data = sc.textFile(self._iFile, self._numPartitions) \
+            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
+        self._lno = data.count()
+        self._minSup = self._convert(self._minSup)
+
+        frequentItems = None
+        frequentItems = data.zipWithIndex() \
+            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
+            .groupByKey() \
+            .filter(lambda x: len(x[1]) >= self._minSup) \
+            .sortBy(lambda x: len(x[1])) \
+            .mapValues(set) \
+            .persist()
+        data.unpersist()
+        # elif 'temporal' in self._iFile:
+        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
+        #         .groupByKey() \
+        #         .filter(lambda x: len(x[1]) >= self._minSup) \
+        #         .mapValues(set) \
+        #         .persist()
+        #     data.unpersist()
+        # else:
+        #     pass
+        #     # print("may be not able to process the input file")
+
+        freqItems = dict(frequentItems.collect())
+        # print(len(freqItems))
+        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
+
+        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
+                            .filter(lambda x: len(x) != 0).collect())
+        for value in freqPatterns:
+            self._finalPatterns.update(value)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
+        sc.stop()
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+        _finalPatterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_finalPatterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# cudaAprioriTID is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 #
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
 #
-#             obj = alg.cuAprioriBit(iFile, minSup)
+#             obj = alg.cuAprioriGCT(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -30,15 +30,14 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,278 +47,227 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
 from deprecated import deprecated
-import abstract as _ab
+from PAMI.frequentPattern.basic import abstract as _ab
+# import abstract as _ab
 
 import os
-import csv
 import time
 import numpy as np
-import pycuda.gpuarray as _gpuarray
-import pycuda.autoinit
+import pycuda.gpuarray as gpuarray
 import psutil
-import pycuda.driver as cuda
-from pycuda.compiler import SourceModule
-import pycuda
-
-deviceIntersection = SourceModule("""
-    __global__ void intersection(int *compareThis, int *compareThat, int *resultStart,
-                                 int *values, int *result, int resultX, int resultY){
-        const int tidX = blockIdx.x * blockDim.x + threadIdx.x;
-        const int tidY = blockIdx.y * blockDim.y + threadIdx.y;
-        int resultIndex = resultStart[tidX] + tidY;
-
-        // ignore if tidX or tidY is out of bounds or if the value comparing with is 0
-        if (tidX > resultX-1 || tidY > resultY-1 || values[compareThis[tidX] + tidY] == 0) return;
-
-        for (int i = 0; i < resultY; i++){
-            if ( values[compareThat[tidX] + i] == 0) return;
-            if ( values[compareThis[tidX] + tidY] == values[compareThat[tidX] + i]){
-                result[resultIndex] = values[compareThis[tidX] + tidY];
-                return;
-            }
-        }
-
-        //result[resultIndex] = values[compareThis[tidX] + tidY];
-
-    }
-
-"""
-                                  )
 
 
-class cudaAprioriTID:
+class cudaAprioriGCT(_ab._frequentPatterns):
     """
-    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-            In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         startTime : float
-          To record the start time of the mining process
+              To record the start time of the mining process
 
         endTime : float
-          To record the completion time of the mining process
+              To record the completion time of the mining process
 
         finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
+              Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
+              To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
+              To store the total amount of RSS memory consumed by the program
 
         Database : list
-          To store the transactions of a database in list
+              To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cudaAprioriTID.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 cudaAprioriTID.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
-             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
 
-             obj = alg.cuAprioriBit(iFile, minSup)
+            obj = alg.cuAprioriGCT(iFile, minSup)
 
-             obj.mine()
+            obj.mine()
 
-             frequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-             print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-             obj.save(oFile)
+            obj.save(oFile)
 
-             Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternInDataFrame()
 
-             memUSS = obj.getMemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
-             print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-             memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-             print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-             run = obj.getRuntime()
+            run = obj.getRuntime()
 
-             print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
     -------------
 
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
     __time = 0
     __memRSS = 0
     __memUSS = 0
     __GPU_MEM = 0
-    filePath = ""
-    _iFile = " "
-    _sep = ""
     _minSup = 0
-    Patterns = {}
+    _finalPatterns = {}
 
-    def __init__(self, filePath, sep, minSup):
-        self.filePath = filePath
-        self.sep = sep
-        self.minSup = minSup
+    def __init__(self, filePath, minSup, sep):
+        self._iFile = filePath
+        self._sep = sep
+        self._minSup = minSup
         self.__time = 0
         self.__memRSS = 0
         self.__memUSS = 0
 
-    def _creatingItemSets(self):
+    def __creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._Database = {}
-        lineNumber = 1
+        self.__Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
+                self.__Database = self._iFile['Transactions'].tolist()
 
-            for k in temp:
-                self._Database.append(set(k))
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
-                    for i in range(len(line)):
-                        if line[i] in self._Database:
-                            self._Database[i].append(lineNumber)
-                        else:
-                            self._Database[i] = [lineNumber]
-                    lineNumber += 1
-
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    self.__Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            line = line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value):
+    def __convert(self, value):
         """
 
         To convert the type of user specified minSup value
 
         :param value: user specified minSup value
 
         :type value: int or float or str
 
         :return: converted type
 
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    """def _readFile(self, fileName, separator):
-        
-        Reads a file and stores the data in a dictionary
-
-        Args:
-            fileName: string
-            separator: string
-
-        Returns:
-            dictionary: dictionary
-        
-        file = open(fileName, 'r')
-        dictionary = {}
-        lineNumber = 1
-        for line in file:
-            line = line.strip()
-            line = line.split(separator)
-            for i in range(len(line)):
-                if line[i] in dictionary:
-                    dictionary[line[i]].append(lineNumber)
-                else:
-                    dictionary[line[i]] = [lineNumber]
-            lineNumber += 1
-
-        # sort dictionary by size of values
-        dictionary = dict(
-            sorted(dictionary.items(), key=lambda x: len(x[1]), reverse=True))
-        return dictionary, lineNumber
+    def compute_vertical_bitvector_data(self):
+        """
+        Converting database into bit vector
         """
+        # ---build item to idx mapping---#
+        idx = 0
+        item2idx = {}
+        for transaction in self.__Database:
+            for item in transaction:
+                if not item in item2idx:
+                    item2idx[item] = idx
+                    idx += 1
+        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
+        # ---build vertical data---#
+        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
+        for trans_id, transaction in enumerate(self.__Database):
+            for item in transaction:
+                vb_data[item2idx[item], trans_id] = 1
+        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
+        return vb_data, idx2item
+
     def getRuntime(self):
         """
         Calculating the total amount of time taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
         return self.__time
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+
         return self.__memRSS
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -337,155 +285,174 @@
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self.Patterns
+        return self._finalPatterns
 
     def get_numberOfPatterns(self):
-        return len(self.Patterns)
+        return len(self._finalPatterns)
+
+    def getPatternsAsDataFrame(self):
+        """
+        Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
+        :rtype: pd.DataFrame
+        """
+
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
+
+    def save(self, outFile):
+        """
+        Complete set of frequent patterns will be loaded in to an output file
+        :param outFile: name of the output file
+        :type outFile: csvfile
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            if type(x) == tuple:
+                pattern = ""
+                for item in x:
+                    pattern = pattern + str(item) + " "
+                s1 = pattern + ":" + str(y)
+            else:
+                s1 = str(x) + ":" + str(y)
+            writer.write("%s \n" % s1)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+        startTime = time.time()
+        basePattern = {}
+        final = {}
+
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern[idx2item[i]] = [i]
+                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
+
+        while len(basePattern) > 0:
+            temp = {}
+            keysList = list(basePattern.keys())
+            valuesList = list(basePattern.values())
+            for i in range(len(basePattern) - 1):
+                keyI = keysList[i].split(" ")
+                keyI = [int(x) for x in keyI]
+
+                for j in range(i + 1, len(basePattern)):
+                    keyJ = keysList[j].split(" ")
+                    keyJ = [int(x) for x in keyJ]
+                    values = set(valuesList[i])
+                    for val in valuesList[j]:
+                        values.add(val)
+                    values = list(sorted(values))
+                    totalArray = vb_data[values[0]]
+                    for k in range(1, len(values)):
+                        totalArray = totalArray.__mul__(vb_data[values[k]])
+                    support = gpuarray.sum(totalArray).get()
+                    if support >= self._minSup:
+                        combinedKey = " ".join(
+                            str(x) for x in sorted(set(keyI) | set(keyJ)))
+                        temp[combinedKey] = values
+                        final[str(combinedKey)] = support
+            basePattern = temp
+
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
-        dev_Intersection = deviceIntersection.get_function("intersection")
         startTime = time.time()
+        basePattern = {}
         final = {}
 
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
         minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
 
-
-        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
-        for key, value in data.items():
-            final[key] = len(value)
-
-        while len(data) > 1:
-            # sort data by size of values
-            data = dict(
-                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
-
-            values = list(data.values())
-            maxLength = values[0]
-            for i in range(1, len(values)):
-                while len(values[i]) != len(maxLength):
-                    values[i].append(0)
-
-            values = np.array(values)
-            resultSize = 0
-
-            compareThis = []
-            compareThat = []
-            resultStart = []
-            counter = 0
-
-            for i in range(len(values)):
-                for j in range(i+1, len(values)):
-                    resultSize += 1
-                    compareThis.append(i*len(maxLength))
-                    compareThat.append(j*len(maxLength))
-                    resultStart.append(counter)
-                    counter += len(maxLength)
-            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
-
-            # convert all to uint32
-            compareThis = np.array(compareThis, dtype=np.uint32)
-            compareThat = np.array(compareThat, dtype=np.uint32)
-            resultStart = np.array(resultStart, dtype=np.uint32)
-            values = np.array(values, dtype=np.uint32)
-            result = np.array(result, dtype=np.uint32)
-
-            # allocate memory on GPU
-            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
-            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
-            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
-            values_gpu = cuda.mem_alloc(values.nbytes)
-            result_gpu = cuda.mem_alloc(result.nbytes)
-
-            # add all nbytes to GPU_MEM
-            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
-            if sumBytes > self.__GPU_MEM:
-                self.__GPU_MEM = sumBytes
-
-            # copy data to GPU
-            cuda.memcpy_htod(compareThis_gpu, compareThis)
-            cuda.memcpy_htod(compareThat_gpu, compareThat)
-            cuda.memcpy_htod(resultStart_gpu, resultStart)
-            cuda.memcpy_htod(values_gpu, values)
-            cuda.memcpy_htod(result_gpu, result)
-
-            blockDim = (32, 32, 1)
-            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
-
-            dev_Intersection(compareThis_gpu, compareThat_gpu,
-                             resultStart_gpu, values_gpu, result_gpu,
-                             np.uint32(resultSize), np.uint32(len(maxLength)),
-                             block=blockDim, grid=gridDim)
-
-            # copy data back to CPU
-            cuda.Context.synchronize()
-            cuda.memcpy_dtoh(result, result_gpu)
-
-            # free GPU memory
-            cuda.DeviceAllocation.free(compareThis_gpu)
-            cuda.DeviceAllocation.free(compareThat_gpu)
-            cuda.DeviceAllocation.free(resultStart_gpu)
-            cuda.DeviceAllocation.free(values_gpu)
-            cuda.DeviceAllocation.free(result_gpu)
-
-            keys = list(data.keys())
-            # convert all to string and add " "
-            for i in range(len(keys)):
-                keys[i] = str(keys[i]) + " "
-            data = {}
-            index = 0
-            for i in range(len(keys)):
-                for j in range(i+1, len(keys)):
-                    newResult = list(sorted(set(result[index])))
-                    newResult = list(filter(lambda x: x > 0, newResult))
-                    if len(newResult) >= self.minSup:
-                        keyI = keys[i].split()
-                        keyJ = keys[j].split()
-                        combinedKey = " ".join(list(str(x) for x in (
-                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
-                        if combinedKey not in final:
-                            data[combinedKey] = newResult
-                            final[combinedKey] = len(newResult)
-                    index += 1
-
+        for i in range(len(vb_data)):
+            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern[idx2item[i]] = [i]
+                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
+
+        while len(basePattern) > 0:
+            temp = {}
+            keysList = list(basePattern.keys())
+            valuesList = list(basePattern.values())
+            for i in range(len(basePattern) - 1):
+                keyI = keysList[i].split(" ")
+                keyI = [int(x) for x in keyI]
+
+                for j in range(i + 1, len(basePattern)):
+                    keyJ = keysList[j].split(" ")
+                    keyJ = [int(x) for x in keyJ]
+                    values = set(valuesList[i])
+                    for val in valuesList[j]:
+                        values.add(val)
+                    values = list(sorted(values))
+                    totalArray = vb_data[values[0]]
+                    for k in range(1, len(values)):
+                        totalArray = totalArray.__mul__(vb_data[values[k]])
+                    support = gpuarray.sum(totalArray).get()
+                    if support >= self._minSup:
+                        combinedKey = " ".join(
+                            str(x) for x in sorted(set(keyI) | set(keyJ)))
+                        temp[combinedKey] = values
+                        final[str(combinedKey)] = support
+            basePattern = temp
 
         self.__time = time.time() - startTime
         self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
         self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self.Patterns = final
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
+
+    def printResults(self):
+        """
+        This function is used to print the results
+        """
+        print("Total number of Coverage Patterns:", len(self.getPatterns()))
+        print("GPU MEM: ", _ap.getGPUMemory())
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("GPU MEM: ", _ap.getGPUMemory())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
-
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,21 +1,20 @@
-# cudaEclatGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.frequentPattern.cuda.cudaEclatGCT as alg
+
+#             import PAMI.periodicFrequentPattern.kPFPMiner as alg
 #
-#             obj = alg.FPGrowth(iFile, minSup)
+#             obj = alg.kPFPMiner(iFile, k)
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -28,115 +27,131 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
 """
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
-from PAMI.frequentPattern.basic import abstract as _ab
-
-minSup = str()
-_ab._sys.setrecursionlimit(20000)
 
-import os
-import csv
-import time
-import numpy as np
-import pycuda.gpuarray as _gpuarray
-import pycuda.autoinit
-import psutil
+from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
 
 
-class cudaEclatGCT:
+class kPFPMiner(_ab._periodicFrequentPatterns):
     """
-    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
 
-    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
+                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
+                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   Name of the output file to store complete set of periodic frequent pattern's
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        startTime : float
+        iFile : str
+            Input file name or path of the input file
+        k: int
+            User specified counte of top-k periodic frequent patterns
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+        oFile : str
+            Name of the output file or the path of the output file
+        startTime:float
             To record the start time of the mining process
-
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
-
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
-
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
-        Database : list
-            To store the transactions of a database in list
-
+    :Methods:
 
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        savePatterns(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Generates one frequent patterns
+        eclatGeneration(candidateList)
+            It will generate the combinations of frequent items
+        generateFrequentPatterns(tidList)
+            It will generate the combinations of frequent items from a list of items
 
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
-
+    **Executing the code on terminal:**
+    ------------------------------------------
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 cudaEclatGCT.py <inputFile> <outputFile> <minSup>
+       Format:
+
 
-      Example Usage:
+       (.venv) $ python3 kPFPMiner.py <inputFile> <outputFile> <k>
 
-      (.venv) $ python3 cudaEclatGCT.py sampleDB.txt patterns.txt 10.0
+       Examples :
 
-    .. note:: minSup will be considered in percentage of database transactions
+       (.venv) $  python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    **Sample run of the importing code:
+    --------------------------------------
     .. code-block:: python
 
-            import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+            import PAMI.periodicFrequentPattern.kPFPMiner as alg
 
-            obj = alg.cuAprioriBit(iFile, minSup)
+            obj = alg.kPFPMiner(iFile, k)
 
-            obj.mine()
+            obj.startMine()
 
-            frequentPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -146,255 +161,324 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-
     **Credits:**
-    -------------
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    --------------
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    __time = 0
-    __memRSS = 0
-    __memUSS = 0
-    __GPU_MEM = 0
-    _minSup = 0
+    _startTime = float()
+    _endTime = float()
+    _k = int()
     _finalPatterns = {}
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _Database = []
+    _tidList = {}
+    lno = int()
+    _maximum = int()
 
-    def __init__(self, filePath, minSup, sep):
-        self._iFile = filePath
-        self._sep = sep
-        self._minSup = minSup
-        self.__time = 0
-        self.__memRSS = 0
-        self.__memUSS = 0
-
-    """def read_data(self, data_path, sep):
-
-
-        data = []
-        if not os.path.isfile(data_path):
-            raise ValueError('Invalid data path.' + data_path)
-        with open(data_path, 'r') as f:
-            file = csv.reader(f, delimiter=sep, quotechar='\r')
-            lineNo = 1
-            for row in file:
-                data.append([str(item) for item in row if item != ''])
-                lineNo += 1
-        return data, lineNo"""
-
-    def __creatingItemSets(self):
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self.__Database = []
+
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self.__Database.append(temp)
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.strip()
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self.__Database.append(temp)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
+                    
+    def getPer_Sup(self, tids):
+        tids.sort()
+        cur=0
+        per=list()
+        sup=0
+        #print(tids)
+        for i in range(len(tids)-1):
+            j = i + 1
+            #if tids[j] - cur <= periodicity:
+                #return [0,0]
+            per.append(tids[j] - cur)
+            cur = tids[j]
+        per.append(self.lno - cur)
+        return max(per)
+
+    def _frequentOneItem(self):
+        """
+        Generating one frequent patterns
+        """
+        self._mapSupport = {}
+        self._tidList = {}
+        n = 0
+        for line in self._Database:
+            self.lno += 1
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [1, abs(0 - n), n]
+                    self._tidList[si] = [n]
+                else:
+                    self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    self._mapSupport[si][2] = n
+                    self._tidList[si].append(n)
+        for x, y in self._mapSupport.items():
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        for i in plist:
+            if len(self._finalPatterns) >= self._k:
+                break
+            else:
+                self._finalPatterns[i] = self._mapSupport[i][1]
+        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+        plist = list(self._finalPatterns.keys())
+        return plist
+
+
+    def _save(self, prefix, suffix, tidSetI):
+        """Saves the patterns that satisfy the periodic frequent property.
+
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: list
+        """
+
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = self.getPer_Sup(tidSetI)
+        sample = str()
+        for i in prefix:
+            sample = sample + i + " "
+        if len(self._finalPatterns) < self._k:
+            if val < self._maximum:
+                self._finalPatterns[sample] = val
+                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._maximum = max([i for i in self._finalPatterns.values()])
+        else:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
+                if val < y:
+                    del self._finalPatterns[x]
+                    self._finalPatterns[sample] = val
+                    self._finalPatterns = {k: v for k, v in
+                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
+                                                     reverse=True)}
+                    self._maximum = max([i for i in self._finalPatterns.values()])
+                    return
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetI).intersection(tidSetJ))
+                if self.getPer_Sup(y) <= self._maximum:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
 
-    def __convert(self, value):
+    def _convert(self, value):
         """
-
-        To convert the type of user specified minSup value
+        to convert the type of user specified minSup value
 
         :param value: user specified minSup value
-
-        :type value: int or float or str
-
         :return: converted type
-
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self.__Database) * value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self.__Database) * value)
+                value = ((len(self._Database)) * value)
             else:
                 value = int(value)
         return value
 
-    def compute_vertical_bitvector_data(self):
-        """
-        Converting  database into bit vector
+    def startMine(self):
         """
-        # ---build item to idx mapping---#
-        idx = 0
-        item2idx = {}
-        for transaction in self.__Database:
-            for item in transaction:
-                if not item in item2idx:
-                    item2idx[item] = idx
-                    idx += 1
-        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
-        # ---build vertical data---#
-        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
-        for trans_id, transaction in enumerate(self.__Database):
-            for item in transaction:
-                vb_data[item2idx[item], trans_id] = 1
-        vb_data = _gpuarray.to_gpu(vb_data.astype(np.uint16))
-        return vb_data, idx2item
+        Main function of the program
 
-    def getRuntime(self):
         """
-        Calculating the total amount of time taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._k = self._convert(self._k)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if self.getPer_Sup(y1) <= self._maximum:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("kPFPMiner has successfully generated top-k frequent patterns")
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
+    def getMemoryUSS(self):
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self.__time
+
+        return self._memoryUSS
 
     def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-        return self.__memRSS
 
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        return self._memoryRSS
+
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self.__memUSS
 
-    def getGPUMemory(self):
-        """
-        To calculate the total memory consumed by GPU
-        :return: return GPU memory
-        :rtype: int
+        return self._endTime - self._startTime
+
+    def getPatternsAsDataFrame(self):
+        """Storing final frequent patterns in a dataframe
+
+        :return: returning frequent patterns in a dataframe
+        :rtype: pd.DataFrame
         """
 
-        return self.__GPU_MEM
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
+        return dataFrame
+
+    def save(self, outFile):
+        """Complete set of frequent patterns will be loaded in to a output file
+
+        :param outFile: name of the output file
+
+        :type outFile: file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def get_numberOfPatterns(self):
-
-        return len(self._finalPatterns)
-
-    def eclat(self, basePattern, final, vb_data, idx2item, item2idx):
-        """
-        param basePattern: base pattern used for the mining process after completion of the mining process
-        type basePattern:
-        param final: final pattern used for the mining process after completion of the mining process
-        type final:
-        param vb_data: vb_data used for the mining process after completion of the mining process
-        type vb_data:
-        param idx2item: idx2item used for the mining process after completion of the mining process
-        type idx2item:
-        param item2idx: item2idx used for the mining process after completion of the mining process
-        type item2idx:
-        """
-        newBasePattern = []
-        for i in range(0, len(basePattern)):
-            item1 = basePattern[i]
-            i1_list = item1.split()
-            for j in range(i + 1, len(basePattern)):
-                item2 = basePattern[j]
-                i2_list = item2.split()
-                if i1_list[:-1] == i2_list[:-1]:
-                    unionOfKey = list(set(i1_list) | set(i2_list))
-                    unionOfKey.sort()
-                    valueList = []
-                    for key in unionOfKey:
-                        valueList.append(item2idx[key])
-                    total = vb_data[valueList[0]]
-                    for k in range(1, len(valueList)):
-                        total = total.__mul__(vb_data[valueList[k]])
-                    support = _gpuarray.sum(total).get()
-                    if support >= self._minSup:
-                        newBasePattern.append(" ".join(unionOfKey))
-                        final[" ".join(unionOfKey)] = support
-
-        if len(newBasePattern) > 0:
-            self.eclat(newBasePattern, final, vb_data, idx2item, item2idx)
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self.mine()
-
-    def mine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        startTime = time.time()
-        basePattern = []
-        final = {}
-
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
-
-        for i in range(len(vb_data)):
-            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern.append(idx2item[i])
-                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
-
-        # reverse idx2item
-        item2idx = {idx2item[i]: i for i in idx2item}
-        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
+    def printResults(self):
+        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ap._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
+        _ap.save(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py` & `pami-2024.4.9.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -645,15 +645,46 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Mining process will start from this function
         """
 
-        self.mine()
+        global _minSup
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        _minSup = self._minSup
+        generatedItems, pfList = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(generatedItems)
+        for x, y in self._rank.items():
+            self._rankdup[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        patterns = {}
+        self._finalPatterns = {}
+        self._maximalTree = _MPTree()
+        Tree = self._buildTree(updatedTransactions, info)
+        Tree.generatePatterns([], patterns, self._maximalTree)
+        for x, y in patterns.items():
+            pattern = str()
+            x = self._convertItems(x)
+            for i in x:
+                pattern = pattern + i + "\t"
+            self._finalPatterns[pattern] = y
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Maximal Frequent patterns were generated successfully using MaxFp-Growth algorithm ")
 
     def mine(self):
         """
         Mining process will start from this function
         """
 
         global _minSup
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/maximal/__init__.py` & `pami-2024.4.9.1/PAMI/frequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/maximal/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/pyspark/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/pyspark/parallelApriori.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelApriori.py`

 * *Files 3% similar despite different names*

```diff
@@ -361,15 +361,38 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+
+        # setting SparkConf and SparkContext to process in parallel
+        conf = _ab._SparkConf().setAppName("parallelApriori").setMaster("local[*]")
+        sc = _ab._SparkContext(conf=conf)
+        # sc.addFile("file:///home/hadoopuser/Spark_code/abstract.py")
+
+        # read database from iFile
+        database = sc.textFile(self._iFile, self._numPartitions).map(
+            lambda x: {int(y) for y in x.rstrip().split(self._sep)})
+        self._lno = database.count()
+        # Calculating minSup as a percentage
+        self._minSup = self._convert(self._minSup)
+
+        oneFrequentItems = self._genFrequentItems(database)
+        self._finalPatterns = oneFrequentItems
+        self._getAllFrequentPatterns(database, oneFrequentItems)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel Apriori algorithm")
+        sc.stop()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/pyspark/parallelECLAT.py` & `pami-2024.4.9.1/PAMI/frequentPattern/topk/FAE.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,22 +1,21 @@
-# ParallelEclat is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+# Top - K is and algorithm to discover top frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
-#  ----------------------------------------------------
+# ---------------------------------------------------------
 #
+#             import PAMI.frequentPattern.topK.FAE as alg
 #
-#             import PAMI.frequentPattern.pyspark.parallelECLAT as alg
-#
-#             obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+#             obj = alg.FAE(iFile, K)
 #
 #             obj.mine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             topKFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -48,328 +47,442 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from pyspark import SparkConf, SparkContext
-# import abstract as _ab
-from PAMI.frequentPattern.pyspark import abstract as _ab
-from abc import ABC as _ABC, abstractmethod as _abstractmethod
+from PAMI.frequentPattern.topk import abstract as _ab
 from deprecated import deprecated
 
 
-class parallelECLAT(_ab._frequentPatterns):
+class FAE(_ab._frequentPatterns):
     """
-    :Description: ParallelEclat is an algorithm to discover frequent patterns in a transactional database.
-     This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+    :Description: Top - K is and algorithm to discover top frequent patterns in a transactional database.
+
 
-    :Reference:
+    :Reference:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261  Source: IEEE Xplore
+                  https://ieeexplore.ieee.org/document/4370261
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param  k: int :
+                    User specified count of top frequent patterns
+    :param minimum: int :
+                    Minimum number of frequent patterns to consider in analysis
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  numPartitions: int :
-                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
+
 
 
     :Attributes:
 
         startTime : float
-            To record the start time of the mining process
+          To record the start time of the mining process
 
         endTime : float
-            To record the completion time of the mining process
+          To record the completion time of the mining process
 
         finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+          Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+          To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+          To store the total amount of RSS memory consumed by the program
 
-        lno : int
-            the number of transactions
+        finalPatterns : dict
+            it represents to store the patterns
 
 
     **Methods to execute code on terminal**
-    ----------------------------------------------------
+    -------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 parallelECLAT.py <inputFile> <outputFile> <minSup> <numWorkers>
+      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
 
       Example Usage:
 
-      (.venv) $ python3 parallelECLAT.py sampleDB.txt patterns.txt 10.0 3
+      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: k will be considered as count of top frequent patterns to consider in analysis
 
 
 
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    ---------------------------------------------------------
     .. code-block:: python
 
-            import PAMI.frequentPattern.pyspark.parallelECLAT as alg
-
-            obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+        import PAMI.frequentPattern.topK.FAE as alg
 
-            obj.mine()
+        obj = alg.FAE(iFile, K)
 
-            frequentPatterns = obj.getPatterns()
+        obj.mine()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+        topKFrequentPatterns = obj.getPatterns()
 
-            obj.save(oFile)
+        print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
 
-            Df = obj.getPatternInDataFrame()
+        obj.save(oFile)
 
-            memUSS = obj.getMemoryUSS()
+        Df = obj.getPatternInDataFrame()
 
-            print("Total Memory in USS:", memUSS)
+        memUSS = obj.getMemoryUSS()
 
-            memRSS = obj.getMemoryRSS()
+        print("Total Memory in USS:", memUSS)
 
-            print("Total Memory in RSS", memRSS)
+        memRSS = obj.getMemoryRSS()
 
-            run = obj.getRuntime()
+        print("Total Memory in RSS", memRSS)
 
-            print("Total ExecutionTime in seconds:", run)
+        run = obj.getRuntime()
 
+        print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    ----------------------------------------------------
-             The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+    Credits:
+    --------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _minSup = float()
-    _numPartitions = int()
     _startTime = float()
     _endTime = float()
+    _k = int()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _lno = int()
+    _Database = []
+    _tidList = {}
+    _minimum = int()
+
+    def _creatingItemSets(self):
+        """
+            Storing the complete transactions of the database/input file in a database variable
+
+        """
 
-    def __init__(self, iFile, minSup, numWorkers, sep="\t"):
-        super().__init__(iFile, minSup, int(numWorkers), sep)
+        self._Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'Transactions' in i:
+                self._Database = self._iFile['Transactions'].tolist()
+
+            # print(self.Database)
+        if isinstance(self._iFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
+            else:
+                try:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(temp)
+                except IOError:
+                    print("File Not Found")
+                    quit()
+
+    def _frequentOneItem(self):
+        """
+        Generating one frequent patterns
+        """
+        candidate = {}
+        self._tidList = {}
+        for i in range(len(self._Database)):
+            for j in self._Database[i]:
+                if j not in candidate:
+                    candidate[j] = 1
+                    self._tidList[j] = [i]
+                else:
+                    candidate[j] += 1
+                    self._tidList[j].append(i)
+        self._finalPatterns = {}
+        plist = [key for key, value in sorted(candidate.items(), key=lambda x: x[1], reverse=True)]
+        for i in plist:
+            if len(self._finalPatterns) >= self._k:
+                break
+            else:
+                self._finalPatterns[i] = candidate[i]
+        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+        plist = list(self._finalPatterns.keys())
+        return plist
+
+    def _save(self, prefix, suffix, tidSetI):
+        """Saves the patterns that satisfy the periodic frequent property.
+
+            :param prefix: the prefix of a pattern
+            :type prefix: list
+            :param suffix: the suffix of a patterns
+            :type suffix: list
+            :param tidSetI: the timestamp of a patterns
+            :type tidSetI: list
+        """
+
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = len(tidSetI)
+        sample = str()
+        for i in prefix:
+            sample = sample + i + "\t"
+        if len(self._finalPatterns) < self._k:
+            if val > self._minimum:
+                self._finalPatterns[sample] = val
+                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._minimum = min([i for i in self._finalPatterns.values()])
+        else:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
+                if val > y:
+                    del self._finalPatterns[x]
+                    self._finalPatterns[sample] = val
+                    self._finalPatterns = {k: v for k, v in
+                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
+                                                     reverse=True)}
+                    self._minimum = min([i for i in self._finalPatterns.values()])
+                    return
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+            :param prefix:  main equivalence prefix
+            :type prefix: periodic-frequent item or pattern
+            :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
+                            and frequent with their timestamps
+            :type itemSets: list
+            :param tidSets: timestamps of the items in the argument itemSets
+            :type tidSets: list
+
+
+                    """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetI).intersection(tidSetJ))
+                if len(y) >= self._minimum:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
+
+    def _convert(self, value):
+        """
+        to convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :type value: int or float or str
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = ((len(self._Database)) * value)
+            else:
+                value = int(value)
+        return value
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+            Main function of the program
+        """
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._k = self._convert(self._k)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if len(y1) >= self._minimum:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
+    def mine(self):
+        """
+            Main function of the program
+        """
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._k = self._convert(self._k)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if len(y1) >= self._minimum:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
+
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
+
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
+
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
+
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
+            data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
-        :type outFile: csvfile
+
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
-            writer.write("%s \n" % s1)
-            
-    def printResults(self):
-        """
-        This method prints all the stats
-        """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
+
         :rtype: dict
         """
         return self._finalPatterns
 
-    def _genPatterns(self, suffix, pattern, data):
-        """
-        This function is used to generate patterns
-        :param suffix: the suffix of the generated ptterns
-
-        :type suffix: str
-
-        :param pattern: the pattern of the generated ptterns
-
-        :type pattern: str
-
-        :param data: the data of the generated ptterns after completion of the mining process
-
-        :type data: str
-        """
-        freqPatterns = {}
-        index = data.index(suffix)
-        for i in range(index + 1, len(data)):
-            tid = pattern[1].intersection(data[i][1])
-            if len(tid) >= self._minSup:
-                freqPattern = pattern[0] + ' ' + data[i][0]
-                freqPatterns[freqPattern] = len(tid)
-                freqPatterns.update(self._genPatterns(data[i], (freqPattern, tid), data))
-        return freqPatterns
-
-    def printResults(self):
+    def printTOPK(self):
         """
         This function is used to print the results
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Top K Frequent  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-
-    def _convert(self, value):
-        """
-        To convert the user specified minSup value
-        :param value: user specified minSup value
-        :type value: int or float or str
-        :return: converted type
-        """
-        print(value, type(value))
-        if type(value) is int:
-            value = int(value)
-        elif type(value) is float:
-            value = (self._lno * value)
-        elif type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        else:
-            print("None")
-        print(type(value), value)
-        return value
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self.mine()
-
-    def mine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-
-        self._startTime = _ab._time.time()
-        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
-        sc = SparkContext(conf=conf)
-
-        data = sc.textFile(self._iFile, self._numPartitions) \
-            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
-        self._lno = data.count()
-        self._minSup = self._convert(self._minSup)
-
-        frequentItems = None
-        frequentItems = data.zipWithIndex() \
-            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
-            .groupByKey() \
-            .filter(lambda x: len(x[1]) >= self._minSup) \
-            .sortBy(lambda x: len(x[1])) \
-            .mapValues(set) \
-            .persist()
-        data.unpersist()
-        # elif 'temporal' in self._iFile:
-        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
-        #         .groupByKey() \
-        #         .filter(lambda x: len(x[1]) >= self._minSup) \
-        #         .mapValues(set) \
-        #         .persist()
-        #     data.unpersist()
-        # else:
-        #     pass
-        #     # print("may be not able to process the input file")
-
-        freqItems = dict(frequentItems.collect())
-        # print(len(freqItems))
-        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
-
-        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
-                            .filter(lambda x: len(x) != 0).collect())
-        for value in freqPatterns:
-            self._finalPatterns.update(value)
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
-        sc.stop()
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        _finalPatterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_finalPatterns))
+        print("Top K Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py` & `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -261,28 +261,65 @@
         super().__init__(iFile, minSup, int(numWorkers), sep)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+
+        self._startTime = _ab._time.time()
+
+        conf = _SparkConf().setAppName("Parallel FPGrowth").setMaster("local[*]")
+        sc = _SparkContext(conf=conf)
+
+        rdd = sc.textFile(self._iFile, self._numPartitions)\
+            .map(lambda x: x.rstrip().split('\t'))\
+            .persist()
+
+        self._lno = rdd.count()
+        self._minSup = self._convert(self._minSup)
+
+        freqItems = rdd.flatMap(lambda trans: [(item, 1) for item in trans])\
+            .reduceByKey(add)\
+            .filter(lambda x: x[1] >= self._minSup)\
+            .sortBy(lambda x: x[1], ascending=False)\
+            .collect()
+        self._finalPatterns = dict(freqItems)
+        self._FPList = [x[0] for x in freqItems]
+        rank = dict([(item, index) for (index, item) in enumerate(self._FPList)])
+
+        workByPartition = rdd.flatMap(lambda x: self.genCondTransaction(x, rank)).groupByKey()
+
+        trees = workByPartition.foldByKey(Tree(), lambda tree, data: self.buildTree(tree, data))
+        freqPatterns = trees.flatMap(lambda tree_tuple: self.genAllFrequentPatterns(tree_tuple))
+        result = freqPatterns.map(lambda ranks_count: (tuple([self._FPList[z] for z in ranks_count[0]]), ranks_count[1]))\
+            .collect()
+
+        self._finalPatterns.update(dict(result))
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        sc.stop()
+
+        print("Frequent patterns were generated successfully using Parallel FPGrowth algorithm")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
 
         conf = _SparkConf().setAppName("Parallel FPGrowth").setMaster("local[*]")
         sc = _SparkContext(conf=conf)
 
         rdd = sc.textFile(self._iFile, self._numPartitions)\
-            .map(lambda x: x.rstrip().split(self._sep))\
+            .map(lambda x: x.rstrip().split('\t'))\
             .persist()
 
         self._lno = rdd.count()
         self._minSup = self._convert(self._minSup)
 
         freqItems = rdd.flatMap(lambda trans: [(item, 1) for item in trans])\
             .reduceByKey(add)\
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/topk/FAE.py` & `pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-# Top - K is and algorithm to discover top frequent patterns in a transactional database.
+# VBFTMine is one of the fundamental algorithm to discover fault-tolerant frequent patterns in an uncertain transactional database based on bitset representation.
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+# --------------------------------------------------------
 #
-#             import PAMI.frequentPattern.topK.FAE as alg
 #
-#             obj = alg.FAE(iFile, K)
+#             import PAMI.uncertainFaultTolerantFrequentPattern.basic.VBFTMine as alg
 #
-#             obj.mine()
+#             obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 #
-#             topKFrequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+#             faultTolerantFrequentPattern = obj.getPatterns()
+#
+#             print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -26,20 +27,16 @@
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
-
-
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,40 +44,49 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.frequentPattern.topk import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
 
+import numpy as _np
+from PAMI.faultTolerantFrequentPattern.basic import abstract as _ab
 
-class FAE(_ab._frequentPatterns):
+class VBFTMine(_ab._faultTolerantFrequentPatterns):
     """
-    :Description: Top - K is and algorithm to discover top frequent patterns in a transactional database.
-
-
-    :Reference:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261  Source: IEEE Xplore
-                  https://ieeexplore.ieee.org/document/4370261
+    
+    :Description:  VBFTMine is one of the fundamental algorithm to discover fault tolerant frequent patterns in an uncertain transactional database based on
+                   bitset representation.
+                   This program employs apriori property (or downward closure property) to  reduce the search space effectively.
+
+    :Reference:   Koh, JL., Yo, PW. (2005). An Efficient Approach for Mining Fault-Tolerant Frequent Patterns Based on Bit Vector Representations.
+                  In: Zhou, L., Ooi, B.C., Meng, X. (eds) Database Systems for Advanced Applications. DASFAA 2005. Lecture Notes in Computer Science,
+                  vol 3453. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11408079_51
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of uncertain Fault Tolerant FrequentFrequent Patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  k: int :
-                    User specified count of top frequent patterns
-    :param minimum: int :
-                    Minimum number of frequent patterns to consider in analysis
-
+                   Name of the output file to store complete set of uncertain Fault Tolerant FrequentFrequent Patterns
+    :param  minSup: float or int or str :
+                    The user can specify minSup either in count or proportion of database size.
+                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                    Otherwise, it will be treated as float.
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  itemSup: int or float :
+                    Frequency of an item
+    :param minLength: int
+                    minimum length of a pattern
+    :param faultTolerance: int :
+                    The ability of a pattern mining algorithm to handle errors or inconsistencies in the data without completely failing or producing incorrect results.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-
-
     :Attributes:
 
         startTime : float
           To record the start time of the mining process
 
         endTime : float
           To record the completion time of the mining process
@@ -90,371 +96,377 @@
 
         memoryUSS : float
           To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
-        finalPatterns : dict
-            it represents to store the patterns
-
+        Database : list
+          To store the transactions of a database in list
 
-    **Methods to execute code on terminal**
-    -------------------------------------------
 
+    **Executing the code on terminal**:
+    ------------------------------------
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
+       Format:
 
-      Example Usage:
+       (.venv) $ python3 VBFTMine.py <inputFile> <outputFile> <minSup> <itemSup> <minLength> <faultTolerance>
 
-      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10
+       Examples usage:
 
-    .. note:: k will be considered as count of top frequent patterns to consider in analysis
-
-
-
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------------
-    .. code-block:: python
+       (.venv) $ python3 VBFTMine.py sampleDB.txt patterns.txt 10.0 3.0 3 1
 
-        import PAMI.frequentPattern.topK.FAE as alg
 
-        obj = alg.FAE(iFile, K)
+               .. note:: minSup will be considered in times of minSup and count of database transactions
 
-        obj.mine()
 
-        topKFrequentPatterns = obj.getPatterns()
+    **Sample run of the importing code**:
+    --------------------------------------------
+    .. code-block:: python
+    
+            import PAMI.faultTolerantFrequentPattern.basic.VBFTMine as alg
 
-        print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+            obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 
-        obj.save(oFile)
+            obj.startMine()
 
-        Df = obj.getPatternInDataFrame()
+            faultTolerantFrequentPattern = obj.getPatterns()
 
-        memUSS = obj.getMemoryUSS()
+            print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 
-        print("Total Memory in USS:", memUSS)
+            obj.save(oFile)
 
-        memRSS = obj.getMemoryRSS()
+            Df = obj.getPatternInDataFrame()
 
-        print("Total Memory in RSS", memRSS)
+            print("Total Memory in USS:", obj.getMemoryUSS())
 
-        run = obj.getRuntime()
+            print("Total Memory in RSS", obj.getMemoryRSS())
 
-        print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", obj.getRuntime())
 
-    Credits:
-    --------
+    **Credits**:
+    ------------
         The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
+    _minSup = float()
+    _itemSup = float()
+    _minLength = int()
+    _faultTolerance = int()
     _startTime = float()
     _endTime = float()
-    _k = int()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
+    _plist = []
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
-    _minimum = int()
+    _mapSupport = {}
 
     def _creatingItemSets(self):
         """
-            Storing the complete transactions of the database/input file in a database variable
-
+        Storing the complete transactions of the database/input file in a database variable
         """
-
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                temp = self._iFile['Transactions'].tolist()
 
-            # print(self.Database)
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            for i in temp:
+                                if i not in self._plist:
+                                    self._plist.append(i)
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _frequentOneItem(self):
-        """
-        Generating one frequent patterns
-        """
-        candidate = {}
-        self._tidList = {}
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                if j not in candidate:
-                    candidate[j] = 1
-                    self._tidList[j] = [i]
-                else:
-                    candidate[j] += 1
-                    self._tidList[j].append(i)
-        self._finalPatterns = {}
-        plist = [key for key, value in sorted(candidate.items(), key=lambda x: x[1], reverse=True)]
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = candidate[i]
-        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-    def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
-
-            :param prefix: the prefix of a pattern
-            :type prefix: list
-            :param suffix: the suffix of a patterns
-            :type suffix: list
-            :param tidSetI: the timestamp of a patterns
-            :type tidSetI: list
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = len(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + "\t"
-        if len(self._finalPatterns) < self._k:
-            if val > self._minimum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([i for i in self._finalPatterns.values()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
-                if val > y:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[sample] = val
-                    self._finalPatterns = {k: v for k, v in
-                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
-                                                     reverse=True)}
-                    self._minimum = min([i for i in self._finalPatterns.values()])
-                    return
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-            :param prefix:  main equivalence prefix
-            :type prefix: periodic-frequent item or pattern
-            :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
-                            and frequent with their timestamps
-            :type itemSets: list
-            :param tidSets: timestamps of the items in the argument itemSets
-            :type tidSets: list
-
-
-                    """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                if len(y) >= self._minimum:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
-
     def _convert(self, value):
         """
-        to convert the type of user specified minSup value
+        To convert the user specified minSup value
+
         :param value: user specified minSup value
-        :type value: int or float or str
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = ((len(self._Database)) * value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def _Count(self, tids):
+        count = 0
+        for i in tids:
+            if i == 1:
+                count += 1
+        return count
+
+    def _save(self, prefix, suffix, tidsetx):
+        if (prefix == None):
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        prefix = list(set(prefix))
+        prefix.sort()
+        val = self._Count(tidsetx)
+        if len(prefix) > self._faultTolerance:
+            self._finalPatterns[tuple(prefix)] = val
+
+    def _processEquivalenceClass(self, prefix, itemsets, tidsets):
+        if (len(itemsets) == 1):
+            i = itemsets[0]
+            tidi = tidsets[0]
+            self._save(prefix, [i], tidi)
+            return
+        for i in range(len(itemsets)):
+            itemx = itemsets[i]
+            if (itemx == None):
+                continue
+            tidsetx = tidsets[i]
+            classItemsets = []
+            classtidsets = []
+            itemsetx = [itemx]
+            for j in range(i + 1, len(itemsets)):
+                itemj = itemsets[j]
+                tidsetj = tidsets[j]
+                y = list(_np.array(tidsetx) & _np.array(tidsetj))
+                total = self._Count(y)
+                if total >= self._minSup:
+                    classItemsets.append(itemj)
+                    classtidsets.append(y)
+            if (len(classItemsets) > 0):
+                newprefix = list(set(itemsetx)) + prefix
+                self._processEquivalenceClass(newprefix, classItemsets, classtidsets)
+            self._save(prefix, list(set(itemsetx)), tidsetx)
+
+    def _oneLengthFrequentItems(self):
+        """
+        To calculate the one Length items
+        """
+        Vector = {}
+        items = []
+        for i in self._Database:
+            for j in self._plist:
+                count = 0
+                if j in i:
+                    count = 1
+                if j in Vector:
+                    Vector[j].append(count)
+                else:
+                    Vector[j] = [count]
+        for x, y in Vector.items():
+            v = self._Count(y)
+            if v >= self._itemSup:
+                items.append(x)
+        return Vector, items
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-            Main function of the program
+        Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._itemSup = self._convert(self._itemSup)
+        self._minLength = int(self._minLength)
+        self._faultTolerance = int(self._faultTolerance)
+        Vector, plist = self._oneLengthFrequentItems()
+        for i in range(len(plist)):
+            itemx = plist[i]
+            tidsetx = Vector[itemx]
+            itemsetx = [itemx]
+            itemsets = []
+            tidsets = []
+            for j in range(i + 1, len(plist)):
+                itemj = plist[j]
+                tidsetj = Vector[itemj]
+                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
+                total = self._Count(y1)
+                if total >= self._minSup:
+                    itemsets.append(itemj)
+                    tidsets.append(y1)
+            if (len(itemsets) > 0):
+                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
+            self._save(None, itemsetx, tidsetx)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
 
-    def mine(self):
+    def Mine(self):
         """
-            Main function of the program
+        Frequent pattern mining process will start from here
         """
+        self._Database = []
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        self._k = self._convert(self._k)
-        plist = self._frequentOneItem()
+        self._minSup = self._convert(self._minSup)
+        self._itemSup = self._convert(self._itemSup)
+        self._minLength = int(self._minLength)
+        self._faultTolerance = int(self._faultTolerance)
+        Vector, plist = self._oneLengthFrequentItems()
         for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
+            itemx = plist[i]
+            tidsetx = Vector[itemx]
+            itemsetx = [itemx]
+            itemsets = []
+            tidsets = []
             for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                if len(y1) >= self._minimum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+                itemj = plist[j]
+                tidsetj = Vector[itemj]
+                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
+                total = self._Count(y1)
+                if total >= self._minSup:
+                    itemsets.append(itemj)
+                    tidsets.append(y1)
+            if (len(itemsets) > 0):
+                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
+            self._save(None, itemsetx, tidsetx)
         self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
-        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
-
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
-
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
-
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
-
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
+            s = str()
+            for i in a:
+                s = s + i + ' '
+            data.append([s, b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
+            s = str()
+            for i in x:
+                s = s + i + '\t'
+            s1 = s.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
-
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printTOPK(self):
+    def printResults(self):
         """
         This function is used to print the results
         """
-        print("Top K Frequent  Patterns:", len(self.getPatterns()))
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 8:
+        if len(_ab._sys.argv) == 8:
+            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3],  _ab._sys.argv[4],
+                            _ab._sys.argv[5], _ab._sys.argv[6], _ab._sys.argv[7],)
+        if len(_ab._sys.argv) == 7:
+            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         _ap.startMine()
-        _ap.mine()
-        print("Top K Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        _ap = VBFTMine('/Users/Likhitha/Downloads/fault/sample4.txt', 5, 3, 2, 1, ' ')
+        _ap.startMine()
+        _ap.printResults()
+        print(_ap.getPatternsAsDataFrame())
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.24.1/PAMI/frequentPattern/topk/abstract.py` & `pami-2024.4.9.1/PAMI/frequentPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py`

 * *Files 12% similar despite different names*

```diff
@@ -472,15 +472,113 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ 
         Frequent pattern mining process will startTime from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        for tr in range(len(self._transactions)):
+            items = self._transactions[tr]
+            quantities = self._fuzzyValues[tr]
+            for i in range(0, len(items)):
+                item = items[i]
+                regions = _Regions(item, float(quantities[i]), 3, self._mapItemRegionSum)
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
+        listOfFFIList = []
+        mapItemsToFFLIST = {}
+        self._minSup = self._convert(self._minSup)
+        #minSup = self._minSup
+        self._minAllConf = float(self._minAllConf)
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            region = 'N'
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+                region = 'L'
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+                region = 'M'
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                region = 'H'
+                self._mapItemSum[item] = high
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item, region)
+                mapItemsToFFLIST[item] = fuList
+                listOfFFIList.append(fuList)
+        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for tr in range(len(self._transactions)):
+            items = self._transactions[tr]
+            quantities = self._fuzzyValues[tr]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                regions = _Regions(pair.item, float(quantities[i]), 3, self._temp)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                        pair.region = 'L'
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.region = 'M'
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                        pair.region = 'H'
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i - 1, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                if pair.quantity > remainUtil:
+                    remainingUtility = pair.quantity
+                else:
+                    remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Fuzzy Correlated Patterns Successfully generated using FCPGrowth algorithms")
 
 
     def mine(self) -> None:
         """
         Frequent pattern mining process will startTime from here
         """
         self._startTime = _ab._time.time()
@@ -634,14 +732,15 @@
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
+
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
@@ -752,15 +851,15 @@
     inputFile = 'https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_T10I4D100K.csv'
 
     minimumSupportCount=1200  #Users can also specify this constraint between 0 to 1.
     ratioExample=0.8
     seperator='\t' 
 
     obj = FCPGrowth(inputFile, minimumSupportCount,ratioExample,seperator)    #initialize
-    obj.mine()
+    obj.startMine()       
 
 
 if __name__ == "__main__":
     main()
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -102,14 +102,15 @@
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         :type minSup: int or float or str
         :param minAllConf: The user can specify minimum all confidence ratio
         :type minAllConf: float
         :param sep: separator used to distinguish items from each other. The default separator is tab space. However, users can override the default separator
         :type sep: str
         """
+
         self._iFile = iFile
         self._minSup = minSup
         self._minAllConf = minAllConf
         self._sep = sep
         self._startTime = float()
         self._endTime = float()
         self._finalPatterns = {}
@@ -120,15 +121,15 @@
         """Code for the mining process will start from this function"""
 
         pass
 
     @_abstractmethod
     def getPatterns(self):
         """Complete set of frequent patterns generated will be retrieved from this function"""
-        
+
         pass
 
     @_abstractmethod
     def save(self, oFile):
         """Complete set of frequent patterns will be saved in to an output file from this function
 
         :param oFile: Name of the output file
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py` & `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py`

 * *Files 9% similar despite different names*

```diff
@@ -61,29 +61,25 @@
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
 
         item: int
             the item name
-
         sumIUtil: float
             the sum of utilities of a fuzzy item in database
-
         sumRUtil: float
             the sum of resting values of a fuzzy item in database
-
         elements: list
             a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
-
         printElement(e)
             Method to print elements
     """
 
     def __init__(self, itemName: int) -> None:
         self.item = itemName
         self.sumIUtil = 0.0
@@ -91,15 +87,17 @@
         self.elements = []
 
     def addElement(self, element) -> None:
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
+
         :type element: Element
+
         :return: None
         """
         self.sumIUtil += element.iUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
 
     def printElement(self) -> None:
@@ -114,18 +112,16 @@
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-
         iUtils: float
             the utility of a fuzzy item in the transaction
-
         rUtils : float
             the  resting value of a fuzzy item in the transaction
     """
 
     def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
         self.tid = tid
         self.iUtils = iUtil
@@ -158,73 +154,71 @@
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param maxPer: float :
                    The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
     :param fuzFile: str :
                     The user can specify fuzFile.
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : string
             Name of the input file to mine complete set of fuzzy  frequent patterns
-
         fmFile : string
             Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
-
         oFile : string
             Name of the oFile file to store complete set of fuzzy  frequent patterns
-
         minSup : float
             The user given minimum support
-
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-
         startTime:float
             To record the start time of the mining process
-
         endTime:float
             To record the completion time of the mining process
-
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
-
+        mapItemsLowSum: map
+            To keep track of low region values of items
+        mapItemsMidSum: map
+            To keep track of middle region values of items
+        mapItemsHighSum: map
+            To keep track of high region values of items
         mapItemSum: map
             To keep track of sum of Fuzzy Values of items
-
-        joinsCnt: int
+        mapItemRegions: map
+            To Keep track of fuzzy regions of item
+        jointCnt: int
             To keep track of the number of ffi-list that was constructed
-
         BufferSize: int
             represent the size of Buffer
-
-        itemSetBuffer list
+        itemBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value)
+        convert(value):
             To convert the given user specified value
         compareItems(o1, o2)
             A Function that sort all ffi-list in ascending order of Support
         FSFIMining(prefix, prefixLen, FSFIM, minSup)
             Method generate ffi from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
@@ -312,18 +306,23 @@
         self._dbLen = 0
 
     def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
         """
         A Function that sort all ffi-list in ascending order of Support
 
         :param o1: First FFI-list
+
         :type o1: _FFList
+
         :param o2: Second FFI-list
+
         :type o1: _FFList
+
         :return: Comparision Value
+
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             if o1.item < o2.item:
                 return -1
             elif o1.item > o2.item:
@@ -334,16 +333,19 @@
             return compare
 
     def _convert(self, value) -> Union[int, float]:
         """
         To convert the given user specified value
 
         :param value: user specified value
+
         :type value: int or float or str
+
         :return: converted value
+
         :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
@@ -398,15 +400,69 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         fuzzy-Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfffilist = []
+        mapItemsToFFLIST = {}
+        #self._minSup = self._convert(self._minSup)
+        # minSup = self.minSup
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.quantity = quantities[i]
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._FFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
         fuzzy-Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._creatingItemsets()
@@ -471,15 +527,15 @@
         :param prefix: the prefix patterns of ffi
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup: int or float
+        :type minSup: int or flaot
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
             if X.sumRUtil >= minSup:
                 exULs = []
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,24 @@
-# Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
-#
-# to its huge search space.we are using efficient pruning techniques to reduce the search space.
+# Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
+# which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
+# techniques to reduce the search space.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# ---------------------------------------------------------
+# .. code-block:: python
 #
-#             from PAMI.fuzzyFrequentPattern import FFIMiner_old as alg
+#             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 #
-#             obj = alg.FFIMiner("input.txt", 2)
+#             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
 #
 #             obj.mine()
 #
-#             fuzzyFrequentPattern = obj.getPatterns()
+#             fuzzySpatialFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+#             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
 #
 #             obj.save("outputFile")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -47,15 +48,16 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-from PAMI.fuzzyFrequentPattern.basic import abstract as _ab
+
+from PAMI.fuzzyGeoreferencedFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
@@ -73,17 +75,18 @@
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
             Method to print elements
+
     """
 
-    def __init__(self, itemName: int) -> None:
+    def __init__(self, itemName: str) -> None:
         self.item = itemName
         self.sumIUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
 
     def addElement(self, element) -> None:
         """
@@ -95,16 +98,15 @@
         """
         self.sumIUtil += element.iUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
 
     def printElement(self) -> None:
         """
-        A method to print elements
-        :return: None
+        A Method to Print elements in the FFList
         """
         for ele in self.elements:
             print(ele.tid, ele.iUtils, ele.rUtils)
 
 
 class _Element:
     """
@@ -113,109 +115,71 @@
     :Attributes:
 
         tid : int
             keep tact of transaction id
         iUtils: float
             the utility of a fuzzy item in the transaction
         rUtils : float
-            the  resting value of a fuzzy item in the transaction
+            the neighbourhood resting value of a fuzzy item in the transaction
     """
 
     def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
         self.tid = tid
         self.iUtils = iUtil
         self.rUtils = rUtil
 
 
-class _Regions:
-    """
-    A class calculate the regions
-
-    :Attributes:
-
-        low : int
-            low region value
-        middle: int
-            middle region value
-        high : int
-            high region values
-    """
-
-    def __init__(self, quantity: int, regionsNumber: int) -> None:
-        self.low = 0
-        self.middle = 0
-        self.high = 0
-        if regionsNumber == 3:  # if we have 3 regions
-            if 0 < quantity <= 1:
-                self.low = 1
-                self.high = 0
-                self.middle = 0
-            elif 1 < quantity <= 6:
-                self.low = float((6 - quantity) / 5)
-                self.middle = float((quantity - 1) / 5)
-                self.high = 0
-            elif 6 < quantity <= 11:
-                self.low = 0
-                self.middle = float((11 - quantity) / 5)
-                self.high = float((quantity - 6) / 5)
-            else:
-                self.low = 0
-                self.middle = 0
-                self.high = 1
-
-
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
     def __init__(self) -> None:
         self.item = 0
         self.quantity = 0
 
 
-class FFIMiner(_ab._fuzzyFrequentPattenrs):
+class FFSPMiner(_ab._fuzzySpatialFrequentPatterns):
     """
-    :Description:   Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
-                    to its huge search space.we are using efficient pruning techniques to reduce the search space.
-
-    :Reference:   Lin, Chun-Wei & Li, Ting & Fournier Viger, Philippe & Hong, Tzung-Pei. (2015).
-                  A fast Algorithm for mining fuzzy frequent itemsets. Journal of Intelligent & Fuzzy Systems. 29.
-                  2373-2379. 10.3233/IFS-151936.
-                  https://www.researchgate.net/publication/286510908_A_fast_Algorithm_for_mining_fuzzy_frequent_itemSets
+    :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
+                    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
+                    techniques to reduce the search space.
+
+    :Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
+                  Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
+                  (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param maxPer: float :
                    The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-    :param fuzFile: str :
-                    The user can specify fuzFile.
-
+    :param nFile: str :
+                   Name of the input file to mine complete set of frequent patterns
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
-        iFile : string
-            Name of the input file to mine complete set of fuzzy  frequent patterns
-        fmFile : string
-            Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
-        oFile : string
-            Name of the oFile file to store complete set of fuzzy  frequent patterns
+        iFile : file
+            Name of the input file to mine complete set of fuzzy spatial frequent patterns
+        oFile : file
+               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
+        neighbors: map
+            keep track of neighbours of elements
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+                To store the total amount of RSS memory consumed by the program
         startTime:float
-            To record the start time of the mining process
+               To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
         mapItemsLowSum: map
             To keep track of low region values of items
         mapItemsMidSum: map
@@ -223,402 +187,434 @@
         mapItemsHighSum: map
             To keep track of high region values of items
         mapItemSum: map
             To keep track of sum of Fuzzy Values of items
         mapItemRegions: map
             To Keep track of fuzzy regions of item
         jointCnt: int
-            To keep track of the number of ffi-list that was constructed
+            To keep track of the number of FFI-list that was constructed
         BufferSize: int
             represent the size of Buffer
         itemBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value)
+        convert(value):
             To convert the given user specified value
-        compareItems(o1, o2)
-            A Function that sort all ffi-list in ascending order of Support
-        FSFIMining(prefix, prefixLen, FSFIM, minSup)
-            Method generate ffi from prefix
+        FSFIMining( prefix, prefixLen, fsFim, minSup)
+            Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
+        Intersection(neighbourX,neighbourY)
+            Return common neighbours of 2 itemSet Neighbours
         findElementWithTID(uList, tid)
             To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil)
+        WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
 
     **Executing the code on terminal :**
-    -----------------------------------------
+    ----------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 FFIMinerMiner.py <inputFile> <outputFile> <minSup> <separator>
+      (.venv) $ python3 FFSPMiner.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
 
       Example Usage:
 
-      (.venv) $ python3  FFIMinerMiner.py sampleTDB.txt output.txt 6
-
-      (.venv) $ python3  FFIMinerMiner.py sampleTDB.txt output.txt 0.3
+      (.venv) $ python3  FFSPMiner.py sampleTDB.txt output.txt sampleN.txt 3
 
     .. note:: minSup will be considered in percentage of database transactions
 
-
     **Sample run of importing the code:**
     ----------------------------------------
+    .. code-block:: python
 
-        from PAMI.fuzzyFrequentPattern import FFIMiner as alg
-
-        obj = alg.FFIMiner("input.txt", "fuzzyMembership.txt" 2)
+            from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 
-        obj.mine()
+            obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
 
-        fuzzyFrequentPattern = obj.getPatterns()
+            obj.mine()
 
-        print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+            fuzzySpatialFrequentPatterns = obj.getPatterns()
 
-        obj.save("outputFile")
+            print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
 
-        memUSS = obj.getMemoryUSS()
+            obj.save("outputFile")
 
-        print("Total Memory in USS:", memUSS)
+            memUSS = obj.getMemoryUSS()
 
-        memRSS = obj.getMemoryRSS()
+            print("Total Memory in USS:", memUSS)
 
-        print("Total Memory in RSS", memRSS)
+            memRSS = obj.getMemoryRSS()
 
-        run = obj.getRuntime()
+            print("Total Memory in RSS", memRSS)
 
-        print("Total ExecutionTime in seconds:", run)
+            run = obj.getRuntime()
 
+            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -------------
-        The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
-
+    --------------
+            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
     """
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _fuzFile = " "
+    _nFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = "\t"
+    _transactions = []
+    _fuzzyValues = []
 
-    def __init__(self, iFile: str, fuzFile: str, minSup: float, sep: str="\t") -> None:
-        super().__init__(iFile, fuzFile, minSup, sep)
+    def __init__(self, iFile: str, nFile: str, minSup: float, sep: str="\t") -> None:
+        super().__init__(iFile, nFile, minSup, sep)
+        self._mapItemNeighbours = {}
         self._startTime = 0
         self._endTime = 0
-        self._itemsCnt = 0
-        self._mapItemsLowSum = {}
-        self._mapItemsMidSum = {}
-        self._mapItemsHighSum = {}
         self._mapItemSum = {}
-        self._mapItemRegions = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
-        self._transactions = []
-        self._fuzzyValues = []
         self._finalPatterns = {}
-        self._RegionsCal = []
-        self._LabelKeyOne = {}
-        self._LabelKey = {}
-        self._RegionsLabel = []
         self._dbLen = 0
+        self._itemsCnt = 0
 
-    def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
+    def _compareItems(self, o1, o2) -> int:
         """
         A Function that sort all ffi-list in ascending order of Support
 
         :param o1: First FFI-list
+
         :type o1: _FFList
+
         :param o2: Second FFI-list
-        :type o2: _FFList
-        :return: Comparison Value
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
-            if o1.item < o2.item:
-                return -1
-            elif o1.item > o2.item:
-                return 1
-            else:
-                return 0
+            return 0
         else:
             return compare
 
-    def _convert(self, value: Union[int, float, str]) -> float:
+    def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
         :type value: int or float or str
         :return: converted value
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _fuzzyMembershipFunc(self) -> None:
-        try:
-            with open(self._fuzFile, 'r', encoding='utf-8') as f:
-                count = 0
-                for line in f:
-                    line = line.split("\n")[0]
-                    parts = line.split(" ")
-                    lowerBound = parts[0].strip()
-                    upperBound = parts[1].strip()
-                    lb_Label = parts[2].strip()
-                    ub_Label = parts[3].strip()
-                    self._RegionsCal.append([int(lowerBound), int(upperBound)])
-                    self._RegionsLabel.append([lb_Label, ub_Label])
-                    for i in range(0, 2):
-                        if lb_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[lb_Label.capitalize()] = count
-                            count += 1
-                        if ub_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[ub_Label.capitalize()] = count
-                            count += 1
-            self._LabelKeyOne = {v:k for k,v in self._LabelKey.items()}
-            print(self._LabelKey)
-            print(self._LabelKeyOne)
-            print(self._RegionsLabel)
-            print(self._RegionsCal)
-        except IOError:
-            print("File Not Found")
-            quit()
-
-    def _creatingItemsets(self) -> None:
+    def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
-        self._transactions, self._fuzzyValues, self._Database = [], [], []
+        self._transactions, self._fuzzyValues = [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._transactions = self._iFile['Transactions'].tolist()
+                self.transactions = self._iFile['Transactions'].tolist()
             if 'fuzzyValues' in i:
-                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
-            # print(self.Database)
+                self.fuzzyValues = self._iFile['Utilities'].tolist()
+
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
+                    parts[1] = parts[1].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
+                    quantities = parts[1].split(self._sep)
                     self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([x for x in quantities])
+                    self._fuzzyValues.append([float(x) for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
-                            parts[2] = parts[2].strip()
+                            parts[1] = parts[1].strip()
                             items = parts[0].split(self._sep)
-                            quantities = parts[2].split(self._sep)
+                            quantities = parts[1].split(self._sep)
                             self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([x for x in quantities])
+                            self._fuzzyValues.append([float(x) for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _Regions(self, quantity: float) -> None:
-        """
-        :param quantity: Quantity to calculate regions
-        :type quantity: float
-        :return: None
-        """
-        self.list = [0] * len(self._LabelKey)
-        if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
-            self.list[0] = 1
-            return
-        elif quantity >= self._RegionsCal[-1][0]:
-            self.list[-1] = 1
-            return
-        else:
-            for i in range(1, len(self._RegionsCal) - 1):
-                if self._RegionsCal[i][0] < quantity <= self._RegionsCal[i][1]:
-                    base = self._RegionsCal[i][1] - self._RegionsCal[i][0]
-                    for pos in range(0, 2):
-                        if self._RegionsLabel[i][pos].islower():
-                            self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (self._RegionsCal[i][1] - quantity) / base)
-                        else:
-                            self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (quantity - self._RegionsCal[i][0]) / base)
-            return
+    def _mapNeighbours(self) -> None:
+        self._mapItemNeighbours = {}
+        if isinstance(self._nFile, _ab._pd.DataFrame):
+            data, items = [], []
+            if self._nFile.empty:
+                print("its empty..")
+            i = self._nFile.columns.values.tolist()
+            if 'items' in i:
+                items = self._nFile['items'].tolist()
+            if 'Neighbours' in i:
+                data = self._nFile['Neighbours'].tolist()
+            for k in range(len(items)):
+                self._mapItemNeighbours[items[k]] = data[k]
+
+        if isinstance(self._nFile, str):
+            if _ab._validators.url(self._nFile):
+                data = _ab._urlopen(self._nFile)
+                for line in data:
+                    line = line.decode("utf-8")
+                    line = line.split("\n")[0]
+                    parts = [i.rstrip() for i in line.split(self._sep)]
+                    parts = [x for x in parts]
+                    item = parts[0]
+                    neigh1 = []
+                    for i in range(1, len(parts)):
+                        neigh1.append(parts[i])
+                    self._mapItemNeighbours[item] = neigh1
+            else:
+                try:
+                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line = line.split("\n")[0]
+                            parts = [i.rstrip() for i in line.split(self._sep)]
+                            parts = [x for x in parts]
+                            item = parts[0]
+                            neigh1 = []
+                            for i in range(1, len(parts)):
+                                neigh1.append(parts[i])
+                            self._mapItemNeighbours[item] = neigh1
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        fuzzy-Frequent pattern mining process will start from here
+        Frequent pattern mining process will start from here
+        :return: None
         """
-        self.mine()
-
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._mapNeighbours()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfFFList = []
+        mapItemsToFFLIST = {}
+        #self._minSup = self._convert(self._minSup)
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfFFList.append(fuList)
+        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.quantity = quantities[i]
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    if self._mapItemNeighbours.get(pair.item[0]) is None:
+                        continue
+                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
+                        remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        itemNeighbours = list(self._mapItemNeighbours.keys())
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
-        fuzzy-Frequent pattern mining process will start from here
+        Frequent pattern mining process will start from here
+        :return: None
         """
         self._startTime = _ab._time.time()
-        self._creatingItemsets()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._mapNeighbours()
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             self._dbLen += 1
             for i in range(0, len(items)):
-                regions = self._Regions(float(quantities[i]))
-                print(regions)
                 item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
                 else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemsMidSum.keys():
-                    mid = self._mapItemsMidSum[item]
-                    mid += regions.middle
-                    self._mapItemsMidSum[item] = mid
-                else:
-                    self._mapItemsMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
-        listOfffilist = []
+                    self._mapItemSum[item] = quantities[i]
+        listOfFFList = []
         mapItemsToFFLIST = {}
-        self._minSup = self._convert(self._minSup)
-        # minSup = self.minSup
-        for item1 in self._mapItemsLowSum.keys():
+        #self._minSup = self._convert(self._minSup)
+        for item1 in self._mapItemSum.keys():
             item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemsMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
             if self._mapItemSum[item] >= self._minSup:
                 fuList = _FFList(item)
                 mapItemsToFFLIST[item] = fuList
-                listOfffilist.append(fuList)
-        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+                listOfFFList.append(fuList)
+        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             revisedTransaction = []
             for i in range(0, len(items)):
                 pair = _Pair()
                 pair.item = items[i]
-                regions = self._Regions(float(quantities[i]), 3)
+                pair.quantity = quantities[i]
                 item = pair.item
                 if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
                 remainUtil = 0
                 for j in range(len(revisedTransaction) - 1, i, -1):
-                    remainUtil += revisedTransaction[j].quantity
+                    if self._mapItemNeighbours.get(pair.item[0]) is None:
+                        continue
+                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
+                        remainUtil += revisedTransaction[j].quantity
                 remainingUtility = remainUtil
                 if mapItemsToFFLIST.get(pair.item) is not None:
                     FFListOfItem = mapItemsToFFLIST[pair.item]
                     element = _Element(tid, pair.quantity, remainingUtility)
                     FFListOfItem.addElement(element)
             tid += 1
-        self._FSFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        itemNeighbours = list(self._mapItemNeighbours.keys())
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FSFIMining(self, prefix: List[int], prefixLen: int, FSFIM: List[_FFList], minSup: float) -> None:
-        """Generates ffi from prefix
+    def _FSFIMining(self, prefix: List, prefixLen: int, FSFIM: List, minSup: float, itemNeighbours: List):
+        """
+        Generates FFSPMiner from prefix
 
-        :param prefix: the prefix patterns of ffi
-        :type prefix: list
+        :param prefix: the prefix patterns of FFSPMiner
+        :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup: float
-        :return: None
+        :type minSup:int
+        :param itemNeighbours: the set of common neighbours of prefix
+        :type itemNeighbours: list or set
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
+            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item[0]), itemNeighbours)
             if X.sumRUtil >= minSup:
                 exULs = []
                 for j in range(i + 1, len(FSFIM)):
                     Y = FSFIM[j]
-                    exULs.append(self._construct(X, Y))
-                    self._joinsCnt += 1
+                    if Y.item[0] in newNeighbours:
+                        exULs.append(self._construct(X, Y))
+                        self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
+                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbours)
+
+    def _Intersection(self, neighbourX: List, neighbourY: List) -> List:
+        """
+        A function to get common neighbours from 2 itemSets
+
+        :param neighbourX: the set of neighbours of itemSet 1
+        :type neighbourX: set or list
+        :param neighbourY: the set of neighbours of itemSet 2
+        :type neighbourY: set or list
+        :return : set of common neighbours of 2 itemSets
+        :rtype :set
+        """
+        result = []
+        if neighbourX is None or neighbourY is None:
+            return result
+        for i in range(0, len(neighbourX)):
+            if neighbourX[i] in neighbourY:
+                result.append(neighbourX[i])
+        return result
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -635,64 +631,65 @@
         """
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
 
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
     def _construct(self, px: _FFList, py: _FFList) -> _FFList:
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
         :param px:the itemSet px
-        :type px:ffi-List
+        :type px:FFI-List
         :param py:itemSet py
-        :type py:ffi-List
+        :type py:FFI-List
         :return :the itemSet of pxy(px and py)
-        :rtype :ffi-List
+        :rtype :FFI-List
         """
         pxyUL = _FFList(py.item)
         for ex in px.elements:
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
             eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
             pxyUL.addElement(eXY)
         return pxyUL
 
-    def _findElementWithTID(self, uList: _FFList, tid: int) -> Union[_Element, None]:
+    def _findElementWithTID(self, uList: _FFList, tid: int) -> _Element:
         """
         To find element with same tid as given
 
-        :param uList: fuzzyList
-        :type uList: ffi-List
-        :param tid: transaction id
-        :type tid: int
-        :return: element  tid as given
-        :rtype: element if exit or None
+        :param uList:fuzzyList
+        :type uList:FFI-List
+        :param tid:transaction id
+        :type tid:int
+        :return:element tid as given
+        :rtype: element if exist or None
         """
         List = uList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
             if List[mid].tid < tid:
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix: List[int], prefixLen: int, item: int, sumIUtil: float) -> None:
+    def _WriteOut(self, prefix: List, prefixLen: int, item: int, sumIUtil: float) -> None:
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
@@ -701,16 +698,16 @@
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
         :return: None
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
-        res += str(item) + "." + str(self._mapItemRegions.get(item))
+            res += str(prefix[i]) + "\t"
+        res += str(item)
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
 
@@ -738,40 +735,47 @@
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
+            patternsAndSupport = x.strip() + " : " + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Fuzzy Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Spatial Fuzzy Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Spatial Fuzzy Frequent  Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
+        _ap = FFSPMiner('sample.txt', 'nei.txt', 1, ' ')
+        _ap.startMine()
+        _ap.mine()
+        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save('output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,63 +46,49 @@
         employ in PAMI
 
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-
         minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-
         startTime:float
             To record the start time of the algorithm
-
         endTime:float
             To record the completion time of the algorithm
-
         finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
-
         oFile : str
             Name of the output file to store complete set of frequent patterns
-
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
-
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
     :Method:
 
         startMine()
             Calling this function will start the actual mining process
-
         getPatterns()
             This function will output all interesting patterns discovered by an algorithm
-
         save(oFile)
             This function will store the discovered patterns in an output file specified by the user
-
         getPatternsAsDataFrame()
             The function outputs the patterns generated by an algorithm as a data frame
-
         getMemoryUSS()
             This function outputs the total amount of USS memory consumed by a mining algorithm
-
         getMemoryRSS()
             This function outputs the total amount of RSS memory consumed by a mining algorithm
-
         getRuntime()
             This function outputs the total runtime of a mining algorithm
 
     """
 
     def __init__(self, iFile, minSup, sep="\t"):
         """
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py` & `pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,26 @@
-# Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-# which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-# techniques to reduce the search space.
+# F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
+#
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
-# .. code-block:: python
+# ----------------------------------------------------
 #
-#             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
+#             import PAMI.fuzzyPartialPeriodicPattern.basic.F3PMiner as alg
 #
-#             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
+#             obj = alg.F3PMiner(iFile, minSup, sep)
 #
 #             obj.mine()
 #
-#             fuzzySpatialFrequentPatterns = obj.getPatterns()
+#             fuzzyPartialPeriodicPatterns = obj.getPatterns()
+#
+#             print("Total number of Fuzzy Partial Periodic Patterns:", len(fuzzyPartialPeriodicPatterns))
 #
-#             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
+#             obj.save(oFile)
 #
-#             obj.save("outputFile")
+#             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -30,440 +30,479 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
+
 __copyright__ = """
+
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
 
+     This program is free software: you can redistribute it and/or modify
+     it under the terms of the GNU General Public License as published by
+     the Free Software Foundation, either version 3 of the License, or
+     (at your option) any later version.
+
+     This program is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU General Public License for more details.
+
+     You should have received a copy of the GNU General Public License
+     along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.fuzzyGeoreferencedFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from PAMI.fuzzyPartialPeriodicPatterns.basic import abstract as _ab
 from deprecated import deprecated
 
 
 class _FFList:
     """
-    A class represent a Fuzzy List of an element
+     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-         item : int
+         item: int
              the item name
-         sumIUtil : float
-             the sum of utilities of a fuzzy item in database
-         sumRUtil : float
+         sumIUtil: float
+             the sum of utilities of an fuzzy item in database
+         sumRUtil: float
              the sum of resting values of a fuzzy item in database
-         elements : list
+         elements: list
              a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
+
         printElement(e)
             Method to print elements
 
     """
 
-    def __init__(self, itemName: str) -> None:
+    def __init__(self, itemName):
         self.item = itemName
         self.sumIUtil = 0.0
-        self.sumRUtil = 0.0
         self.elements = []
 
-    def addElement(self, element) -> None:
+    def addElement(self, element):
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :param element: Element
-        :return: None
+        :type element: Element
         """
         self.sumIUtil += element.iUtils
-        self.sumRUtil += element.rUtils
         self.elements.append(element)
 
-    def printElement(self) -> None:
+    def printElement(self):
         """
-        A Method to Print elements in the FFList
+        A method to print elements
         """
         for ele in self.elements:
             print(ele.tid, ele.iUtils, ele.rUtils)
 
 
 class _Element:
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils : float
-            the utility of a fuzzy item in the transaction
+        iUtils: float
+            the utility of an fuzzy item in the transaction
         rUtils : float
-            the neighbourhood resting value of a fuzzy item in the transaction
+            the  resting value of an fuzzy item in the transaction
     """
 
-    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
+    def __init__(self, tid, iUtil):
         self.tid = tid
         self.iUtils = iUtil
-        self.rUtils = rUtil
 
 
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.item = 0
         self.quantity = 0
 
 
-class FFSPMiner(_ab._fuzzySpatialFrequentPatterns):
+class F3PMiner(_ab._fuzzyPartialPeriodicPatterns):
     """
-    :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-                    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-                    techniques to reduce the search space.
-
-    :Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
-                  Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
-                  (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
+    :Description:   F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
+    
+    :Reference:
+
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param maxPer: float :
                    The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-    :param nFile: str :
-                   Name of the input file to mine complete set of frequent patterns
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
+
     :Attributes:
 
-        iFile : file
+        iFile : string
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
-        oFile : file
-            Name of the oFile file to store complete set of fuzzy spatial frequent patterns
+        oFile : string
+               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
-        neighbors : map
-            keep track of neighbours of elements
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        startTime : float
-            To record the start time of the mining process
-        endTime : float
+                To store the total amount of RSS memory consumed by the program
+        startTime:float
+               To record the start time of the mining process
+        endTime:float
             To record the completion time of the mining process
-        itemsCnt : int
+        itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
-        mapItemSum : map
+        mapItemsGSum: map
+            To keep track of G region values of items
+        mapItemsMidSum: map
+            To keep track of M region values of items
+        mapItemsHSum: map
+            To keep track of H region values of items
+        mapItemSum: map
             To keep track of sum of Fuzzy Values of items
-        mapItemRegions : map
+        mapItemRegions: map
             To Keep track of fuzzy regions of item
-        joinsCnt : int
-            To keep track of the number of FFI-list that was constructed
-        BufferSize : int
+        jointCnt: int
+            To keep track of the number of ffi-list that was constructed
+        BufferSize: int
             represent the size of Buffer
-        itemSetBuffer : list
+        itemBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value)
+        convert(value):
             To convert the given user specified value
-        FSFIMining( prefix, prefixLen, fsFim, minSup)
-            Method generate FFI from prefix
+        compareItems(o1, o2)
+            A Function that sort all ffi-list in ascending order of Support
+        F3PMining(prefix, prefixLen, FSFIM, minSup)
+            Method generate ffi from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        Intersection(neighbourX,neighbourY)
-            Return common neighbours of 2 itemSet Neighbours
         findElementWithTID(uList, tid)
             To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil,period)
+        WriteOut(prefix, prefixLen, item, sumIUtil)
             To Store the patten
 
     **Executing the code on terminal :**
-    ----------------------------------------
+    ---------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 FFSPMiner.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
+      (.venv) $ python3 F3PMiner.py <inputFile> <outputFile> <minSup> <separator>
 
       Example Usage:
 
-      (.venv) $ python3  FFSPMiner.py sampleTDB.txt output.txt sampleN.txt 3
+      (.venv) $ python3  F3PMiner.py sampleTDB.txt output.txt 6
 
     .. note:: minSup will be considered in percentage of database transactions
 
+
     **Sample run of importing the code:**
-    ----------------------------------------
-    .. code-block:: python
+    --------------------------------------
 
-            from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
+        from PAMI.fuzzyPartialPeriodicPatterns import F3PMiner as alg
 
-            obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
+        obj = alg.F3PMiner("input.txt", 2)
 
-            obj.mine()
+        obj.mine()
 
-            fuzzySpatialFrequentPatterns = obj.getPatterns()
+        fuzzyPartialPeriodicPatterns = obj.getPatterns()
 
-            print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
+        print("Total number of Fuzzy Frequent Patterns:", len(fuzzyPartialPeriodicPatterns))
 
-            obj.save("outputFile")
+        obj.save("outputFile")
 
-            memUSS = obj.getMemoryUSS()
+        memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+        print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+        memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+        print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+        run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+        print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+    -------------
+        The complete program was written by PALLA Likhitha under the supervision of Professor Rage Uday Kiran.
     """
-
+    
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _nFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = "\t"
-    _transactions = []
-    _fuzzyValues = []
 
-    def __init__(self, iFile: str, nFile: str, minSup: float, sep: str="\t") -> None:
-        super().__init__(iFile, nFile, minSup, sep)
-        self._mapItemNeighbours = {}
+    def __init__(self, iFile, minSup, sep="\t"):
+        super().__init__(iFile, minSup, sep)
         self._startTime = 0
         self._endTime = 0
+        self._itemsCnt = 0
         self._mapItemSum = {}
+        self._mapItemRegions = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
+        self._transactions = []
+        self._fuzzyValues = []
+        self._ts = []
         self._finalPatterns = {}
         self._dbLen = 0
-        self._itemsCnt = 0
 
-    def _compareItems(self, o1, o2) -> int:
+    def _compareItems(self, o1, o2):
         """
         A Function that sort all ffi-list in ascending order of Support
 
         :param o1: First FFI-list
+
         :type o1: _FFList
+
         :param o2: Second FFI-list
-        :type o2: _FFList
+
+        :type o1: _FFList
+
         :return: Comparision Value
 
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
-            return 0
+            if o1.item < o2.item:
+                return -1
+            elif o1.item > o2.item:
+                return 1
+            else:
+                return 0
         else:
             return compare
 
-    def _convert(self, value) -> float:
+    def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
+
         :type value: int or float or str
+
         :return: converted value
+
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
+                value = float(value)
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self) -> None:
+    def _creatingItemsets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
-
-        :return: None
         """
-        self._transactions, self._fuzzyValues = [], []
+        self._transactions, self._fuzzyValues, self._Database, self._ts = [], [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self.transactions = self._iFile['Transactions'].tolist()
+                self._transactions = self._iFile['Transactions'].tolist()
             if 'fuzzyValues' in i:
-                self.fuzzyValues = self._iFile['Utilities'].tolist()
-
+                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
-                    parts[1] = parts[1].strip()
+                    parts[2] = parts[2].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[1].split(self._sep)
+                    quantities = parts[2].split(self._sep)
                     self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([float(x) for x in quantities])
+                    self._fuzzyValues.append([x for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.split("\n")[0]
+                            line = line.strip()
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
                             parts[1] = parts[1].strip()
-                            items = parts[0].split(self._sep)
-                            quantities = parts[1].split(self._sep)
-                            self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([float(x) for x in quantities])
-                except IOError:
-                    print("File Not Found")
-                    quit()
-
-    def _mapNeighbours(self) -> None:
-        self._mapItemNeighbours = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            data, items = [], []
-            if self._nFile.empty:
-                print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'items' in i:
-                items = self._nFile['items'].tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for k in range(len(items)):
-                self._mapItemNeighbours[items[k]] = data[k]
-
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._nFile):
-                data = _ab._urlopen(self._nFile)
-                for line in data:
-                    line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = [i.rstrip() for i in line.split(self._sep)]
-                    parts = [x for x in parts]
-                    item = parts[0]
-                    neigh1 = []
-                    for i in range(1, len(parts)):
-                        neigh1.append(parts[i])
-                    self._mapItemNeighbours[item] = neigh1
-            else:
-                try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.split("\n")[0]
-                            parts = [i.rstrip() for i in line.split(self._sep)]
-                            parts = [x for x in parts]
-                            item = parts[0]
-                            neigh1 = []
-                            for i in range(1, len(parts)):
-                                neigh1.append(parts[i])
-                            self._mapItemNeighbours[item] = neigh1
+                            parts[2] = parts[2].strip()
+                            times = parts[0].split(self._sep)
+                            items = parts[1].split(self._sep)
+                            quantities = parts[2].split(self._sep)
+                            #print(times, items, quantities)
+                            _time = [x for x in times if x]
+                            items = [x for x in items if x]
+                            quantities = [float(x) for x in quantities if x]
+                            tempList = []
+                            for k in range(len(_time)):
+                                ite = "(" + _time[k] + "," + items[k] + ")"
+                                tempList.append(ite)
+                            self._ts.append([x for x in times])
+                            self._transactions.append([x for x in tempList])
+                            self._fuzzyValues.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
+    def startMine(self):
         """
-        Frequent pattern mining process will start from here
-
-        :return: None
+        fuzzy-Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        for line in range(len(self._transactions)):
+            times = self._ts[line]
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfffilist = []
+        mapItemsToFFLIST = {}
+        #self._minSup = float(self._minSup)
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            # print(type(self._mapItemSum[item]))
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.quantity = quantities[i]
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._F3PMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def mine(self) -> None:
+    def mine(self):
         """
-        Frequent pattern mining process will start from here
-
-        :return: None
+        fuzzy-Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        self._mapNeighbours()
+        self._creatingItemsets()
         for line in range(len(self._transactions)):
+            times = self._ts[line]
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             self._dbLen += 1
             for i in range(0, len(items)):
                 item = items[i]
                 if item in self._mapItemSum:
                     self._mapItemSum[item] += quantities[i]
                 else:
                     self._mapItemSum[item] = quantities[i]
-        listOfFFList = []
+        listOfffilist = []
         mapItemsToFFLIST = {}
-        #self._minSup = self._convert(self._minSup)
+        #self._minSup = float(self._minSup)
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
         for item1 in self._mapItemSum.keys():
             item = item1
+            # print(type(self._mapItemSum[item]))
             if self._mapItemSum[item] >= self._minSup:
                 fuList = _FFList(item)
                 mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             revisedTransaction = []
             for i in range(0, len(items)):
                 pair = _Pair()
@@ -472,243 +511,201 @@
                 item = pair.item
                 if self._mapItemSum[item] >= self._minSup:
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    if self._mapItemNeighbours.get(pair.item[0]) is None:
-                        continue
-                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
-                        remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
                 if mapItemsToFFLIST.get(pair.item) is not None:
                     FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
+                    element = _Element(tid, pair.quantity)
                     FFListOfItem.addElement(element)
             tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        self._F3PMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FSFIMining(self, prefix: List, prefixLen: int, FSFIM: List, minSup: float, itemNeighbours: List):
+    def _F3PMining(self, prefix, prefixLen, FSFIM, minSup):
         """
-        Generates FFSPMiner from prefix
+        Generates ffi from prefix
 
-        :param prefix: the prefix patterns of FFSPMiner
+        :param prefix: the prefix patterns of ffi
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup: int
-        :param itemNeighbours: the set of common neighbours of prefix
-        :type itemNeighbours: list or set
+        :type minSup:int
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
+            exULs = []
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item[0]), itemNeighbours)
-            if X.sumRUtil >= minSup:
-                exULs = []
                 for j in range(i + 1, len(FSFIM)):
                     Y = FSFIM[j]
-                    if Y.item[0] in newNeighbours:
-                        exULs.append(self._construct(X, Y))
-                        self._joinsCnt += 1
+                    exULs.append(self._construct(X, Y))
+                    self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbours)
-
-    def _Intersection(self, neighbourX: List, neighbourY: List) -> List:
-        """
-        A function to get common neighbours from 2 itemSets
-
-        :param neighbourX: the set of neighbours of itemSet 1
-        :type neighbourX: set or list
-        :param neighbourY: the set of neighbours of itemSet 2
-        :type neighbourY: set or list
-        :return: set of common neighbours of 2 itemSets
-        :rtype: set
-        """
-        result = []
-        if neighbourX is None or neighbourY is None:
-            return result
-        for i in range(0, len(neighbourX)):
-            if neighbourX[i] in neighbourY:
-                result.append(neighbourX[i])
-        return result
+                self._F3PMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-        """
+       """
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
+    def _construct(self, px, py):
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
-        :param px: the itemSet px
-        :type px: FFI-List
-        :param py: itemSet py
-        :type py: FFI-List
-        :return: the itemSet of pxy(px and py)
-        :rtype: FFI-List
+        :param px:the itemSet px
+        :type px:ffi-List
+        :param py:itemSet py
+        :type py:ffi-List
+        :return :the itemSet of pxy(px and py)
+        :rtype :ffi-List
         """
         pxyUL = _FFList(py.item)
         for ex in px.elements:
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
-            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
+            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)))
             pxyUL.addElement(eXY)
         return pxyUL
 
-    def _findElementWithTID(self, uList: _FFList, tid: int) -> _Element:
+    def _findElementWithTID(self, uList, tid):
         """
         To find element with same tid as given
 
         :param uList: fuzzyList
-        :type uList: FFI-List
+        :type uList: ffi-List
         :param tid: transaction id
         :type tid: int
-        :return: element tid as given
-        :rtype: element if exist or None
+        :return: element  tid as given
+        :rtype: element if exit or None
         """
         List = uList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
             if List[mid].tid < tid:
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix: List, prefixLen: int, item: int, sumIUtil: float) -> None:
+    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
-        :return: None
+
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
             res += str(prefix[i]) + "\t"
         res += str(item)
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def getPatterns(self) -> Dict[str, str]:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
         :type outFile: csv file
-        :return: None
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + " : " + str(y)
+            patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Spatial Fuzzy Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Fuzzy Partial Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Spatial Fuzzy Frequent  Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        _ap.printResults()
     else:
-        _ap = FFSPMiner('sample.txt', 'nei.txt', 1, ' ')
-        _ap.startMine()
-        _ap.mine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py`

 * *Files 10% similar despite different names*

```diff
@@ -107,15 +107,15 @@
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils : float
+        iUtils: float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the neighbourhood resting value of a fuzzy item in the transaction
     """
 
     def __init__(self, tid, iUtil, rUtil):
         self.tid = tid
@@ -127,15 +127,15 @@
     """
     A class calculate the regions
 
     :Attributes:
 
         low : int
             low region value
-        middle : int
+        middle: int
             middle region value
         high : int
             high region values
     """
 
     def __init__(self, quantity, regionsNumber):
         self.low = 0
@@ -198,58 +198,58 @@
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
             Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
-        neighbors : map
+        neighbors: map
             keep track of neighbours of elements
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-        itemsCnt : int
+        itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum : map
+        mapItemsLowSum: map
             To keep track of low region values of items
-        mapItemsMidSum : map
+        mapItemsMidSum: map
             To keep track of middle region values of items
-        mapItemsHighSum : map
+        mapItemsHighSum: map
             To keep track of high region values of items
-        mapItemSum : map
+        mapItemSum: map
             To keep track of sum of Fuzzy Values of items
-        mapItemRegions : map
+        mapItemRegions: map
             To Keep track of fuzzy regions of item
-        joinsCnt : int
+        jointCnt: int
             To keep track of the number of FFI-list that was constructed
-        BufferSize : int
+        BufferSize: int
             represent the size of Buffer
-        itemSetBuffer : list
+        itemBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function            
-        convert(value)
+        convert(value):
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         Intersection(neighbourX,neighbourY)
             Return common neighbours of 2 itemSet Neighbours
@@ -457,15 +457,107 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._mapNeighbours()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                regions = _Regions(int(quantities[i]), 3)
+                item = items[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
+        listOfFFList = []
+        mapItemsToFFLIST = {}
+        self._minSup = self._convert(self._minSup)
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfFFList.append(fuList)
+        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                regions = _Regions(int(quantities[i]), 3)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    if self._mapItemNeighbours.get(pair.item) is None:
+                        continue
+                    if revisedTransaction[j].item in self._mapItemNeighbours[pair.item]:
+                        remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        itemNeighbours = list(self._mapItemNeighbours.keys())
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -568,15 +660,15 @@
         :param prefix: the prefix patterns of FFSPMiner
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup: int
+        :type minSup:int
         :param itemNeighbours: the set of common neighbours of prefix
         :type itemNeighbours: list or set
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
@@ -595,16 +687,16 @@
         """
         A function to get common neighbours from 2 itemSets
 
         :param neighbourX: the set of neighbours of itemSet 1
         :type neighbourX: set or list
         :param neighbourY: the set of neighbours of itemSet 2
         :type neighbourY: set or list
-        :return: set of common neighbours of 2 itemSets
-        :rtype: set
+        :return : set of common neighbours of 2 itemSets
+        :rtype :set
         """
         result = []
         if neighbourX is None or neighbourY is None:
             return result
         for i in range(0, len(neighbourX)):
             if neighbourX[i] in neighbourY:
                 result.append(neighbourX[i])
@@ -629,48 +721,49 @@
         """
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
     def _construct(self, px, py):
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
-        :param px: the itemSet px
-        :type px: FFI-List
-        :param py: itemSet py
-        :type py: FFI-List
-        :return: the itemSet of pxy(px and py)
-        :rtype: FFI-List
+        :param px:the itemSet px
+        :type px:FFI-List
+        :param py:itemSet py
+        :type py:FFI-List
+        :return :the itemSet of pxy(px and py)
+        :rtype :FFI-List
         """
         pxyUL = _FFList(py.item)
         for ex in px.elements:
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
             eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
             pxyUL.addElement(eXY)
         return pxyUL
 
     def _findElementWithTID(self, uList, tid):
         """
         To find element with same tid as given
 
-        :param uList: fuzzyList
-        :type uList: FFI-List
-        :param tid: transaction id
-        :type tid: int
-        :return: element tid as given
+        :param uList:fuzzyList
+        :type uList:FFI-List
+        :param tid:transaction id
+        :type tid:int
+        :return:element tid as given
         :rtype: element if exist or None
         """
         List = uList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,27 +46,27 @@
                     employ in PAMI
 
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        minSup : integer or float or str
+        minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime : float
+        startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py`

 * *Files 4% similar despite different names*

```diff
@@ -48,24 +48,23 @@
 import PAMI.fuzzyGeoreferencedPeriodicFrequentPattern.basic.abstract as _ab
 from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
-
     :Attributes:
 
-         item : int
+         item: int
              the item name
-         sumIUtil : float
+         sumIUtil: float
              the sum of utilities of a fuzzy item in database
-         sumRUtil : float
+         sumRUtil: float
              the sum of resting values of a fuzzy item in database
-         elements : list
+         elements: list
              a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
@@ -102,15 +101,15 @@
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils : float
+        iUtils: float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the neighbourhood resting value of a fuzzy item in the transaction
     """
 
     def __init__(self, tid, iUtil, rUtil):
         self.tid = tid
@@ -156,50 +155,58 @@
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
             Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
-        neighbors : map
+        neighbors: map
             keep track of neighbours of elements
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-        itemsCnt : int
+        itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
-        mapItemSum : map
+        mapItemsLowSum: map
+            To keep track of low region values of items
+        mapItemsMidSum: map
+            To keep track of middle region values of items
+        mapItemsHighSum: map
+            To keep track of high region values of items
+        mapItemSum: map
             To keep track of sum of Fuzzy Values of items
-        joinsCnt : int
+        mapItemRegions: map
+            To Keep track of fuzzy regions of item
+        jointCnt: int
             To keep track of the number of FFI-list that was constructed
-        BufferSize : int
+        BufferSize: int
             represent the size of Buffer
-        itemSetBuffer list
+        itemBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value)
+        convert(value):
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         Intersection(neighbourX,neighbourY)
             Return common neighbours of 2 itemSet Neighbours
@@ -282,33 +289,41 @@
         self._dbLen = 0
 
     def _compareItems(self, o1, o2) -> int:
         """
         A Function that sort all FFI-list in ascending order of Support
 
         :param o1: First FFI-list
+
         :type o1: _FFList
+
         :param o2: Second FFI-list
-        :type o2: _FFList
-        :return: Comparison Value
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
     def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
+
         :type value: int or float or str
+
         :return: converted value
+
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
@@ -409,15 +424,106 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._mapNeighbours()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        recent_occur = {}
+        for line in range(len(self._transactionsDB)):
+            item_list = self._transactionsDB[line]
+            fuzzyValues_list = self._fuzzyValuesDB[line]
+            ts = self._ts[line]
+            self._dbLen += 1
+            """
+            The section below is for:
+            1.Finding the support of each item's region in the entire database
+            2.Finding the periodic patterns of the data
+            3.Trimming off the patterns whose support is less than minSupport
+            """
+            for i in range(0, len(item_list)):
+                item = item_list[i]
+                if item in self._tidList:
+                    self._tidList[item].append(ts - recent_occur[item][-1])
+                    recent_occur[item].append(ts)
+                else:
+                    self._tidList[item] = [ts]
+                    recent_occur[item] = [ts]
+                fuzzy_ref = fuzzyValues_list[i]
+                if item[0] in self._mapItemNeighbours:
+                    if item in self._itemSupData.keys():
+                        self._itemSupData[item] += fuzzy_ref
+                    else:
+                        self._itemSupData[item] = fuzzy_ref
+        for item in self._tidList.keys():
+            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
+        del recent_occur
+        """
+        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
+        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
+        Step2. prune out all items whose regional support is less than the given minSup
+        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
+        """
+
+        listOfFFList = []
+        mapItemsToFFLIST = {}
+        region_label = []
+        #self._minSup = self._convert(self._minSup)
+        for item in self._itemSupData.keys():
+            if self._itemSupData[item] >= self._minSup:
+                self._mapItemSum[item] = self._itemSupData[item]
+                fuList = _FFList(item)
+                if int(self._maxPer) >= max(self._tidList[item]):
+                    fuList.isPeriodic = True
+                mapItemsToFFLIST[item] = fuList
+                listOfFFList.append(fuList)
+        del self._itemSupData
+        del self._tidList
+        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for j in range(len(self._transactionsDB)):
+            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
+            fuzzy_list = [self._fuzzyValuesDB[j][i] for i in range(len(self._fuzzyValuesDB[j])) if self._transactionsDB[j][i] in self._mapItemSum.keys()]
+            revisedTransaction = []
+            for i in range(0, len(item_list)):
+                pair = _Pair()
+                pair.item = item_list[i]
+                fuzzy_ref = fuzzy_list[i]
+                pair.quantity = fuzzy_ref
+                if pair.quantity > 0:
+                    revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            qaunt = {}
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                qaunt[pair.item[0]] = pair.quantity
+                remainUtil = 0
+                temp = list(set(self._mapItemNeighbours[pair.item[0]]).intersection(set(qaunt.keys())))
+                # print(temp, self._mapItemNeighbours[pair.item[0]], qaunt)
+                for j in temp:
+                    remainUtil += float(qaunt[j])
+                del temp
+                remainingUtility = remainUtil
+                FFListObject = mapItemsToFFLIST[pair.item]
+                element = _Element(tid, pair.quantity, remainingUtility)
+                FFListObject.addElement(element)
+            del qaunt
+            tid += 1
+        itemNeighbours = list(self._mapItemNeighbours.keys())
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._mapNeighbours()
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py`

 * *Files 11% similar despite different names*

```diff
@@ -48,21 +48,21 @@
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-         item : int
+         item: int
              the item name
-         sumIUtil : float
+         sumIUtil: float
              the sum of utilities of a fuzzy item in database
-         sumRUtil : float
+         sumRUtil: float
              the sum of resting values of a fuzzy item in database
-         elements : list
+         elements: list
              a list of elements contain tid,Utility and resting values of element in each transaction
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
             Method to print elements
@@ -98,15 +98,15 @@
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils : float
+        iUtils: float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the neighbourhood resting value of a fuzzy item in the transaction
     """
 
     def __init__(self, tid, iUtil, rUtil):
         self.tid = tid
@@ -153,52 +153,58 @@
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
             Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
-        neighbors : map
+        neighbors: map
             keep track of neighbours of elements
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-        itemsCnt : int
+        itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
-        mapItemSum : map
+        mapItemsLowSum: map
+            To keep track of low region values of items
+        mapItemsMidSum: map
+            To keep track of middle region values of items
+        mapItemsHighSum: map
+            To keep track of high region values of items
+        mapItemSum: map
             To keep track of sum of Fuzzy Values of items
-        mapItemRegions : map
+        mapItemRegions: map
             To Keep track of fuzzy regions of item
-        joinsCnt : int
+        jointCnt: int
             To keep track of the number of FFI-list that was constructed
-        BufferSize : int
+        BufferSize: int
             represent the size of Buffer
-        itemSetBuffer list
+        itemBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value)
+        convert(value):
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         Intersection(neighbourX,neighbourY)
             Return common neighbours of 2 itemSet Neighbours
@@ -286,47 +292,56 @@
         self._LabelKey = {}
 
     def _compareItems(self, o1, o2):
         """
         A Function that sort all FFI-list in ascending order of Support
 
         :param o1: First FFI-list
+
         :type o1: _FFList
+
         :param o2: Second FFI-list
-        :type o2: _FFList
-        :return: Comparison Value
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
     def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
+
         :type value: int or float or str
+
         :return: converted value
+
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
             else:
                 value = int(value)
         return value
 
     def _fuzzyMembershipFunc(self):
+
         try:
             with open(self._FuzFile, 'r', encoding='utf-8') as f:
                 count = 0
                 for line in f:
                     line = line.split("\n")[0]
                     parts = line.split(" ")
                     lowerBound = parts[0].strip()
@@ -384,17 +399,14 @@
                             self._transactionsDB.append([x for x in items])
                             self._fuzzyValuesDB.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _mapNeighbours(self):
-        """
-        A function to map items to their Neighbours
-        """
         self._mapItemNeighbours = {}
         if isinstance(self._nFile, _ab._pd.DataFrame):
             data, items = [], []
             if self._nFile.empty:
                 print("its empty..")
             i = self._nFile.columns.values.tolist()
             if 'items' in i:
@@ -461,15 +473,117 @@
         return
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._mapNeighbours()
+        self._creatingItemSets()
+        self._fuzzyMembershipFunc()
+        self._finalPatterns = {}
+        recent_occur = {}
+        for line in range(len(self._transactionsDB)):
+            item_list = self._transactionsDB[line]
+            fuzzyValues_list = self._fuzzyValuesDB[line]
+            self._dbLen += 1
+            """
+            This section below is for:
+            1.Finding the support of each item's region in the entire database
+            2.Finding the periodic patterns of the data
+            3.Trimming off the patterns whose support is less than minSupport
+            """
+            for i in range(0, len(item_list)):
+                item = item_list[i]
+                if item in self._tidList:
+                    self._tidList[item].append(self._dbLen - recent_occur[item][-1])
+                    recent_occur[item].append(self._dbLen)
+                else:
+                    self._tidList[item] = [self._dbLen]
+                    recent_occur[item] = [self._dbLen]
+                fuzzy_ref = fuzzyValues_list[i]
+                if item in self._mapItemNeighbours:
+                    if fuzzy_ref not in self._fuzzyRegionReferenceMap:
+                        self._Regions(int(fuzzy_ref))
+                        self._fuzzyRegionReferenceMap[fuzzy_ref] = self._list
+
+                    if item in self._itemSupData.keys():
+                        self._itemSupData[item] = [sum(i) for i in zip(self._itemSupData[item],
+                                                                       self._fuzzyRegionReferenceMap[fuzzy_ref])]
+                    else:
+                        self._itemSupData[item] = self._fuzzyRegionReferenceMap[fuzzy_ref]
+
+        for item in self._tidList.keys():
+            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
+        del recent_occur
+        """
+        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
+        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
+        Step2. prune out all items whose regional support is less than the given minSup
+        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
+        """
+
+        listOfFFList = []
+        mapItemsToFFLIST = {}
+        region_label = []
+        for i in range(0, len(self._RegionsLabel)):
+            if self._RegionsLabel[i][1] not in region_label:
+                region_label.append(str(self._RegionsLabel[i][1]))
+
+        self._minSup = self._convert(self._minSup)
+        for item in self._itemSupData.keys():
+            if max(self._itemSupData[item]) >= self._minSup:
+                self._mapItemSum[item] = max(self._itemSupData[item])
+                self._mapItemRegions[item] = region_label[self._itemSupData[item].index(self._mapItemSum[item])]
+                fuList = _FFList(item)
+                if int(self._maxPer) >= max(self._tidList[item]):
+                    fuList.isPeriodic = True
+                mapItemsToFFLIST[item] = fuList
+                listOfFFList.append(fuList)
+
+        del self._itemSupData
+        del self._tidList
+        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for j in range(len(self._transactionsDB)):
+            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
+            revisedTransaction = []
+            for i in range(0, len(item_list)):
+                pair = _Pair()
+                pair.item = item_list[i]
+                fuzzy_ref = str(self._fuzzyValuesDB[j][self._transactionsDB[j].index(pair.item)])
+                pair.quantity = self._fuzzyRegionReferenceMap[fuzzy_ref][
+                    region_label.index(self._mapItemRegions[pair.item])]
+                if pair.quantity > 0:
+                    revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            qaunt = {}
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                qaunt[pair.item] = pair.quantity
+                remainUtil = 0
+                temp = list(set(self._mapItemNeighbours[pair.item]).intersection(set(qaunt.keys())))
+                for j in temp:
+                    remainUtil += float(qaunt[j])
+                del temp
+                remainingUtility = remainUtil
+                FFListObject = mapItemsToFFLIST[pair.item]
+                element = _Element(tid, pair.quantity, remainingUtility)
+                FFListObject.addElement(element)
+            del qaunt
+            tid += 1
+        itemNeighbours = list(self._mapItemNeighbours.keys())
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._mapNeighbours()
@@ -582,15 +696,15 @@
         :param prefix: the prefix patterns of FFSPMiner
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup: int
+        :type minSup:int
         :param itemNeighbours: the set of common neighbours of prefix
         :type itemNeighbours: list or set
         """
         for i in range(0, len(FSFIM)):
             _FFListObject1 = FSFIM[i]
             if _FFListObject1.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, _FFListObject1, _FFListObject1.sumIUtil)
@@ -609,16 +723,16 @@
         """
         A function to get common neighbours from 2 itemSets
 
         :param neighbourX: the set of neighbours of itemSet 1
         :type neighbourX: set or list
         :param neighbourY: the set of neighbours of itemSet 2
         :type neighbourY: set or list
-        :return: set of common neighbours of 2 itemSets
-        :rtype: set
+        :return : set of common neighbours of 2 itemSets
+        :rtype :set
         """
         result = []
         if neighbourX is None or neighbourY is None:
             return result
         for i in range(0, len(neighbourX)):
             if neighbourX[i] in neighbourY:
                 result.append(neighbourX[i])
@@ -652,20 +766,20 @@
         """
         return self._endTime - self._startTime
 
     def _construct(self, _FFListObject1, _FFListObject2):
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
-        :param _FFListObject1: the itemSet px
-        :type _FFListObject1: FFI-List
-        :param _FFListObject2: itemSet py
-        :type _FFListObject2: FFI-List
-        :return: the itemSet of pxy(px and py)
-        :rtype: FFI-List
+        :param _FFListObject1:the itemSet px
+        :type _FFListObject1:FFI-List
+        :param _FFListObject2:itemSet py
+        :type _FFListObject2:FFI-List
+        :return :the itemSet of pxy(px and py)
+        :rtype :FFI-List
         """
         recent_occur, first_occur, tid = 0, 0, 0
         periodlist = []
         _newFFListObject = _FFList(_FFListObject2.item)
         for Ob1Element in _FFListObject1.elements:
             Ob2Element = self._findElementWithTID(_FFListObject2, Ob1Element.tid)
             if Ob2Element is None:
@@ -687,19 +801,19 @@
             _newFFListObject.isPeriodic = False
         return _newFFListObject
 
     def _findElementWithTID(self, uList, tid):
         """
         To find element with same tid as given
 
-        :param uList: fuzzyList
-        :type uList: FFI-List
-        :param tid: transaction id
-        :type tid: int
-        :return: element tid as given
+        :param uList:fuzzyList
+        :type uList:FFI-List
+        :param tid:transaction id
+        :type tid:int
+        :return:element tid as given
         :rtype: element if exist or None
         """
         List = uList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,31 +40,32 @@
 from urllib.request import urlopen as _urlopen
 import functools as _functools
 
 class _fuzzySpatialFrequentPatterns(_ABC):
     """ This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
         employ in PAMI
 
+
     Attributes :
     ----------
         iFile : str
             Input file name or path of the input file
-        minSup : integer or float or str
+        minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime : float
+        startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,645 +1,509 @@
-# F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
-#
+# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# --------------------------------------------------------
 #
-#             import PAMI.fuzzyPartialPeriodicPattern.basic.F3PMiner as alg
 #
-#             obj = alg.F3PMiner(iFile, minSup, sep)
+#     from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
 #
-#             obj.mine()
+#     obj = alg.UVEclat(iFile, minSup)
 #
-#             fuzzyPartialPeriodicPatterns = obj.getPatterns()
+#     obj.startMine()
 #
-#             print("Total number of Fuzzy Partial Periodic Patterns:", len(fuzzyPartialPeriodicPatterns))
+#     frequentPatterns = obj.getPatterns()
 #
-#             obj.save(oFile)
+#     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             Df = obj.getPatternInDataFrame()
+#     obj.save(oFile)
 #
-#             memUSS = obj.getMemoryUSS()
+#     Df = obj.getPatternsAsDataFrame()
 #
-#             print("Total Memory in USS:", memUSS)
+#     memUSS = obj.getMemoryUSS()
 #
-#             memRSS = obj.getMemoryRSS()
+#     print("Total Memory in USS:", memUSS)
 #
-#             print("Total Memory in RSS", memRSS)
+#     memRSS = obj.getMemoryRSS()
 #
-#             run = obj.getRuntime()
+#     print("Total Memory in RSS", memRSS)
 #
-#             print("Total ExecutionTime in seconds:", run)
+#     run = obj.getRuntime()
+#
+#     print("Total ExecutionTime in seconds:", run)
 #
-
 
 __copyright__ = """
-
-Copyright (C)  2021 Rage Uday Kiran
-
-     This program is free software: you can redistribute it and/or modify
-     it under the terms of the GNU General Public License as published by
-     the Free Software Foundation, either version 3 of the License, or
-     (at your option) any later version.
-
-     This program is distributed in the hope that it will be useful,
-     but WITHOUT ANY WARRANTY; without even the implied warranty of
-     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-     GNU General Public License for more details.
-
-     You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
+ Copyright (C)  2021 Rage Uday Kiran
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
 """
 
-from PAMI.fuzzyPartialPeriodicPatterns.basic import abstract as _ab
-from deprecated import deprecated
+import operator as _operator
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 
 
-class _FFList:
-    """
-     A class represent a Fuzzy List of an element
-
-    :Attributes:
-
-         item : int
-             the item name
-         sumIUtil : float
-             the sum of utilities of an fuzzy item in database
-         sumRUtil : float
-             the sum of resting values of a fuzzy item in database
-         elements : list
-             a list of elements contain tid,Utility and resting values of element in each transaction
-
-    :Methods:
-
-        addElement(element)
-            Method to add an element to this fuzzy list and update the sums at the same time.
-        printElement(e)
-            Method to print elements
-
-    """
-
-    def __init__(self, itemName):
-        self.item = itemName
-        self.sumIUtil = 0.0
-        self.elements = []
-
-    def addElement(self, element):
-        """
-        A Method that add a new element to FFList
-
-        :param element: an element to be added to FFList
-        :type element: Element
-        """
-        self.sumIUtil += element.iUtils
-        self.elements.append(element)
-
-    def printElement(self):
-        """
-        A method to print elements
-        """
-        for ele in self.elements:
-            print(ele.tid, ele.iUtils, ele.rUtils)
+_minSup = float()
+_finalPatterns = {}
 
 
-class _Element:
+class _Item:
     """
-    A class represents an Element of a fuzzy list
-
+    A class used to represent the item with probability in transaction of dataset
     :Attributes:
-
-        tid : int
-            keep tact of transaction id
-        iUtils : float
-            the utility of an fuzzy item in the transaction
-        rUtils : float
-            the  resting value of an fuzzy item in the transaction
-    """
-
-    def __init__(self, tid, iUtil):
-        self.tid = tid
-        self.iUtils = iUtil
-
-
-class _Pair:
-    """
-    A class to store item and it's quantity together
+        item : int or word
+            Represents the name of the item
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self):
-        self.item = 0
-        self.quantity = 0
+    def __init__(self, item, probability):
+        self.item = item
+        self.probability = probability
 
 
-class F3PMiner(_ab._fuzzyPartialPeriodicPatterns):
+class UVEclat(_ab._frequentPatterns):
     """
-    :Description:   F3PMiner algorithm discovers the fuzzy partial periodic patterns in quantitative Irregulat multiple timeseries databases.
-    
+    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
     :Reference:
-
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param maxPer: float :
-                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-
-
+    Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
+    SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983984,
+    https://doi.org/10.1145/1982185.1982399
     :Attributes:
-
-        iFile : string
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
-        oFile : string
-               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given minimum support
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
         memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
-        startTime : float
-               To record the start time of the mining process
-        endTime : float
+            To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
             To record the completion time of the mining process
-        itemsCnt : int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsGSum : map
-            To keep track of G region values of items
-        mapItemsMidSum: map
-            To keep track of M region values of items
-        mapItemsHSum: map
-            To keep track of H region values of items
-        mapItemSum: map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        joinsCnt: int
-            To keep track of the number of ffi-list that was constructed
-        BufferSize: int
-            represent the size of Buffer
-        itemSetBuffer list
-            to keep track of items in buffer
-
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            To represent the total no of transaction
+        tree : class
+            To represents the Tree class
+        itemSetCount : int
+            To represents the total no of patterns
+        finalPatterns : dict
+            To store the complete patterns
     :Methods:
-
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        storePatternsInFile(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
+        getPatternsInDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value)
-            To convert the given user specified value
-        compareItems(o1, o2)
-            A Function that sort all ffi-list in ascending order of Support
-        F3PMining(prefix, prefixLen, FSFIM, minSup)
-            Method generate ffi from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        findElementWithTID(uList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil)
-            To Store the patten
-
-    **Executing the code on terminal :**
-    ---------------------------------------
-
-    .. code-block:: console
-
-      Format:
-
-      (.venv) $ python3 F3PMiner.py <inputFile> <outputFile> <minSup> <separator>
-
-      Example Usage:
-
-      (.venv) $ python3  F3PMiner.py sampleTDB.txt output.txt 6
-
-    .. note:: minSup will be considered in percentage of database transactions
-
-
-    **Sample run of importing the code:**
-    --------------------------------------
-
-        from PAMI.fuzzyPartialPeriodicPatterns import F3PMiner as alg
-
-        obj = alg.F3PMiner("input.txt", 2)
-
-        obj.mine()
-
-        fuzzyPartialPeriodicPatterns = obj.getPatterns()
-
-        print("Total number of Fuzzy Frequent Patterns:", len(fuzzyPartialPeriodicPatterns))
-
-        obj.save("outputFile")
-
-        memUSS = obj.getMemoryUSS()
-
-        print("Total Memory in USS:", memUSS)
-
-        memRSS = obj.getMemoryRSS()
-
-        print("Total Memory in RSS", memRSS)
-
-        run = obj.getRuntime()
-
-        print("Total ExecutionTime in seconds:", run)
-
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+    **Methods to execute code on terminal**
+    ------------------------------------------
+            Format:
+                      >>> python3 uveclat.py <inputFile> <outputFile> <minSup>
+            Example:
+                      >>>  python3 uveclat.py sampleTDB.txt patterns.txt 3
+                      .. note:: minSup  will be considered in support count or frequency
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    .. code-block:: python
+            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+            obj = alg.UVEclat(iFile, minSup)
+            obj.startMine()
+            frequentPatterns = obj.getPatterns()
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            obj.save(oFile)
+            Df = obj.getPatternsAsDataFrame()
+            memUSS = obj.getmemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = obj.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = obj.getRuntime()
+            print("Total ExecutionTime in seconds:", run)
     **Credits:**
-    -------------
-        The complete program was written by PALLA Likhitha under the supervision of Professor Rage Uday Kiran.
+    ---------------
+         The complete program was written by   P.Likhitha    under the supervision of Professor Rage Uday Kiran.
     """
-    
     _startTime = float()
     _endTime = float()
     _minSup = str()
-    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _sep = "\t"
-
-    def __init__(self, iFile, minSup, sep="\t"):
-        super().__init__(iFile, minSup, sep)
-        self._startTime = 0
-        self._endTime = 0
-        self._itemsCnt = 0
-        self._mapItemSum = {}
-        self._mapItemRegions = {}
-        self._joinsCnt = 0
-        self._BufferSize = 200
-        self._itemSetBuffer = []
-        self._transactions = []
-        self._fuzzyValues = []
-        self._ts = []
-        self._finalPatterns = {}
-        self._dbLen = 0
-
-    def _compareItems(self, o1, o2):
-        """
-        A Function that sort all ffi-list in ascending order of Support
-
-        :param o1: First FFI-list
-        :type o1: _FFList
-        :param o2: Second FFI-list
-        :type o2: _FFList
-        :return: Comparison Value
-        :rtype: int
-        """
-        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
-        if compare == 0:
-            if o1.item < o2.item:
-                return -1
-            elif o1.item > o2.item:
-                return 1
-            else:
-                return 0
-        else:
-            return compare
+    _Database = []
+    _tidList = {}
+    _rank = {}
 
-    def _convert(self, value):
+    def _creatingItemSets(self):
         """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :type value: int or float or str
-        :return: converted value
-        :rtype: float
+        Scans the dataset
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbLen * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._dbLen * value)
-            else:
-                value = int(value)
-        return value
-
-    def _creatingItemsets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        self._transactions, self._fuzzyValues, self._Database, self._ts = [], [], [], []
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._transactions = self._iFile['Transactions'].tolist()
-            if 'fuzzyValues' in i:
-                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
+                    line.strip()
                     line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = line.split(":")
-                    parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
-                    items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
-                    self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([x for x in quantities])
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line = line.strip()
-                            parts = line.split(":")
-                            parts[0] = parts[0].strip()
-                            parts[1] = parts[1].strip()
-                            parts[2] = parts[2].strip()
-                            times = parts[0].split(self._sep)
-                            items = parts[1].split(self._sep)
-                            quantities = parts[2].split(self._sep)
-                            #print(times, items, quantities)
-                            _time = [x for x in times if x]
-                            items = [x for x in items if x]
-                            quantities = [float(x) for x in quantities if x]
-                            tempList = []
-                            for k in range(len(_time)):
-                                ite = "(" + _time[k] + "," + items[k] + ")"
-                                tempList.append(ite)
-                            self._ts.append([x for x in times])
-                            self._transactions.append([x for x in tempList])
-                            self._fuzzyValues.append([x for x in quantities])
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def _frequentOneItem(self):
         """
-        fuzzy-Frequent pattern mining process will start from here
+        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
         """
-        self.mine()
 
-    def mine(self):
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[str(j.item)] = j.probability
+                    self._tidList[str(j.item)] = {k: j.probability}
+                else:
+                    mapSupport[str(j.item)] += j.probability
+                    self._tidList[str(j.item)].update({k: j.probability})
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
+        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
+        return list(plist.keys())
+
+    @staticmethod
+    def _check(i, x):
+        """
+        To check the presence of item or pattern in transaction
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    @staticmethod
+    def _convert(value):
         """
-        fuzzy-Frequent pattern mining process will start from here
+        To convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :return: converted type minSup value
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemsets()
-        for line in range(len(self._transactions)):
-            times = self._ts[line]
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = float(value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+            else:
+                value = int(value)
+        return value
+
+    def _removeFalsePositives(self):
+        """
+        To remove the false positive patterns generated in frequent patterns
+        :return: patterns with accurate probability
+        """
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
                 else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfffilist = []
-        mapItemsToFFLIST = {}
-        #self._minSup = float(self._minSup)
+                    s = 1
+                    check = self._check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
+
+    @staticmethod
+    def _Intersection(tidSetx, tidSetY):
+        """
+        This function is used to find the intersection
+        :param tidSetx: the timestamp of a patterns
+        :type tidSetx: dict
+        :param tidSetY: the timestamp of a patterns
+        :type tidSetY: dict
+        """
+        tids = []
+        support = []
+        tidDict = {}
+        for x, y in tidSetx.items():
+            for x1, y1 in tidSetY.items():
+                if x == x1:
+                    tids.append(x)
+                    support.append(y * y1)
+                    tidDict.update({x: y * y1})
+        return tidDict
+
+    def _calculateExpSup(self, tidList):
+        """
+        This function is used to calculate support of tidList
+        :param tidList: timestamp of a list.
+        :type tidList: List
+        """
+        return sum(tidList.values())
+
+    def _save(self, prefix, suffix, tidSetI):
+        """
+        Saves the patterns that satisfy the periodic frequent property.
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
+        """
+
+        global _finalPatterns
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = self._calculateExpSup(tidSetI)
+        _finalPatterns[tuple(prefix)] = val
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y) >= self._minSup:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
+
+    def startMine(self):
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        """
+        global _minSup
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            # print(type(self._mapItemSum[item]))
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfffilist.append(fuList)
-        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.quantity = quantities[i]
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        self._F3PMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i+1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y1) >= self._minSup:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetI)
+        self._removeFalsePositives()
+        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
         self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _F3PMining(self, prefix, prefixLen, FSFIM, minSup):
-        """
-        Generates ffi from prefix
-
-        :param prefix: the prefix patterns of ffi
-        :type prefix: len
-        :param prefixLen: the length of prefix
-        :type prefixLen: int
-        :param FSFIM: the Fuzzy list of prefix itemSets
-        :type FSFIM: list
-        :param minSup: the minimum support of
-        :type minSup:int
-        """
-        for i in range(0, len(FSFIM)):
-            X = FSFIM[i]
-            exULs = []
-            if X.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-                for j in range(i + 1, len(FSFIM)):
-                    Y = FSFIM[j]
-                    exULs.append(self._construct(X, Y))
-                    self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, X.item)
-                self._F3PMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
-
     def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-       """
+        """
+
         return self._memoryRSS
 
     def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
-
+        """Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self._endTime - self._startTime
 
-    def _construct(self, px, py):
-        """
-        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
-
-        :param px:the itemSet px
-        :type px:ffi-List
-        :param py:itemSet py
-        :type py:ffi-List
-        :return :the itemSet of pxy(px and py)
-        :rtype :ffi-List
-        """
-        pxyUL = _FFList(py.item)
-        for ex in px.elements:
-            ey = self._findElementWithTID(py, ex.tid)
-            if ey is None:
-                continue
-            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)))
-            pxyUL.addElement(eXY)
-        return pxyUL
-
-    def _findElementWithTID(self, uList, tid):
-        """
-        To find element with same tid as given
-
-        :param uList: fuzzyList
-        :type uList: ffi-List
-        :param tid: transaction id
-        :type tid: int
-        :return: element  tid as given
-        :rtype: element if exit or None
-        """
-        List = uList.elements
-        first = 0
-        last = len(List) - 1
-        while first <= last:
-            mid = (first + last) >> 1
-            if List[mid].tid < tid:
-                first = mid + 1
-            elif List[mid].tid > tid:
-                last = mid - 1
-            else:
-                return List[mid]
-        return None
-
-    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
-        """
-        To Store the patten
-
-        :param prefix: prefix of itemSet
-        :type prefix: list
-        :param prefixLen: length of prefix
-        :type prefixLen: int
-        :param item: the last item
-        :type item: int
-        :param sumIUtil: sum of utility of itemSet
-        :type sumIUtil: float
-
-        """
-        self._itemsCnt += 1
-        res = ""
-        for i in range(0, prefixLen):
-            res += str(prefix[i]) + "\t"
-        res += str(item)
-        res1 = str(sumIUtil)
-        self._finalPatterns[res] = res1
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """
-        Storing final frequent patterns in a dataframe
-
+        """Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def getPatterns(self):
+    def save(self, oFile):
+        """Complete set of frequent patterns will be loaded in to an output file
+        :param oFile: name of the output file
+        :type oFile: csv file
         """
-        Function to send the set of frequent patterns after completion of the mining process
+        self.oFile = oFile
+        writer = open(self.oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
+    def getPatterns(self):
+        """ Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to a output file
-
-        :param outFile: name of the output file
-        :type outFile: csv file
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
-
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Fuzzy Partial Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = F3PMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
+        print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _ap.printResults()
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,27 +46,27 @@
     :Description: This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                   employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        minSup : integer or float or str
+        minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime : float
+        startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py`

 * *Files 8% similar despite different names*

```diff
@@ -70,23 +70,23 @@
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
     :Attributes:
 
-        item : int
+        item: int
             the item name
-        sumLUtil : float
+        sumLUtil: float
             the sum of utilities of a fuzzy item in database
-        sumRUtil : float
+        sumRUtil: float
             the sum of resting values of a fuzzy item in database
-        elements : list
+        elements: list
             list of elements contain tid,Utility and resting values of element in each transaction
-        maxPeriod : int
+        maxPeriod: int
             it represents the max period of a item
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
@@ -217,15 +217,15 @@
         lastTIDs: map
             represent the last tid of fuzzy items
         itemsToRegion: map
             represent items with respective regions
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -422,15 +422,89 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Fuzzy periodic Frequent pattern mining process will start from here
         """
-        self.mine()
+        maxTID = 0
+        lastTIDs = {}
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        tid = int()
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            self._dbLen += 1
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            if tid < maxTID:
+                maxTID = tid
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfFFIList = []
+        mapItemsToFFLIST = {}
+        # self._minSup = self._convert(self._minSup)
+        self._minSup = float(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fUList = _FFList(item)
+                k = tuple([item])
+                mapItemsToFFLIST[k] = fUList
+                listOfFFIList.append(fUList)
+                lastTIDs[item] = tid
+        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                item = pair.item
+                pair.quantity = quantities[i]
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i - 1, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                if pair.quantity > remainUtil:
+                    remainingUtility = pair.quantity
+                else:
+                    remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(tuple([pair.item])) is not None:
+                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item])]
+                    if len(FFListOfItem.elements) == 0:
+                        element = _Element(tid, pair.quantity, remainingUtility, 0)
+                    else:
+                        if lastTIDs[pair.item] == tid:
+                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
+                        else:
+                            lastTid = FFListOfItem.elements[-1].tid
+                            curPer = tid - lastTid
+                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
+                    FFListOfItem.addElement(element)
+        self._FPFPMining(self._itemSetBuffer, 0, listOfFFIList)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
         Fuzzy periodic Frequent pattern mining process will start from here
         """
         maxTID = 0
         lastTIDs = {}
@@ -584,18 +658,18 @@
         return pxyUL
 
     def _findElementWithTID(self, UList, tid) -> _Element:
         """
         To find element with same tid as given
 
         :param UList: fuzzy list
-        :type UList: FFI-List
+        :type UList:FFI-List
         :param tid:transaction id
-        :type tid: int
-        :return: element eith tid as given
+        :type tid:int
+        :return:element eith tid as given
         :rtype: element if exist or None
         """
         List = UList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/HMiner.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,24 @@
-# Sample run of importing the code:
-# -------------------------------------
+#  High Utility itemSet Mining (HMinER) is an important algorithm to miner High utility items from the database.
 #
-#             from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
+# **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#             obj =alg.FPFPMiner("input.txt",2,3)
+#
+#             from PAMI.highUtilityPattern.basic import HMiner as alg
+#
+#             obj = alg.HMiner("input.txt", 35)
 #
 #             obj.mine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of high utility Patterns:", len(Patterns))
 #
-#             obj.save("output.txt")
+#             obj.save("output")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -25,14 +28,15 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -40,719 +44,784 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
 
-     This program is free software: you can redistribute it and/or modify
-     it under the terms of the GNU General Public License as published by
-     the Free Software Foundation, either version 3 of the License, or
-     (at your option) any later version.
-
-     This program is distributed in the hope that it will be useful,
-     but WITHOUT ANY WARRANTY; without even the implied warranty of
-     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-     GNU General Public License for more details.
-
-     You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
-from PAMI.fuzzyPeriodicFrequentPattern.basic import abstract as _ab
+from PAMI.highUtilityPattern.basic import abstract as _ab
 from deprecated import deprecated
 
 
-class _FFList:
+class _Element:
     """
-    A class represent a Fuzzy List of an element
+    A class represents an Element of a utility list.
 
-    :Attributes:
-
-        item : int
-            the item name
-        sumLUtil : float
-            the sum of utilities of a fuzzy item in database
-        sumRUtil : float
-            the sum of resting values of a fuzzy item in database
-        elements : list
-            list of elements contain tid,Utility and resting values of element in each transaction
-        maxPeriod : int
-            it represents the max period of a item
-
-    :Methods:
-
-        addElement(element)
-            Method to add an element to this fuzzy list and update the sums at the same time.
-        printElement(e)
-            Method to print elements
+    :Attributes :
 
+        ts : int
+            keep tact of transaction id
+        nu : int
+            non-closed itemSet utility
+        nru : int
+             non-closed remaining utility
+        pu : int
+            prefix utility
+        ppos: int
+            position of previous item in the list
     """
 
-    def __init__(self, itemName):
-        self.item = itemName
-        self.sumLUtil = 0.0
-        self.sumRUtil = 0.0
-        self.elements = []
-        self.maxPeriod = 0
-
-    def addElement(self, element):
-        """
-        A Method that add a new element to FFList
-
-        :param element: an element to be added to FFList
-        :type element: Element
-        """
-        self.sumLUtil += element.lUtils
-        self.sumRUtil += element.rUtils
-        self.elements.append(element)
-        self.maxPeriod = max(self.maxPeriod, element.period)
-
-    def printElement(self):
-        """
-        A Method to Print elements in the FFList
-        """
-        for ele in self.elements:
-            print(ele.tid, ele.lUtils, ele.rUtils, ele.period)
-
-
-class _Element:
-    """
-    A class represents an Element of a fuzzy list
+    def __init__(self, tid, nu, nru, pu, ppos):
+        self.tid = tid
+        self.nu = nu
+        self.nru = nru
+        self.pu = pu
+        self.ppos = ppos
 
-    :Attributes:
 
-        tid : int
-            keep tact of transaction id
-        lUtils : float
-            the utility of a fuzzy item in the transaction
-        rUtils : float
-            the resting value of a fuzzy item in the transaction
-        period : int
-            represent the period of the element
+class _CUList:
     """
+    A class represents a UtilityList
 
-    def __init__(self, tid, iUtil, rUtil, period):
-        self.tid = tid
-        self.lUtils = iUtil
-        self.rUtils = rUtil
-        self.period = period
+    :Attributes :
 
+        item: int
+            item 
+        sumNu: long
+            the sum of item utilities
+        sumNru: long
+            the sum of remaining utilities
+        sumCu : long
+            the sum of closed utilities
+        sumCru: long
+            the sum of closed remaining utilities
+        sumCpu: long
+            the sum of closed prefix utilities
+        elements: list
+            the list of elements 
+    :Methods :
 
-class _Regions:
+        addElement(element)
+            Method to add an element to this utility list and update the sums at the same time.
     """
-    A class calculate the regions
 
-    :Attributes:
+    def __init__(self, item):
+        self.item = item
+        self.sumnu = 0
+        self.sumnru = 0
+        self.sumCu = 0
+        self.sumCru = 0
+        self.sumCpu = 0
+        self.elements = []
 
-        low : int
-            low region value
-        middle : int
-            middle region value
-        high : int
-            high region values
-        """
-
-    def __init__(self, quantity, regionsNumber):
-        self.low = 0
-        self.middle = 0
-        self.high = 0
-        if regionsNumber == 3:  # if we have 3 regions
-            if 0 < quantity <= 1:
-                self.low = 1
-                self.high = 0
-                self.middle = 0
-            elif 1 < quantity <= 6:
-                self.low = float((6 - quantity) / 5)
-                self.middle = float((quantity - 1) / 5)
-                self.high = 0
-            elif 6 < quantity <= 11:
-                self.low = 0
-                self.middle = float((11 - quantity) / 5)
-                self.high = float((quantity - 6) / 5)
-            else:
-                self.low = 0
-                self.middle = 0
-                self.high = 1
+    def addElements(self, element):
+        """
+        A method to add new element to CUList
+        :param element: element to be added to CUList
+        :type element: Element
+        """
+        self.sumnu += element.nu
+        self.sumnru += element.nru
+        self.elements.append(element)
 
 
 class _Pair:
     """
-    A class to store item name and quantity together.
+    A class represent an item and its utility in a transaction
     """
 
     def __init__(self):
         self.item = 0
-        self.quantity = 0
+        self.utility = 0
 
 
-class FPFPMiner(_ab._fuzzyPeriodicFrequentPatterns):
+class HMiner(_ab._utilityPatterns):
     """
-    :Description:   Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
-                    on-trivial and challenging problem to its huge search space.we are using efficient pruning
-                    techniques to reduce the search space.
+    :Description:   High Utility itemSet Mining (HMIER) is an importent algorithm to miner High utility items from the database.
 
     :Reference:
 
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of High Utility patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of High Utility patterns
+    :param minUtil: int :
+                   The user given minUtil value.
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param maxPer: float :
                    The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
+            Name of the input file to mine complete set of frequent patterns
         oFile : file
-               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given support
-        period : int
-            periodicity of an element
+            Name of the output file to store complete set of frequent patterns
         memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
-        startTime : float
-               To record the start time of the mining process
-        endTime : float
+            To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
             To record the completion time of the mining process
-        itemsCnt : int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum : map
-            To keep track of low region values of items
-        mapItemsMidSum : map
-            To keep track of middle region values of items
-        mapItemsHighSum : map
-            To keep track of high region values of items
-        mapItemSum : map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions : map
-            To Keep track of fuzzy regions of item
-        joinsCnt : int
-            To keep track of the number of FFI-list that was constructed
-        BufferSize : int
-            represent the size of Buffer
-        itemSetBuffer list
-            to keep track of items in buffer
-        maxTID : int
-            represent the maximum tid of the database
-        lastTIDs : map
-            represent the last tid of fuzzy items
-        itemsToRegion : map
-            represent items with respective regions
+        minUtil : int
+            The user given minUtil
+        mapFMAP: list
+            EUCS map of the FHM algorithm
+        candidates: int
+            candidates genetated
+        huiCnt: int
+            huis created
+        neighbors: map
+            keep track of nighboues of elements
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value)
-            To convert the given user specified value
-        FSFIMining( prefix, prefixLen, fsFim, minSup)
-            Method generate FFI from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        findElementWithTID(UList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil,period)
-            To Store the patten
+        Explore_SearchTree(prefix, uList, minUtil)
+            A method to find all high utility itemSets
+        UpdateCLosed(x, culs, st, excul, newT, ex, ey_ts, length)
+            A method to update closed values
+        saveitemSet(prefix, prefixLen, item, utility)
+            A method to save itemSets
+        updateElement(z, culs, st, excul, newT, ex, duppos, ey_ts)
+            A method to updates vales for duplicates
+        construcCUL(x, culs, st, minUtil, length, exnighbors)
+            A method to construct CUL's database
 
-    **Executing the code on terminal :**
-    ---------------------------------------
+    **Executing the code on terminal:**
+    --------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 FPFPMiner_old.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
+      (.venv) $ python3 HMiner.py <inputFile> <outputFile> <minUtil>
 
       Example Usage:
 
-      (.venv) $ python3  FPFPMiner_old.py sampleTDB.txt output.txt 2 3
+      (.venv) $ python3 HMiner.py sampleTDB.txt output.txt 35
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
-    **Sample run of importing the code:**
+    Sample run of importing the code:
     --------------------------------------
+    .. code-block:: python
 
-        from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
-
-        obj =alg.FPFPMiner("input.txt",2,3)
-
-        obj.mine()
-
-        periodicFrequentPatterns = obj.getPatterns()
-
-        print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
-
-        obj.save("output.txt")
-
-        memUSS = obj.getMemoryUSS()
-
-        print("Total Memory in USS:", memUSS)
-
-        memRSS = obj.getMemoryRSS()
-
-        print("Total Memory in RSS", memRSS)
-
-        run = obj.getRuntime()
-
-        print("Total ExecutionTime in seconds:", run)
-
+            from PAMI.highUtilityPattern.basic import HMiner as alg
+        
+            obj = alg.HMiner("input.txt",35)
+        
+            obj.mine()
+        
+            Patterns = obj.getPatterns()
+        
+            print("Total number of high utility Patterns:", len(Patterns))
+        
+            obj.save("output")
+        
+            memUSS = obj.getMemoryUSS()
+        
+            print("Total Memory in USS:", memUSS)
+        
+            memRSS = obj.getMemoryRSS()
+        
+            print("Total Memory in RSS", memRSS)
+        
+            run = obj.getRuntime()
+        
+            print("Total ExecutionTime in seconds:", run)
+    
     **Credits:**
-    ---------------
-            The complete program was written by Sai Chitra.B under the supervision of Professor Rage Uday Kiran.
+    -----------------------------
+            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
     """
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
+    _Database = {}
+    _transactions = []
+    _utilities = []
+    _utilitySum = []
     _iFile = " "
     _oFile = " "
+    _minUtil = 0
+    _sep = "\t"
     _memoryUSS = float()
     _memoryRSS = float()
-    _sep = " "
-    _Database = []
-    _transactions = []
-    _fuzzyValues = []
-    _ts = []
 
-    def __init__(self, iFile, minSup, period, sep="\t"):
-        super().__init__(iFile, minSup, period, sep)
-        self._oFile = ""
-        self._BufferSize = 200
-        self._itemSetBuffer = []
-        self._mapItemRegions = {}
-        self._mapItemSum = {}
-        self._mapItemsHighSum = {}
+    def __init__(self, iFile1, minUtil, sep="\t"):
+        super().__init__(iFile1, minUtil, sep)
+        self._huiCount = 0
+        self._candidates = 0
+        self._mapOfTWU = {}
+        self._minutil = 0
+        self._mapFMAP = {}
         self._finalPatterns = {}
-        self._joinsCnt = 0
-        self._itemsCnt = 0
-        self._mapItemMidSum = {}
-        self._startTime = float()
-        self._endTime = float()
-        self._mapItemsLowSum = {}
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._dbLen = 0
 
-    def _compareItems(self, o1, o2):
+    def _HMiner(self, o1, o2) -> int:
         """
         A Function that sort all FFI-list in ascending order of Support
 
         :param o1: First FFI-list
+
         :type o1: _FFList
+
         :param o2: Second FFI-list
-        :type o2: _FFList
-        :return: Comparison Value
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
         :rtype: int
         """
-        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
+        compare = self._mapOfTWU[o1.item] - self._mapOfTWU[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
-    def _convert(self, value):
-        """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :type value: int  or float or str
-        :return: converted value
-        :rtype: float
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbLen * value)
-        if type(value) is str:
-            if '.' in value:
-                value = (self._dbLen * value)
-            else:
-                value = int(value)
-        return value
-
-    def _creatingItemSets(self):
+    def _creteItemsets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        data, self._transactions, self._fuzzyValues, ts = [], [], [], []
+        self._transactions, self._utilities, self._utilitySum = [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                self._ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 self._transactions = self._iFile['Transactions'].tolist()
-            if 'fuzzyValues' in i:
-                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
+            if 'Utilities' in i:
+                self._utilities = self._iFile['Utilities'].tolist()
+            if 'UtilitySum' in i:
+                self._utilitySum = self._iFile['UtilitySum'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
+                #print("hey")
                 data = _ab._urlopen(self._iFile)
-                count = 0
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
-                    parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
-                    self._ts.append(count)
-                    self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([x for x in quantities])
-                    count += 1
+                    self._transactions.append([x for x in items if x])
+                    utilities = parts[2].split(self._sep)
+                    self._utilities.append(utilities)
+                    self._utilitySum.append(int(parts[1]))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
-                        count = 0
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
-                            parts[0] = parts[0].strip()
-                            parts[2] = parts[2].strip()
                             items = parts[0].split(self._sep)
-                            quantities = parts[2].split(self._sep)
-                            self._ts.append(count)
-                            self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([x for x in quantities])
-                            count += 1
+                            self._transactions.append([x for x in items if x])
+                            utilities = parts[2].split(self._sep)
+                            self._utilities.append(utilities)
+                            self._utilitySum.append(int(parts[1]))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
-        Fuzzy periodic Frequent pattern mining process will start from here
+        Main program to start the operation
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._creteItemsets()
+        self._finalPatterns = {}
+        for line in range(len(self._transactions)):
+            items_str = self._transactions[line]
+            utility_str = self._utilities[line]
+            transUtility = self._utilitySum[line]
+            for i in range(0, len(items_str)):
+                item = items_str[i]
+                twu = self._mapOfTWU.get(item)
+                if twu == None:
+                    twu = transUtility
+                else:
+                    twu += transUtility
+                self._mapOfTWU[item] = twu
+        listOfCUList = []
+        hashTable = {}
+        mapItemsToCUList = {}
+        minutil = self._minUtil
+        for item in self._mapOfTWU.keys():
+            if self._mapOfTWU.get(item) >= self._minUtil:
+                uList = _CUList(item)
+                mapItemsToCUList[item] = uList
+                listOfCUList.append(uList)
+        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._HMiner))
+        tid = 1
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            utilities = self._utilities[line]
+            ru = 0
+            newTwu = 0
+            tx_key = []
+            revisedTrans = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.utility = int(utilities[i])
+                if self._mapOfTWU.get(pair.item) >= self._minUtil:
+                    revisedTrans.append(pair)
+                    tx_key.append(pair.item)
+                    newTwu += pair.utility
+            revisedTrans.sort(key=_ab._functools.cmp_to_key(self._HMiner))
+            tx_key1 = tuple(tx_key)
+            if len(revisedTrans) > 0:
+                if tx_key1 not in hashTable.keys():
+                    hashTable[tx_key1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
+                    for i in range(len(revisedTrans) - 1, -1, -1):
+                        pair = revisedTrans[i]
+                        cuListoFItems = mapItemsToCUList.get(pair.item)
+                        element = _Element(tid, pair.utility, ru, 0, 0)
+                        if i > 0:
+                            element.ppos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
+                        else:
+                            element.ppos = - 1
+                        cuListoFItems.addElements(element)
+                        ru += pair.utility
+                else:
+                    pos = hashTable[tx_key1]
+                    ru = 0
+                    for i in range(len(revisedTrans) - 1, -1, -1):
+                        cuListoFItems = mapItemsToCUList[revisedTrans[i].item]
+                        cuListoFItems.elements[pos].nu += revisedTrans[i].utility
+                        cuListoFItems.elements[pos].nru += ru
+                        cuListoFItems.sumnu += revisedTrans[i].utility
+                        cuListoFItems.sumnru += ru
+                        ru += revisedTrans[i].utility
+                        pos = cuListoFItems.elements[pos].ppos
+                    # EUCS
+            for i in range(len(revisedTrans) - 1, -1, -1):
+                pair = revisedTrans[i]
+                mapFMAPItem = self._mapFMAP.get(pair.item)
+                if mapFMAPItem == None:
+                    mapFMAPItem = {}
+                    self._mapFMAP[pair.item] = mapFMAPItem
+                for j in range(i + 1, len(revisedTrans)):
+                    pairAfter = revisedTrans[j]
+                    twuSUm = mapFMAPItem.get(pairAfter.item)
+                    if twuSUm is None:
+                        mapFMAPItem[pairAfter.item] = newTwu
+                    else:
+                        mapFMAPItem[pairAfter.item] = twuSUm + newTwu
+            tid += 1
+        self._ExploreSearchTree([], listOfCUList, minutil)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility patterns were generated successfully using HMiner algorithm")
 
     def mine(self):
         """
-        Fuzzy periodic Frequent pattern mining process will start from here
+        Main program to start the operation
         """
-        maxTID = 0
-        lastTIDs = {}
         self._startTime = _ab._time.time()
-        self._creatingItemSets()
+        self._creteItemsets()
         self._finalPatterns = {}
-        tid = int()
         for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            self._dbLen += 1
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            if tid < maxTID:
-                maxTID = tid
-            for i in range(0, len(items)):
-                regions = _Regions(int(quantities[i]), 3)
-                item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemMidSum.keys():
-                    mid = self._mapItemMidSum[item]
-                    mid += regions.middle
-                    self._mapItemMidSum[item] = mid
+            items_str = self._transactions[line]
+            utility_str = self._utilities[line]
+            transUtility = self._utilitySum[line]
+            for i in range(0, len(items_str)):
+                item = items_str[i]
+                twu = self._mapOfTWU.get(item)
+                if twu == None:
+                    twu = transUtility
                 else:
-                    self._mapItemMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
-        listOfFFIList = []
-        mapItemsToFFLIST = {}
-        itemsToRegion = {}
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        for item1 in self._mapItemsLowSum.keys():
-            item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-                itemsToRegion[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-                itemsToRegion[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
-                itemsToRegion[item] = "H"
-            if self._mapItemSum[item] >= self._minSup:
-                fUList = _FFList(item)
-                k = tuple([item, itemsToRegion.get(item)])
-                mapItemsToFFLIST[k] = fUList
-                listOfFFIList.append(fUList)
-                lastTIDs[item] = tid
-        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+                    twu += transUtility
+                self._mapOfTWU[item] = twu
+        listOfCUList = []
+        hashTable = {}
+        mapItemsToCUList = {}
+        minutil = self._minUtil
+        for item in self._mapOfTWU.keys():
+            if self._mapOfTWU.get(item) >= self._minUtil:
+                uList = _CUList(item)
+                mapItemsToCUList[item] = uList
+                listOfCUList.append(uList)
+        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._HMiner))
+        tid = 1
         for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
             items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
+            utilities = self._utilities[line]
+            ru = 0
+            newTwu = 0
+            tx_key = []
+            revisedTrans = []
             for i in range(0, len(items)):
                 pair = _Pair()
                 pair.item = items[i]
-                regions = _Regions(int(quantities[i]), 3)
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i - 1, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                if pair.quantity > remainUtil:
-                    remainingUtility = pair.quantity
+                pair.utility = int(utilities[i])
+                if self._mapOfTWU.get(pair.item) >= self._minUtil:
+                    revisedTrans.append(pair)
+                    tx_key.append(pair.item)
+                    newTwu += pair.utility
+            revisedTrans.sort(key=_ab._functools.cmp_to_key(self._HMiner))
+            tx_key1 = tuple(tx_key)
+            if len(revisedTrans) > 0:
+                if tx_key1 not in hashTable.keys():
+                    hashTable[tx_key1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
+                    for i in range(len(revisedTrans) - 1, -1, -1):
+                        pair = revisedTrans[i]
+                        cuListoFItems = mapItemsToCUList.get(pair.item)
+                        element = _Element(tid, pair.utility, ru, 0, 0)
+                        if i > 0:
+                            element.ppos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
+                        else:
+                            element.ppos = - 1
+                        cuListoFItems.addElements(element)
+                        ru += pair.utility
                 else:
-                    remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(tuple([pair.item, itemsToRegion[pair.item]])) is not None:
-                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item, itemsToRegion[pair.item]])]
-                    if len(FFListOfItem.elements) == 0:
-                        element = _Element(tid, pair.quantity, remainingUtility, 0)
+                    pos = hashTable[tx_key1]
+                    ru = 0
+                    for i in range(len(revisedTrans) - 1, -1, -1):
+                        cuListoFItems = mapItemsToCUList[revisedTrans[i].item]
+                        cuListoFItems.elements[pos].nu += revisedTrans[i].utility
+                        cuListoFItems.elements[pos].nru += ru
+                        cuListoFItems.sumnu += revisedTrans[i].utility
+                        cuListoFItems.sumnru += ru
+                        ru += revisedTrans[i].utility
+                        pos = cuListoFItems.elements[pos].ppos
+                    # EUCS
+            for i in range(len(revisedTrans) - 1, -1, -1):
+                pair = revisedTrans[i]
+                mapFMAPItem = self._mapFMAP.get(pair.item)
+                if mapFMAPItem == None:
+                    mapFMAPItem = {}
+                    self._mapFMAP[pair.item] = mapFMAPItem
+                for j in range(i + 1, len(revisedTrans)):
+                    pairAfter = revisedTrans[j]
+                    twuSUm = mapFMAPItem.get(pairAfter.item)
+                    if twuSUm is None:
+                        mapFMAPItem[pairAfter.item] = newTwu
                     else:
-                        if lastTIDs[pair.item] == tid:
-                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
-                        else:
-                            lastTid = FFListOfItem.elements[-1].tid
-                            curPer = tid - lastTid
-                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
-                    FFListOfItem.addElement(element)
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
+                        mapFMAPItem[pairAfter.item] = twuSUm + newTwu
+            tid += 1
+        self._ExploreSearchTree([], listOfCUList, minutil)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
         self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("High Utility patterns were generated successfully using HMiner algorithm")
 
-    def _FSFIMining(self, prefix, prefixLen, fsFim, minSup):
-
-        """
-        Generates FPFP from prefix
-
-        :param prefix: the prefix patterns of FPFP
-        :type prefix: len
-        :param prefixLen: the length of prefix
-        :type prefixLen: int
-        :param fsFim: the Fuzzy list of prefix itemSets
-        :type fsFim: list
-        :param minSup: the minimum support of
-        :type minSup:int
-        """
-        for i in range(0, len(fsFim)):
-            X = fsFim[i]
-            if X.sumLUtil >= minSup and X.maxPeriod <= self._maxPer:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumLUtil, X.maxPeriod)
-            if X.sumRUtil >= minSup:
-                exULs = []
-                for j in range(i + 1, len(fsFim)):
-                    Y = fsFim[j]
-                    exULs.append(self._construct(X, Y))
-                    self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, )
-
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-
-        :return: returning USS memory consumed by the mining process
-        :rtype: float
-        """
-
-        return self._memoryUSS
-
-    def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
-        :return: returning RSS memory consumed by the mining process
-        :rtype: float
-        """
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
-
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def _ExploreSearchTree(self, prefix, uList, minutil):
         """
-        return self._endTime - self._startTime
-
-    def _construct(self, px, py):
-        """
-        A function to construct a new Fuzzy item set from 2 fuzzy itemSets
-
-        :param px: the item set px
-        :type px: FFI-List
-        :param py: item set py
-        :type py: FFI-List
-        :return: the item set of pxy(px and py)
-        :rtype: FFI-List
-        """
-        pxyUL = _FFList(py.item)
-        prev = 0
-        for ex in px.elements:
-            ey = self._findElementWithTID(py, ex.tid)
-            if ey is None:
+        A method to find all high utility itemSets
+        :parm prefix:it represents all items in prefix
+        :type prefix:list
+        :parm uList:projected Utility list
+        :type uList: lists
+        :parm minutil:user minUtil
+        :type minutil:int
+        """
+        for i in range(0, len(uList)):
+            x = uList[i]
+            soted_prefix = [0] * (len(prefix) + 1)
+            soted_prefix = prefix[0:len(prefix) + 1]
+            soted_prefix.append(x.item)
+            if x.sumnu + x.sumCu >= minutil:
+                self._saveitemSet(prefix, len(prefix), x.item, x.sumnu + x.sumCu)
+            self._candidates += 1
+            if x.sumnu + x.sumCu + x.sumnru + x.sumCru >= minutil:
+                exULs = self._construcCUL(x, uList, i, minutil, len(soted_prefix))
+                self._ExploreSearchTree(soted_prefix, exULs, minutil)
+
+    def _construcCUL(self, x, culs, st, minutil, length):
+        """
+        A method to construct CUL's database
+        :parm x: Compact utility list
+        :type x: Node
+        :parm culs:list of Compact utility list
+        :type culs:lists
+        :parm st: starting pos of culs
+        :type st:int
+        :parm minutil: user minUtil
+        :type minutil:int
+        :parm length: length of x
+        :type length:int
+        :return: projectd database of list X
+        :rtype: list
+        """
+        excul = []
+        lau = []
+        cutil = []
+        ey_tid = []
+        for i in range(0, len(culs)):
+            uList = _CUList(culs[i].item)
+            excul.append(uList)
+            lau.append(0)
+            cutil.append(0)
+            ey_tid.append(0)
+        sz = len(culs) - (st + 1)
+        exSZ = sz
+        for j in range(st + 1, len(culs)):
+            mapOfTWUF = self._mapFMAP[x.item]
+            if mapOfTWUF != None:
+                twuf = mapOfTWUF.get(culs[j].item)
+                if twuf != None and twuf < minutil:
+                    excul[j] = None
+                    exSZ = sz - 1
+                else:
+                    uList = _CUList(culs[j].item)
+                    excul[j] = uList
+                    ey_tid[j] = 0
+                    lau[j] = x.sumCu + x.sumCru + x.sumnu + x.sumnru
+                    cutil[j] = x.sumCu + x.sumCru
+        hashTable = {}
+        for ex in x.elements:
+            newT = []
+            for j in range(st + 1, len(culs)):
+                if excul[j] is None:
+                    continue
+                eylist = culs[j].elements
+                while ey_tid[j] < len(eylist) and eylist[ey_tid[j]].tid < ex.tid:
+                    ey_tid[j] = ey_tid[j] + 1
+                if ey_tid[j] < len(eylist) and eylist[ey_tid[j]].tid == ex.tid:
+                    newT.append(j)
+                else:
+                    lau[j] = lau[j] - ex.nu - ex.nru
+                    if lau[j] < minutil:
+                        excul[j] = None
+                        exSZ = exSZ - 1
+            if len(newT) == exSZ:
+                self._UpdateCLosed(x, culs, st, excul, newT, ex, ey_tid, length)
+            else:
+                if len(newT) == 0:
+                    continue
+                ru = 0
+                newT1 = tuple(newT)
+                if newT1 not in hashTable.keys():
+                    hashTable[newT1] = len(excul[newT[len(newT) - 1]].elements)
+                    for i in range(len(newT) - 1, -1, -1):
+                        cuListoFItems = excul[newT[i]]
+                        y = culs[newT[i]].elements[ey_tid[newT[i]]]
+                        element = _Element(ex.tid, ex.nu + y.nu - ex.pu, ru, ex.nu, 0)
+                        if i > 0:
+                            element.ppos = len(excul[newT[i - 1]].elements)
+                        else:
+                            element.ppos = - 1
+                        cuListoFItems.addElements(element)
+                        ru += y.nu - ex.pu
+                else:
+                    dppos = hashTable[newT1]
+                    self._updateElement(x, culs, st, excul, newT, ex, dppos, ey_tid)
+            for j in range(st + 1, len(culs)):
+                cutil[j] = cutil[j] + ex.nu + ex.nru
+        filter_culs = []
+        for j in range(st + 1, len(culs)):
+            if cutil[j] < minutil or excul[j] is None:
                 continue
-            eXY = _Element(ex.tid, min([ex.lUtils, ey.lUtils], key=lambda x: float(x)), ey.rUtils, ex.tid - prev)
-            pxyUL.addElement(eXY)
-            prev = ex.tid
-        return pxyUL
-
-    def _findElementWithTID(self, UList, tid):
-        """
-        To find element with same tid as given
-
-        :param UList: fuzzy list
-        :type UList: FFI-List
-        :param tid: transaction id
-        :type tid: int
-        :return: element with tid as given
-        :rtype: element if exist or None
-        """
-        List = UList.elements
-        first = 0
-        last = len(List) - 1
-        while first <= last:
-            mid = (first + last) >> 1
-            if List[mid].tid < tid:
-                first = mid + 1
-            elif List[mid].tid > tid:
-                last = mid - 1
             else:
-                return List[mid]
-        return None
-
-    def _WriteOut(self, prefix, prefixLen, item, sumLUtil, period):
-        """
-        To Store the patten
-
-        :param prefix: prefix of itemSet
-        :type prefix: list
-        :param prefixLen: length of prefix
-        :type prefixLen: int
-        :param item: the last item
+                if length > 1:
+                    excul[j].sumCu += culs[j].sumCu + x.sumCu - x.sumCpu
+                    excul[j].sumCru += culs[j].sumCru
+                    excul[j].sumCpu += x.sumCu
+                filter_culs.append(excul[j])
+        return filter_culs
+
+    def _UpdateCLosed(self, x, culs, st, excul, newT, ex, ey_tid, length):
+        """
+        A method to update closed values
+        :parm x: Compact utility list
+        :type x: lists
+        :parm culs:list of Compact utility list
+        :type culs:lists
+        :parm st: starting pos of culs
+        :type st:int
+        :parm excul: list of culs
+        :type excul: list
+        :parm newT:transaction to be updated
+        :type newT:list
+        :parm ex: element ex
+        :type ex:element
+        :parm ey_tid:list of tss
+        :type ey_tid:ts
+        :parm length: length of x
+        :type length:int
+        """
+        nru = 0
+        for j in range(len(newT) - 1, -1, -1):
+            ey = culs[newT[j]]
+            eyy = ey.elements[ey_tid[newT[j]]]
+            excul[newT[j]].sumCu += ex.nu + eyy.nu - ex.pu
+            excul[newT[j]].sumCru += nru
+            excul[newT[j]].sumCpu += ex.nu
+            nru = nru + eyy.nu - ex.pu
+
+    def _updateElement(self, z, culs, st, excul, newT, ex, duppos, ey_tid):
+        """
+        A method to updates vales for duplicates
+
+        :Attributes:
+
+        :parm z: Compact utility list
+        :type z: lists
+        :parm culs:list of Compact utility list
+        :type culs:lists
+        :parm st: starting pos of culs
+        :type st:int
+        :parm excul:list of culs
+        :type excul:list
+        :parm newT:transaction to be updated
+        :type newT:list
+        :parm ex: element ex
+        :type ex:element
+        :parm duppos: position of z in excul
+        :type duppos:int
+        :parm ey_tid:list of tss
+        :type ey_tid:ts
+        """
+        nru = 0
+        pos = duppos
+        for j in range(len(newT) - 1, -1, -1):
+            ey = culs[newT[j]]
+            eyy = ey.elements[ey_tid[newT[j]]]
+            excul[newT[j]].elements[pos].nu += ex.nu + eyy.nu - ex.pu
+            excul[newT[j]].sumnu += ex.nu + eyy.nu - ex.pu
+            excul[newT[j]].elements[pos].nru += nru
+            excul[newT[j]].sumnru += nru
+            excul[newT[j]].elements[pos].pu += ex.nu
+            nru = nru + eyy.nu - ex.pu
+            pos = excul[newT[j]].elements[pos].ppos
+
+    def _saveitemSet(self, prefix, prefixLen, item, utility):
+        """
+        A method to save itemSets
+        :parm prefix: it represents all items in prefix
+        :type prefix :list
+        :parm prefixLen: length of prefix
+        :type prefixLen:int
+        :parm item:item
         :type item: int
-        :param sumLUtil: sum of utility of itemSet
-        :type sumLUtil: float
-        :param period: represent the period of itemSet
-        :type period: int
+        :parm utility:utility of itemSet
+        :type utility:int
         """
-        self._itemsCnt += 1
-        res = ""
+        self._huiCount += 1
+        res = str()
         for i in range(0, prefixLen):
-            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
-        res += str(item) + "." + str(self._mapItemRegions.get(item))
-        #res1 = str(sumLUtil) + " : " + str(period)
-        self._finalPatterns[res] = [sumLUtil, period]
+            res += str(prefix[i]) + "\t"
+        res += str(item)
+        self._finalPatterns[str(res)] = str(utility)
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a.replace('\t', ' '), b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility'])
         return dataFrame
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
-        :type outFile: csv ile
+        :type outFile: csv file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
-            writer.write("%s \n" % patternsAndSupport)
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s\n" % patternsAndSupport)
 
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
+        :rtype: float
+        """
+
+        return self._memoryUSS
+
+    def getMemoryRSS(self):
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
+       """
+        return self._memoryRSS
+
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
+        """
+        return self._endTime - self._startTime
+    
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of High Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:  # to  include a user specified separator
-            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:  # to consider "\t" as a separator
-            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:  # includes separator
+            _ap = HMiner(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:  # to consider "\t" as a separator
+            _ap = HMiner(_ab._sys.argv[1], int(_ab._sys.argv[3]))
         _ap.startMine()
         _ap.mine()
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of huis:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        print("Total Memory in RSS",  _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,29 +46,29 @@
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        minSup : integer or float or str
+        minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=0.1 will be treated as float
-        maxPer : int
+        maxPer: int
             The user specified Maximum periodicity
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
         startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py` & `pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py`

 * *Files 8% similar despite different names*

```diff
@@ -53,71 +53,71 @@
 """
 
 from  PAMI.geoReferencedPeriodicFrequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
 
 class GPFPMiner(_ab._geoReferencedPeriodicFrequentPatterns):
-    """
-    :Description:   GPFPMiner is an Extension of CLAT algorithm,which  stands for Equivalence Class Clustering and
-    bottom-up Lattice Traversal to mine the geo referenced periodic frequent patterns.
+    """ 
+    :Description:   GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
+                    Lattice Traversal to mine the geo referenced peridoic frequent patterns.
         
     :Reference:
 
-    :param  iFile: str
+    :param  iFile: str :
                    Name of the Input file to mine complete set of Geo-referenced periodic frequent patterns
-    :param  oFile: str
+    :param  oFile: str :
                    Name of the output file to store complete set of Geo-referenced periodic frequent patterns
-    :param  minSup: int or float or str
+    :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param maxPer: float
+    :param maxPer: float :
                    The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-    :param nFile: str
+    :param nFile: str :
                    Name of the input file to mine complete set of Geo-referenced periodic frequent patterns
-    :param  sep: str
+    :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile : str
+        nFile: str:
            Name of Neighbourhood file name
-        minSup : float or int or str
+        minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : float or int or str
+        maxPer: float or int or str
             The user can specify maxPer either in count or proportion of database size.
             If the program detects the data type of maxPer is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         Database : list
             To store the complete set of transactions available in the input database/file
 
     :Methods:
 
-            mine()
+            startMine()
                 Mining process will start from here
             getPatterns()
                 Complete set of patterns will be retrieved with this function
             save(oFile)
                 Complete set of frequent patterns will be loaded in to a output file
             getPatternsAsDataFrames()
                 Complete set of frequent patterns will be loaded in to a dataframe
@@ -127,19 +127,19 @@
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function
             getRuntime()
                 Total amount of runtime taken by the mining process will be retrieved from this function
             creatingItemSets(iFileName)
                 Storing the complete transactions of the database/input file in a database variable
             frequentOneItem()
                 Generating one frequent patterns
-            convert(value)
+            convert(value):
                 To convert the given user specified value    
-            getNeighbourItems(keySet)
+            getNeighbourItems(keySet):
                 A function to get common neighbours of a itemSet
-            mapNeighbours(file)
+             mapNeighbours(file):
                 A function to map items to their neighbours
 
     **Executing the code on terminal :**
     ----------------------------------------
 
     .. code-block:: console
 
@@ -380,18 +380,18 @@
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
     def _getNeighbourItems(self, keySet):
         """
         A function to get Neighbours of a item
 
-        :param keySet: itemSet
-        :type keySet: str or tuple
+        :param keySet:itemSet
+        :type keySet:str or tuple
         :return: set of common neighbours
-        :rtype: set
+        :rtype:set
         """
         itemNeighbours = self._NeighboursMap.keys()
         if isinstance(keySet, str):
             if self._NeighboursMap.get(keySet) is None:
                 return []
             itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
         if isinstance(keySet, tuple):
@@ -438,15 +438,48 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
-        self.mine()
+        # global items_sets, endTime, startTime
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self.mapNeighbours()
+        self._finalPatterns = {}
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemX = plist[i]
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            neighboursItems = self._getNeighbourItems(plist[i])
+            for j in range(i + 1, len(plist)):
+                if not plist[j] in neighboursItems:
+                    continue
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) >= self._minSup:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         # global items_sets, endTime, startTime
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.24.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -31,34 +31,34 @@
     """
     :Description: This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                   employ in PAMI
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile : str
-            Neighbourhood file name
-        minSup : integer or float or str
+        nFile: str
+            Neighbourhoof file name
+        minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : integer or float or str
+        maxPer: integer or float or str
             The user can specify maxPer either in count or proportion of database size.
             If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
             Otherwise, it will be treated as float.
             Example: maxPer = 10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator
-        startTime : float
+        startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
@@ -124,15 +124,14 @@
         """Complete set of frequent patterns generated will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def save(self, oFile):
         """Complete set of frequent patterns will be saved in to an output file from this function
-
         :param oFile: Name of the output file
         :type oFile: csv file
         """
 
         pass
 
     @_abstractmethod
```

### Comparing `pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -288,15 +288,15 @@
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
     :Methods:
 
-        mine()
+        startMine()
             This function starts pattern mining.
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsInDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -508,15 +508,30 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start pattern mining from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        self._readDatabase()
+        print(len(self._Database), len(self._neighbourList))
+        self._minSup = self._convert(self._minSup)
+        self._getFrequentItems()
+        self._sortTransaction()
+        _FPTree = self._createFPTree()
+        self._finalPatterns.update(dict(_FPTree.mining(self._minSup, self._neighbourList)))
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent Spatial Patterns successfully generated using FSPGrowth")
 
     def mine(self):
         """
         Start pattern mining from here
         """
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
```

### Comparing `pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py`

 * *Files 4% similar despite different names*

```diff
@@ -79,39 +79,39 @@
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile : str
+        nFile: str
             Name of Neighbourhood file name
-        minSup : int or float or str
+        minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         Database : list
             To store the complete set of transactions available in the input database/file
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -127,19 +127,19 @@
             Generating one frequent patterns
         dictKeysToInt(iList)
             Converting dictionary keys to integer elements
         eclatGeneration(cList)
             It will generate the combinations of frequent items
         generateSpatialFrequentPatterns(tidList)
             It will generate the combinations of frequent items from a list of items
-        convert(value)
+        convert(value):
             To convert the given user specified value    
-        getNeighbourItems(keySet)
+        getNeighbourItems(keySet):
             A function to get common neighbours of a itemSet
-        mapNeighbours(file)
+        mapNeighbours(file):
             A function to map items to their neighbours
 
     **Executing the code on terminal :**
     ----------------------------------------
 
     .. code-block:: console
 
@@ -320,15 +320,15 @@
                         tidList[tuple(set(itemList))] = intersectionList
         return tidList
 
     def _generateSpatialFrequentPatterns(self, tidList):
         """
         It will generate the combinations of frequent items from a list of items
 
-        :param tidList: it represents the items with their respective transaction identifiers
+        :param tidList :it represents the items with their respective transaction identifiers
         :type tidList: dictionary
         :return: returning transaction dictionary
         :rtype: dict
         """
         tidList1 = {}
         if len(tidList) == 0:
             print("There are no more candidate sets")
@@ -347,19 +347,18 @@
                         tidList1[tuple(itemList)] = intersectionList
 
         return tidList1
 
     def _getNeighbourItems(self, keySet):
         """
         A function to get Neighbours of a item
-
-        :param keySet: itemSet
-        :type keySet: str or tuple
+        :param keySet:itemSet
+        :type keySet:str or tuple
         :return: set of common neighbours
-        :rtype: set
+        :rtype:set
         """
         itemNeighbours = self._NeighboursMap.keys()
         if isinstance(keySet, str):
             if self._NeighboursMap.get(keySet) is None:
                 return []
             itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
         if isinstance(keySet, tuple):
@@ -410,15 +409,41 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
-        self.mine()
+        # global items_sets, endTime, startTime
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._mapNeighbours()
+        self._finalPatterns = {}
+        self._frequentOneItem()
+        frequentSet = self._generateSpatialFrequentPatterns(self._finalPatterns)
+        for x, y in frequentSet.items():
+            if x not in self._finalPatterns:
+                self._finalPatterns[x] = y
+        while 1:
+            frequentSet = self._eclatGeneration(frequentSet)
+            for x, y in frequentSet.items():
+                if x not in self._finalPatterns:
+                    self._finalPatterns[x] = y
+            if len(frequentSet) == 0:
+                break
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Spatial Frequent patterns were generated successfully using SpatialECLAT algorithm")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         # global items_sets, endTime, startTime
```

### Comparing `pami-2024.4.24.1/PAMI/georeferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,37 +46,36 @@
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile : str
+        nFile: str
             Neighbourhood file name
-        minSup : integer or float or str
+        minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime : float
+        startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-
     :Methods:
 
         startMine()
             Calling this function will start the actual mining process
         getPatterns()
             This function will output all interesting patterns discovered by an algorithm
         save(oFile)
```

### Comparing `pami-2024.4.24.1/PAMI/georeferencedFrequentSequencePattern/abstract.py` & `pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,29 +46,29 @@
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile : str
+        nFile: str
             Neighbourhood file name
-        minSup : integer or float or str
+        minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime : float
+        startTime:float
             To record the start time of the algorithm
-        endTime : float
+        endTime:float
             To record the completion time of the algorithm
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
@@ -164,10 +164,11 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
+
         """ To print the results of execution"""
 
         pass
```

### Comparing `pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py` & `pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py`

 * *Files 2% similar despite different names*

```diff
@@ -76,47 +76,47 @@
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile : str
+        nFile: str:
            Name of Neighbourhood file name
-        maxIAT : float or int or str
+        maxIAT: float or int or str
             The user can specify maxIAT either in count or proportion of database size.
             If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count.
             Otherwise, it will be treated as float.
             Example: maxIAT=10 will be treated as integer, while maxIAT=10.0 will be treated as float
-        minPS : float or int or str
+        minPS: float or int or str
             The user can specify minPS either in count or proportion of database size.
             If the program detects the data type of minPS is integer, then it treats minPS is expressed in count.
             Otherwise, it will be treated as float.
             Example: minPS=10 will be treated as integer, while minPS=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-        finalPatterns : dict
+        finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         Database : list
             To store the complete set of transactions available in the input database/file
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrames()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -128,17 +128,17 @@
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets(iFileName)
             Storing the complete transactions of the database/input file in a database variable
         frequentOneItem()
             Generating one frequent patterns
         convert(value):
             To convert the given user specified value
-        getNeighbourItems(keySet)
+        getNeighbourItems(keySet):
             A function to get common neighbours of a itemSet
-        mapNeighbours(file)
+         mapNeighbours(file):
             A function to map items to their neighbours
 
     **Executing the code on terminal :**
     ----------------------------------------
 
     .. code-block:: console
 
@@ -366,18 +366,18 @@
             self._Generation(newprefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
     def _getNeighbourItems(self, keySet):
         """
         A function to get Neighbours of an item
 
-        :param keySet: itemSet
-        :type keySet: str or tuple
+        :param keySet:itemSet
+        :type keySet:str or tuple
         :return: set of common neighbours
-        :rtype: set
+        :rtype:set
         """
         itemNeighbours = self._NeighboursMap.keys()
         if isinstance(keySet, str):
             if self._NeighboursMap.get(keySet) is None:
                 return []
             itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
         if isinstance(keySet, tuple):
@@ -424,15 +424,49 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
-        self.mine()
+        # global items_sets, endTime, startTime
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        #self._minSup = self._convert(self._minSup)
+        self.mapNeighbours()
+        self._finalPatterns = {}
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemX = plist[i]
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            neighboursItems = self._getNeighbourItems(plist[i])
+            for j in range(i + 1, len(plist)):
+                if not plist[j] in neighboursItems:
+                    continue
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                val = self._getPeriodicSupport(y1)
+                if val >= self._minPS:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         # global items_sets, endTime, startTime
@@ -470,45 +504,41 @@
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -518,15 +548,14 @@
             data.append([pat, b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             pat = ""
@@ -534,15 +563,14 @@
                 pat += str(i) + '\t'
             patternsAndSupport = pat.strip() + ": " + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
```

### Comparing `pami-2024.4.24.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py` & `pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py`

 * *Files 2% similar despite different names*

```diff
@@ -73,30 +73,29 @@
             represent total sum of all utilities in the database
         prefixUtility:
             prefix Utility values of item
         offset:
             an offset pointer, used by projected transactions
         support:
             maintains the support of the transaction
-
     :Methods:
 
-        projectedTransaction(offsetE)
+        projectedTransaction(offsetE):
             A method to create new Transaction from existing starting from offsetE until the end
-        getItems()
+        getItems():
             return items in transaction
-        getUtilities()
+        getUtilities():
             return utilities in transaction
-        getLastPosition()
+        getLastPosition():
             return last position in a transaction
-        removeUnpromisingItems()
+        removeUnpromisingItems():
             A method to remove items which are having low values when compared with minUtil
-        insertionSort()
+        insertionSort():
             A method to sort all items in the transaction
-        getSupport()
+        getSupport():
             returns the support of the transaction
     """
     offset = 0
     prefixUtility = 0
     support = 1
 
     def __init__(self, items: List[int], utilities: List[int], transactionUtility: int) -> None:
@@ -106,16 +105,19 @@
         self.support = 1
 
     def projectTransaction(self, offsetE: int) -> '_Transaction':
         """
         A method to create new Transaction from existing transaction starting from offsetE until the end
 
         :param offsetE: an offset over the original transaction for projecting the transaction
+
         :type offsetE: int
+
         :return: a new transaction starting from offsetE until the end of the transaction
+
         :rtype: _Transaction
         """
         new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
         new_transaction.prefixUtility = self.prefixUtility + utilityE
         new_transaction.transactionUtility = self.transactionUtility - utilityE
         new_transaction.support = self.support
@@ -125,52 +127,59 @@
         return new_transaction
 
     def getItems(self) -> List[int]:
         """
         A method to return items in transaction
 
         :return: the list of items in transaction starting from offsetE until the end of the transactions
+
         :rtype: list
         """
         return self.items
 
     def getUtilities(self) -> List[int]:
         """
         A method to return utilities in transaction
 
         :return: the list of utilities in transaction starting from offsetE until the end of the transaction
+
         :rtype: list
         """
         return self.utilities
 
     def getLastPosition(self) -> int:
         """
         A method to return last position in a transaction
+
         :return: the last position in a transaction
+
         :rtype: int
         """
 
         return len(self.items) - 1
 
     def getSupport(self) -> int:
         """
         A method to return support in a transaction
 
         :return: the support in a transaction
+
         :rtype: int
         """
 
         return self.support
 
     def removeUnpromisingItems(self, oldNamesToNewNames: Dict[int, int]) -> None:
         """
         A method to remove items which are not present in the map passed to the function
 
         :param oldNamesToNewNames: A map represent old names to new names
+
         :type oldNamesToNewNames: map
+
         :return: None
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
                 tempItems.append(oldNamesToNewNames[item])
@@ -180,15 +189,14 @@
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
     def insertionSort(self) -> None:
         """
         A method to sort items in order
-
         :return: None
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
@@ -208,19 +216,19 @@
         transactions :
             the list of transactions in this dataset
         maxItem:
             the largest item name
         
     :methods:
 
-        createTransaction(line)
+        createTransaction(line):
             Create a transaction object from a line from the input file
-        getMaxItem()
+        getMaxItem():
             return Maximum Item
-        getTransactions()
+        getTransactions():
             return transactions in database
 
     """
     transactions = []
     maxItem = 0
     
     def __init__(self, datasetPath: Union[str, _ab._pd.DataFrame], sep: str) -> None:
@@ -231,15 +239,17 @@
         self.createItemSets(datasetPath)
 
     def createItemSets(self, datasetPath: List[str]) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
 
         :param datasetPath: list of paths to the input file to store
+
         :type datasetPath: list
+
         :return: None
 
         """
         self.Database = []
         self.transactions = []
         if isinstance(datasetPath, _ab._pd.DataFrame):
             utilities, data, utilitySum = [], [], []
@@ -282,20 +292,27 @@
                     quit()
 
     def createTransaction(self, items: List[str], utilities: List[str], utilitySum: int) -> _Transaction:
         """
         A method to create Transaction from dataset given
 
         :param items: represent a single line of database
+
         :type items: list
+
         :param utilities: represent the utilities of items
+
         :type utilities: list
+
         :param utilitySum: represent  the utilitySum
+
         :type utilitySum: int
+
         :return: a Transaction from given dataset
+
         :rtype: _Transaction
         """
         transactionUtility = utilitySum
         itemsString = items
         utilityString = utilities
         items = []
         utilities = []
@@ -312,23 +329,25 @@
         return _Transaction(items, utilities, transactionUtility)
 
     def getMaxItem(self) -> int:
         """
         A method to return name of the largest item
 
         :return: the name of the largest item in the dataset
+
         :rtype: int
         """
         return self.maxItem
 
     def getTransactions(self) -> List[_Transaction]:
         """
         A method to return transactions from database
 
         :return: the list of transactions from database which have the highest utility
+
         :rtype: list
         """
         return self.transactions
 
 
 class HUFIM(_ab._utilityPatterns):
     """
@@ -397,15 +416,15 @@
         itemsToKeep: list
             keep only the promising items i.e items that can extend other items to form RHUIs
         itemsToExplore: list
             list of items that needs to be explored
 
     :Methods:
 
-        mine()
+        startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of patterns will be loaded in to a dataframe
@@ -512,15 +531,14 @@
 
     def __init__(self, iFile: str, minUtil: Union[int, float], minSup: Union[int, float], sep: str="\t") -> None:
         super().__init__(iFile, minUtil, minSup, sep)
 
     def _convert(self, value) -> Union[int, float]:
         """
         To convert the given user specified value
-
         :param value: user specified value
         :type value: int or float or str
         :return: converted value
         :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
@@ -534,23 +552,73 @@
                 value = int(value)
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         High Utility Frequent Pattern mining start here
-
         :return: None
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        self._dataset = []
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._singleItemSetsSupport = _ab._defaultdict(int)
+        self._singleItemSetsUtility = _ab._defaultdict(int)
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        self._minUtil = int(self._minUtil)
+        self._minSup = self._convert(self._minSup)
+        itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        # calculating suffix utility values
+        totalUtility = 0
+        for item in itemsToKeep:
+            totalUtility += self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+        # piItems
+        piItems = []
+        for item in itemsToKeep:
+            if totalUtility >= self._minUtil:
+                piItems.append(item)
+                totalUtility -= self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+            else:
+                break
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        itemsToExplore = []
+        for item in piItems:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                itemsToExplore.append(item)
+        self._backTrackingHUFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility Frequent patterns were generated successfully using HUFIM algorithm")
 
     def mine(self) -> None:
         """
         High Utility Frequent Pattern mining start here
-
         :return: None
         """
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
         self._dataset = []
         self._dataset = _Dataset(self._iFile, self._sep)
         self._singleItemSetsSupport = _ab._defaultdict(int)
@@ -602,15 +670,14 @@
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("High Utility Frequent patterns were generated successfully using HUFIM algorithm")
 
     def _backTrackingHUFIM(self, transactionsOfP: List[_Transaction], itemsToKeep: List[int], itemsToExplore: List[int], prefixLength: int) -> None:
         """
         A method to mine the HUFIs Recursively
-
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
@@ -749,15 +816,14 @@
             if i != tempPosition:
                 s1 += "\t"
         self._finalPatterns[s1] = [utility, support]
 
     def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
         """
         A method to Check if two transaction are identical
-
         :param  transaction1: the first transaction
         :type  transaction1: Trans
         :param  transaction2:    the second transaction
         :type  transaction2: Trans
         :return : whether both are identical or not
         :rtype: bool
         """
@@ -773,15 +839,14 @@
             position1 += 1
             position2 += 1
         return True
 
     def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         Scan the initial database to calculate the subtree utility of each item using a utility-bin array
-
         :param dataset: the transaction database
         :type dataset: Dataset
         :return : None
         """
         for transaction in dataset.getTransactions():
             sumSU = 0
             i = len(transaction.getItems()) - 1
@@ -794,26 +859,24 @@
                 else:
                     self._utilityBinArraySU[item] = sumSU
                 i -= 1
 
     def _sortDatabase(self, transactions: List[_Transaction]) -> None:
         """
         A Method to sort transaction
-
         :param transactions: transactions of items
         :type transactions: list
         :return: None
         """
         compareItems = _ab._functools.cmp_to_key(self._sortTransaction)
         transactions.sort(key=compareItems)
 
     def _sortTransaction(self, trans1: _Transaction, trans2: _Transaction) -> int:
         """
         A Method to sort transaction
-
         :param trans1: the first transaction
         :type trans1: Trans
         :param trans2:the second transaction
         :type trans2: Trans
         :return: sorted transaction
         :rtype: int
         """
@@ -845,15 +908,14 @@
                 pos1 -= 1
                 pos2 -= 1
             return 0
 
     def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         A method to calculate local utility of single itemSets
-
         :param dataset: the transaction database
         :type dataset: databases
         :return: None
         """
         for transaction in dataset.getTransactions():
             for idx, item in enumerate(transaction.getItems()):
                 self._singleItemSetsSupport[item] += 1
@@ -862,72 +924,66 @@
                     self._utilityBinArrayLU[item] += transaction.transactionUtility
                 else:
                     self._utilityBinArrayLU[item] = transaction.transactionUtility
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final patterns in a dataframe
-
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
         """
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'Support'])
 
         return dataFrame
     
     def getPatterns(self) -> Dict[str, List[Union[int, float]]]:
         """
         Function to send the set of patterns after completion of the mining process
-
         :return: returning patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime-self._startTime
     
     def printResults(self) -> None:
         """
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py` & `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py`

 * *Files 3% similar despite different names*

```diff
@@ -390,15 +390,15 @@
         itemsToKeep: list
             keep only the promising items ie items whose supersets can be required patterns
         itemsToExplore: list
             keep items that subtreeUtility grater than minUtil
 
     :Methods :
 
-        mine()
+        startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of frequent patterns will be loaded in to a dataframe
@@ -529,15 +529,87 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         High Utility Frequent Pattern mining start here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._patternCount = 0
+        self._finalPatterns = {}
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._singleItemSetsSupport = _ab._defaultdict(int)
+        self._singleItemSetsUtility = _ab._defaultdict(int)
+        self._minUtil = int(self._minUtil)
+        self._minSup = self._convert(self._minSup)
+        with open(self._nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self._sep)
+                item = self._dataset.strToInt.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self._dataset.strToInt.get(line_split[i]))
+                self._Neighbours[item] = lst
+        o.close()
+        InitialMemory = _ab._psutil.virtual_memory()[3]
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        _itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
+                _itemsToKeep.append(key)
+        # sorting items in decreasing order of their utilities
+        _itemsToKeep = sorted(_itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
+        _currentName = 1
+        for idx, item in enumerate(_itemsToKeep):
+            self._oldNamesToNewNames[item] = _currentName
+            self._newNamesToOldNames[_currentName] = item
+            _itemsToKeep[idx] = _currentName
+            _currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        _emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                _emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[_emptyTransactionCount:]
+        # calculating neighborhood suffix utility values
+        _secondary = []
+        for idx, item in enumerate(_itemsToKeep):
+            _cumulativeUtility = self._singleItemSetsUtility[self._newNamesToOldNames[item]]
+            if self._newNamesToOldNames[item] in self._Neighbours:
+                neighbors = [self._oldNamesToNewNames[y] for y in self._Neighbours[self._newNamesToOldNames[item]] if y in self._oldNamesToNewNames]
+                for i in range(idx+1, len(_itemsToKeep)):
+                    _nextItem = _itemsToKeep[i]
+                    if _nextItem in neighbors:
+                        _cumulativeUtility += self._singleItemSetsUtility[self._newNamesToOldNames[_nextItem]]
+            if _cumulativeUtility >= self._minUtil:
+                _secondary.append(item)         
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        _itemsToExplore = []
+        for item in _secondary:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                _itemsToExplore.append(item)
+        _commonitems = []
+        for i in range(self._dataset.maxItem):
+            _commonitems.append(i)
+        self._backtrackingEFIM(self._dataset.getTransactions(), _itemsToKeep, _itemsToExplore, 0)
+        _finalMemory = _ab._psutil.virtual_memory()[3]
+        memory = (_finalMemory - InitialMemory) / 10000
+        if memory > self._maxMemory:
+            self._maxMemory = memory
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print('Spatial High Utility Frequent Itemsets generated successfully using SHUFIM algorithm')
 
     def mine(self):
         """
         High Utility Frequent Pattern mining start here
         """
         self._startTime = _ab._time.time()
         self._patternCount = 0
@@ -995,15 +1067,15 @@
     inputFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
     neighborFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
 
     minUtilCount = 10000
     minSup = 100
     seperator = ' '  
     obj = SHUFIM(iFile=inputFile, nFile=neighborFile, minUtil=minUtilCount, minSup=minSup, sep=seperator)    #initialize
-    obj.mine()
+    obj.startMine()   
     obj.printResults()
     print(obj.getPatterns())
 
 
 if __name__ == '__main__':
     main()
     # _ap = str()
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPattern/basic/EFIM.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/EFIM.py`

 * *Files 3% similar despite different names*

```diff
@@ -361,15 +361,15 @@
         itemsToKeep: list
             keep only the promising items ie items having local utility values greater than or equal to minUtil
         itemsToExplore: list
             list of items that have subtreeUtility value greater than or equal to minUtil
 
     :Methods :
 
-        mine()
+        startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of patterns will be loaded in to a dataframe
@@ -492,15 +492,50 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Start the EFIM algorithm.
         :return: None
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        self._minUtil = int(self._minUtil)
+        itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        itemsToExplore = []
+        for item in itemsToKeep:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                itemsToExplore.append(item)
+        self._backTrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility patterns were generated successfully using EFIM algorithm")
 
     def mine(self) -> None:
         """
         Start the EFIM algorithm.
         :return: None
         """
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPattern/basic/HMiner.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/PPPClose.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,43 @@
-#  High Utility itemSet Mining (HMinER) is an important algorithm to miner High utility items from the database.
-#
+
+
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#             from PAMI.partialPeriodicPattern.closed import PPPClose as alg
 #
-#             from PAMI.highUtilityPattern.basic import HMiner as alg
+#             obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
 #
-#             obj = alg.HMiner("input.txt", 35)
+#             obj.startMine()
 #
-#             obj.mine()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             Patterns = obj.getPatterns()
+#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#             print("Total number of high utility Patterns:", len(Patterns))
+#             obj.save("patterns")
 #
-#             obj.save("output")
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
-
-
-
-
+#
+#
 
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -48,691 +47,585 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.highUtilityPattern.basic import abstract as _ab
-from deprecated import deprecated
-
-
-class _Element:
-    """
-    A class represents an Element of a utility list.
-
-    :Attributes :
-
-        ts : int
-            keep tact of transaction id
-        nu : int
-            non-closed itemSet utility
-        nru : int
-             non-closed remaining utility
-        pu : int
-            prefix utility
-        ppos: int
-            position of previous item in the list
-    """
-
-    def __init__(self, tid, nu, nru, pu, ppos):
-        self.tid = tid
-        self.nu = nu
-        self.nru = nru
-        self.pu = pu
-        self.ppos = ppos
 
 
-class _CUList:
-    """
-    A class represents a UtilityList
+import sys as _sys
+import validators as _validators
+from urllib.request import urlopen as _urlopen
+from PAMI.partialPeriodicPattern.closed import abstract as _abstract
 
-    :Attributes :
-
-        item: int
-            item 
-        sumNu: long
-            the sum of item utilities
-        sumNru: long
-            the sum of remaining utilities
-        sumCu : long
-            the sum of closed utilities
-        sumCru: long
-            the sum of closed remaining utilities
-        sumCpu: long
-            the sum of closed prefix utilities
-        elements: list
-            the list of elements 
-    :Methods :
-
-        addElement(element)
-            Method to add an element to this utility list and update the sums at the same time.
-    """
-
-    def __init__(self, item):
-        self.item = item
-        self.sumnu = 0
-        self.sumnru = 0
-        self.sumCu = 0
-        self.sumCru = 0
-        self.sumCpu = 0
-        self.elements = []
-
-    def addElements(self, element):
-        """
-        A method to add new element to CUList
-        :param element: element to be added to CUList
-        :type element: Element
-        """
-        self.sumnu += element.nu
-        self.sumnru += element.nru
-        self.elements.append(element)
 
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
 
-class _Pair:
+class PPPClose(_abstract._partialPeriodicPatterns):
     """
-    A class represent an item and its utility in a transaction
-    """
-
-    def __init__(self):
-        self.item = 0
-        self.utility = 0
+    :Description:
 
+    PPPClose algorithm is used to discover the closed partial periodic patterns in temporal databases.
+    It uses depth-first search.
 
-class HMiner(_ab._utilityPatterns):
-    """
-    :Description:   High Utility itemSet Mining (HMIER) is an importent algorithm to miner High utility items from the database.
+    :Reference: R. Uday Kiran1 , J. N. Venkatesh2 , Philippe Fournier-Viger3 , Masashi Toyoda1 , P. Krishna Reddy2 and Masaru Kitsuregawa
+                 https://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/799/PAKDD.pdf
 
-    :Reference:
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of High Utility patterns
+                   Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of High Utility patterns
-    :param minUtil: int :
-                   The user given minUtil value.
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param maxPer: float :
-                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+                   Name of the output file to store complete set of frequent patterns
+    :param  period: float:
+                   Minimum partial periodic...
+    :param  periodicSupport: float:
+                   Minimum partial periodic...
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-
     :Attributes:
 
-        iFile : file
-            Name of the input file to mine complete set of frequent patterns
-        oFile : file
-            Name of the output file to store complete set of frequent patterns
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+        iFile : str
+            Input file name or path of the input file
+        oFile : str
+            Name of the output file or path of the input file
+        periodicSupport: int or float or str
+            The user can specify periodicSupport either in count or proportion of database size.
+            If the program detects the data type of periodicSupport is integer, then it treats periodicSupport is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: periodicSupport=10 will be treated as integer, while periodicSupport=10.0 will be treated as float
+        period: int or float or str
+            The user can specify period either in count or proportion of database size.
+            If the program detects the data type of period is integer, then it treats period is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            However, the users can override their default separator.
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
-        minUtil : int
-            The user given minUtil
-        mapFMAP: list
-            EUCS map of the FHM algorithm
-        candidates: int
-            candidates genetated
-        huiCnt: int
-            huis created
-        neighbors: map
-            keep track of nighboues of elements
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        Explore_SearchTree(prefix, uList, minUtil)
-            A method to find all high utility itemSets
-        UpdateCLosed(x, culs, st, excul, newT, ex, ey_ts, length)
-            A method to update closed values
-        saveitemSet(prefix, prefixLen, item, utility)
-            A method to save itemSets
-        updateElement(z, culs, st, excul, newT, ex, duppos, ey_ts)
-            A method to updates vales for duplicates
-        construcCUL(x, culs, st, minUtil, length, exnighbors)
-            A method to construct CUL's database
 
     **Executing the code on terminal:**
-    --------------------------------------------
-
+    -------------------------------------
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 HMiner.py <inputFile> <outputFile> <minUtil>
+       Format:
 
-      Example Usage:
+       (.venv) $ python3 PPPClose.py <inputFile> <outputFile> <periodicSupport> <period>
 
-      (.venv) $ python3 HMiner.py sampleTDB.txt output.txt 35
+       Examples:
 
-    .. note:: minSup will be considered in percentage of database transactions
+       (.venv) $ python3 PPPClose.py sampleTDB.txt patterns.txt 0.3 0.4
 
 
-    Sample run of importing the code:
+    **Sample run of the imported code:**
     --------------------------------------
     .. code-block:: python
 
-            from PAMI.highUtilityPattern.basic import HMiner as alg
-        
-            obj = alg.HMiner("input.txt",35)
-        
-            obj.mine()
-        
-            Patterns = obj.getPatterns()
-        
-            print("Total number of high utility Patterns:", len(Patterns))
-        
-            obj.save("output")
-        
+            from PAMI.partialPeriodicPattern.closed import PPPClose as alg
+
+            obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
+
+            obj.startMine()
+
+            periodicFrequentPatterns = obj.getPatterns()
+
+            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+
+            obj.save("patterns")
+
+            Df = obj.getPatternsAsDataFrame()
+
             memUSS = obj.getMemoryUSS()
-        
+
             print("Total Memory in USS:", memUSS)
-        
+
             memRSS = obj.getMemoryRSS()
-        
+
             print("Total Memory in RSS", memRSS)
-        
+
             run = obj.getRuntime()
-        
+
             print("Total ExecutionTime in seconds:", run)
-    
+
     **Credits:**
-    -----------------------------
-            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+    --------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
 
     """
 
+    _periodicSupport = float()
+    _period = float()
     _startTime = float()
     _endTime = float()
-    _minSup = str()
-    _maxPer = float()
     _finalPatterns = {}
-    _Database = {}
-    _transactions = []
-    _utilities = []
-    _utilitySum = []
+    _Database = []
     _iFile = " "
     _oFile = " "
-    _minUtil = 0
-    _sep = "\t"
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
+    _transaction = []
+    _hashing = {}
+    _mapSupport = {}
+    _itemSetCount = 0
+    _maxItemId = 0
+    _tableSize = 10000
+    _tidList = {}
+    _lno = 0
+
+    def _convert(self, value):
+        """
+        To convert the given user specified value
+
+        :param value: user specified value
+        :return: converted value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._lno * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._lno * value)
+            else:
+                value = int(value)
+        return value
 
-    def __init__(self, iFile1, minUtil, sep="\t"):
-        super().__init__(iFile1, minUtil, sep)
-        self._huiCount = 0
-        self._candidates = 0
-        self._mapOfTWU = {}
-        self._minutil = 0
-        self._mapFMAP = {}
-        self._finalPatterns = {}
-
-    def _HMiner(self, o1, o2) -> int:
-        """
-        A Function that sort all FFI-list in ascending order of Support
-
-        :param o1: First FFI-list
-
-        :type o1: _FFList
-
-        :param o2: Second FFI-list
-
-        :type o1: _FFList
-
-        :return: Comparision Value
-
-        :rtype: int
-        """
-        compare = self._mapOfTWU[o1.item] - self._mapOfTWU[o2.item]
-        if compare == 0:
-            return int(o1.item) - int(o2.item)
-        else:
-            return compare
-
-    def _creteItemsets(self):
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._transactions, self._utilities, self._utilitySum = [], [], []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
+        self._Database = []
+        if isinstance(self._iFile, _abstract._pd.DataFrame):
+            timeStamp, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                timeStamp = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._transactions = self._iFile['Transactions'].tolist()
-            if 'Utilities' in i:
-                self._utilities = self._iFile['Utilities'].tolist()
-            if 'UtilitySum' in i:
-                self._utilitySum = self._iFile['UtilitySum'].tolist()
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [timeStamp[i]]
+                tr = tr + data[i]
+                self._Database.append(tr)
+            self._lno = len(self._Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                #print("hey")
-                data = _ab._urlopen(self._iFile)
+            if _validators.url(self._iFile):
+                data = _urlopen(self._iFile)
                 for line in data:
+                    self._lno += 1
                     line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = line.split(":")
-                    items = parts[0].split(self._sep)
-                    self._transactions.append([x for x in items if x])
-                    utilities = parts[2].split(self._sep)
-                    self._utilities.append(utilities)
-                    self._utilitySum.append(int(parts[1]))
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.split("\n")[0]
-                            parts = line.split(":")
-                            items = parts[0].split(self._sep)
-                            self._transactions.append([x for x in items if x])
-                            utilities = parts[2].split(self._sep)
-                            self._utilities.append(utilities)
-                            self._utilitySum.append(int(parts[1]))
+                            self._lno += 1
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def _OneLengthPartialItems(self):
         """
-        Main program to start the operation
+        To scan the database and extracts the 1-length periodic-frequent items
+
+        :return: Returns the 1-length periodic-frequent items
+        """
+        self._mapSupport = {}
+        self._tidList = {}
+        self._period = self._convert(self._period)
+        for line in self._Database:
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [1, 0, n]
+                    self._tidList[si] = [n]
+                else:
+                    self._mapSupport[si][0] += 1
+                    period = abs(n - self._mapSupport[si][2])
+                    if period <= self._period:
+                        self._mapSupport[si][1] += 1
+                    self._mapSupport[si][2] = n
+                    self._tidList[si].append(n)
+        for x, y in self._mapSupport.items():
+            period = abs(self._lno - self._mapSupport[x][2])
+            if period <= self._period:
+                self._mapSupport[x][1] += 1
+        self._periodicSupport = self._convert(self._periodicSupport)
+        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items() if v[1] >= self._periodicSupport}
+        periodicFrequentItems = {}
+        self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
+        for x, y in self._tidList.items():
+            t1 = 0
+            for i in y:
+                t1 += i
+            periodicFrequentItems[x] = t1
+        periodicFrequentItems = [key for key, value in sorted(periodicFrequentItems.items(), key=lambda x: x[1])]
+        return periodicFrequentItems
+
+    def _calculate(self, tidSet):
+        """
+        To calculate the weight if pattern based on the respective timeStamps
+
+        :param tidSet: timeStamps of the pattern
+        :return: the calculated weight of the timeStamps
+        """
+        hashcode = 0
+        for i in tidSet:
+            hashcode += i
+        if hashcode < 0:
+            hashcode = abs(0 - hashcode)
+        return hashcode % self._tableSize
+
+    def _contains(self, itemSet, val, hashcode):
+        """
+        To check if the key(hashcode) is in dictionary(hashing) variable
+
+        :param itemSet: generated periodic-frequent itemSet
+        :param val: support and period of itemSet
+        :param hashcode: the key generated in calculate() method for every itemSet
+
+        :return: true if itemSet with same support present in dictionary(hashing) or else returns false
+        """
+        if self._hashing.get(hashcode) is None:
+            return False
+        for i in self._hashing[hashcode]:
+            itemSetX = i
+            if val == self._hashing[hashcode][itemSetX] and set(itemSetX).issuperset(itemSet):
+                return True
+        return False
+
+    def _getPeriodicSupport(self, timeStamps):
+        """
+        Calculates the period and support of timeStamps
+
+        :param: timeStamps: timeStamps of itemSet
+        :return: period and support
+        """
+        timeStamps.sort()
+        sup = 0
+        for j in range(len(timeStamps) - 1):
+            per = abs(timeStamps[j + 1] - timeStamps[j])
+            if per <= self._period:
+                sup += 1
+        return sup
+
+    def _save(self, prefix, suffix, tidSetX):
+        """
+        Saves the generated pattern which satisfies the closed property
+
+        :param prefix: the prefix part of itemSet
+        :param suffix: the suffix part of itemSet
+        :param tidSetX: the timeStamps of the generated itemSet
+        :return: saves the closed periodic-frequent pattern
+        """
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        prefix = list(set(prefix))
+        prefix.sort()
+        val = self._getPeriodicSupport(tidSetX)
+        if val >= self._periodicSupport:
+            hashcode = self._calculate(tidSetX)
+            if self._contains(prefix, val, hashcode) is False:
+                self._itemSetCount += 1
+                sample = str()
+                for i in prefix:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = val
+            if hashcode not in self._hashing:
+                self._hashing[hashcode] = {tuple(prefix): val}
+            else:
+                self._hashing[hashcode][tuple(prefix)] = val
+
+    def _processEquivalenceClass(self, prefix, itemSets, tidSets):
         """
-        self.mine()
 
-    def mine(self):
+        :param prefix: Prefix class of an itemSet
+        :param itemSets: suffix items in periodicFrequentItems that satisfies the periodicSupport condition
+        :param tidSets: timeStamps of items in itemSets respectively
+        :return: closed periodic patterns with length more than 2
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidList = tidSets[0]
+            self._save(prefix, [i], tidList)
+            return
+        if len(itemSets) == 2:
+            itemI = itemSets[0]
+            tidSetI = tidSets[0]
+            itemJ = itemSets[1]
+            tidSetJ = tidSets[1]
+            y1 = list(set(tidSetI).intersection(tidSetJ))
+            if len(y1) >= self._periodicSupport:
+                suffix = []
+                suffix += [itemI, itemJ]
+                suffix = list(set(suffix))
+                self._save(prefix, suffix, y1)
+            if len(y1) != len(tidSetI):
+                self._save(prefix, [itemI], tidSetI)
+            if len(y1) != len(tidSetJ):
+                self._save(prefix, [itemJ], tidSetJ)
+            return
+        for i in range(len(itemSets)):
+            itemX = itemSets[i]
+            if itemX is None:
+                continue
+            tidSetX = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemX]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                if itemJ is None:
+                    continue
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetX).intersection(tidSetJ))
+                if len(y) < self._periodicSupport:
+                    continue
+                if len(tidSetX) == len(tidSetJ) and len(y) == len(tidSetX):
+                    itemSets.insert(j, None)
+                    tidSets.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y) == len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y) == len(tidSetJ):
+                    itemSets.insert(j, None)
+                    tidSets.insert(j, None)
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+                else:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            if len(classItemSets) > 0:
+                newPrefix = list(set(itemSetX)) + prefix
+                self._processEquivalenceClass(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetX)
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self):
         """
-        Main program to start the operation
+        Mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._creteItemsets()
+        self._startTime = _abstract._time.time()
+        self._creatingItemSets()
+        self._hashing = {}
         self._finalPatterns = {}
-        for line in range(len(self._transactions)):
-            items_str = self._transactions[line]
-            utility_str = self._utilities[line]
-            transUtility = self._utilitySum[line]
-            for i in range(0, len(items_str)):
-                item = items_str[i]
-                twu = self._mapOfTWU.get(item)
-                if twu == None:
-                    twu = transUtility
-                else:
-                    twu += transUtility
-                self._mapOfTWU[item] = twu
-        listOfCUList = []
-        hashTable = {}
-        mapItemsToCUList = {}
-        minutil = self._minUtil
-        for item in self._mapOfTWU.keys():
-            if self._mapOfTWU.get(item) >= self._minUtil:
-                uList = _CUList(item)
-                mapItemsToCUList[item] = uList
-                listOfCUList.append(uList)
-        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._HMiner))
-        tid = 1
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            utilities = self._utilities[line]
-            ru = 0
-            newTwu = 0
-            tx_key = []
-            revisedTrans = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.utility = int(utilities[i])
-                if self._mapOfTWU.get(pair.item) >= self._minUtil:
-                    revisedTrans.append(pair)
-                    tx_key.append(pair.item)
-                    newTwu += pair.utility
-            revisedTrans.sort(key=_ab._functools.cmp_to_key(self._HMiner))
-            tx_key1 = tuple(tx_key)
-            if len(revisedTrans) > 0:
-                if tx_key1 not in hashTable.keys():
-                    hashTable[tx_key1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
-                    for i in range(len(revisedTrans) - 1, -1, -1):
-                        pair = revisedTrans[i]
-                        cuListoFItems = mapItemsToCUList.get(pair.item)
-                        element = _Element(tid, pair.utility, ru, 0, 0)
-                        if i > 0:
-                            element.ppos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
-                        else:
-                            element.ppos = - 1
-                        cuListoFItems.addElements(element)
-                        ru += pair.utility
+        periodicFrequentItems = self._OneLengthPartialItems()
+        for i in range(len(periodicFrequentItems)):
+            itemX = periodicFrequentItems[i]
+            if itemX is None:
+                continue
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(periodicFrequentItems)):
+                itemJ = periodicFrequentItems[j]
+                if itemJ is None:
+                    continue
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) < self._periodicSupport:
+                    continue
+                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
+                    periodicFrequentItems.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
+                    periodicFrequentItems.insert(j, None)
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
                 else:
-                    pos = hashTable[tx_key1]
-                    ru = 0
-                    for i in range(len(revisedTrans) - 1, -1, -1):
-                        cuListoFItems = mapItemsToCUList[revisedTrans[i].item]
-                        cuListoFItems.elements[pos].nu += revisedTrans[i].utility
-                        cuListoFItems.elements[pos].nru += ru
-                        cuListoFItems.sumnu += revisedTrans[i].utility
-                        cuListoFItems.sumnru += ru
-                        ru += revisedTrans[i].utility
-                        pos = cuListoFItems.elements[pos].ppos
-                    # EUCS
-            for i in range(len(revisedTrans) - 1, -1, -1):
-                pair = revisedTrans[i]
-                mapFMAPItem = self._mapFMAP.get(pair.item)
-                if mapFMAPItem == None:
-                    mapFMAPItem = {}
-                    self._mapFMAP[pair.item] = mapFMAPItem
-                for j in range(i + 1, len(revisedTrans)):
-                    pairAfter = revisedTrans[j]
-                    twuSUm = mapFMAPItem.get(pairAfter.item)
-                    if twuSUm is None:
-                        mapFMAPItem[pairAfter.item] = newTwu
-                    else:
-                        mapFMAPItem[pairAfter.item] = twuSUm + newTwu
-            tid += 1
-        self._ExploreSearchTree([], listOfCUList, minutil)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
+            self._save([], itemSetX, tidSetX)
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("High Utility patterns were generated successfully using HMiner algorithm")
+        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
 
-    def _ExploreSearchTree(self, prefix, uList, minutil):
+    def Mine(self):
         """
-        A method to find all high utility itemSets
-        :parm prefix:it represents all items in prefix
-        :type prefix:list
-        :parm uList:projected Utility list
-        :type uList: lists
-        :parm minutil:user minUtil
-        :type minutil:int
-        """
-        for i in range(0, len(uList)):
-            x = uList[i]
-            soted_prefix = [0] * (len(prefix) + 1)
-            soted_prefix = prefix[0:len(prefix) + 1]
-            soted_prefix.append(x.item)
-            if x.sumnu + x.sumCu >= minutil:
-                self._saveitemSet(prefix, len(prefix), x.item, x.sumnu + x.sumCu)
-            self._candidates += 1
-            if x.sumnu + x.sumCu + x.sumnru + x.sumCru >= minutil:
-                exULs = self._construcCUL(x, uList, i, minutil, len(soted_prefix))
-                self._ExploreSearchTree(soted_prefix, exULs, minutil)
-
-    def _construcCUL(self, x, culs, st, minutil, length):
-        """
-        A method to construct CUL's database
-        :parm x: Compact utility list
-        :type x: Node
-        :parm culs:list of Compact utility list
-        :type culs:lists
-        :parm st: starting pos of culs
-        :type st:int
-        :parm minutil: user minUtil
-        :type minutil:int
-        :parm length: length of x
-        :type length:int
-        :return: projectd database of list X
-        :rtype: list
-        """
-        excul = []
-        lau = []
-        cutil = []
-        ey_tid = []
-        for i in range(0, len(culs)):
-            uList = _CUList(culs[i].item)
-            excul.append(uList)
-            lau.append(0)
-            cutil.append(0)
-            ey_tid.append(0)
-        sz = len(culs) - (st + 1)
-        exSZ = sz
-        for j in range(st + 1, len(culs)):
-            mapOfTWUF = self._mapFMAP[x.item]
-            if mapOfTWUF != None:
-                twuf = mapOfTWUF.get(culs[j].item)
-                if twuf != None and twuf < minutil:
-                    excul[j] = None
-                    exSZ = sz - 1
-                else:
-                    uList = _CUList(culs[j].item)
-                    excul[j] = uList
-                    ey_tid[j] = 0
-                    lau[j] = x.sumCu + x.sumCru + x.sumnu + x.sumnru
-                    cutil[j] = x.sumCu + x.sumCru
-        hashTable = {}
-        for ex in x.elements:
-            newT = []
-            for j in range(st + 1, len(culs)):
-                if excul[j] is None:
+        Mining process will start from here
+        """
+        self._startTime = _abstract._time.time()
+        self._creatingItemSets()
+        self._hashing = {}
+        self._finalPatterns = {}
+        periodicFrequentItems = self._OneLengthPartialItems()
+        for i in range(len(periodicFrequentItems)):
+            itemX = periodicFrequentItems[i]
+            if itemX is None:
+                continue
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(periodicFrequentItems)):
+                itemJ = periodicFrequentItems[j]
+                if itemJ is None:
                     continue
-                eylist = culs[j].elements
-                while ey_tid[j] < len(eylist) and eylist[ey_tid[j]].tid < ex.tid:
-                    ey_tid[j] = ey_tid[j] + 1
-                if ey_tid[j] < len(eylist) and eylist[ey_tid[j]].tid == ex.tid:
-                    newT.append(j)
-                else:
-                    lau[j] = lau[j] - ex.nu - ex.nru
-                    if lau[j] < minutil:
-                        excul[j] = None
-                        exSZ = exSZ - 1
-            if len(newT) == exSZ:
-                self._UpdateCLosed(x, culs, st, excul, newT, ex, ey_tid, length)
-            else:
-                if len(newT) == 0:
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) < self._periodicSupport:
                     continue
-                ru = 0
-                newT1 = tuple(newT)
-                if newT1 not in hashTable.keys():
-                    hashTable[newT1] = len(excul[newT[len(newT) - 1]].elements)
-                    for i in range(len(newT) - 1, -1, -1):
-                        cuListoFItems = excul[newT[i]]
-                        y = culs[newT[i]].elements[ey_tid[newT[i]]]
-                        element = _Element(ex.tid, ex.nu + y.nu - ex.pu, ru, ex.nu, 0)
-                        if i > 0:
-                            element.ppos = len(excul[newT[i - 1]].elements)
-                        else:
-                            element.ppos = - 1
-                        cuListoFItems.addElements(element)
-                        ru += y.nu - ex.pu
+                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
+                    periodicFrequentItems.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
+                    periodicFrequentItems.insert(j, None)
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
                 else:
-                    dppos = hashTable[newT1]
-                    self._updateElement(x, culs, st, excul, newT, ex, dppos, ey_tid)
-            for j in range(st + 1, len(culs)):
-                cutil[j] = cutil[j] + ex.nu + ex.nru
-        filter_culs = []
-        for j in range(st + 1, len(culs)):
-            if cutil[j] < minutil or excul[j] is None:
-                continue
-            else:
-                if length > 1:
-                    excul[j].sumCu += culs[j].sumCu + x.sumCu - x.sumCpu
-                    excul[j].sumCru += culs[j].sumCru
-                    excul[j].sumCpu += x.sumCu
-                filter_culs.append(excul[j])
-        return filter_culs
-
-    def _UpdateCLosed(self, x, culs, st, excul, newT, ex, ey_tid, length):
-        """
-        A method to update closed values
-        :parm x: Compact utility list
-        :type x: lists
-        :parm culs:list of Compact utility list
-        :type culs:lists
-        :parm st: starting pos of culs
-        :type st:int
-        :parm excul: list of culs
-        :type excul: list
-        :parm newT:transaction to be updated
-        :type newT:list
-        :parm ex: element ex
-        :type ex:element
-        :parm ey_tid:list of tss
-        :type ey_tid:ts
-        :parm length: length of x
-        :type length:int
-        """
-        nru = 0
-        for j in range(len(newT) - 1, -1, -1):
-            ey = culs[newT[j]]
-            eyy = ey.elements[ey_tid[newT[j]]]
-            excul[newT[j]].sumCu += ex.nu + eyy.nu - ex.pu
-            excul[newT[j]].sumCru += nru
-            excul[newT[j]].sumCpu += ex.nu
-            nru = nru + eyy.nu - ex.pu
-
-    def _updateElement(self, z, culs, st, excul, newT, ex, duppos, ey_tid):
-        """
-        A method to updates vales for duplicates
-
-        :Attributes:
-
-        :parm z: Compact utility list
-        :type z: lists
-        :parm culs:list of Compact utility list
-        :type culs:lists
-        :parm st: starting pos of culs
-        :type st:int
-        :parm excul:list of culs
-        :type excul:list
-        :parm newT:transaction to be updated
-        :type newT:list
-        :parm ex: element ex
-        :type ex:element
-        :parm duppos: position of z in excul
-        :type duppos:int
-        :parm ey_tid:list of tss
-        :type ey_tid:ts
-        """
-        nru = 0
-        pos = duppos
-        for j in range(len(newT) - 1, -1, -1):
-            ey = culs[newT[j]]
-            eyy = ey.elements[ey_tid[newT[j]]]
-            excul[newT[j]].elements[pos].nu += ex.nu + eyy.nu - ex.pu
-            excul[newT[j]].sumnu += ex.nu + eyy.nu - ex.pu
-            excul[newT[j]].elements[pos].nru += nru
-            excul[newT[j]].sumnru += nru
-            excul[newT[j]].elements[pos].pu += ex.nu
-            nru = nru + eyy.nu - ex.pu
-            pos = excul[newT[j]].elements[pos].ppos
-
-    def _saveitemSet(self, prefix, prefixLen, item, utility):
-        """
-        A method to save itemSets
-        :parm prefix: it represents all items in prefix
-        :type prefix :list
-        :parm prefixLen: length of prefix
-        :type prefixLen:int
-        :parm item:item
-        :type item: int
-        :parm utility:utility of itemSet
-        :type utility:int
-        """
-        self._huiCount += 1
-        res = str()
-        for i in range(0, prefixLen):
-            res += str(prefix[i]) + "\t"
-        res += str(item)
-        self._finalPatterns[str(res)] = str(utility)
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
+            self._save([], itemSetX, tidSetX)
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
 
-    def getPatternsAsDataFrame(self):
+    def getMemoryUSS(self):
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning USS memory consumed by the mining process
+        :rtype: float
+        """
+
+        return self._memoryUSS
+
+    def getMemoryRSS(self):
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
+        """
+
+        return self._memoryRSS
+
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
         """
-        Storing final frequent patterns in a dataframe
+
+        return self._endTime - self._startTime
+
+    def getPatternsAsDataFrame(self):
+        """Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility'])
+            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
         return dataFrame
 
-    def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
-        :return: returning frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
-
     def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to an output file
+        """Complete set of frequent patterns will be loaded in to a output file
+
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s\n" % patternsAndSupport)
-
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
-        :rtype: float
-        """
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
-        return self._memoryUSS
+    def getPatterns(self):
+        """ Function to send the set of frequent patterns after completion of the mining process
 
-    def getMemoryRSS(self):
+        :return: returning frequent patterns
+        :rtype: dict
         """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        :return: returning RSS memory consumed by the mining process
-        :rtype: float
-       """
-        return self._memoryRSS
+        return self._finalPatterns
 
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
-        """
-        return self._endTime - self._startTime
-    
     def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Total number of High Utility Patterns:", len(self.getPatterns()))
+        print("Total number of  Closed Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:  # includes separator
-            _ap = HMiner(_ab._sys.argv[1], int(_ab._sys.argv[3]), _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:  # to consider "\t" as a separator
-            _ap = HMiner(_ab._sys.argv[1], int(_ab._sys.argv[3]))
+    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
+        if len(_sys.argv) == 6:
+            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
+        if len(_sys.argv) == 5:
+            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of huis:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
+        print("Total number of  Patterns:", len(_ap.getPatterns()))
+        _ap.save(_sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS",  _ap.getMemoryRSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPattern/basic/UPGrowth.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/UPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -378,15 +378,15 @@
         phuis : list
             A list to store the phuis
         MapItemToTwu : map
             A map to store the twu of each item in database
 
     :Methods:
 
-        mine()
+        startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         createLocalTree(tree, item)
             A Method to Construct conditional pattern base
         UPGrowth( tree, alpha)
             A Method to Mine UP Tree recursively
@@ -512,15 +512,96 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process will start from here
         :return: None
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        tree = _UPTree()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        for line in self._Database:
+            line = line.split("\n")[0]
+            transaction = line.strip().split(':')
+            items = transaction[0].split(self._sep)
+            transactionUtility = int(transaction[1])
+            for item in items:
+                Item = int(item)
+                if Item in self._MapItemToTwu:
+                    self._MapItemToTwu[Item] += transactionUtility
+                else:
+                    self._MapItemToTwu[Item] = transactionUtility
+        for line in self._Database:
+            line = line.split("\n")[0]
+            transaction = line.strip().split(':')
+            items = transaction[0].split(self._sep)
+            utilities = transaction[2].split(self._sep)
+            remainingUtility = 0
+            revisedTransaction = []
+            for idx, item in enumerate(items):
+                Item = int(item)
+                utility = int(utilities[idx])
+                if self._MapItemToTwu[Item] >= self._minUtil:
+                    element = _UPItem(Item, utility)
+                    revisedTransaction.append(element)
+                    remainingUtility += utility
+                    if Item in self._MapItemToMinimumUtility:
+                        minItemUtil = self._MapItemToMinimumUtility[Item]
+                        if minItemUtil >= utility:
+                            self._MapItemToMinimumUtility[Item] = utility
+                    else:
+                        self._MapItemToMinimumUtility[Item] = utility
+            revisedTransaction = sorted(revisedTransaction, key=lambda x: self._MapItemToTwu[x.name], reverse=True)
+            self._ParentNumberOfNodes += tree.addTransaction(revisedTransaction, remainingUtility)
+        tree.createHeaderList(self._MapItemToTwu)
+        alpha = []
+        self._finalPatterns = {}
+        # print("number of nodes in parent tree", self.ParentNumberOfNodes)
+        self._UPGrowth(tree, alpha)
+        # self.phuis = sorted(self.phuis, key=lambda x: len(x))
+        # print(self.phuis[0:10])
+        for line in self._Database:
+            line = line.split("\n")[0]
+            transaction = line.strip().split(':')
+            items = transaction[0].split(self._sep)
+            utilities = transaction[2].split(self._sep)
+            mapItemToUtility = {}
+            for idx, item in enumerate(items):
+                Item = int(item)
+                utility = int(utilities[idx])
+                if self._MapItemToTwu[Item] >= self._minUtil:
+                    mapItemToUtility[Item] = utility
+            for itemset in self._phuis:
+                l = len(itemset)
+                count = 0
+                utility = 0
+                for item in itemset:
+                    item = int(item)
+                    if item in mapItemToUtility:
+                        utility += mapItemToUtility[item]
+                        count += 1
+                if count == l:
+                    self._MapItemsetsToUtilities[tuple(itemset)] += utility
+
+        for itemset in self._phuis:
+            util = self._MapItemsetsToUtilities[tuple(itemset)]
+            if util >= self._minUtil:
+                s = str()
+                for item in itemset:
+                    s = s + str(item)
+                    s = s + "\t"
+                self._finalPatterns[s] = util
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("High Utility patterns were generated successfully using UPGrowth algorithm")
 
     def mine(self) -> None:
         """
         Mining process will start from here
         :return: None
         """
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPattern/basic/efimParallel.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/efimParallel.py`

 * *Files 2% similar despite different names*

```diff
@@ -126,15 +126,15 @@
             Read the input file and return the filtered transactions, primary items, and secondary items.
         binarySearch(arr, item):
             Perform a binary search on the given array to find the given item.
         project(beta, file_data, secondary):
             Project the given beta itemset on the given database.
         search(collections):
             Search for high utility itemsets in the given collections.
-        mine():
+        startMine():
             Start the EFIM algorithm.
         savePatterns(outputFile):
             Save the patterns discovered by the algorithm to an output file.
         getPatterns():
             Get the patterns discovered by the algorithm.
         getRuntime():
             Get the runtime of the algorithm.
@@ -447,15 +447,29 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start the EFIM algorithm.
         """
 
-        self.mine()
+        ps = psutil.Process(os.getpid())
+
+        self.start = time.time()
+
+        fileData, primary, secondary = self._read_file()
+
+        collection = [[[], fileData, primary, secondary]]
+
+        self._search(collection)
+
+        self.memoryRSS = ps.memory_info().rss
+        self.memoryUSS = ps.memory_full_info().uss
+
+        end = time.time()
+        self.runtime = end - self.start
 
     def mine(self):
         """
         Start the EFIM algorithm.
         """
 
         ps = psutil.Process(os.getpid())
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPattern/parallel/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPattern/parallel/efimparallel.py` & `pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/efimparallel.py`

 * *Files 2% similar despite different names*

```diff
@@ -108,15 +108,15 @@
             Read the input file and return the filtered transactions, primary items, and secondary items.
         binarySearch(arr, item):
             Perform a binary search on the given array to find the given item.
         project(beta, file_data, secondary):
             Project the given beta itemset on the given database.
         search(collections):
             Search for high utility itemsets in the given collections.
-        mine():
+        startMine():
             Start the EFIM algorithm.
         savePatterns(outputFile):
             Save the patterns discovered by the algorithm to an output file.
         getPatterns():
             Get the patterns discovered by the algorithm.
         getRuntime():
             Get the runtime of the algorithm.
@@ -386,15 +386,29 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start the EFIM algorithm.
         """
 
-        self.mine()
+        ps = psutil.Process(os.getpid())
+
+        self.start = time.time()
+
+        fileData, primary, secondary = self._read_file()
+
+        collection = [[[], fileData, primary, secondary]]
+
+        self._search(collection)
+
+        self.memoryRSS = ps.memory_info().rss
+        self.memoryUSS = ps.memory_full_info().uss
+
+        end = time.time()
+        self.runtime = end - self.start
 
     def mine(self):
         """
         Start the EFIM algorithm.
         """
 
         ps = psutil.Process(os.getpid())
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/HUPMS.py` & `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/HUPMS.py`

 * *Files 4% similar despite different names*

```diff
@@ -468,15 +468,15 @@
 
         contains(superset, subset)
             Checks if the superset contains the subset
 
         treeGenerations(root, netUtil, candidatePattern, curItem)
             Generates the tree of the high utility patterns
 
-        mine()
+        startMine()
             Starts the mining process
 
         printTree(root, level)
             Prints the HUS-tree in a readable format
 
         getMemoryRSS()
             Returns the memory usage of the algorithm in resident set size
@@ -748,15 +748,85 @@
                     self.treeGenerations(conditionalTree, netUtil, candidatePattern, newItemset)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         This function will start the mining process
         """
-        self.mine()
+        global _minUtil
+        self.__startTime = _hus._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minUtil is None:
+            raise Exception("Please enter the Minimum Support")
+        if self._windowSize is None:
+            raise Exception("Please enter the Window Size")
+        if self._paneSize is None:
+            raise Exception("Please enter the Pane Size")
+        self.__windowSize = int(self._windowSize)
+        self.__paneSize = int(self._paneSize)
+        
+        self._createItemsets()
+        self._minUtil = float(self._minUtil)
+        self.__tree = _HUSTree(self.__windowSize, self.__paneSize)
+        
+        transactionwiseUtility = []
+
+        for i in range(len(self._transactions)):
+            curTrans = {}
+            for j in range(len(self._transactions[i])):
+                curTrans[self._transactions[i][j]] = self._utilities[i][j]
+            transactionwiseUtility.append(curTrans)
+
+        for i in range(0, self.__windowSize):
+            self.__tree.batchIndex = i
+            for j in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[i * self.__paneSize + j], self._utilitySum[i * self.__paneSize + j])
+
+        startIndex = 0
+        endIndex = self.__windowSize * self.__paneSize
+
+        while(endIndex <= len(self._transactions)):
+            
+            filteredItemsets = {}
+
+            self.treeGenerations(self.__tree, self._minUtil, filteredItemsets)
+
+            results = []
+
+            for itemSetLen in filteredItemsets:
+                for itemSet in filteredItemsets[itemSetLen]:
+                    itemSetUtility = 0
+                    for transId in range(startIndex, endIndex):
+                        if(self.contains(list(transactionwiseUtility[transId].keys()), itemSet)):
+                            for item in itemSet:
+                                itemSetUtility += transactionwiseUtility[transId][item]
+                        
+                    if(itemSetUtility >= self._minUtil):
+                        results.append([itemSet, itemSetUtility])
+
+            self.__finalPatterns[(startIndex, endIndex)] = results
+
+            if(endIndex >= len(self._transactions)):
+                break
+
+            self.__tree.removeBatch()
+
+            for i in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[endIndex + i], self._utilitySum[endIndex + i])
+
+            startIndex += self.__paneSize
+            endIndex += self.__paneSize
+
+        self.__endTime = _hus._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _hus._psutil.Process(_hus._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
     def mine(self):
         """
         This function will start the mining process
         """
         global _minUtil
         self.__startTime = _hus._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py` & `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -510,15 +510,15 @@
 
         contains(superset, subset)
             Checks if the superset contains the subset
 
         treeGenerations(root, netUtil, candidatePattern, curItem)
             Generates the tree of the high utility patterns
 
-        mine()
+        startMine()
             Starts the mining process
 
         printTree(root, level)
             Prints the SHU-tree in a readable format
 
         getMemoryRSS()
             Returns the memory usage of the algorithm in resident set size
@@ -828,15 +828,85 @@
                     self.treeGenerations(conditionalTree, netUtil, candidatePattern, newItemset)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         This function will start the mining process
         """
-        self.mine()
+        global _minUtil
+        self.__startTime = _hus._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minUtil is None:
+            raise Exception("Please enter the Minimum Support")
+        if self._windowSize is None:
+            raise Exception("Please enter the Window Size")
+        if self._paneSize is None:
+            raise Exception("Please enter the Pane Size")
+        self.__windowSize = int(self._windowSize)
+        self.__paneSize = int(self._paneSize)
+        
+        self._createItemsets()
+        self._minUtil = float(self._minUtil)
+        self.__tree = _SHUTree(self.__windowSize, self.__paneSize)
+        
+        transactionwiseUtility = []
+
+        for i in range(len(self._transactions)):
+            curTrans = {}
+            for j in range(len(self._transactions[i])):
+                curTrans[self._transactions[i][j]] = self._utilities[i][j]
+            transactionwiseUtility.append(curTrans)
+
+        for i in range(0, self.__windowSize):
+            self.__tree.batchIndex = i
+            for j in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[i * self.__paneSize + j], self._utilitySum[i * self.__paneSize + j], self._utilities[i * self.__paneSize + j])
+
+        startIndex = 0
+        endIndex = self.__windowSize * self.__paneSize
+
+        while(endIndex <= len(self._transactions)):
+            
+            filteredItemsets = {}
+            
+            self.treeGenerations(self.__tree, self._minUtil, filteredItemsets)
+
+            results = []
+
+            for itemSetLen in filteredItemsets:
+                for itemSet in filteredItemsets[itemSetLen]:
+                    itemSetUtility = 0
+                    for transId in range(startIndex, endIndex):
+                        if(self.contains(list(transactionwiseUtility[transId].keys()), itemSet)):
+                            for item in itemSet:
+                                itemSetUtility += transactionwiseUtility[transId][item]
+                        
+                    if(itemSetUtility >= self._minUtil):
+                        results.append([itemSet, itemSetUtility])
+
+            self.__finalPatterns[(startIndex, endIndex)] = results
+
+            if(endIndex >= len(self._transactions)):
+                break
+
+            self.__tree.removeBatch()
+
+            for i in range(0, self.__paneSize):
+                self.__tree.addTransaction(self._transactions[endIndex + i], self._utilitySum[endIndex + i], self._utilities[endIndex + i])
+
+            startIndex += self.__paneSize
+            endIndex += self.__paneSize
+
+        self.__endTime = _hus._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _hus._psutil.Process(_hus._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
     def mine(self):
         """
         This function will start the mining process
         """
         global _minUtil
         self.__startTime = _hus._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilityPatternsInStreams/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/__init__.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py`

 * *Files 10% similar despite different names*

```diff
@@ -198,15 +198,15 @@
             huis created
         neighbors: map
             keep track of neighbours of elements
         mapOfPMU: map
             a map to keep track of Probable Maximum utility(PMU) of each item
     :Methods:
 
-            mine()
+            startMine()
                 Mining process will start from here
             getPatterns()
                 Complete set of patterns will be retrieved with this function
             save(oFile)
                 Complete set of frequent patterns will be loaded in to a output file
             constructCUL(x, compactUList, st, minUtil, length, exNeighbours)
                 A method to construct CUL's database
@@ -325,17 +325,129 @@
             return compare
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main program to start the operation
         """
-        self.mine()
-
-
+        minUtil = self._minUtil
+        self._startTime = _ab._time.time()
+        with open(self._nFile, 'r') as file1:
+            for line in file1:
+                line = line.split("\n")[0]
+                parts = line.split(self._sep)
+                parts = [i.strip() for i in parts]
+                item = parts[0]
+                neigh1 = list()
+                for i in range(1, len(parts)):
+                    neigh1.append(parts[i])
+                self._neighbors[item] = set(neigh1)
+        with open(self._iFile, 'r') as file:
+            for line in file:
+                parts = line.split(":")
+                itemString = (parts[0].split("\n")[0]).split(self._sep)
+                utilityString = (parts[2].split("\n")[0]).split(self._sep)
+                transUtility = int(parts[1])
+                trans1 = set()
+                for i in range(0, len(itemString)):
+                    trans1.add(itemString[i])
+                for i in range(0, len(itemString)):
+                    item = itemString[i]
+                    twu = self._mapOfPMU.get(item)
+                    if twu is None:
+                        twu = int(utilityString[i])
+                    else:
+                        twu += int(utilityString[i])
+                    self._mapOfPMU[item] = twu
+                    if self._neighbors.get(item) is None:
+                        continue
+                    neighbours2 = trans1.intersection(self._neighbors.get(item))
+                    for item2 in neighbours2:
+                        if self._mapOfPMU.get(item2) is None:
+                            self._mapOfPMU[item2] = int(utilityString[i])
+                        else:
+                            self._mapOfPMU[item2] += int(utilityString[i])
+
+        listOfCUList = []
+        hashTable = {}
+        mapItemsToCUList = {}
+        for item in self._mapOfPMU.keys():
+            if self._mapOfPMU.get(item) >= minUtil:
+                uList = _CUList(item)
+                mapItemsToCUList[item] = uList
+                listOfCUList.append(uList)
+        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        ts = 1
+        with open(self._iFile, 'r') as file:
+            for line in file:
+                parts = line.split(":")
+                items = (parts[0].split("\n")[0]).split(self._sep)
+                utilities = (parts[2].split("\n")[0]).split(self._sep)
+                ru = 0
+                newTwu = 0
+                txKey = []
+                revisedTrans = []
+                for i in range(0, len(items)):
+                    pair = _Pair()
+                    pair.item = items[i]
+                    pair.utility = int(utilities[i])
+                    if self._mapOfPMU.get(pair.item) >= minUtil:
+                        revisedTrans.append(pair)
+                        txKey.append(pair.item)
+                        newTwu += pair.utility
+                revisedTrans.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+                txKey1 = tuple(txKey)
+                if len(revisedTrans) > 0:
+                    if txKey1 not in hashTable.keys():
+                        hashTable[txKey1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
+                        for i in range(len(revisedTrans) - 1, -1, -1):
+                            pair = revisedTrans[i]
+                            cuListOfItems = mapItemsToCUList.get(pair.item)
+                            element = _Element(ts, pair.utility, ru, 0, 0)
+                            if i > 0:
+                                element.prevPos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
+                            else:
+                                element.prevPos = -1
+                            cuListOfItems.addElements(element)
+                            ru += pair.utility
+                    else:
+                        pos = hashTable[txKey1]
+                        ru = 0
+                        for i in range(len(revisedTrans) - 1, -1, -1):
+                            cuListOfItems = mapItemsToCUList[revisedTrans[i].item]
+                            cuListOfItems.elements[pos].snu += revisedTrans[i].utility
+                            cuListOfItems.elements[pos].remainingUtility += ru
+                            cuListOfItems.sumSnu += revisedTrans[i].utility
+                            cuListOfItems.sumRemainingUtility += ru
+                            ru += revisedTrans[i].utility
+                            pos = cuListOfItems.elements[pos].prevPos
+                # EUCS
+                for i in range(len(revisedTrans) - 1, -1, -1):
+                    pair = revisedTrans[i]
+                    mapFMAPItem = self._mapFMAP.get(pair.item)
+                    if mapFMAPItem is None:
+                        mapFMAPItem = {}
+                        self._mapFMAP[pair.item] = mapFMAPItem
+                    for j in range(i + 1, len(revisedTrans)):
+                        pairAfter = revisedTrans[j]
+                        twuSUm = mapFMAPItem.get(pairAfter.item)
+                        if twuSUm is None:
+                            mapFMAPItem[pairAfter.item] = newTwu
+                        else:
+                            mapFMAPItem[pairAfter.item] = twuSUm + newTwu
+                ts += 1
+        exNeighbours = set(self._mapOfPMU.keys())
+        # print(self.Neighbours)
+        self._ExploreSearchTree([], listOfCUList, exNeighbours, minUtil)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
         main program to start the operation
         """
         minUtil = self._minUtil
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py`

 * *Files 2% similar despite different names*

```diff
@@ -348,15 +348,15 @@
         itemsToKeep: list
             keep only the promising items ie items having twu >= minUtil
         itemsToExplore: list
             keep items that subtreeUtility grater than minUtil
 
     :Methods:
 
-        mine()
+        startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of frequent patterns will be loaded in to a dataframe
@@ -463,15 +463,71 @@
         super().__init__(iFile, nFile, minUtil, sep)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main program to start the operation
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._patternCount = 0
+        self._finalPatterns = {}
+        self._dataset = _Dataset(self._iFile, self._sep)
+        with open(self._nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self._sep)
+                line_split = [i.strip() for i in line_split]
+                item = self._dataset.strToInt.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self._dataset.strToInt.get(line_split[i]))
+                self._Neighbours[item] = lst
+        o.close()
+        #print(len(self._Neighbours))
+        InitialMemory = _ab._psutil.virtual_memory()[3]
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        itemsToKeep = []
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= self._minUtil:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self._sortDatabase(self._dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self._dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
+        itemsToExplore = []
+        for item in itemsToKeep:
+            if self._utilityBinArraySU[item] >= self._minUtil:
+                itemsToExplore.append(item)
+        commonitems = []
+        for i in range(self._dataset.maxItem):
+            commonitems.append(i)
+        self._backtrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        finalMemory = _ab._psutil.virtual_memory()[3]
+        memory = (finalMemory - InitialMemory) / 10000
+        if memory > self._maxMemory:
+            self._maxMemory = memory
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
         main program to start the operation
         """
         self._startTime = _ab._time.time()
         self._patternCount = 0
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py`

 * *Files 2% similar despite different names*

```diff
@@ -323,15 +323,15 @@
         itemsToKeep: list
             keep only the promising items ie items having twu >= minUtil
         itemsToExplore: list
             keep items that subtreeUtility grater than minUtil
 
     :Methods:
 
-        mine()
+        startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of patterns will be loaded in to a dataframe
@@ -434,15 +434,72 @@
         super().__init__(iFile, nFile, k, sep)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Main function of the program.
         """
-        self.mine()
+        self.startTime = time.time()
+        self.finalPatterns = {}
+        self.dataset = Dataset(self.iFile, self.sep)
+        with open(self.nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self.sep)
+                item = self.dataset.strToint.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self.dataset.strToint.get(line_split[i]))
+                self.Neighbours[item] = lst
+        o.close()
+        InitialMemory = psutil.virtual_memory()[3]
+        self.useUtilityBinArrayToCalculateLocalUtilityFirstTime(self.dataset)
+        itemsToKeep = []
+        for key in self.utilityBinArrayLU.keys():
+            if self.utilityBinArrayLU[key] >= self.minUtil:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self.utilityBinArrayLU[x])
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self.oldNamesToNewNames[item] = currentName
+            self.newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self.dataset.getTransactions():
+            transaction.removeUnpromisingItems(self.oldNamesToNewNames)
+        self.sortDatabase(self.dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self.dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self.dataset.transactions = self.dataset.transactions[emptyTransactionCount:]
+        self.useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self.dataset)
+        self.heapList = []
+        itemsToExplore = []
+        for item in itemsToKeep:
+            if self.utilityBinArraySU[item] >= self.minUtil:
+                itemsToExplore.append(item)
+        commonitems = []
+        for i in range(self.dataset.maxItem):
+            commonitems.append(i)
+        self.backtrackingEFIM(self.dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        finalMemory = psutil.virtual_memory()[3]
+        memory = (finalMemory - InitialMemory) / 10000
+        if memory > self.maxMemory:
+            self.maxMemory = memory
+        self.endTime = time.time()
+        process = psutil.Process(os.getpid())
+        self.memoryUSS = float()
+        self.memoryRSS = float()
+        self.memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
+        for item in self.heapList:
+            self.finalPatterns[item[1]] = item[0]
+        print('TOP-K mining process is completed by TKSHUIM')
 
     def mine(self):
         """
         Main function of the program.
         """
         self.startTime = time.time()
         self.finalPatterns = {}
```

### Comparing `pami-2024.4.24.1/PAMI/highUtilitySpatialPattern/topk/abstract.py` & `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,47 +4,45 @@
 # lengths where a pattern is continuously periodic, while the minDur (minimal duration) measure ensures that those
 # time-intervals have a minimum duration.
 #
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.localPeriodicPattern.basic import LPPGrowth as alg
+#     from PAMI.localPeriodicPattern.basic import LPPGrowth as alg
 #
-#             obj = alg.LPPGrowth(iFile, maxPer, maxSoPer, minDur)
+#     obj = alg.LPPGrowth(iFile, maxPer, maxSoPer, minDur)
 #
-#             obj.mine()
+#     obj.startMine()
 #
-#             localPeriodicPatterns = obj.getPatterns()
+#     localPeriodicPatterns = obj.getPatterns()
 #
-#             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
+#     print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 #
-#             obj.save(oFile)
+#     obj.save(oFile)
 #
-#             Df = obj.getPatternsAsDataFrame()
+#     Df = obj.getPatternsAsDataFrame()
 #
-#             memUSS = obj.getMemoryUSS()
+#     memUSS = obj.getMemoryUSS()
 #
-#             print(f'Total memory in USS: {memUSS}')
+#     print(f'Total memory in USS: {memUSS}')
 #
-#             memRSS = obj.getMemoryRSS()
+#     memRSS = obj.getMemoryRSS()
 #
-#             print(f'Total memory in RSS: {memRSS}')
+#     print(f'Total memory in RSS: {memRSS}')
 #
-#             runtime = obj.getRuntime()
+#     runtime = obj.getRuntime()
+#
+#     print(f'Total execution time in seconds: {runtime})
 #
-#             print(f'Total execution time in seconds: {runtime})
 #
-
-
-
 
 
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -58,15 +56,14 @@
 
 """
 
 
 
 from PAMI.localPeriodicPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
-from deprecated import deprecated
 
 class Node:
     """
     A class used to represent the node of localPeriodicPatternTree
 
     :Attributes:
 
@@ -93,22 +90,16 @@
         self.child = []
         self.nodeLink = None
         self.tidList = set()
 
     def getChild(self, item: int) -> 'Node':
         """
         This function is used to get child node from the parent node
-
-        :param item: item of the parent node
-
-        :type item: int
-
+        :param item:
         :return: if node have node of item, then return it. if node don't have return []
-
-        :rtype: Node
         """
         for child in self.child:
             if child.item == item:
                 return child
         return []
 
 
@@ -146,15 +137,14 @@
         """
         add transaction into tree
 
         :param transaction: it represents the one transaction in database
         :type transaction: list
         :param tid: represents the timestamp of transaction
         :type tid: list or int
-        :return: None
         """
         current = self.root
         for item in transaction:
             child = current.getChild(item)
             if not child:
                 newNode = Node()
                 newNode.item = item
@@ -170,30 +160,28 @@
         """
         fix node link
 
         :param item: it represents item name of newNode
         :type item: string
         :param newNode: it represents node which is added
         :type newNode: Node
-        :return: None
         """
         if item in self.nodeLinks:
             lastNode = self.nodeLinks[item]
             lastNode.nodeLink = newNode
         self.nodeLinks[item] = newNode
         if item not in self.firstNodeLink:
             self.firstNodeLink[item] = newNode
 
     def deleteNode(self, item: int) -> None:
         """
         delete the node from tree
 
         :param item: it represents the item name of node
         :type item: str
-        :return: None
         """
         deleteNode = self.firstNodeLink[item]
         parentNode = deleteNode.parent
         parentNode.child.remove(deleteNode)
         parentNode.child += deleteNode.child
         parentNode.tidList |= deleteNode.tidList
         for child in deleteNode.child:
@@ -211,15 +199,14 @@
         """
         create prefix tree by path
 
         :param path: it represents path to root from prefix node
         :type path: list
         :param tidList: it represents tid of each item
         :type tidList: list
-        :return: None
         """
         currentNode = self.root
         for item in path:
             child = currentNode.getChild(item)
             if not child:
                 newNode = Node()
                 newNode.item = item
@@ -244,17 +231,17 @@
 
     :Reference:
 
         Fournier-Viger, P., Yang, P., Kiran, R. U., Ventura, S., Luna, J. M.. (2020). Mining Local Periodic Patterns in
         a Discrete Sequence. Information Sciences, Elsevier, to appear. [ppt] DOI: 10.1016/j.ins.2020.09.044
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of local periodic pattern's
+                   Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of local periodic patterns
+                   Name of the output file to store complete set of frequent patterns
     :param  minDur: str:
                    Minimal duration in seconds between consecutive periods of time-intervals where a pattern is continuously periodic.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  maxSoPer: float:
                    Controls the maximum number of time periods between consecutive periods of time-intervals where a pattern is continuously periodic.
 
@@ -308,15 +295,15 @@
                 Create LPPTree of local periodic item from input data.
             patternGrowth(tree, prefix, prefixPFList)
                 Execute pattern growth algorithm. It is important function in this program.
             calculatePTL(tsList)
                 Calculate PTL from input tsList as integer list.
             calculatePTLbit(tsList)
                 Calculate PTL from input tsList as bit vector.
-            mine()
+            startMine()
                 Mining process will start from here.
             getMemoryUSS()
                 Total amount of USS memory consumed by the mining process will be retrieved from this function.
             getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function.
             getRuntime()
                 Total amount of runtime taken by the mining process will be retrieved from this function.
@@ -324,38 +311,31 @@
                 return local periodic patterns and its PTL
             save(oFile)
                 Complete set of local periodic patterns will be loaded in to an output file.
             getPatternsAsDataFrame()
                 Complete set of local periodic patterns will be loaded in to a dataframe.
 
     **Executing the code on terminal:**
-    ---------------------------------------
-
-    .. code-block:: console
-
-      Format:
-
-      (.venv) $ python3 LPPMGrowth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
-
-      Example Usage:
+    -------------------------------------
+            Format:
+                    >>> python3 LPPMGrowth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
 
-      (.venv) $ python3 LPPMGrowth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
-
-    .. note: minDur will be considered as time interval between two consecutive periods
+            Examples:
+                    >>> python3 LPPMGrowth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
 
 
     **Sample run of importing the code:**
     ----------------------------------------
     .. code-block:: python
 
             from PAMI.localPeriodicPattern.basic import LPPGrowth as alg
 
             obj = alg.LPPGrowth(iFile, maxPer, maxSoPer, minDur)
 
-            obj.mine()
+            obj.startMine()
 
             localPeriodicPatterns = obj.getPatterns()
 
             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 
             obj.save(oFile)
 
@@ -601,15 +581,14 @@
 
         :param tree: The root node of prefix tree.
         :type tree: Node or Tree
         :param prefix: Prefix item list.
         :type prefix: list
         :param prefixPFList: tsList of prefix patterns.
         :type prefixPFList: dict or list
-        :return: None
         """
         items = list(prefixPFList)
         if not prefix:
             items = reversed(items)
         for item in items:
             prefixCopy = prefix.copy()
             prefixCopy.append(item)
@@ -658,15 +637,14 @@
     def __calculatePTL(self, tsList: List[int]) -> set:
         """
         Calculate PTL from input tsList as integer list
 
         :param tsList: It is tsList which store time stamp as integer.
         :type tsList: list
         :return: PTL
-        :rtype: set
         """
         start = -1
         PTL = set()
         tsList = sorted(tsList)
         tsPre = tsList[0]
         soPer = ' '
         for ts in tsList[1:]:
@@ -692,15 +670,14 @@
     def __calculatePTLbit(self, tsList: List[int]) -> set:
         """
         Calculate PTL from input tsList as bit vector.
 
         :param tsList: It is tsList which store time stamp as bit vector.
         :type tsList: list
         :return: PTL
-        :rtype: set
         """
         tsList = list(bin(tsList))
         tsList = tsList[2:]
         start = -1
         currentTs = 1
         PTL = set()
         tsPre = ' '
@@ -739,41 +716,32 @@
         return PTL
 
     def __convert(self, value: Any) -> float:
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :type value: int or float or str
         :return: converted type
-        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process start from here.
         """
-        self.mine()
-
-    def mine(self) -> None:
-        """
-        Mining process start from here.
-        """
         self._localPeriodicPatterns__startTime = _ab._time.time()
         self._localPeriodicPatterns__finalPatterns = {}
         self.__creatingItemSets()
         self._localPeriodicPatterns__maxPer = self.__convert(self._localPeriodicPatterns__maxPer)
         self._localPeriodicPatterns__maxSoPer = self.__convert(self._localPeriodicPatterns__maxSoPer)
         self._localPeriodicPatterns__minDur = self.__convert(self._localPeriodicPatterns__minDur)
         self.__createTSList()
@@ -784,46 +752,42 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._localPeriodicPatterns__memoryUSS = float()
         self._localPeriodicPatterns__memoryRSS = float()
         self._localPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._localPeriodicPatterns__memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryRSS
 
     def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__endTime - self._localPeriodicPatterns__startTime
 
     def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
-        """
-        Storing final local periodic patterns in a dataframe
+        """Storing final local periodic patterns in a dataframe
 
         :return: returning local periodic patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
@@ -837,31 +801,29 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of local periodic patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
-        :return: None
         """
         self._localPeriodicPatterns__oFile = outFile
         writer = open(self._localPeriodicPatterns__oFile, 'w+')
         for x, y in self._localPeriodicPatterns__finalPatterns.items():
             pat = str()
             for i in x:
                 pat = pat + i + '\t'
             pat = pat + ":"
             for i in y:
                 pat = pat + str(i) + '\t'
             patternsAndPTL = pat.strip()
             writer.write("%s \n" % patternsAndPTL)
 
     def getPatterns(self) -> Dict:
-        """
-        Function to send the set of local periodic patterns after completion of the mining process
+        """ Function to send the set of local periodic patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._localPeriodicPatterns__finalPatterns
 
     def printResults(self) -> None:
@@ -878,15 +840,14 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = LPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = LPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
-        _ap.mine()
         print("Total number of Local Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,46 +5,43 @@
 # time-intervals have a minimum duration.
 #
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.localPeriodicPattern.basic import LPPMBreadth as alg
+#     from PAMI.localPeriodicPattern.basic import LPPMBreadth as alg
 #
-#             obj = alg.LPPMBreadth(iFile, maxPer, maxSoPer, minDur)
+#     obj = alg.LPPMBreadth(iFile, maxPer, maxSoPer, minDur)
 #
-#             obj.mine()
+#     obj.startMine()
 #
-#             localPeriodicPatterns = obj.getPatterns()
+#     localPeriodicPatterns = obj.getPatterns()
 #
-#             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
+#     print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 #
-#             obj.save(oFile)
+#     obj.save(oFile)
 #
-#             Df = obj.getPatternsAsDataFrame()
+#     Df = obj.getPatternsAsDataFrame()
 #
-#             memUSS = obj.getMemoryUSS()
+#     memUSS = obj.getMemoryUSS()
 #
-#             print(f'Total memory in USS: {memUSS}')
+#     print(f'Total memory in USS: {memUSS}')
 #
-#             memRSS = obj.getMemoryRSS()
+#     memRSS = obj.getMemoryRSS()
 #
-#             print(f'Total memory in RSS: {memRSS}')
+#     print(f'Total memory in RSS: {memRSS}')
 #
-#             runtime = obj.getRuntime()
+#     runtime = obj.getRuntime()
 #
-#             print(f'Total execution time in seconds: {runtime})
-#
-
-
+#     print(f'Total execution time in seconds: {runtime})
 
 
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -57,16 +54,14 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.localPeriodicPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
-from deprecated import deprecated
-
 
 class LPPMBreadth(_ab._localPeriodicPatterns):
 
     """
     :Description:
 
         Local Periodic Patterns, which are patterns (sets of events) that have a periodic behavior in some non predefined
@@ -77,17 +72,17 @@
 
     :Reference:
 
         Fournier-Viger, P., Yang, P., Kiran, R. U., Ventura, S., Luna, J. M.. (2020). Mining Local Periodic Patterns in
         a Discrete Sequence. Information Sciences, Elsevier, to appear. [ppt] DOI: 10.1016/j.ins.2020.09.044
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of local periodic pattern's
+                   Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of local periodic patterns
+                   Name of the output file to store complete set of frequent patterns
     :param  minDur: str:
                    Minimal duration in seconds between consecutive periods of time-intervals where a pattern is continuously periodic.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  maxSoPer: float:
                    Controls the maximum number of time periods between consecutive periods of time-intervals where a pattern is continuously periodic.
 
@@ -128,15 +123,15 @@
             Create the tsList as bit vector from input data.
         generateLPP()
             Generate 1 length local periodic pattens by tsList and execute depth first search.
         calculatePTL(tsList)
             Calculate PTL from input tsList as bit vector
         LPPMBreathSearch(extensionOfP)
             Mining local periodic patterns using breadth first search.
-        mine()
+        startMine()
             Mining process will start from here.
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function.
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function.
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function.
@@ -144,37 +139,29 @@
             return local periodic patterns and its PTL
         save(oFile)
             Complete set of local periodic patterns will be loaded in to an output file.
         getPatternsAsDataFrame()
             Complete set of local periodic patterns will be loaded in to a dataframe.
 
     **Executing the code on terminal:**
-    --------------------------------------
-
-    .. code-block:: console
-
-      Format:
-
-      (.venv) $ python3 LPPBreadth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
-
-      Example Usage:
-
-      (.venv) $ python3 LPPMBreadth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
-
-    .. note: minDur will be considered as time interval between two consecutive periods
+    ------------------------------------
+            Format:
+                >>> python3 LPPBreadth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
+            Examples:
+                >>> python3 LPPMBreadth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
 
     **Sample run of importing the code:**
     -------------------------------------
     .. code-block:: python
     
             from PAMI.localPeriodicPattern.basic import LPPMBreadth as alg
 
             obj = alg.LPPMBreadth(iFile, maxPer, maxSoPer, minDur)
 
-            obj.mine()
+            obj.startMine()
 
             localPeriodicPatterns = obj.getPatterns()
 
             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 
             obj.save(oFile)
 
@@ -358,15 +345,14 @@
     def __calculatePTL(self, tsList: int) -> Set[Tuple[int, int]]:
         """
         calculate PTL from tsList as bit vector.
 
         :param tsList: it is one item's tsList which is used bit vector.
         :type tsList: int
         :return: it is PTL of input item.
-        :rtype: set
         """
         tsList = list(bin(tsList))
         tsList = tsList[2:]
         start = -1
         currentTs = 1
         PTL = set()
         tsPre = ' '
@@ -451,41 +437,32 @@
         return w1map
 
     def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :type value: int or float or str
         :return: converted type
-        :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process start from here.
         """
-        self.mine()
-
-    def mine(self) -> None:
-        """
-        Mining process start from here.
-        """
         self._localPeriodicPatterns__startTime = _ab._time.time()
         self.__creatingItemSets()
         self._localPeriodicPatterns__maxPer = self.__convert(self._localPeriodicPatterns__maxPer)
         self._localPeriodicPatterns__maxSoPer = self.__convert(self._localPeriodicPatterns__maxSoPer)
         self._localPeriodicPatterns__minDur = self.__convert(self._localPeriodicPatterns__minDur)
         self._localPeriodicPatterns__finalPatterns = {}
         self.__createTSList()
@@ -494,46 +471,42 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._localPeriodicPatterns__memoryUSS = float()
         self._localPeriodicPatterns__memoryRSS = float()
         self._localPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._localPeriodicPatterns__memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryRSS
 
     def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__endTime - self._localPeriodicPatterns__startTime
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
-        """
-        Storing final local periodic patterns in a dataframe
+        """Storing final local periodic patterns in a dataframe
 
         :return: returning local periodic patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
@@ -542,36 +515,33 @@
             for i in a:
                 pat = pat + i + ' '
             data.append([pat, b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'PTL'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
-        """
-        Complete set of local periodic patterns will be loaded in to an output file
+        """Complete set of local periodic patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
-        :return: None
         """
         self._localPeriodicPatterns__oFile = outFile
         writer = open(self._localPeriodicPatterns__oFile, 'w+')
         for x, y in self._localPeriodicPatterns__finalPatterns.items():
             pat = str()
             for i in x:
                 pat = pat + i + '\t'
             pat = pat + ":"
             for i in y:
                 pat = pat + str(i) + '\t'
             patternsAndPTL = pat.strip()
             writer.write("%s \n" % patternsAndPTL)
 
     def getPatterns(self) -> Dict[Union[Tuple[str, ...], str], Set[Tuple[int, int]]]:
-        """
-        Function to send the set of local periodic patterns after completion of the mining process
+        """ Function to send the set of local periodic patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._localPeriodicPatterns__finalPatterns
 
     def printResults(self) -> None:
@@ -588,15 +558,14 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = LPPMBreadth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = LPPMBreadth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
-        _ap.mine()
         print("Total number of Local Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,46 +4,42 @@
 # lengths where a pattern is continuously periodic, while the minDur (minimal duration) measure ensures that those
 # time-intervals have a minimum duration.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.localPeriodicPattern.basic import LPPMDepth as alg
+#     from PAMI.localPeriodicPattern.basic import LPPMDepth as alg
 #
-#             obj = alg.LPPMDepth(iFile, maxPer, maxSoPer, minDur)
+#     obj = alg.LPPMDepth(iFile, maxPer, maxSoPer, minDur)
 #
-#             obj.mine()
+#     obj.startMine()
 #
-#             localPeriodicPatterns = obj.getPatterns()
+#     localPeriodicPatterns = obj.getPatterns()
 #
-#             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
+#     print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 #
-#             obj.save(oFile)
+#     obj.save(oFile)
 #
-#             Df = obj.getPatternsAsDataFrame()
+#     Df = obj.getPatternsAsDataFrame()
 #
-#             memUSS = obj.getMemoryUSS()
+#     memUSS = obj.getMemoryUSS()
 #
-#             print(f'Total memory in USS: {memUSS}')
+#     print(f'Total memory in USS: {memUSS}')
 #
-#             memRSS = obj.getMemoryRSS()
+#     memRSS = obj.getMemoryRSS()
 #
-#             print(f'Total memory in RSS: {memRSS}')
+#     print(f'Total memory in RSS: {memRSS}')
 #
-#             runtime = obj.getRuntime()
+#     runtime = obj.getRuntime()
 #
-#             print(f'Total execution time in seconds: {runtime})
-#
-
-
-
+#     print(f'Total execution time in seconds: {runtime})
 
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -56,16 +52,14 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.localPeriodicPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
-from deprecated import deprecated
-
 
 class LPPMDepth(_ab._localPeriodicPatterns):
 
     """
     :Description:
 
         Local Periodic Patterns, which are patterns (sets of events) that have a periodic behavior in some non predefined
@@ -76,17 +70,17 @@
 
     :Reference:
 
         Fournier-Viger, P., Yang, P., Kiran, R. U., Ventura, S., Luna, J. M.. (2020). Mining Local Periodic Patterns in
         a Discrete Sequence. Information Sciences, Elsevier, to appear. [ppt] DOI: 10.1016/j.ins.2020.09.044
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of local periodic pattern's
+                   Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of local periodic patterns
+                   Name of the output file to store complete set of frequent patterns
     :param  minDur: str:
                    Minimal duration in seconds between consecutive periods of time-intervals where a pattern is continuously periodic.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  maxSoPer: float:
                    Controls the maximum number of time periods between consecutive periods of time-intervals where a pattern is continuously periodic.
 
@@ -126,15 +120,15 @@
             Create the TSlist as bit vector from input data.
         generateLPP()
             Generate 1 length local periodic pattens by TSlist and execute depth first search.
         calculatePTL(tsList)
             Calculate PTL from input tsList as bit vector
         LPPMDepthSearch(extensionOfP)
             Mining local periodic patterns using depth first search.
-        mine()
+        startMine()
             Mining process will start from here.
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function.
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function.
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function.
@@ -142,38 +136,32 @@
             return local periodic patterns and its PTL
         save(oFile)
             Complete set of local periodic patterns will be loaded in to an output file.
         getPatternsAsDataFrame()
             Complete set of local periodic patterns will be loaded in to a dataframe.
 
     **Executing the code on terminal:**
-    --------------------------------------
-
-    .. code-block:: console
-
-      Format:
-
-      (.venv) $ python3 LPPMDepth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
-
-      Example Usage:
+    -------------------------------------
+            Format:
 
-      (.venv) $ python3 LPPMDepth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
+                >>> python3 LPPMDepth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur> <sep>
+            Examples:
 
-    .. note: minDur will be considered as time interval between two consecutive periods
+                >>> python3 LPPMDepth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
 
 
     **Sample run of importing the code:**
     ----------------------------------------
     .. code-block:: python
 
             from PAMI.localPeriodicPattern.basic import LPPMDepth as alg
 
             obj = alg.LPPMDepth(iFile, maxPer, maxSoPer, minDur)
 
-            obj.mine()
+            obj.startMine()
 
             localPeriodicPatterns = obj.getPatterns()
 
             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 
             obj.save(oFile)
 
@@ -341,20 +329,19 @@
                 self._localPeriodicPatterns__finalPatterns[item] = PTL[item]
         I = sorted(list(I))
         # I = set(I)
         self.__LPPMDepthSearch(I)
 
     def __calculatePTL(self, tsList: int) -> Set[Tuple[int, int]]:
         """
-        calculate PTL from tsList as bit vector.
+         calculate PTL from tsList as bit vector.
 
         :param tsList: it is one item's tsList which is used bit vector.
         :type tsList: int
         :return: it is PTL of input item.
-        :rtype: set
         """
         tsList = list(bin(tsList))
         tsList = tsList[2:]
         start = -1
         currentTs = 1
         PTL = set()
         tsPre = ' '
@@ -394,15 +381,14 @@
 
     def __LPPMDepthSearch(self, extensionsOfP: List[Union[Tuple[str, ...], str]]) -> None:
         """
         Mining n-length local periodic pattens from n-1-length patterns by depth first search.
 
         :param extensionsOfP: it is n-1 length patterns list.
         :type extensionsOfP: list
-        :return: None
         """
         for x in range(len(extensionsOfP)-1):
             extensionsOfPx = set()
             for y in range(x+1,len(extensionsOfP)):
                 tspxy = self.__tsList[extensionsOfP[x]] & self.__tsList[extensionsOfP[y]]
                 PTL = self.__calculatePTL(tspxy)
                 if len(PTL) > 0:
@@ -422,38 +408,30 @@
                 self.__LPPMDepthSearch(list(extensionsOfPx))
 
     def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :type value: int or float or str
         :return: converted type
-        :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
-        """
-        Mining process start from here. This function calls createTSlist and generateLPP.
-        """
-        self.mine()
 
-    def mine(self) -> None:
+    def startMine(self) -> None:
         """
         Mining process start from here. This function calls createTSlist and generateLPP.
         """
         self._localPeriodicPatterns__startTime = _ab._time.time()
         self._localPeriodicPatterns__finalPatterns = {}
         self.__creatingItemSets()
         self._localPeriodicPatterns__maxPer = self.__convert(self._localPeriodicPatterns__maxPer)
@@ -465,36 +443,33 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._localPeriodicPatterns__memoryRSS = float()
         self._localPeriodicPatterns__memoryUSS = float()
         self._localPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._localPeriodicPatterns__memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryRSS
 
     def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__endTime - self._localPeriodicPatterns__startTime
 
@@ -518,31 +493,29 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of local periodic patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
-        :return: None
         """
         self._localPeriodicPatterns__oFile = outFile
         writer = open(self._localPeriodicPatterns__oFile, 'w+')
         for x, y in self._localPeriodicPatterns__finalPatterns.items():
             pat = str()
             for i in x:
                 pat = pat + i + '\t'
             pat = pat + ":"
             for i in y:
                 pat = pat + str(i) + '\t'
             patternsAndPTL = pat.strip()
             writer.write("%s \n" % patternsAndPTL)
 
     def getPatterns(self) -> Dict[Union[Tuple[str, ...], str], Set[Tuple[int, int]]]:
-        """
-        Function to send the set of local periodic patterns after completion of the mining process
+        """ Function to send the set of local periodic patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._localPeriodicPatterns__finalPatterns
 
     def printResults(self) -> None:
@@ -559,15 +532,14 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = LPPMDepth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = LPPMDepth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
-        _ap.mine()
         print("Total number of Local Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/localPeriodicPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py` & `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -546,15 +546,40 @@
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         main program to start the operation
         :return: none
 
         """
-        self.mine()
+        global _MIS
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self.__creatingItemSets()
+        self._getMISValues()
+        #MIS = self._MISValues
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            _MIS[y] = self._MISValues[x]
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using basic algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
     def Mine(self) -> None:
         """
         main program to start the operation
         :return: none
 
         """
```

### Comparing `pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py` & `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py`

 * *Files 2% similar despite different names*

```diff
@@ -500,15 +500,39 @@
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         main program to start the operation
 
         """
-        self.mine()
+        global MIS
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self.__creatingItemSets()
+        self._getMISValues()
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            MIS[y] = self._MISValues[x]
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
     def Mine(self):
         """
         main program to start the operation
 
         """
         global MIS
```

### Comparing `pami-2024.4.24.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-import deprecated
+import sys
 from PAMI.partialPeriodicFrequentPattern.basic.abstract import *
 
 orderOfItem = {}
 
 class Node:
     """
     A class used to represent the node of frequentPatternTree
@@ -659,20 +659,16 @@
                             temp = [i.rstrip() for i in line.split(self._partialPeriodicPatterns__sep)]
                             temp = [x for x in temp if x]
                             self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
-
-        self.mine()
 
-    def mine(self):
+    def startMine(self):
         self.__inputFile = self._partialPeriodicPatterns__iFile
         self._partialPeriodicPatterns__startTime = time.time()
         self._partialPeriodicPatterns__finalPatterns = {}
         self.__readDatabase()
         self._partialPeriodicPatterns__minSup = self.__convert(self._partialPeriodicPatterns__minSup)
         self._partialPeriodicPatterns__maxPer = self.__convert(self._partialPeriodicPatterns__maxPer)
         # self.minPR = self.convert(self.minPR)
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py` & `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
 from PAMI.partialPeriodicFrequentPattern.basic.abstract import *
-import deprecated
+
 
 class PPF_DFS(partialPeriodicPatterns):
     """
     :Description:   PPF_DFS is algorithm to mine the partial periodic frequent patterns.
 
     :References:    (Has to be added)
 
@@ -416,28 +416,19 @@
                 if len(y) >= self._partialPeriodicPatterns__minSup and val / (self._partialPeriodicPatterns__minSup + 1) >= self._partialPeriodicPatterns__minPR:
                     classItemsets.append(itemj)
                     classtidsets.append(y)
             newprefix = list(set(itemsetx)) + prefix
             self.__Generation(newprefix, classItemsets, classtidsets)
             self.__save(prefix, list(set(itemsetx)), tidsetx)
 
-
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main program start with extracting the periodic frequent items from the database and
         performs prefix equivalence to form the combinations and generates closed periodic frequent patterns.
         """
-        self.mine()
-
-    def mine(self):
-        """
-        Main program start with extracting the periodic frequent items from the database and
-        performs prefix equivalence to form the combinations and generates closed periodic frequent patterns.
-        """
         self.__path = self._partialPeriodicPatterns__iFile
         self._partialPeriodicPatterns__startTime = time.time()
         self.__creatingItemSets()
         plist = self.__oneItems(self.__path)
         self._partialPeriodicPatterns__finalPatterns = {}
         for i in range(len(plist)):
             itemx = plist[i]
@@ -458,14 +449,16 @@
         self._partialPeriodicPatterns__endTime = time.time()
         self.__runTime = self._partialPeriodicPatterns__endTime - self._partialPeriodicPatterns__startTime
         process = psutil.Process(os.getpid())
         self._partialPeriodicPatterns__memoryUSS = float()
         self._partialPeriodicPatterns__memoryRSS = float()
         self._partialPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._partialPeriodicPatterns__memoryRSS = process.memory_info().rss
+        # print("eclat Time taken:",temp)
+        # print("eclat Memory Space:",resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -45,15 +45,15 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from pandas.core.arrays import period
-import deprecated
+
 from PAMI.partialPeriodicPattern.basic import Gabstract as _abstract
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
 
 _minPS = float()
@@ -567,27 +567,19 @@
             if '%' in value:
                 value = value[:-1]
                 value = float(int(value)/100)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
 
         """
-        self.mine()
-
-    def mine(self) -> None:
-        """
-        Main method where the patterns are mined by constructing tree.
-
-        """
         global _minPS, _period, _relativePS, _lno
         self._startTime = float()
         self._startTime = _abstract._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minPS is None:
             raise Exception("Please enter the Minimum Support")
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/Gabstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/Gabstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,14 +54,16 @@
 
 
 from PAMI.partialPeriodicPattern.basic import abstract as _abstract
 from typing import List, Dict, Tuple, Set, Union, Any, Iterable, Generator
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
+
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
 _minPS = float()
 _period = float()
 _lno = int()
 
@@ -578,20 +580,44 @@
 
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
         :return: None
-        """
-
-        self.mine()
 
+        """
+        global _minPS, _period, _lno
+        self._startTime = _abstract._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minPS is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        generatedItems, pfList = self._partialPeriodicOneItem()
+        _minPS, _period, _lno = self._minPS, self._period, len(self._Database)
+        updatedTransactions = self._updateTransactions(generatedItems)
+        for x, y in self._rank.items():
+            self._rankdup[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedTransactions, info)
+        patterns = Tree._generatePatterns([])
+        self._finalPatterns = {}
+        for i in patterns:
+            s = self._savePeriodic(i[0])
+            self._finalPatterns[s] = i[1]
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Partial Periodic Patterns were generated successfully using 3PGrowth algorithm ")
 
-    def mine(self) -> None:
+    def Mine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
         :return: None
 
         """
         global _minPS, _period, _lno
         self._startTime = _abstract._time.time()
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py`

 * *Files 3% similar despite different names*

```diff
@@ -383,15 +383,41 @@
     def startMine(self) -> None:
         """
         Main program start with extracting the periodic frequent items from the database and
         performs prefix equivalence to form the combinations and generates partial-periodic patterns.
         :return: None
 
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        plist = self._creatingOneitemSets()
+        self._finalPatterns = {}
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetX = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                val = self._getPeriodicSupport(y1)
+                if val >= self._minPS:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
+        print("Partial Periodic Patterns were generated successfully using 3PEclat algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def Mine(self) -> None:
         """
         Main program start with extracting the periodic frequent items from the database and
         performs prefix equivalence to form the combinations and generates partial-periodic patterns.
         :return: None
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/closed/PPPClose.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,17 @@
-
-
+#  CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
+#  It uses depth-first search.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.partialPeriodicPattern.closed import PPPClose as alg
 #
-#             obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
+#             from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
+#
+#             obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
 #
 #             obj.startMine()
 #
 #             periodicFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
@@ -25,16 +27,16 @@
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
-#
-#
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -47,71 +49,59 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-
-
-import sys as _sys
-import validators as _validators
-from urllib.request import urlopen as _urlopen
-from PAMI.partialPeriodicPattern.closed import abstract as _abstract
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-class PPPClose(_abstract._partialPeriodicPatterns):
-    """
-    :Description:
+from PAMI.periodicFrequentPattern.closed import abstract as _ab
+
 
-    PPPClose algorithm is used to discover the closed partial periodic patterns in temporal databases.
-    It uses depth-first search.
+class CPFPMiner(_ab._periodicFrequentPatterns):
+    """ 
+    :Description:   CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
+                    It uses depth-first search.
 
-    :Reference: R. Uday Kiran1 , J. N. Venkatesh2 , Philippe Fournier-Viger3 , Masashi Toyoda1 , P. Krishna Reddy2 and Masaru Kitsuregawa
-                 https://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/799/PAKDD.pdf
+    :Reference:   P. Likhitha et al., "Discovering Closed Periodic-Frequent Patterns in Very Large Temporal Databases"
+                  2020 IEEE International Conference on Big Data (Big Data), 2020, https://ieeexplore.ieee.org/document/9378215
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of periodic frequent pattern's
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  period: float:
-                   Minimum partial periodic...
-    :param  periodicSupport: float:
-                   Minimum partial periodic...
+    :param  minSup: float:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: float:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
         oFile : str
             Name of the output file or path of the input file
-        periodicSupport: int or float or str
-            The user can specify periodicSupport either in count or proportion of database size.
-            If the program detects the data type of periodicSupport is integer, then it treats periodicSupport is expressed in count.
+        minSup: int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
-            Example: periodicSupport=10 will be treated as integer, while periodicSupport=10.0 will be treated as float
-        period: int or float or str
-            The user can specify period either in count or proportion of database size.
-            If the program detects the data type of period is integer, then it treats period is expressed in count.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
             Otherwise, it will be treated as float.
-            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
@@ -123,94 +113,99 @@
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
 
-    **Executing the code on terminal:**
-    -------------------------------------
+    **Methods to execute code on terminal**
+    --------------------------------------------
     .. code-block:: console
 
 
        Format:
 
-       (.venv) $ python3 PPPClose.py <inputFile> <outputFile> <periodicSupport> <period>
+       (.venv) $  python3 CPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer>
 
-       Examples:
+       Example:
 
-       (.venv) $ python3 PPPClose.py sampleTDB.txt patterns.txt 0.3 0.4
+       (.venv) $ python3 CPFPMiner.py sampleTDB.txt patterns.txt 0.3 0.4
 
-
-    **Sample run of the imported code:**
-    --------------------------------------
+        
+               .. note:: minSup will be considered in percentage of database transactions
+        
+        
+    **Importing this algorithm into a python program**
+    -------------------------------------------------------
     .. code-block:: python
-
-            from PAMI.partialPeriodicPattern.closed import PPPClose as alg
-
-            obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
-
-            obj.startMine()
-
-            periodicFrequentPatterns = obj.getPatterns()
-
-            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
-
-            obj.save("patterns")
-
-            Df = obj.getPatternsAsDataFrame()
-
-            memUSS = obj.getMemoryUSS()
-
-            print("Total Memory in USS:", memUSS)
-
-            memRSS = obj.getMemoryRSS()
-
-            print("Total Memory in RSS", memRSS)
-
-            run = obj.getRuntime()
-
-            print("Total ExecutionTime in seconds:", run)
-
+        
+                    from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
+        
+                    obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
+        
+                    obj.startMine()
+        
+                    periodicFrequentPatterns = obj.getPatterns()
+        
+                    print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+        
+                    obj.save("patterns")
+        
+                    Df = obj.getPatternsAsDataFrame()
+        
+                    memUSS = obj.getMemoryUSS()
+        
+                    print("Total Memory in USS:", memUSS)
+        
+                    memRSS = obj.getMemoryRSS()
+        
+                    print("Total Memory in RSS", memRSS)
+        
+                    run = obj.getRuntime()
+        
+                    print("Total ExecutionTime in seconds:", run)
+        
     **Credits:**
-    --------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
-
+    ------------------
+    The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
     """
 
-    _periodicSupport = float()
-    _period = float()
+    _minSup = float()
+    _maxPer = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
-    _Database = []
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _transaction = []
     _hashing = {}
     _mapSupport = {}
     _itemSetCount = 0
     _maxItemId = 0
     _tableSize = 10000
     _tidList = {}
     _lno = 0
 
+    def __init__(self, iFile, minSup, maxPer, sep='\t'):
+        super().__init__(iFile, minSup, maxPer, sep)
+        self._finalPatterns = {}
+    
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
@@ -222,83 +217,76 @@
             if '.' in value:
                 value = float(value)
                 value = (self._lno * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self):
+    def _scanDatabase(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        To scan the database and extracts the 1-length periodic-frequent items
+        :return:   Returns the 1-length periodic-frequent items
         """
-        self._Database = []
-        if isinstance(self._iFile, _abstract._pd.DataFrame):
-            timeStamp, data = [], []
+        Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
-                timeStamp = self._iFile['TS'].tolist()
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
-                tr = [timeStamp[i]]
+                tr = [ts[i][0]]
                 tr = tr + data[i]
-                self._Database.append(tr)
-            self._lno = len(self._Database)
+                Database.append(tr)
+
         if isinstance(self._iFile, str):
-            if _validators.url(self._iFile):
-                data = _urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    self._lno += 1
+                    line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            self._lno += 1
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-
-    def _OneLengthPartialItems(self):
-        """
-        To scan the database and extracts the 1-length periodic-frequent items
-
-        :return: Returns the 1-length periodic-frequent items
-        """
-        self._mapSupport = {}
         self._tidList = {}
-        self._period = self._convert(self._period)
-        for line in self._Database:
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
+        self._mapSupport = {}
+        for line in Database:
+            self._lno += 1
+            s = line
+            n = int(s[0])
+            for i in range(1, len(s)):
+                si = s[i]
                 if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, 0, n]
+                    self._mapSupport[si] = [1, abs(0 - n), n]
                     self._tidList[si] = [n]
                 else:
                     self._mapSupport[si][0] += 1
-                    period = abs(n - self._mapSupport[si][2])
-                    if period <= self._period:
-                        self._mapSupport[si][1] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
                     self._mapSupport[si][2] = n
                     self._tidList[si].append(n)
         for x, y in self._mapSupport.items():
-            period = abs(self._lno - self._mapSupport[x][2])
-            if period <= self._period:
-                self._mapSupport[x][1] += 1
-        self._periodicSupport = self._convert(self._periodicSupport)
-        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items() if v[1] >= self._periodicSupport}
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(self._lno - self._mapSupport[x][2]))
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if
+                           v[0] >= self._minSup and v[1] <= self._maxPer}
         periodicFrequentItems = {}
         self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
         for x, y in self._tidList.items():
             t1 = 0
             for i in y:
                 t1 += i
             periodicFrequentItems[x] = t1
@@ -320,91 +308,101 @@
         return hashcode % self._tableSize
 
     def _contains(self, itemSet, val, hashcode):
         """
         To check if the key(hashcode) is in dictionary(hashing) variable
 
         :param itemSet: generated periodic-frequent itemSet
-        :param val: support and period of itemSet
+        :param val: support and periodicity of itemSet
         :param hashcode: the key generated in calculate() method for every itemSet
 
         :return: true if itemSet with same support present in dictionary(hashing) or else returns false
         """
         if self._hashing.get(hashcode) is None:
             return False
         for i in self._hashing[hashcode]:
             itemSetX = i
-            if val == self._hashing[hashcode][itemSetX] and set(itemSetX).issuperset(itemSet):
+            if val[0] == self._hashing[hashcode][itemSetX][0] and set(itemSetX).issuperset(itemSet):
                 return True
         return False
 
-    def _getPeriodicSupport(self, timeStamps):
+    def _getPeriodAndSupport(self, timeStamps):
         """
-        Calculates the period and support of timeStamps
+        Calculates the periodicity and support of timeStamps
 
-        :param: timeStamps: timeStamps of itemSet
-        :return: period and support
+        :param timeStamps: timeStamps of itemSet
+        :return: periodicity and support
         """
         timeStamps.sort()
+        cur = 0
+        per = 0
         sup = 0
-        for j in range(len(timeStamps) - 1):
-            per = abs(timeStamps[j + 1] - timeStamps[j])
-            if per <= self._period:
-                sup += 1
-        return sup
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > self._maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+            sup += 1
+        per = max(per, self._lno - cur)
+        return [sup, per]
 
     def _save(self, prefix, suffix, tidSetX):
         """
         Saves the generated pattern which satisfies the closed property
+        Parameters:
+        -----------
+            prefix: the prefix part of itemSet
+            suffix: the suffix part of itemSet
+            tidSetX: the timeStamps of the generated itemSet
+
+        Returns:
+        --------
+            saves the closed periodic-frequent pattern
 
-        :param prefix: the prefix part of itemSet
-        :param suffix: the suffix part of itemSet
-        :param tidSetX: the timeStamps of the generated itemSet
-        :return: saves the closed periodic-frequent pattern
         """
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
         prefix = list(set(prefix))
         prefix.sort()
-        val = self._getPeriodicSupport(tidSetX)
-        if val >= self._periodicSupport:
+        val = self._getPeriodAndSupport(tidSetX)
+        if val[0] >= self._minSup and val[1] <= self._maxPer:
             hashcode = self._calculate(tidSetX)
             if self._contains(prefix, val, hashcode) is False:
                 self._itemSetCount += 1
                 sample = str()
                 for i in prefix:
-                    sample = sample + i + "\t"
+                    sample = sample + i + " "
                 self._finalPatterns[sample] = val
             if hashcode not in self._hashing:
                 self._hashing[hashcode] = {tuple(prefix): val}
             else:
                 self._hashing[hashcode][tuple(prefix)] = val
 
     def _processEquivalenceClass(self, prefix, itemSets, tidSets):
         """
 
         :param prefix: Prefix class of an itemSet
-        :param itemSets: suffix items in periodicFrequentItems that satisfies the periodicSupport condition
+        :param itemSets: suffix items in periodicFrequentItems that satisfies the minSup condition
         :param tidSets: timeStamps of items in itemSets respectively
-        :return: closed periodic patterns with length more than 2
+        :return:  closed periodic patterns with length more than 2
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidList = tidSets[0]
             self._save(prefix, [i], tidList)
             return
         if len(itemSets) == 2:
             itemI = itemSets[0]
             tidSetI = tidSets[0]
             itemJ = itemSets[1]
             tidSetJ = tidSets[1]
             y1 = list(set(tidSetI).intersection(tidSetJ))
-            if len(y1) >= self._periodicSupport:
+            if len(y1) >= self._minSup:
                 suffix = []
                 suffix += [itemI, itemJ]
                 suffix = list(set(suffix))
                 self._save(prefix, suffix, y1)
             if len(y1) != len(tidSetI):
                 self._save(prefix, [itemI], tidSetI)
             if len(y1) != len(tidSetJ):
@@ -420,15 +418,15 @@
             itemSetX = [itemX]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 if itemJ is None:
                     continue
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetX).intersection(tidSetJ))
-                if len(y) < self._periodicSupport:
+                if len(y) < self._minSup:
                     continue
                 if len(tidSetX) == len(tidSetJ) and len(y) == len(tidSetX):
                     itemSets.insert(j, None)
                     tidSets.insert(j, None)
                     itemSetX.append(itemJ)
                 elif len(tidSetX) < len(tidSetJ) and len(y) == len(tidSetX):
                     itemSetX.append(itemJ)
@@ -446,66 +444,109 @@
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._finalPatterns = {}
+        self._hashing = {}
+        periodicFrequentItems = self._scanDatabase()
+        for i in range(len(periodicFrequentItems)):
+            itemX = periodicFrequentItems[i]
+            if itemX is None:
+                continue
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(periodicFrequentItems)):
+                itemJ = periodicFrequentItems[j]
+                if itemJ is None:
+                    continue
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) < self._minSup:
+                    continue
+                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
+                    periodicFrequentItems.insert(j, None)
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
+                    itemSetX.append(itemJ)
+                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
+                    periodicFrequentItems.insert(j, None)
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+                else:
+
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            if len(itemSets) > 0:
+                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
+            self._save([], itemSetX, tidSetX)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
 
-    def mine(self):
+    def Mine(self):
         """
         Mining process will start from here
         """
-        self._startTime = _abstract._time.time()
-        self._creatingItemSets()
-        self._hashing = {}
+        self._startTime = _ab._time.time()
         self._finalPatterns = {}
-        periodicFrequentItems = self._OneLengthPartialItems()
+        self._hashing = {}
+        periodicFrequentItems = self._scanDatabase()
         for i in range(len(periodicFrequentItems)):
             itemX = periodicFrequentItems[i]
             if itemX is None:
                 continue
             tidSetX = self._tidList[itemX]
             itemSetX = [itemX]
             itemSets = []
             tidSets = []
             for j in range(i + 1, len(periodicFrequentItems)):
                 itemJ = periodicFrequentItems[j]
                 if itemJ is None:
                     continue
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._periodicSupport:
+                if len(y1) < self._minSup:
                     continue
                 if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
                     periodicFrequentItems.insert(j, None)
                     itemSetX.append(itemJ)
                 elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
                     itemSetX.append(itemJ)
                 elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
                     periodicFrequentItems.insert(j, None)
                     itemSets.append(itemJ)
                     tidSets.append(y1)
                 else:
+
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             if len(itemSets) > 0:
                 self._processEquivalenceClass(itemSetX, itemSets, tidSets)
             self._save([], itemSetX, tidSetX)
-        self._endTime = _abstract._time.time()
-        process = _abstract._psutil.Process(_abstract._os.getpid())
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
+        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
 
     def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
@@ -533,54 +574,57 @@
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
+            data.append([a, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+        """
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        print("Total number of  Closed Partial Periodic Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of Closed Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-
+        print("Total ExecutionTime in ms:", self.getRuntime())
+        
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
-        if len(_sys.argv) == 6:
-            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
-        if len(_sys.argv) == 5:
-            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of  Patterns:", len(_ap.getPatterns()))
-        _ap.save(_sys.argv[2])
+        print("Total number of Closed Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/closed/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py`

 * *Files 0% similar despite different names*

```diff
@@ -52,15 +52,14 @@
 
 """
 
 import sys as _sys
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 from PAMI.partialPeriodicPattern.maximal import abstract as _abstract
-import deprecated
 
 global maximalTree
 _periodicSupport = float()
 _period = float()
 _lno = int()
 
 
@@ -680,27 +679,19 @@
         :return: pattern with original item names
         """
         t1 = []
         for i in itemSet:
             t1.append(self._pfList[i])
         return t1
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from this function
         """
 
-        self.mine()
-
-    def mine(self):
-        """
-        Mining process will start from this function
-        """
-
         global _periodicSupport, _period, _lno
         self._startTime = _abstract._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._periodicSupport is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingitemSets()
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/maximal/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/maximal/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/pyspark/__init__.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/pyspark/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py`

 * *Files 8% similar despite different names*

```diff
@@ -51,14 +51,16 @@
 """
 
 from PAMI.partialPeriodicPattern.pyspark import abstract as _ab
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
 from pyspark import SparkContext, SparkConf
+
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
 _periodicSupport = float()
 _period = float()
 _lno = int()
 
@@ -478,17 +480,73 @@
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree.
         """
         
-        self.mine()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minPS is None:
+            raise Exception("Please enter the Minimum Period-Support")
+            
+        self._period = self._convert(self._period)
+        self._minPS = self._convert(self._minPS)
+        minPS = self._minPS
+        period = self._period
+
+        
+        APP_NAME = "4PGrowth"
+        conf = SparkConf().setAppName(APP_NAME)
+        sc  = SparkContext(conf=conf).getOrCreate()
+
+        self._startTime = _ab._time.time()
+        
+        data = sc.textFile(self._iFile,self.numPartitions).map(lambda x: [y for y in x.strip().split(self._sep)])
+        # self.numPartitions = data.getNumPartitions()
+        # numPartitions = 50
+        freqItems,RecItems = self.getFrequentItems(data)
+        # print(RecItems)
+
+        trans = self.getFrequentItemsets(data,freqItems,self._period,self._minPS, dict(RecItems))
+        a = trans.collect()
+        
+        # print(type(a))
+        for k,v in a:
+            string = "\t".join(k)
+            # print(string,":",v)
+            self._finalPatterns[string] = v
+
+        # print(self._finalPatterns)
+        #     print(k,":",v)
+        # trans.saveAsTextFile('temp')
+        self._endTime = _ab._time.time()
+        sc.stop()
+        
+        # self._creatingItemSets()
+        # generatedItems, pfList = self._partialPeriodicOneItem()
+        # _minPS, _period, _lno = self._minPS, self._period, len(self._Database)
+        # updatedTransactions = self._updateTransactions(generatedItems)
+        # for x, y in self._rank.items():
+        #     self._rankdup[y] = x
+        # info = {self._rank[k]: v for k, v in generatedItems.items()}
+        # Tree = self._buildTree(updatedTransactions, info)
+        # patterns = Tree._generatePatterns([])
+        # self._finalPatterns = {}
+        # for i in patterns:
+        #     s = self._savePeriodic(i[0])
+        #     self._finalPatterns[s] = i[1]
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Partial Periodic Patterns were generated successfully using 4PGrowth algorithm ")
 
-    def mine(self):
+    def Mine(self):
         """
         Main method where the patterns are mined by constructing tree.
         """
 
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minPS is None:
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/topk/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-# k3PMiner is and algorithm to discover top - k partial periodic patterns in a temporal  database.
-#
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
-
 #
-#             import PAMI.partialPeriodicPattern.topk.k3PMiner as alg
 #
-#             obj = alg.k3PMiner(iFile, k, periodicity)
+#             import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+#
+#             obj = alg.TopkPFPGrowth(iFile, k, maxPer,oFile)
 #
 #             obj.startMine()
 #
-#             partialPeriodicPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of top partial periodic Patterns:", len(partialPeriodicPatterns))
+#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -26,14 +23,15 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
+#
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -48,119 +46,110 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.partialPeriodicPattern.topk import abstract as _abstract
-import validators as _validators
-from urllib.request import urlopen as _urlopen
-import sys as _sys
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-class k3PMiner(_abstract.partialPeriodicPatterns):
+
+class TopkPFPGrowth(_ab._periodicFrequentPatterns):
     """
-    :Description:   k3PMiner is and algorithm to discover top - k partial periodic patterns in a temporal  database.
+    :Description:   Top - K is and algorithm to discover top periodic frequent patterns in a temporal database.
 
-    :Reference:  Palla Likhitha,Rage Uday Kiran, Discovering Top-K Partial Periodic Patterns in Big Temporal Databases https://dl.acm.org/doi/10.1007/978-3-031-39847-6_28
+    :Reference:   Komate Amphawan, Philippe Lenca, Athasit Surarerks: "Mining Top-K Periodic-Frequent Pattern from Transactional Databases without Support Threshold"
+                  International Conference on Advances in Information Technology: https://link.springer.com/chapter/10.1007/978-3-642-10392-6_3
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of periodic frequent pattern's
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  period: str:
-                   Minimum partial periodic...
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-            iFile : str
-                Input file name or path of the input file
-            k: int
-                User specified count of top partial periodic patterns
-            sep : str
-                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-                However, the users can override their default separator.
-            oFile : str
-                Name of the output file or the path of the output file
-            startTime:float
-                To record the start time of the mining process
-            endTime:float
-                To record the completion time of the mining process
-            finalPatterns: dict
-                Storing the complete set of patterns in a dictionary variable
-            memoryUSS : float
-                To store the total amount of USS memory consumed by the program
-            memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
+        iFile : str
+            Input file name or path of the input file
+        k: int
+            User specified counte of top frequent patterns
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+        oFile : str
+            Name of the output file or the path of the output file
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
 
     :Methods:
 
-            startMine()
-                Mining process will start from here
-            getPatterns()
-                Complete set of patterns will be retrieved with this function
-            save(oFile)
-                Complete set of frequent patterns will be loaded in to a output file
-            getPatternsAsDataFrame()
-                Complete set of frequent patterns will be loaded in to a dataframe
-            getMemoryUSS()
-                Total amount of USS memory consumed by the mining process will be retrieved from this function
-            getMemoryRSS()
-                Total amount of RSS memory consumed by the mining process will be retrieved from this function
-            getRuntime()
-                Total amount of runtime taken by the mining process will be retrieved from this function
-            creatingItemSets()
-                Scans the dataset or dataframes and stores in list format
-            frequentOneItem()
-                Generates one frequent patterns
-            eclatGeneration(candidateList)
-                It will generate the combinations of frequent items
-            generateFrequentPatterns(tidList)
-                It will generate the combinations of frequent items from a list of items
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Generates one frequent patterns
+        eclatGeneration(candidateList)
+            It will generate the combinations of frequent items
+        generateFrequentPatterns(tidList)
+            It will generate the combinations of frequent items from a list of items
 
     **Executing the code on terminal:**
     -------------------------------------
-     .. code-block:: console
+   .. code-block:: console
 
 
        Format:
 
-       python3 k3PMiner.py <iFile> <oFile> <k> <period>
+       (.venv) $ python3 TopkPFP.py <inputFile> <outputFile> <k> <maxPer>
 
        Examples:
 
-       python3 k3PMiner.py sampleDB.txt patterns.txt 10 3
+       (.venv) $ python3 TopkPFP.py sampleDB.txt patterns.txt 10 3
 
 
     **Sample run of the importing code:**
-    --------------------------------------
-    ...     code-block:: python
+    ---------------------------------------
+    .. code-block:: python
 
-            import PAMI.partialPeriodicPattern.topk.k3PMiner as alg
+            import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
 
-            obj = alg.Topk_PPPGrowth(iFile, k, period)
+            obj = alg.TopkPFPGrowth(iFile, k, maxPer)
 
             obj.startMine()
 
-            partialPeriodicPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of top partial periodic Patterns:", len(partialPeriodicPatterns))
+            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -171,23 +160,23 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
+    --------------
             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _k = int()
-    _period = " "
+    _maxPer = " "
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
@@ -195,45 +184,46 @@
     _lno = int()
     _minimum = int()
     _mapSupport = {}
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
+
         """
         self._Database = []
-        if isinstance(self._iFile, _abstract._pd.DataFrame):
-            timeStamp, data = [], []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
-                timeStamp = self._iFile['TS'].tolist()
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
+            if 'Patterns' in i:
+                data = self._iFile['Patterns'].tolist()
             for i in range(len(data)):
-                tr = [timeStamp[i]]
+                tr = [ts[i][0]]
                 tr = tr + data[i]
                 self._Database.append(tr)
-            self._lno = len(self._Database)
-            # print(self.Database)
         if isinstance(self._iFile, str):
-            if _validators.url(self._iFile):
-                data = _urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    self._lno += 1
+                    line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            self._lno += 1
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
@@ -259,67 +249,65 @@
     def _frequentOneItem(self):
         """
         Generating one frequent patterns
         """
 
         self._mapSupport = {}
         self._tidList = {}
-        self._period = self._convert(self._period)
-        self._k = int(self._convert(self._k))
+        n = 0
         for line in self._Database:
+            self._lno += 1
             n = int(line[0])
             for i in range(1, len(line)):
                 si = line[i]
                 if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, 0, n]
+                    self._mapSupport[si] = [1, abs(0 - n), n]
                     self._tidList[si] = [n]
                 else:
                     self._mapSupport[si][0] += 1
-                    period = abs(n - self._mapSupport[si][2])
-                    if period <= self._period:
-                        self._mapSupport[si][1] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
                     self._mapSupport[si][2] = n
                     self._tidList[si].append(n)
         for x, y in self._mapSupport.items():
-            period = abs(self._lno - self._mapSupport[x][2])
-            if period <= self._period:
-                self._mapSupport[x][1] += 1
-        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items()}
-        #print(self._mapSupport)
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        #print(plist)
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
+        self._maxPer = self._convert(self._maxPer)
+        self._k = self._convert(self._k)
+        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if v[1] <= self._maxPer}
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
         self._finalPatterns = {}
+        #print(len(plist))
         for i in plist:
-            if self._mapSupport[i] == 0:
-                continue
             if len(self._finalPatterns) >= self._k:
                 break
             else:
-                self._finalPatterns[i] = self._mapSupport[i]
-        #print(len(self._finalPatterns),  self._k, self._periodicity)
-        #print(self._finalPatterns)
-        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-        #print(self._minimum)
+                self._finalPatterns[i] = [self._mapSupport[i][0], self._mapSupport[i][1]]
+        self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
         plist = list(self._finalPatterns.keys())
         return plist
 
     def _getSupportAndPeriod(self, timeStamps):
         """To calculate the periodicity and support
 
         :param timeStamps: Timestamps of an item set
         :return: support, periodicity
         """
 
+        global lno
         timeStamps.sort()
+        cur = 0
+        per = list()
         sup = 0
-        for j in range(len(timeStamps) - 1):
-            per = abs(timeStamps[j + 1] - timeStamps[j])
-            if per <= self._period:
-                sup += 1
-        return sup
+        for j in range(len(timeStamps)):
+            per.append(timeStamps[j] - cur)
+            cur = timeStamps[j]
+            sup += 1
+        per.append(self._lno - cur)
+        if len(per) == 0:
+            return [0, 0]
+        return [sup, max(per)]
 
     def _save(self, prefix, suffix, tidSetI):
         """Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
         :type prefix: list
         :param suffix: the suffix of a patterns
@@ -328,47 +316,42 @@
         :type tidSetI: list
         """
 
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        #print(prefix)
-        #print(self._minimum)
         val = self._getSupportAndPeriod(tidSetI)
         sample = str()
         for i in prefix:
-            sample = sample + i + "\t"
+            sample = sample + i + " "
         if len(self._finalPatterns) < self._k:
-            if val > self._minimum:
+            if val[0] >= self._minimum:
                 self._finalPatterns[sample] = val
                 self._finalPatterns = {k: v for k, v in
-                                       sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-                #print(self._finalPatterns)
+                                  sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
         else:
-            #print(prefix, val)
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
-                if val > y:
-                    #print("yes")
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1][0]):
+                if val[0] > y[0]:
                     del self._finalPatterns[x]
-                    self._finalPatterns[sample] = val
+                    self._finalPatterns[x] = y
                     self._finalPatterns = {k: v for k, v in
-                                           sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                    self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-                    #print(self._finalPatterns)
+                                          sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                    self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
                     return
 
     def _Generation(self, prefix, itemSets, tidSets):
-        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
 
         :param prefix:  main equivalence prefix
         :type prefix: periodic-frequent item or pattern
         :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
-                        and frequent with their timestamps
+                            and frequent with their timestamps
         :type itemSets: list
         :param tidSets: timestamps of the items in the argument itemSets
         :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidI = tidSets[0]
@@ -383,63 +366,89 @@
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetI).intersection(tidSetJ))
                 val = self._getSupportAndPeriod(y)
-                if val > self._minimum:
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
             newPrefix = list(set(itemSetX)) + prefix
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetI)
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main function of the program
-
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        _plist = self._frequentOneItem()
+        for i in range(len(_plist)):
+            itemI = _plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(_plist)):
+                itemJ = _plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y1)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("TopK Periodic Frequent patterns were generated successfully")
+        self._endTime = _ab._time.time()
+        _process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
 
-    def mine(self):
-            """
-            Main function of the program
-
-            """
-            self._startTime = _abstract._time.time()
-            if self._iFile is None:
-                raise Exception("Please enter the file path or file name:")
-            if self._k is None:
-                raise Exception("Please enter the Minimum Support")
-            self._creatingItemSets()
-            plist = self._frequentOneItem()
-            for i in range(len(plist)):
-                itemI = plist[i]
-                tidSetI = self._tidList[itemI]
-                itemSetX = [itemI]
-                itemSets = []
-                tidSets = []
-                for j in range(i + 1, len(plist)):
-                    itemJ = plist[j]
-                    tidSetJ = self._tidList[itemJ]
-                    y1 = list(set(tidSetI).intersection(tidSetJ))
-                    val = self._getSupportAndPeriod(y1)
-                    if val > self._minimum:
-                        itemSets.append(itemJ)
-                        tidSets.append(y1)
-                self._Generation(itemSetX, itemSets, tidSets)
-            print("TopK partial periodic patterns were generated successfully")
-            self._endTime = _abstract._time.time()
-            process = _abstract._psutil.Process(_abstract._os.getpid())
-            self._memoryUSS = float()
-            self._memoryRSS = float()
-            self._memoryUSS = process.memory_full_info().uss
-            self._memoryRSS = process.memory_info().rss
+    def Mine(self):
+        """
+        Main function of the program
+        """
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        _plist = self._frequentOneItem()
+        for i in range(len(_plist)):
+            itemI = _plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(_plist)):
+                itemJ = _plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y1)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("TopK Periodic Frequent patterns were generated successfully")
+        self._endTime = _ab._time.time()
+        _process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -461,64 +470,63 @@
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to an output file
+        """Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
+            patternsAndSupport = x.replace(' ', '\t') + ":" + f'{y[0]}:{y[1]}'
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        """ This function is used to print the results
-        """
-        print("Top K Partial Periodic Patterns:", len(self.getPatterns()))
+        print("Top K Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
-        if len(_sys.argv) == 6:
-            _ap = k3PMiner(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
-        if len(_sys.argv) == 5:
-            _ap = k3PMiner(_sys.argv[1], _sys.argv[3], _sys.argv[4])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Top K Partial Periodic Patterns:", len(_ap.getPatterns()))
-        _ap.save(_sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Top K Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 # PPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
+# --------------------------------------------------------
+#
 #
 #     from PAMI.periodicFrequentPattern.basic import PPGrowth as alg
 #
 #     obj = alg.PPGrowth(iFile, minSup, maxPer)
 #
 #     obj.startMine()
 #
@@ -21,16 +23,14 @@
 #     print("Total Memory in USS:", memUSS)
 #
 #     memRSS = obj.getMemoryRSS()
 #
 #     print("Total Memory in RSS", memRSS)
 #
 #     run = obj.getRuntime()
-#
-#     print("Total ExecutionTime in seconds:", run)
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -40,15 +40,15 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     
+     Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 import pandas as pd
 from deprecated import deprecated
 from PAMI.partialPeriodicPatternInMultipleTimeSeries import abstract as _ab
 
@@ -308,17 +308,14 @@
                 for q in conditionalTree.generatePatterns(pattern):
                     yield q
             self.removeNode(i)
 
 
 class PPGrowth(_ab._partialPeriodicPatterns):
     """
-    About this algorithm
-    ====================
-
     :Description:   PPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 
     :Reference:   C. Saideep, R. Uday Kiran, K. Zettsu, P. Fournier-Viger, M. Kitsuregawa and P. Krishna Reddy,
                  "Discovering Periodic Patterns in Irregular Time Series," 2019 International Conference on Data Mining Workshops (ICDMW), 2019,
                   pp. 1020-1028, doi: 10.1109/ICDMW.2019.00147.
 
     :param  iFile: str :
@@ -392,23 +389,19 @@
         updateDatabases()
             Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
 
-    Execution methods
-    =================
-
-
-    **Terminal command**
-
-
+    **Executing the code on terminal:**
+    -------------------------------------
     .. code-block:: console
 
+
        Format:
 
        (.venv) $ python3 PPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
 
        Examples:
 
        (.venv) $  python3 PPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
@@ -440,17 +433,15 @@
 
         run = obj.getRuntime()
 
         print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
     --------------
-
-
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
 
     """
     _startTime = float()
     _endTime = float()
     _periodicSupport = str()
     _period = float()
     _finalPatterns = {}
@@ -500,14 +491,15 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
+
     def _periodicFrequentOneItem(self):
         """
         Calculates the support of each item in the database and assign ranks to the items
         by decreasing support and returns the frequent items list
 
         :returns: return the one-length periodic frequent patterns
         """
```

### Comparing `pami-2024.4.24.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,22 +1,7 @@
-#  Copyright (C)  2021 Rage Uday Kiran
-#
-#      This program is free software: you can redistribute it and/or modify
-#      it under the terms of the GNU General Public License as published by
-#      the Free Software Foundation, either version 3 of the License, or
-#      (at your option) any later version.
-#
-#      This program is distributed in the hope that it will be useful,
-#      but WITHOUT ANY WARRANTY; without even the implied warranty of
-#      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-#      GNU General Public License for more details.
-#
-#      You should have received a copy of the GNU General Public License
-#      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
 from abc import ABC as _ABC, abstractmethod as _abstractmethod
 import time as _time
 import math as _math
 import csv as _csv
 import pandas as _pd
 from collections import defaultdict as _defaultdict
 from itertools import combinations as _combinations
@@ -26,17 +11,14 @@
 import sys as _sys
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 
 
 class _partialPeriodicPatterns(_ABC):
     """
-    About this algorithm
-    ====================
-
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
@@ -97,15 +79,14 @@
     def iFile(self):
         """Variable to store the input file path/file name"""
         pass
     @abstractmethod
     def periodicSupport(self):
         """Variable to store the user-specified minimum support value"""
         pass
-    @abstractmethod
     def period(self):
         """Variable to store the user specified maximum periodicity value"""
         pass
     @abstractmethod
     def sep(self):
         """Variable to store the input file path/file name"""
         pass
@@ -177,10 +158,10 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
-        """ To print the results of execution"""
+        """ To print all the results of execution"""
 
         pass
```

### Comparing `pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -642,62 +642,57 @@
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Correlated Periodic-Frequent patterns were generated successfully using EPCPGrowth algorithm ")
 
     def getMemoryUSS(self) -> float:
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
 
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
-        """
-        Storing final periodic-frequent patterns in a dataframe
+        """Storing final periodic-frequent patterns in a dataframe
 
         :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1], b[2], b[3]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity', 'allConf', 'maxPerAllConf'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
-        """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
+        """Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
```

### Comparing `pami-2024.4.24.1/PAMI/periodicCorrelatedPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,17 +26,14 @@
 import sys as _sys
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 
 
 class _periodicCorrelatedPatterns(_ABC):
     """
-    About this algorithm
-    ====================
-
     :Description:   This abstract base class defines the variables and methods that every periodic-frequent pattern mining algorithm must
         employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPMC.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,16 @@
-# PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
-#
+# PFPMC is the fundamental approach to mine the periodic-frequent patterns.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
+#             from PAMI.periodicFrequentPattern.basic import PFPMC as alg
 #
-#             obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
+#             obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
 #
 #             obj.startMine()
 #
 #             periodicFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
@@ -30,15 +29,14 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -53,25 +51,25 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
-import numpy as np
-
+from itertools import groupby as _groupby
+from operator import itemgetter as _itemgetter
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
 
-class PFECLAT(_ab._periodicFrequentPatterns):
+class PFPMC(_ab._periodicFrequentPatterns):
     """
-    :Description:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
+    :Description:   PFPMC is the fundamental approach to mine the periodic-frequent patterns.
 
-    :Reference:   P. Ravikumar, P.Likhitha, R. Uday kiran, Y. Watanobe, and Koji Zettsu, "Towards efficient discovery of
-                  periodic-frequent patterns in columnar temporal databases", 2021 IEA/AIE.
+    :Reference: (has to be added)
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of periodic frequent pattern's
     :param  minSup: str:
                    Controls the minimum number of transactions in which every item must appear in a database.
@@ -99,17 +97,17 @@
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
             it represents the total no of transactions
@@ -127,56 +125,54 @@
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
+            Complete set of periodic-frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingOneItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent 
+            Scan the database and store the items with their timestamps which are periodic frequent
         getPeriodAndSupport()
             Calculates the support and period for a list of timestamps.
         Generation()
             Used to implement prefix class equivalence method to generate the periodic patterns recursively
-            
+
 
     **Methods to execute code on terminal**
     ------------------------------------------
     .. code-block:: console
 
 
        Format:
 
-       (.venv) $ python3 PFECLAT.py <inputFile> <outputFile> <minSup>
+       (.venv) $ python3 PFPMC.py <inputFile> <outputFile> <minSup> <maxPer>
 
        Example usage:
 
-       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
-
-
+       (.venv) $ python3 PFPMC.py sampleDB.txt patterns.txt 10.0 4.0
 
-               .. note:: minSup will be considered in percentage of database transactions
 
+               .. note:: minSup and maxPer will be considered in percentage of database transactions
 
     **Importing this algorithm into a python program**
-    --------------------------------------------------------
+    ----------------------------------------------------
     .. code-block:: python
 
-             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
+                from PAMI.periodicFrequentPattern.basic import PFPMC as alg
 
-                obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
+                obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
 
                 obj.startMine()
 
                 periodicFrequentPatterns = obj.getPatterns()
 
                 print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
@@ -193,33 +189,96 @@
                 print("Total Memory in RSS", memRSS)
 
                 run = obj.getRuntime()
 
                 print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+    ----------------
+
+    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
-    
+
     _iFile = " "
     _oFile = " "
     _sep = " "
     _dbSize = None
     _Database = None
     _minSup = str()
     _maxPer = str()
     _tidSet = set()
     _finalPatterns = {}
     _startTime = None
     _endTime = None
+    _lastTid = int()
     _memoryUSS = float()
     _memoryRSS = float()
 
+    def _getPeriodic(self, tids: set) -> int:
+        """
+        To get  Periodic frequent patterns
+
+        :param tids: represents the timestamp of a transaction
+        :type tids: set
+        :return: None
+        """
+        tids = list(tids)
+        tids.sort()
+        temp = self._maxPer + 1
+        diffs = []
+        if self._lastTid in tids:
+            tids.remove(self._lastTid)
+        for k, g in _groupby(enumerate(tids), lambda ix: ix[0] - ix[1]):
+            diffs.append(len(list(map(_itemgetter(1), g))))
+        if len(diffs) < 1:
+            return temp
+        return max(diffs) + 1
+
+    def _getPeriodic(self, tids: set):
+
+        tids = list(tids)
+        tids.sort()
+        temp = self._maxPer + 1
+        if self._lastTid in tids:
+            tids.remove(self._lastTid)
+        diffs = []
+        # find the longest consecutive period
+
+        count = 0
+        for i in range(len(tids) - 1):
+            if tids[i + 1] == tids[i] + 1:
+                count += 1
+            else:
+                diffs.append(count)
+                count = 0
+        if len(diffs) < 1:
+            return temp
+        return max(diffs) + 1
+
+    def _getPeriodic(self, tids: set):
+        tids = list(tids)
+        tids.sort()
+        temp = self._maxPer + 1
+        if self._lastTid in tids:
+            tids.remove(self._lastTid)
+        diffs = []
+        tempPer = 0
+        period = 0
+        for i in range(len(tids) - 1):
+            if tids[i+1] - tids[i] == 1:
+                tempPer += 1
+            else:
+                period = max(period, tempPer + 1)
+                if period > self._maxPer:
+                    return temp
+                tempPer = 0
+
+        return period
+
     def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
@@ -231,172 +290,145 @@
             if '.' in value:
                 value = float(value)
                 value = (self._dbSize * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self) -> None:
+    def _creatingOneItemSets(self) -> list:
         """
-            Storing the complete transactions of the database/input file in a database variable
-        :return: None
+        Storing the complete transactions of the database/input file in a database variable
+        :return: list
         """
-        self._Database = []
+        plist = []
+        Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+            ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
                 tr = [ts[i][0]]
                 tr = tr + data[i]
-                self._Database.append(tr)
-
+                Database.append(tr)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
+        tid = 0
+        itemsets = {}  # {key: item, value: list of tids}
+        periodicHelper = {}  # {key: item, value: [period, last_tid]}
+        for line in Database:
+            tid = int(line[0])
+            self._tidSet.add(tid)
+            for item in line[1:]:
+                if item in itemsets:
+                    itemsets[item].add(tid)
+                else:
+                    itemsets[item] = {tid}
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self) -> None:
-        """
-        Mining process will start from this function
-        :return: None
-        """
-        self.Mine()
-        # self._startTime = _ab._time.time()
-        # self._finalPatterns = {}
-        # frequentSets = self._creatingOneItemSets()
-        # self._generateEclat(frequentSets)
-        # self._endTime = _ab._time.time()
-        # process = _ab._psutil.Process(_ab._os.getpid())
-        # self._memoryRSS = float()
-        # self._memoryUSS = float()
-        # self._memoryUSS = process.memory_full_info().uss
-        # self._memoryRSS = process.memory_info().rss
-        # print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
-
-    def _getMaxPer(self, arr, maxTS):
-        arr = np.append(list(arr), [0, maxTS])
-        arr = np.sort(arr)
-        arr = np.diff(arr)
+        self._dbSize = len(Database)
+        self._lastTid = max(self._tidSet)
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        del Database
+        candidates = []
+        for item, tids in itemsets.items():
+            diff = self._tidSet.difference(tids)
+            per = self._getPeriodic(diff)
+            sup = len(tids)
+            if sup >= self._minSup and per <= self._maxPer:
+                candidates.append(item)
+                self._finalPatterns[item] = [sup, per, diff]
+        return candidates
+
+    def _generateDiffsetEclat(self, candidates: list) -> None:
+        new_freqList = []
+        for i in range(0, len(candidates)):
+            item1 = candidates[i]
+            i1_list = item1.split()
+            for j in range(i + 1, len(candidates)):
+                item2 = candidates[j]
+                i2_list = item2.split()
+                if i1_list[:-1] == i2_list[:-1]:
+                    union_DiffSet = self._finalPatterns[item2][2].union(self._finalPatterns[item1][2])
+                    sorted(union_DiffSet)
+                    union_supp = self._dbSize - len(union_DiffSet)
+                    period = self._getPeriodic(union_DiffSet)
+                    if union_supp >= self._minSup and period <= self._maxPer:
+                        newKey = item1 + "\t" + i2_list[-1]
+                        self._finalPatterns[newKey] = [union_supp, period, union_DiffSet]
+                        new_freqList.append(newKey)
+                else:
+                    break
 
-        return np.max(arr)
+        if len(new_freqList) > 0:
+            self._generateDiffsetEclat(new_freqList)
 
-    def Mine(self) -> None:
+    def startMine(self) -> None:
         """
         Mining process will start from this function
         :return: None
         """
+        # print(f"Optimized {type(self).__name__}")
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
-        frequentSets = self._creatingItemSets()
-
-        items = {}
-        maxTS = 0
-        for line in self._Database:
-            index = int(line[0])
-            maxTS = max(maxTS, index)
-            for item in line[1:]:
-                if tuple([item]) not in items:
-                    items[tuple([item])] = set()
-                items[tuple([item])].add(index)
-
-        self._dbSize = maxTS
-
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        minSup = self._minSup
-        maxPer = self._maxPer
-
-
-        items = {k: v for k, v in items.items() if len(v) >= minSup}
-        items = {k: v for k, v in sorted(items.items(), key = lambda x: len(x[1]), reverse = True)}
-
-        keys = []
-        for item in list(items.keys()):
-            per = self._getMaxPer(items[item], maxTS)
-            if per <= maxPer:
-                keys.append(item)
-                self._finalPatterns[item] = [len(items[item]), per, set(items[item])]
-
-        while keys:
-            newKeys = []
-            for i in range(len(keys)):
-                for j in range(i + 1, len(keys)):
-                    if keys[i][:-1] == keys[j][:-1] and keys[i][-1] != keys[j][-1]:
-                        # print(keys[i], keys[j])
-                        newKey = tuple(keys[i] + (keys[j][-1],))
-                        intersect = items[keys[i]].intersection(items[keys[j]])
-                        per = self._getMaxPer(intersect, maxTS)
-                        sup = len(intersect)
-                        if sup >= minSup and per <= maxPer:
-                            items[newKey] = intersect
-                            newKeys.append(newKey)
-                            self._finalPatterns[newKey] = [sup, per, set(intersect)]
-                    else:
-                        break
-            keys = newKeys
-
-        newPattern = {}
-        for k, v in self._finalPatterns.items():
-            newPattern["\t".join([str(x) for x in k])] = v
-
-        self._finalPatterns = newPattern
-
-        # self._generateEclat(frequentSets)
+        frequentSets = self._creatingOneItemSets()
+        self._generateDiffsetEclat(frequentSets)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
         self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+        print("Periodic-Frequent patterns were generated successfully using PFPDiffset ECLAT algorithm ")
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
-
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
@@ -413,15 +445,15 @@
         for a, b in self._finalPatterns.items():
             data.append([a, b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of periodic-frequent patterns will be loaded in to a output file
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
@@ -444,23 +476,23 @@
         This function is used to print the results
         :return: None
         """
         print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
-                    
+
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeP.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,587 +1,486 @@
-# PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
+# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#             from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
+#     obj = alg.TUFP(iFile, minSup)
 #
-#             obj = alg.PFPGrowth(iFile, minSup, maxPer)
+#     obj.startMine()
 #
-#             obj.startMine()
+#     frequentPatterns = obj.getPatterns()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#     obj.save(oFile)
 #
-#             obj.save(oFile)
+#     Df = obj.getPatternsAsDataFrame()
 #
-#             Df = obj.getPatternsAsDataFrame()
+#     memUSS = obj.getMemoryUSS()
 #
-#             memUSS = obj.getMemoryUSS()
+#     print("Total Memory in USS:", memUSS)
 #
-#             print("Total Memory in USS:", memUSS)
+#     memRSS = obj.getMemoryRSS()
 #
-#             memRSS = obj.getMemoryRSS()
+#     print("Total Memory in RSS", memRSS)
 #
-#             print("Total Memory in RSS", memRSS)
+#     run = obj.getRuntime()
 #
-#             run = obj.getRuntime()
-#
-#             print("Total ExecutionTime in seconds:", run)
-#
-
-
+#     print("Total ExecutionTime in seconds:", run)
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Union
 import pandas as pd
-from deprecated import deprecated
-import numpy as np
 
-_maxPer = float()
 _minSup = float()
-_lno = int()
+_finalPatterns = {}
 
 
-class _Node(object):
+class _Item:
     """
-    A class used to represent the node of frequentPatternTree
-
+    A class used to represent the item with probability in transaction of dataset
     :Attributes:
-
-        item : int or None
-            Storing item of a node
-        timeStamps : list
-            To maintain the timestamps of a database at the end of the branch
-        parent : node
-            To maintain the parent of every node
-        children : list
-            To maintain the children of a node
-
-    :Methods:
-
-        addChild(itemName)
-            Storing the children to their respective parent nodes
-        """
-
-    def __init__(self, item, locations, parent=None):
-        self.item = item
-        self.locations = locations
-        self.parent = parent
-        self.children = {}
-
-    def addChild(self, item, locations):
-        if item not in self.children:
-            self.children[item] = _Node(item, locations, self)
-        else:
-            self.children[item].locations = locations + self.children[item].locations
-            
-        return self.children[item]
-
-    def traverse(self):
-        transaction = []
-        locs = self.locations
-        node = self.parent
-        while node.parent is not None:
-            transaction.append(node.item)
-            node = node.parent
-        return transaction[::-1], locs
-
-    def traverse(self):
-        transaction = []
-        locs = self.locations
-        node = self.parent
-        while node.parent is not None:
-            transaction.append(node.item)
-            node = node.parent
-        return transaction[::-1], locs
-
-class PFPGrowth(_ab._periodicFrequentPatterns):
+        item : int or word
+            Represents the name of the item
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
     """
-    :Description:   PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 
-    :Reference:   Syed Khairuzzaman Tanbeer, Chowdhury Farhan, Byeong-Soo Jeong, and Young-Koo Lee, "Discovering Periodic-Frequent
-                   Patterns in Transactional Databases", PAKDD 2009, https://doi.org/10.1007/978-3-642-01307-2_24
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-    :param  minSup: str:
-                   Controls the minimum number of transactions in which every item must appear in a database.
-    :param  maxPer: float:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    def __init__(self, item, probability) -> None:
+        self.item = item
+        self.probability = probability
 
 
+class TUFP(_ab._frequentPatterns):
+    """
+    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+    :Reference:
+        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
+        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
     :Attributes:
-
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
-        minSup : int or float or str
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
             To represent the total no of transaction
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
-
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        storePatternsInFile(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsInDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets(fileName)
             Scans the dataset and stores in a list format
-        PeriodicFrequentOneItem()
-            Extracts the one-periodic-frequent patterns from database
-        updateDatabases()
-            Update the database by removing aperiodic items and sort the Database by item decreased support
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
-
-
-
+        startMine()
+            Mining process will start from this function
+    **Methods to execute code on terminal**
+    -----------------------------------------
+            Format:
+                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
+            Example:
+                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
+                      .. note:: minSup  will be considered in support count or frequency
+    **Importing this algorithm into a python program**
+    ------------------------------------------------------
+    .. code-block:: python
+            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+            obj = alg.TUFP(iFile, minSup)
+            obj.startMine()
+            frequentPatterns = obj.getPatterns()
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            obj.save(oFile)
+            Df = obj.getPatternsAsDataFrame()
+            memUSS = obj.getmemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = obj.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = obj.getRuntime()
+            print("Total ExecutionTime in seconds:", run)
     **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    ---------------
+             The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
-    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _rank = {}
-    _rankedUp = {}
-    _lno = 0
+    _cupList = {}
+    _topk = {}
+    _minimum = 9999
 
     def _creatingItemSets(self) -> None:
         """
-            Storing the complete transactions of the database/input file in a database variable
-        :return: None
+        Scans the dataset
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
                 self._Database.append(tr)
 
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _convert(self, value) -> int:
+    def _frequentOneItem(self) -> List[str]:
+        """
+        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
         """
-        To convert the given user specified value
 
-        :param value: user specified value
-        :return: converted value
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = j.probability
+                    self._cupList[j.item] = {k:j.probability}
+                else:
+                    mapSupport[j.item] += j.probability
+                    self._cupList[j.item].update({k: j.probability})
+        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        k = 0
+        for x, in plist:
+            k +=1
+            if k >= self._minSup:
+                break
+            self._finalPatterns[x] = mapSupport[x]
+        self._minimum = min(list(self._finalPatterns.values()))
+        return plist
+
+    @staticmethod
+    def _convert(value: Union[int, float, str]) -> Union[int, float]:
+        """
+        To convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self) -> None:
+    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
         """
-        Mining process will start from this function
-        :return: None
+        Saves the patterns that satisfy the periodic frequent property.
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
         """
 
-        self.Mine()
-
-    def _getMaxPer(self, arr, maxTS):
-        arr = np.append(arr, [0, maxTS])
-        arr = np.sort(arr)
-        arr = np.diff(arr)
-
-        return np.max(arr)
-
-    def _construct(self, items, data, minSup, maxPer, maxTS, patterns):
-
-        # maxPerItems = {k: self.getMaxPer(v, maxTS) for k, v in items.items() if len(v) >= minSup}
-
-        items = {k: v for k, v in items.items() if len(v) >= minSup and self._getMaxPer(v, maxTS) <= maxPer}
-
-        #tested ok
-        for item, ts in items.items():
-            # pat = "\t".join(item)
-            # self.patCount += 1
-            # patterns[pat] = (len(ts), self.getMaxPer(ts, maxTS))
-            patterns[tuple([item])] = [len(ts), self._getMaxPer(ts, maxTS)]
-
-        root = _Node([], None, None)
-        itemNodes = {}
-        for line in data:
-            currNode = root
-            index = int(line[0])
-            line = line[1:]
-            line = sorted([item for item in line if item in items], key = lambda x: len(items[x]), reverse = True)
-            for item in line:
-                currNode = currNode.addChild(item, [index])   # heavy
-                if item in itemNodes:
-                    itemNodes[item].add(currNode)
-                else:
-                    itemNodes[item] = set([currNode])
-
-        return root, itemNodes
-
-
-    def _recursive(self, root, itemNode, minSup, maxPer, patterns, maxTS):
-
-        for item in itemNode:
-            newRoot = _Node(root.item + [item], None, None)
-
-            itemLocs = {}
-            transactions = {}
-            for node in itemNode[item]:
-                transaction, locs = node.traverse()
-                if len(transaction) < 1:
-                    continue
-                # transactions.append((transaction, locs))
-                if tuple(transaction) in transactions:
-                    transactions[tuple(transaction)].extend(locs)
-                else:
-                    transactions[tuple(transaction)] = locs
-
-                for item in transaction:
-                    if item in itemLocs:
-                        itemLocs[item] += locs
-                    else:
-                        itemLocs[item] = list(locs)
-
-            # Precompute getMaxPer results for itemLocs
-            maxPerResults = {item: self._getMaxPer(itemLocs[item], maxTS) for item in itemLocs if len(itemLocs[item]) >= minSup}
-
-            # Filter itemLocs based on minSup and maxPer
-            itemLocs = {k: len(v) for k, v in itemLocs.items() if k in maxPerResults and maxPerResults[k] <= maxPer}
-
-            # Iterate over filtered itemLocs
-            for item in itemLocs:
-                # pat = "\t".join([str(x) for x in newRoot.item + [item]])
-                # self.patCount += 1
-                # patterns[pat] = [itemLocs[item], maxPerResults[item]]
-                patterns[tuple(newRoot.item + [item])] = [itemLocs[item], maxPerResults[item]]
-            
-            if not itemLocs:
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = sum(tidSetI.values())
+        #print(prefix, val)
+        if len(self._finalPatterns) <= self._minSup:
+            sample = str()
+            for i in prefix:
+                sample = sample + i + " "
+            self._finalPatterns[sample] = val
+        if len(self._finalPatterns) == self._minSup:
+            if val > self._minimum:
+                sample = str()
+                for i in prefix:
+                    sample = sample + i + " "
+                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
+                del self._finalPatterns[index]
+                self._finalPatterns[sample] = val
+                self._minimum = min(list(self._finalPatterns.values()))
+        #print(self.finalPatterns, self.minimum, self.minSup)
+
+
+    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(0, len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
                 continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i+1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                sum2 = sum(list(y.values()))
+                #print(prefix, itemJ, y, sum2)
+                #if sum2 >= self.minimum:
+                self._save(prefix, [itemJ], y)
+                classItemSets.append(itemJ)
+                classTidSets.append(y)
+            #print(itemI, tidSetI, classItemSets)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            #self.save(prefix, list(set(itemSetX)), tidSetI)
 
-            newItemNodes = {}
-
-            for transaction, locs in transactions.items():
-                transaction = sorted([item for item in transaction if item in itemLocs], key = lambda x: itemLocs[x], reverse = True)
-                if len(transaction) < 1:
-                    continue
-                currNode = newRoot
-                for item in transaction:
-                    currNode = currNode.addChild(item, locs)
-                    if item in newItemNodes:
-                        newItemNodes[item].add(currNode)
-                    else:
-                        newItemNodes[item] = set([currNode])
-
-            self._recursive(newRoot, newItemNodes, minSup, maxPer, patterns, _lno)
-
-    def Mine(self) -> None:
+    def startMine(self) -> None:
         """
-        Mining process will start from this function
-        :return: None
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-
-        global _minSup, _maxPer, _lno
+        global _minSup
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        if self._maxPer is None:
-            raise Exception("Please enter the Maximum Periodicity")
-        if self._sep is None:
-            raise Exception("Default separator is tab space, please enter the separator if you have different separator in the input file")
-
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        #tested ok
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
-        if self._minSup > len(self._Database):
-            raise Exception("Please enter the minSup in range between 0 to 1")
-        
-
-        items = {}
-
-        # tested ok
-        for line in self._Database:
-            index = int(line[0])
-            for item in line[1:]:
-                if item not in items:
-                    items[item] = []
-                items[item].append(index)
-
-        root, itemNodes = self._construct(items, self._Database, _minSup, _maxPer, _lno, self._finalPatterns)
-
-        self._recursive(root, itemNodes, _minSup, _maxPer, self._finalPatterns, _lno)
-
-    
-
-        newPattern = {}
-        for k, v in self._finalPatterns.items():
-            newPattern["\t".join([str(x) for x in k])] = v
-
-        self._finalPatterns = newPattern
-
-
-
-
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._cupList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i+1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._cupList[itemJ]
+                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
+                self._save(itemSetX, [itemJ], y1)
+                itemSets.append(itemJ)
+                tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic Frequent patterns were generated successfully using PFPGrowth algorithm ")
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
-
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
-
-
+        """
+        Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
-        Storing final periodic-frequent patterns in a dataframe
-
-        :return: returning periodic-frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+            data.append([a, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
-
+        Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: csv file
-        :return: None
+        :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
-            #s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, Tuple[int, int]]:
+    def getPatterns(self) -> Dict[str, float]:
         """
-        Function to send the set of periodic-frequent patterns after completion of the mining process
-
-        :return: returning periodic-frequent patterns
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
-
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
+        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
+        ap.startMine()
+        Patterns = ap.getPatterns()
+        print("Total number of Patterns:", len(Patterns))
+        ap.save("patterns.txt")
+        memUSS = ap.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = ap.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = ap.getRuntime()
+        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
-
-
-    """
-    **Methods to execute code on terminal**
-    --------------------------------------------
-    .. code-block:: console
-
-      Format:
-            
-      (.venv) $ python3 PFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
-
-      Example:
-      
-      (.venv) $ python3 PFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
-
-    .. note:: minSup will be considered in percentage of database transactions
-
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------
-    .. code-block:: python
-
-                from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
-
-                obj = alg.PFPGrowth(iFile, minSup, maxPer)
-
-                obj.startMine()
-
-                periodicFrequentPatterns = obj.getPatterns()
-
-                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
-
-                obj.save(oFile)
-
-                Df = obj.getPatternsAsDataFrame()
-
-                memUSS = obj.getMemoryUSS()
-
-                print("Total Memory in USS:", memUSS)
-
-                memRSS = obj.getMemoryRSS()
-
-                print("Total Memory in RSS", memRSS)
-
-                run = obj.getRuntime()
-
-                print("Total ExecutionTime in seconds:", run)
-    """
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PFPMC.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TUFP.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,499 +1,485 @@
-# PFPMC is the fundamental approach to mine the periodic-frequent patterns.
+# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#             from PAMI.periodicFrequentPattern.basic import PFPMC as alg
+#     obj = alg.TUFP(iFile, minSup)
 #
-#             obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
+#     obj.startMine()
 #
-#             obj.startMine()
+#     frequentPatterns = obj.getPatterns()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#     obj.save(oFile)
 #
-#             obj.save("patterns")
+#     Df = obj.getPatternsAsDataFrame()
 #
-#             Df = obj.getPatternsAsDataFrame()
+#     memUSS = obj.getMemoryUSS()
 #
-#             memUSS = obj.getMemoryUSS()
+#     print("Total Memory in USS:", memUSS)
 #
-#             print("Total Memory in USS:", memUSS)
+#     memRSS = obj.getMemoryRSS()
 #
-#             memRSS = obj.getMemoryRSS()
+#     print("Total Memory in RSS", memRSS)
 #
-#             print("Total Memory in RSS", memRSS)
+#     run = obj.getRuntime()
 #
-#             run = obj.getRuntime()
-#
-#             print("Total ExecutionTime in seconds:", run)
-#
-
-
+#     print("Total ExecutionTime in seconds:", run)
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
+
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Union
 import pandas as pd
-from deprecated import deprecated
-from itertools import groupby as _groupby
-from operator import itemgetter as _itemgetter
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
+_minSup = float()
+_finalPatterns = {}
 
-class PFPMC(_ab._periodicFrequentPatterns):
+
+class _Item:
+    """
+    A class used to represent the item with probability in transaction of dataset
+    :Attributes:
+        item : int or word
+            Represents the name of the item
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
     """
-    :Description:   PFPMC is the fundamental approach to mine the periodic-frequent patterns.
 
-    :Reference: (has to be added)
+    def __init__(self, item, probability) -> None:
+        self.item = item
+        self.probability = probability
 
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-    :param  minSup: str:
-                   Controls the minimum number of transactions in which every item must appear in a database.
-    :param  maxPer: str:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
+class TUFP(_ab._frequentPatterns):
+    """
+    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+    :Reference:
+        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
+        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
     :Attributes:
-
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
-        minSup : int or float or str
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
-            it represents the total no of transactions
+            To represent the total no of transaction
         tree : class
-            it represents the Tree class
+            To represents the Tree class
         itemSetCount : int
-            it represents the total no of patterns
+            To represents the total no of patterns
         finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
-        hashing : dict
-            stores the patterns with their support to check for the closed property
-
+            To store the complete patterns
     :Methods:
-
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to an output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        storePatternsInFile(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsInDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingOneItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent
-        getPeriodAndSupport()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
-
-
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
     **Methods to execute code on terminal**
-    ------------------------------------------
-    .. code-block:: console
-
-
-       Format:
-
-       (.venv) $ python3 PFPMC.py <inputFile> <outputFile> <minSup> <maxPer>
-
-       Example usage:
-
-       (.venv) $ python3 PFPMC.py sampleDB.txt patterns.txt 10.0 4.0
-
-
-               .. note:: minSup and maxPer will be considered in percentage of database transactions
-
+    -----------------------------------------
+            Format:
+                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
+            Example:
+                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
+                      .. note:: minSup  will be considered in support count or frequency
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    ------------------------------------------------------
     .. code-block:: python
-
-                from PAMI.periodicFrequentPattern.basic import PFPMC as alg
-
-                obj = alg.PFPMC("../basic/sampleTDB.txt", "2", "5")
-
-                obj.startMine()
-
-                periodicFrequentPatterns = obj.getPatterns()
-
-                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
-
-                obj.save("patterns")
-
-                Df = obj.getPatternsAsDataFrame()
-
-                memUSS = obj.getMemoryUSS()
-
-                print("Total Memory in USS:", memUSS)
-
-                memRSS = obj.getMemoryRSS()
-
-                print("Total Memory in RSS", memRSS)
-
-                run = obj.getRuntime()
-
-                print("Total ExecutionTime in seconds:", run)
-
+            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+            obj = alg.TUFP(iFile, minSup)
+            obj.startMine()
+            frequentPatterns = obj.getPatterns()
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            obj.save(oFile)
+            Df = obj.getPatternsAsDataFrame()
+            memUSS = obj.getmemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = obj.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = obj.getRuntime()
+            print("Total ExecutionTime in seconds:", run)
     **Credits:**
-    ----------------
-
-    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
+    ---------------
+    The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
 
+    _startTime = float()
+    _endTime = float()
+    _minSup = str()
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _dbSize = None
-    _Database = None
-    _minSup = str()
-    _maxPer = str()
-    _tidSet = set()
-    _finalPatterns = {}
-    _startTime = None
-    _endTime = None
-    _lastTid = int()
     _memoryUSS = float()
     _memoryRSS = float()
+    _Database = []
+    _cupList = {}
+    _topk = {}
+    _minimum = 9999
 
-    def _getPeriodic(self, tids: set) -> int:
-        """
-        To get  Periodic frequent patterns
-
-        :param tids: represents the timestamp of a transaction
-        :type tids: set
-        :return: None
-        """
-        tids = list(tids)
-        tids.sort()
-        temp = self._maxPer + 1
-        diffs = []
-        if self._lastTid in tids:
-            tids.remove(self._lastTid)
-        for k, g in _groupby(enumerate(tids), lambda ix: ix[0] - ix[1]):
-            diffs.append(len(list(map(_itemgetter(1), g))))
-        if len(diffs) < 1:
-            return temp
-        return max(diffs) + 1
-
-    def _getPeriodic(self, tids: set):
-
-        tids = list(tids)
-        tids.sort()
-        temp = self._maxPer + 1
-        if self._lastTid in tids:
-            tids.remove(self._lastTid)
-        diffs = []
-        # find the longest consecutive period
-
-        count = 0
-        for i in range(len(tids) - 1):
-            if tids[i + 1] == tids[i] + 1:
-                count += 1
-            else:
-                diffs.append(count)
-                count = 0
-        if len(diffs) < 1:
-            return temp
-        return max(diffs) + 1
-
-    def _getPeriodic(self, tids: set):
-        tids = list(tids)
-        tids.sort()
-        temp = self._maxPer + 1
-        if self._lastTid in tids:
-            tids.remove(self._lastTid)
-        diffs = []
-        tempPer = 0
-        period = 0
-        for i in range(len(tids) - 1):
-            if tids[i+1] - tids[i] == 1:
-                tempPer += 1
-            else:
-                period = max(period, tempPer + 1)
-                if period > self._maxPer:
-                    return temp
-                tempPer = 0
-
-        return period
-
-    def _convert(self, value) -> float:
+    def _creatingItemSets(self) -> None:
         """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        Scans the dataset
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbSize * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._dbSize * value)
-            else:
-                value = int(value)
-        return value
-
-    def _creatingOneItemSets(self) -> list:
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        :return: list
-        """
-        plist = []
-        Database = []
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            ts, data = [], []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                Database.append(tr)
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    Database.append(temp)
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            Database.append(temp)
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
-        tid = 0
-        itemsets = {}  # {key: item, value: list of tids}
-        periodicHelper = {}  # {key: item, value: [period, last_tid]}
-        for line in Database:
-            tid = int(line[0])
-            self._tidSet.add(tid)
-            for item in line[1:]:
-                if item in itemsets:
-                    itemsets[item].add(tid)
-                else:
-                    itemsets[item] = {tid}
 
-        self._dbSize = len(Database)
-        self._lastTid = max(self._tidSet)
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        del Database
-        candidates = []
-        for item, tids in itemsets.items():
-            diff = self._tidSet.difference(tids)
-            per = self._getPeriodic(diff)
-            sup = len(tids)
-            if sup >= self._minSup and per <= self._maxPer:
-                candidates.append(item)
-                self._finalPatterns[item] = [sup, per, diff]
-        return candidates
-
-    def _generateDiffsetEclat(self, candidates: list) -> None:
-        new_freqList = []
-        for i in range(0, len(candidates)):
-            item1 = candidates[i]
-            i1_list = item1.split()
-            for j in range(i + 1, len(candidates)):
-                item2 = candidates[j]
-                i2_list = item2.split()
-                if i1_list[:-1] == i2_list[:-1]:
-                    union_DiffSet = self._finalPatterns[item2][2].union(self._finalPatterns[item1][2])
-                    sorted(union_DiffSet)
-                    union_supp = self._dbSize - len(union_DiffSet)
-                    period = self._getPeriodic(union_DiffSet)
-                    if union_supp >= self._minSup and period <= self._maxPer:
-                        newKey = item1 + "\t" + i2_list[-1]
-                        self._finalPatterns[newKey] = [union_supp, period, union_DiffSet]
-                        new_freqList.append(newKey)
+    def _frequentOneItem(self) -> List[str]:
+        """
+        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        """
+
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = j.probability
+                    self._cupList[j.item] = {k:j.probability}
                 else:
-                    break
+                    mapSupport[j.item] += j.probability
+                    self._cupList[j.item].update({k: j.probability})
+        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        k = 0
+        for x, in plist:
+            k +=1
+            if k >= self._minSup:
+                break
+            self._finalPatterns[x] = mapSupport[x]
+        self._minimum = min(list(self._finalPatterns.values()))
+        return plist
+
+    @staticmethod
+    def _convert(value: Union[int, float, str]) -> Union[int, float]:
+        """
+        To convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :return: converted type minSup value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = float(value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+            else:
+                value = int(value)
+        return value
 
-        if len(new_freqList) > 0:
-            self._generateDiffsetEclat(new_freqList)
+    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
+        """
+        Saves the patterns that satisfy the periodic frequent property.
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
+        """
+
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = sum(tidSetI.values())
+        # print(prefix, val)
+        if len(self._finalPatterns) <= self._minSup:
+            sample = str()
+            for i in prefix:
+                sample = sample + i + " "
+            self._finalPatterns[sample] = val
+        if len(self._finalPatterns) == self._minSup:
+            if val > self._minimum:
+                sample = str()
+                for i in prefix:
+                    sample = sample + i + " "
+                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
+                del self._finalPatterns[index]
+                self._finalPatterns[sample] = val
+                self._minimum = min(list(self._finalPatterns.values()))
+        # print(self.finalPatterns, self.minimum, self.minSup)
+
+    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(0, len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                sum2 = sum(list(y.values()))
+                # print(prefix, itemJ, y, sum2)
+                # if sum2 >= self.minimum:
+                self._save(prefix, [itemJ], y)
+                classItemSets.append(itemJ)
+                classTidSets.append(y)
+            # print(itemI, tidSetI, classItemSets)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            # self.save(prefix, list(set(itemSetX)), tidSetI)
 
     def startMine(self) -> None:
         """
-        Mining process will start from this function
-        :return: None
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        # print(f"Optimized {type(self).__name__}")
+        global _minSup
         self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        frequentSets = self._creatingOneItemSets()
-        self._generateDiffsetEclat(frequentSets)
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._cupList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._cupList[itemJ]
+                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                self._save(itemSetX, [itemJ], y1)
+                itemSets.append(itemJ)
+                tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using PFPDiffset ECLAT algorithm ")
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
-        Storing final periodic-frequent patterns in a dataframe
-
-        :return: returning periodic-frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
-
+        Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: csv file
-        :return: None
+        :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
-            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self) -> Dict[str, float]:
         """
-        Function to send the set of periodic-frequent patterns after completion of the mining process
-
-        :return: returning periodic-frequent patterns
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
+    if __name__ == "__main__":
+        _ap = str()
+        if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+            if len(_ab._sys.argv) == 5:
+                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            if len(_ab._sys.argv) == 4:
+                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap.startMine()
+            _Patterns = _ap.getPatterns()
+            print("Total number of Patterns:", len(_Patterns))
+            _ap.save(_ab._sys.argv[2])
+            _memUSS = _ap.getMemoryUSS()
+            print("Total Memory in USS:", _memUSS)
+            _memRSS = _ap.getMemoryRSS()
+            print("Total Memory in RSS", _memRSS)
+            _run = _ap.getRuntime()
+            print("Total ExecutionTime in ms:", _run)
+        else:
+            '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
+            ap.startMine()
+            Patterns = ap.getPatterns()
+            print("Total number of Patterns:", len(Patterns))
+            ap.save("patterns.txt")
+            memUSS = ap.getMemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = ap.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = ap.getRuntime()
+            print("Total ExecutionTime in ms:", run)'''
+            print("Error! The number of input parameters do not match the total number of parameters provided")
 
-if __name__ == "__main__":
-    _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = PFPMC(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        _ap.startMine()
-        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
-    else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPAM.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-# PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
-#
-#
+# SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
+# This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
+#  This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
+#             import PAMI.sequentialPatternMining.basic.SPAM as alg
 #
-#             obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
+#             obj = alg.SPAM(iFile, minSup)
 #
 #             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             sequentialPatternMining = obj.getPatterns()
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save("patterns")
+#             obj.save(oFile)
 #
-#             Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -30,15 +30,14 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -50,422 +49,475 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
+
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
+from PAMI.sequentialPatternMining.basic import abstract as _ab
+_ab._sys.setrecursionlimit(10000)
 
-
-class PFECLAT(_ab._periodicFrequentPatterns):
+class SPAM(_ab._sequentialPatterns):
     """
-    :Description:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
+    :Description:    SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
+                     This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
+                     This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
 
-    :Reference:   P. Ravikumar, P.Likhitha, R. Uday kiran, Y. Watanobe, and Koji Zettsu, "Towards efficient discovery of
-                  periodic-frequent patterns in columnar temporal databases", 2021 IEA/AIE.
+    :Reference:   J. Ayres, J. Gehrke, T.Yiu, and J. Flannick. Sequential Pattern Mining Using Bitmaps. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Edmonton, Alberta, Canada, July 2002.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
+                   Name of the Input file to mine complete set of  Sequential frequent patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-    :param  minSup: str:
-                   Controls the minimum number of transactions in which every item must appear in a database.
-    :param  maxPer: str:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+                   Name of the output file to store complete set of  Sequential frequent patterns
+    :param  minSup: float or int or str :
+                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        startTime : float
-            To record the start time of the mining process
-        endTime : float
-            To record the completion time of the mining process
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        itemSetCount : int
-            it represents the total no of patterns
-        finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
-        hashing : dict
-            stores the patterns with their support to check for the closed property
+            iFile : str
+                Input file name or path of the input file
+            oFile : str
+                Name of the output file or the path of output file
+            minSup : float or int or str
+                The user can specify minSup either in count or proportion of database size.
+                If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                Otherwise, it will be treated as float.
+                Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+            sep : str
+                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+                However, the users can override their default separator.
+            startTime : float
+                To record the start time of the mining process
+            endTime : float
+                To record the completion time of the mining process
+            finalPatterns : dict
+                Storing the complete set of patterns in a dictionary variable
+            memoryUSS : float
+                To store the total amount of USS memory consumed by the program
+            memoryRSS : float
+                To store the total amount of RSS memory consumed by the program
+            Database : list
+                To store the sequences of a database in list
+            _idDatabase : dict
+                To store the sequences of a database by bit map
+            _maxSeqLen:
+                the maximum length of subsequence in sequence.
 
     :Methods:
 
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingOneItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent 
-        getPeriodAndSupport()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
-            
+            _creatingItemSets():
+                Storing the complete sequences of the database/input file in a database variable
+            _convert(value):
+                To convert the user specified minSup value
+            make2BitDatabase():
+                To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
+            DfsPruning(items,sStep,iStep):
+                the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
+            Sstep(s):
+                To convert bit to ssteo bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
+            startMine()
+                Mining process will start from here
+            getPatterns()
+                Complete set of patterns will be retrieved with this function
+            savePatterns(oFile)
+                Complete set of frequent patterns will be loaded in to a output file
+            getPatternsAsDataFrame()
+                Complete set of frequent patterns will be loaded in to a dataframe
+            getMemoryUSS()
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
+            getMemoryRSS()
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
+            getRuntime()
+                Total amount of runtime taken by the mining process will be retrieved from this function
+            candidateToFrequent(candidateList)
+                Generates frequent patterns from the candidate patterns
+            frequentToCandidate(frequentList, length)
+                Generates candidate patterns from the frequent patterns
+
 
-    **Methods to execute code on terminal**
-    ------------------------------------------
+    **Executing the code on terminal**:
+    ----------------------------------------
     .. code-block:: console
 
 
        Format:
 
-       (.venv) $ python3 PFECLAT.py <inputFile> <outputFile> <minSup>
-
-       Example usage:
+       (.venv) $ python3 SPAM.py <inputFile> <outputFile> <minSup> (<separator>)
 
-       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
+       Examples usage:
 
+       (.venv) $ python3 SPAM.py sampleDB.txt patterns.txt 10.0
 
 
-               .. note:: minSup will be considered in percentage of database transactions
+               .. note:: minSup will be considered in times of minSup and count of database transactions
 
+    **Sample run of the importing code**:
+    -------------------------------------
+            import PAMI.sequentialPatternMining.basic.SPAM as alg
 
-    **Importing this algorithm into a python program**
-    --------------------------------------------------------
-    .. code-block:: python
+            obj = alg.SPAM(iFile, minSup)
 
-             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
+            obj.startMine()
 
-                obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
+            sequentialPatternMining = obj.getPatterns()
 
-                obj.startMine()
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-                periodicFrequentPatterns = obj.getPatterns()
+            obj.savePatterns(oFile)
 
-                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            Df = obj.getPatternInDataFrame()
 
-                obj.save("patterns")
+            memUSS = obj.getMemoryUSS()
 
-                Df = obj.getPatternsAsDataFrame()
+            print("Total Memory in USS:", memUSS)
 
-                memUSS = obj.getMemoryUSS()
+            memRSS = obj.getMemoryRSS()
 
-                print("Total Memory in USS:", memUSS)
+            print("Total Memory in RSS", memRSS)
 
-                memRSS = obj.getMemoryRSS()
+            run = obj.getRuntime()
 
-                print("Total Memory in RSS", memRSS)
-
-                run = obj.getRuntime()
-
-                print("Total ExecutionTime in seconds:", run)
-
-    **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+            print("Total ExecutionTime in seconds:", run)
 
+    **Credits**:
+    ------------
+            The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
     """
-    
+
+    _minSup = float()
+    _startTime = float()
+    _endTime = float()
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _dbSize = None
-    _Database = None
-    _minSup = str()
-    _maxPer = str()
-    _tidSet = set()
-    _finalPatterns = {}
-    _startTime = None
-    _endTime = None
     _memoryUSS = float()
     _memoryRSS = float()
-
-    def _getPeriodic(self, tids: set) -> int:
-        tidList = list(tids)
-        tidList.sort()
-        tidList.append(self._dbSize)
-        cur = 0
-        per = 0
-        for tid in tidList:
-            per = max(per, tid - cur)
-            if per > self._maxPer:  # early stopping
-                break
-            cur = tid
-        return per
-
-    def _convert(self, value) -> float:
+    _Database = []
+    _idDatabase={}
+    _maxSeqLen=0
+    def _creatingItemSets(self):
         """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        Storing the complete sequences of the database/input file in a database variable
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbSize * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._dbSize * value)
-            else:
-                value = int(value)
-        return value
+        self._Database = []
 
-    def _creatingOneItemSets(self) -> list:
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        :return: list
-        """
-        plist = []
-        Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            ts, data = [], []
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                Database.append(tr)
+                temp = self._iFile['Transactions'].tolist()
+            if "tid" in i:
+                temp2=self._iFile[''].tolist()
+            addList=[]
+            addList.append(temp[0])
+            for k in range(len(temp)-1):
+                if temp2[k]==temp[k+1]:
+                    addList.append(temp[k+1])
+                else:
+                    self._Database.append(addList)
+                    addList=[]
+                    addList.append(temp[k+1])
+            self._Database.append(addList)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    Database.append(temp)
+                    temp.pop()
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            Database.append(temp)
+                            temp = [i.rstrip() for i in line.split('-1')]
+                            temp = [x for x in temp if x ]
+                            temp.pop()
+
+                            seq = []
+                            for i in temp:
+                                k = -2
+                                if len(i)>1:
+                                    seq.append(list(sorted(set(i.split()))))
+
+                                else:
+                                    seq.append(i)
+
+                            self._Database.append(seq)
+
                 except IOError:
                     print("File Not Found")
                     quit()
-        tid = 0
-        itemsets = {}  # {key: item, value: list of tids}
-        periodicHelper = {}  # {key: item, value: [period, last_tid]}
-        for line in Database:
-            tid = int(line[0])
-            self._tidSet.add(tid)
-            for item in line[1:]:
-                if item in itemsets:
-                    itemsets[item].add(tid)
-                    periodicHelper[item][0] = max(periodicHelper[item][0],
-                                                  abs(tid - periodicHelper[item][1]))  # update current max period
-                    periodicHelper[item][1] = tid  # update the last tid
-                else:
-                    itemsets[item] = {tid}
-                    periodicHelper[item] = [abs(0 - tid), tid]  # initialize helper
 
-        # finish all items' period
-        self._dbSize = len(Database)
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        del Database
-        for item, _ in periodicHelper.items():
-            periodicHelper[item][0] = max(periodicHelper[item][0],
-                                          abs(self._dbSize - periodicHelper[item][1]))  # tid of the last transaction
-        candidates = []
-        for item, tids in itemsets.items():
-            per = periodicHelper[item][0]
-            sup = len(tids)
-            if sup >= self._minSup and per <= self._maxPer:
-                candidates.append(item)
-                self._finalPatterns[item] = [sup, per, tids]
-        return candidates
-    
-    def _generateEclat(self, candidates: list) -> None:
-
-        newCandidates = []
-        for i in range(0, len(candidates)):
-            prefixItem = candidates[i]
-            prefixItemSet = prefixItem.split()
-            for j in range(i + 1, len(candidates)):
-                item = candidates[j]
-                itemSet = item.split()
-                if prefixItemSet[:-1] == itemSet[:-1] and prefixItemSet[-1] != itemSet[-1]:
-                    _value = self._finalPatterns[item][2].intersection(self._finalPatterns[prefixItem][2])
-                    sup = len(_value)
-                    per = self._getPeriodic(_value)
-                    if sup >= self._minSup and per <= self._maxPer:
-                        newItem = prefixItem + "\t" + itemSet[-1]
-                        self._finalPatterns[newItem] = [sup, per, _value]
-                        newCandidates.append(newItem)
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
-        if len(newCandidates) > 0:
-            self._generateEclat(newCandidates)
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self) -> None:
+    def make2BitDatabase(self):
         """
-        Mining process will start from this function
-        :return: None
+        To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        frequentSets = self._creatingOneItemSets()
-        self._generateEclat(frequentSets)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+        self._maxSeqLen=max([len(i) for i in self._Database])
+        lineNumber=0
+        idDatabase={}
+        for line in self._Database:
+            seqNumber=1
+            for seq in line:
+
+                for data in seq:
+                    if data in idDatabase:
+                        while lineNumber+1!=len(idDatabase[data]):
+                            idDatabase[data].append(0)
+                        idDatabase[data][lineNumber]+=int(2**(self._maxSeqLen-seqNumber))
+
+                    else:
+                        idDatabase[data]=[]
+                        while lineNumber+1!=len(idDatabase[data]):
+                            idDatabase[data].append(0)
+                        idDatabase[data][lineNumber]+=(int(2 ** (self._maxSeqLen-seqNumber)))
+
+                seqNumber+=1
+            lineNumber+=1
+        for key,val in idDatabase.items():
+
+            sup=self.countSup(val)
+            while lineNumber+1!=len(idDatabase[key]):
+                            idDatabase[key].append(0)
+            if sup>=self._minSup:
+                self._finalPatterns[str(key)+self._sep+"-2"]=sup
+                self._idDatabase[str(key)]=val
+
+    def DfsPruning(self,items,sStep,iStep):
+        """
+        the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
+
+        :Attributes:
+
+        items : str
+            The pattrens I got before
+        sStep : list
+            Items presumed to have "sstep" relationship with "items".(sstep is What appears later like a-b and a-c)
+        iStep : list
+            Items presumed to have "istep" relationship with "items"(istep is What appears in same time like ab and ac)
+
+        """
+        Snext=[]
+        Inext=[]
+        ns = self.Sstep(self._idDatabase[items])
+        for i in sStep:
+            nnext=[]
+            for k in  range(len(self._idDatabase[items])):
+                nandi=ns[k] & self._idDatabase[i][k]
+                nnext.append(nandi)
+
+
+            sup=self.countSup(nnext)
+            if sup>=self._minSup:
+                key=items+self._sep+"-1"+self._sep+i
+                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
+                self._idDatabase[key]=nnext
+                Snext.append(i)
+
+        for i in Snext:
+            key = items+self._sep+"-1"+self._sep+i
+            self.DfsPruning(key,Snext,[k for k in Snext if self._Database.index(i)<self._Database.index(k)])
+        for i in iStep:
+            nnext = []
+
+            for k in range(len(self._idDatabase[items])):
+                nandi = self._idDatabase[items][k] & self._idDatabase[i][k]
+                nnext.append(nandi)
+            sup=self.countSup(nnext)
+            if sup>=self._minSup:
+                key=items+self._sep+str(i)
+                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
+                self._idDatabase[key]=nnext
+                Inext.append(i)
+        for i in Inext:
+            key = items +self._sep +str(i)
+            self.DfsPruning(key,Snext,[k for k in Inext if self._Database.index(i)<self._Database.index(k)])
+
+    def Sstep(self,s):
+        """
+        To convert bit to Sstep bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
+
+
+        :param s:list
+            to store each bit sequence
+        :return:
+            nextS:list to store the bit sequence converted by sstep
+
+        """
+        nextS=[]
+        for bins in s:
+            binS=str(bin(bins))
+
 
-    def Mine(self) -> None:
+            LenNum=2
+            for i in range(len(binS)-2):
+                if binS[LenNum] == "1":
+
+                    binS = binS[:LenNum] + "0" + binS[LenNum + 1:]
+                    while len(binS)-1!=LenNum:
+                        LenNum += 1
+                        binS = binS[:LenNum] + "1" + binS[LenNum + 1:]
+                    break
+                LenNum+=1
+            nextS.append(int(binS, 0))
+
+
+        return nextS
+
+    def countSup(self,n):
+        """
+        count support
+
+        :param n:list
+                to store each bit sequence
+        :return:
+            count: int support of this list
         """
-        Mining process will start from this function
-        :return: None
+        count=0
+        for i in n:
+            if "1" in str(bin(i)):
+                count+=1
+        return count
+
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
         """
+        self._Database = []
         self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        frequentSets = self._creatingOneItemSets()
-        self._generateEclat(frequentSets)
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self.make2BitDatabase()
+        self._Database = [i for i in self._idDatabase.keys()]
+        for i in self._Database:
+            x=[]
+            for j in self._Database:
+                if self._Database.index(i)<self._Database.index(j):
+                    x.append(j)
+
+            self.DfsPruning(i,self._Database,x)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+        print("Frequent patterns were generated successfully using Apriori algorithm ")
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """Calculating the total amount of runtime taken by the mining process
-
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
-        """
-        Storing final periodic-frequent patterns in a dataframe
-
-        :return: returning periodic-frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self):
+        """Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataframe
-
-    def save(self, outFile: str) -> None:
-        """
-        Complete set of periodic-frequent patterns will be loaded in to a output file
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
 
+    def save(self, outFile):
+        """Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: csv file
-        :return: None
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
-            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> dict:
-        """
-        Function to send the set of periodic-frequent patterns after completion of the mining process
-
-        :return: returning periodic-frequent patterns
+    def getPatterns(self):
+        """ Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-                    
+        print("Total ExecutionTime in ms:", self.getRuntime())
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_Patterns))
+        _ap.savePatterns(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
+
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -153,10 +153,10 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
-        """ To print the results of the execution."""
+        """ To print results of the execution."""
 
         pass
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,25 +1,22 @@
-#  CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
-#  It uses depth-first search.
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
+#             from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowthDump as alg
 #
-#             obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
+#             obj = alg.SPPGrowthDump(iFile, minSup, maxPer, maxLa)
 #
 #             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#             obj.save("patterns")
+#             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -49,511 +46,475 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
+from PAMI.stableperiodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.periodicFrequentPattern.closed import abstract as _ab
+from urllib.request import urlopen
+import validators
+import pandas as pd
+import resource
+import time
+import sys
+import os
+import psutil
+
+_minSup = int()
+_maxPer = int()
+_maxLa = int()
+_last = int()
 
-class CPFPMiner(_ab._periodicFrequentPatterns):
-    """
-    About this algorithm
-    ====================
-
-    :Description:   CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
-                    It uses depth-first search.
-
-    :Reference:   P. Likhitha et al., "Discovering Closed Periodic-Frequent Patterns in Very Large Temporal Databases"
-                  2020 IEEE International Conference on Big Data (Big Data), 2020, https://ieeexplore.ieee.org/document/9378215
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-    :param  minSup: float:
-                   Controls the minimum number of transactions in which every item must appear in a database.
-    :param  maxPer: float:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
-
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-    :Attributes:
-
-        iFile : str
-            Input file name or path of the input file
-        oFile : str
-            Name of the output file or path of the input file
-        minSup: int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-
-    :Methods:
-
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of frequent patterns will be loaded in to an output file
-        getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-
-
-    Execution methods
-    =================
-
-
-    **Terminal command**
-
-
-    .. code-block:: console
-
-       Format:
-
-       (.venv) $  python3 CPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer>
-
-       Example:
-
-       (.venv) $ python3 CPFPMiner.py sampleTDB.txt patterns.txt 0.3 0.4
-
-     .. note:: minSup will be considered in percentage of database transactions
-        
-        
-    **Calling from a python program**
-
-    .. code-block:: python
-
-                    from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
-        
-                    obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
-        
-                    obj.startMine()
-        
-                    periodicFrequentPatterns = obj.getPatterns()
-        
-                    print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
-        
-                    obj.save("patterns")
-        
-                    Df = obj.getPatternsAsDataFrame()
-        
-                    memUSS = obj.getMemoryUSS()
-        
-                    print("Total Memory in USS:", memUSS)
-        
-                    memRSS = obj.getMemoryRSS()
-        
-                    print("Total Memory in RSS", memRSS)
-        
-                    run = obj.getRuntime()
-        
-                    print("Total ExecutionTime in seconds:", run)
-        
-    **Credits:**
-    ------------------
-    The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
-    """
 
-    _minSup = float()
-    _maxPer = float()
+class _Node:
+
+    def __init__(self, item, children):
+        """
+        Initializing the Node class
+
+        :param item: Storing the item of a node
+        :type item: int or None
+        :param children: To maintain the children of a node
+        :type children: dict
+        """
+
+        self.item = item
+        self.children = children
+        self.parent = None
+        self.timeStamps = []
+
+    def addChild(self, node):
+        """
+        To add the children to a node
+
+        :param node: parent node in the tree
+        """
+
+        self.children[node.item] = node
+        node.parent = self
+
+class _Tree:
+    def __init__(self):
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction, tid):
+        """
+        Adding a transaction into tree
+
+        :param transaction: To represent the complete database
+        :type transaction: list
+        :param tid: To represent the timestamp of a database
+        :type tid: list
+        :return: pfp-growth tree
+        """
+
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+        currentNode.timeStamps = currentNode.timeStamps + tid
+
+    def getConditionalPatterns(self, alpha):
+        """
+        Generates all the conditional patterns of a respective node
+
+        :param alpha: To represent a Node in the tree
+        :type alpha: Node
+        :return: A tuple consisting of finalPatterns, conditional pattern base and information
+        """
+        finalPatterns = []
+        finalSets = []
+        for i in self.summaries[alpha]:
+            set1 = i.timeStamps
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalSets.append(set1)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
+        return finalPatterns, finalSets, info
+
+    @staticmethod
+    def generateTimeStamps(node):
+        """
+        To get the timestamps of a node
+
+        :param node: A node in the tree
+        :return: Timestamps of a node
+        """
+
+        finalTimeStamps = node.timeStamps
+        return finalTimeStamps
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
+
+        :param nodeValue: To represent a node in the tree
+        :type nodeValue: node
+        :return: Tree with their nodes updated with timestamps
+        """
+
+        for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            del i.parent.children[nodeValue]
+
+    def getTimeStamps(self, alpha):
+        """
+        To get all the timestamps of the nodes which share same item name
+
+        :param alpha: Node in a tree
+        :return: Timestamps of a  node
+        """
+        temporary = []
+        for i in self.summaries[alpha]:
+            temporary += i.timeStamps
+        return temporary
+
+    @staticmethod
+    def getSupportAndPeriod(timeStamps):
+        """
+        To calculate the periodicity and support
+
+        :param timeStamps: Timestamps of an item set
+        :return: support, periodicity
+        """
+        global _maxPer, _last
+        previous = 0
+        la = 0
+        tsList = sorted(timeStamps)
+        for ts in tsList:
+            la = max(0, la + ts - previous - _maxPer)
+            previous = ts
+        la = max(0, la + _last - previous - _maxPer)
+        return len(timeStamps), la
+
+    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps):
+        """
+        It generates the conditional patterns with periodic-frequent items
+
+        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
+        :type conditionalPatterns: list
+        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
+        :type conditionalTimeStamps: list
+        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
+        """
+
+        global _maxPer, _minSup, _maxLa
+        pat = []
+        timeStamps = []
+        data1 = {}
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
+                if j in data1:
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                else:
+                    data1[j] = conditionalTimeStamps[i]
+        updatedDictionary = {}
+        for m in data1:
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _minSup and v[1] <= _maxLa}
+        count = 0
+        for p in conditionalPatterns:
+            p1 = [v for v in p if v in updatedDictionary]
+            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                timeStamps.append(conditionalTimeStamps[count])
+            count += 1
+        return pat, timeStamps, updatedDictionary
+
+    def generatePatterns(self, prefix):
+        """
+        Generates the patterns
+
+        :param prefix: Forms the combination of items
+        :type prefix: list
+        :returns: yields patterns with their support and periodicity
+        """
+
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            yield pattern, self.info[i]
+            patterns, timeStamps, info = self.getConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
+            self.removeNode(i)
+
+class SPPGrowth():
     _startTime = float()
     _endTime = float()
+    _minSup = str()
+    _maxPer = float()
+    _maxLa = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _transaction = []
-    _hashing = {}
-    _mapSupport = {}
-    _itemSetCount = 0
-    _maxItemId = 0
-    _tableSize = 10000
-    _tidList = {}
+    _Database = []
+    _rank = {}
+    _rankedUp = {}
     _lno = 0
+    SPPList = {}
 
-    def __init__(self, iFile, minSup, maxPer, sep='\t'):
-        super().__init__(iFile, minSup, maxPer, sep)
-        self._finalPatterns = {}
-    
-    def _convert(self, value):
-        """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._lno * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        return value
-
-    def _scanDatabase(self):
-        """
-        To scan the database and extracts the 1-length periodic-frequent items
-
-        :return:   Returns the 1-length periodic-frequent items
-        """
-        Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            ts, data = [], []
+    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
+        self._iFile = inputFile
+        self._minSup = minSup
+        self._maxPer = maxPer
+        self._maxLa = maxLa
+        self._sep = sep
+
+    def _creatingItemSets(self):
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        """
+        self._Database = []
+        if isinstance(self._iFile, pd.DataFrame):
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
                 tr = [ts[i][0]]
                 tr = tr + data[i]
-                Database.append(tr)
+                self._Database.append(tr)
 
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if validators.url(self._iFile):
+                data = urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    Database.append(temp)
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            Database.append(temp)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-        self._tidList = {}
-        self._mapSupport = {}
-        for line in Database:
-            self._lno += 1
-            s = line
-            n = int(s[0])
-            for i in range(1, len(s)):
-                si = s[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
-                    self._tidList[si] = [n]
-                else:
-                    self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(self._lno - self._mapSupport[x][2]))
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if
-                           v[0] >= self._minSup and v[1] <= self._maxPer}
-        periodicFrequentItems = {}
-        self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
-        for x, y in self._tidList.items():
-            t1 = 0
-            for i in y:
-                t1 += i
-            periodicFrequentItems[x] = t1
-        periodicFrequentItems = [key for key, value in sorted(periodicFrequentItems.items(), key=lambda x: x[1])]
-        return periodicFrequentItems
-
-    def _calculate(self, tidSet):
-        """
-        To calculate the weight if pattern based on the respective timeStamps
-
-        :param tidSet: timeStamps of the pattern
-        :return: the calculated weight of the timeStamps
-        """
-        hashcode = 0
-        for i in tidSet:
-            hashcode += i
-        if hashcode < 0:
-            hashcode = abs(0 - hashcode)
-        return hashcode % self._tableSize
-
-    def _contains(self, itemSet, val, hashcode):
-        """
-        To check if the key(hashcode) is in dictionary(hashing) variable
-
-        :param itemSet: generated periodic-frequent itemSet
-        :param val: support and periodicity of itemSet
-        :param hashcode: the key generated in calculate() method for every itemSet
-
-        :return: true if itemSet with same support present in dictionary(hashing) or else returns false
-        """
-        if self._hashing.get(hashcode) is None:
-            return False
-        for i in self._hashing[hashcode]:
-            itemSetX = i
-            if val[0] == self._hashing[hashcode][itemSetX][0] and set(itemSetX).issuperset(itemSet):
-                return True
-        return False
-
-    def _getPeriodAndSupport(self, timeStamps):
-        """
-        Calculates the periodicity and support of timeStamps
-
-        :param timeStamps: timeStamps of itemSet
-        :return: periodicity and support
-        """
-        timeStamps.sort()
-        cur = 0
-        per = 0
-        sup = 0
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > self._maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-            sup += 1
-        per = max(per, self._lno - cur)
-        return [sup, per]
-
-    def _save(self, prefix, suffix, tidSetX):
-        """
-        Saves the generated pattern which satisfies the closed property
-        Parameters:
-        -----------
-            prefix: the prefix part of itemSet
-            suffix: the suffix part of itemSet
-            tidSetX: the timeStamps of the generated itemSet
-
-        Returns:
-        --------
-            saves the closed periodic-frequent pattern
-
-        """
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        prefix = list(set(prefix))
-        prefix.sort()
-        val = self._getPeriodAndSupport(tidSetX)
-        if val[0] >= self._minSup and val[1] <= self._maxPer:
-            hashcode = self._calculate(tidSetX)
-            if self._contains(prefix, val, hashcode) is False:
-                self._itemSetCount += 1
-                sample = str()
-                for i in prefix:
-                    sample = sample + i + " "
-                self._finalPatterns[sample] = val
-            if hashcode not in self._hashing:
-                self._hashing[hashcode] = {tuple(prefix): val}
-            else:
-                self._hashing[hashcode][tuple(prefix)] = val
 
-    def _processEquivalenceClass(self, prefix, itemSets, tidSets):
+    def _periodicFrequentOneItem(self):
         """
-        identifies and saves closed periodic patterns of length more than 2 in a dataset, by processing equivalence classes of item sets that satisfy a minimum support condition.
-        :param prefix: Prefix class of an itemSet
-        :param itemSets: suffix items in periodicFrequentItems that satisfies the minSup condition
-        :param tidSets: timeStamps of items in itemSets respectively
-        :return:  closed periodic patterns with length more than 2
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidList = tidSets[0]
-            self._save(prefix, [i], tidList)
-            return
-        if len(itemSets) == 2:
-            itemI = itemSets[0]
-            tidSetI = tidSets[0]
-            itemJ = itemSets[1]
-            tidSetJ = tidSets[1]
-            y1 = list(set(tidSetI).intersection(tidSetJ))
-            if len(y1) >= self._minSup:
-                suffix = []
-                suffix += [itemI, itemJ]
-                suffix = list(set(suffix))
-                self._save(prefix, suffix, y1)
-            if len(y1) != len(tidSetI):
-                self._save(prefix, [itemI], tidSetI)
-            if len(y1) != len(tidSetJ):
-                self._save(prefix, [itemJ], tidSetJ)
-            return
-        for i in range(len(itemSets)):
-            itemX = itemSets[i]
-            if itemX is None:
-                continue
-            tidSetX = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemX]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                if itemJ is None:
-                    continue
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetX).intersection(tidSetJ))
-                if len(y) < self._minSup:
-                    continue
-                if len(tidSetX) == len(tidSetJ) and len(y) == len(tidSetX):
-                    itemSets.insert(j, None)
-                    tidSets.insert(j, None)
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) < len(tidSetJ) and len(y) == len(tidSetX):
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) > len(tidSetJ) and len(y) == len(tidSetJ):
-                    itemSets.insert(j, None)
-                    tidSets.insert(j, None)
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
+        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
+
+        :returns: return the one-length periodic frequent patterns
+        """
+        global _last
+        tidLast = {}
+        la = {}
+        for transaction in self._Database:
+            ts = int(transaction[0])
+            for item in transaction[1:]:
+                if item not in self.SPPList:
+                    la[item] = max(0, ts - self._maxPer)
+                    self.SPPList[item] = [1, la[item]]
                 else:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            if len(classItemSets) > 0:
-                newPrefix = list(set(itemSetX)) + prefix
-                self._processEquivalenceClass(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetX)
+                    s = self.SPPList[item][0] + 1
+                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
+                    self.SPPList[item] = [s, max(la[item], self.SPPList[item][1])]
+                tidLast[item] = ts
+            _last = ts
+        for item in self.SPPList:
+            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
+            self.SPPList[item][1] = max(la[item], self.SPPList[item][1])
+        self.SPPList = {k: v for k, v in self.SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
+        self.SPPList = {k: v for k, v in sorted(self.SPPList.items(), key=lambda x: x[1][0], reverse=True)}
+        data = self.SPPList
+        pfList = [k for k, v in sorted(data.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
+        #print(len(pfList))
+        return data, pfList
+
+    def _updateDatabases(self, dict1):
+        """
+        Remove the items which are not frequent from database and updates the database with rank of items
+
+        :param dict1: frequent items with support
+        :type dict1: dictionary
+        :return: Sorted and updated transactions
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
+                if tr[i] in dict1:
+                    list2.append(self._rank[tr[i]])
+            if len(list2) >= 2:
+                basket = list2[1:]
+                basket.sort()
+                list2[1:] = basket[0:]
+                list1.append(list2)
+        return list1
+
+    @staticmethod
+    def _buildTree(data, info):
+        """
+        It takes the database and support of each item and construct the main tree by setting root node as a null
+
+        :param data: it represents the one Database in database
+        :type data: list
+        :param info: it represents the support of each item
+        :type info: dictionary
+        :return: returns root node of tree
+        """
+
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            set1 = [data[i][0]]
+            rootNode.addTransaction(data[i][1:], set1)
+        return rootNode
+
+    def _savePeriodic(self, itemSet):
+        """
+        To convert the ranks of items in to their original item names
+
+        :param itemSet: frequent pattern.
+        :return: frequent pattern with original item names
+        """
+        t1 = str()
+        for i in itemSet:
+            t1 = t1 + self._rankedUp[i] + " "
+        return t1
+
+    def _convert(self, value):
+        """
+        To convert the given user specified value
+
+        :param value: user specified value
+        :return: converted value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Mining process will start from here
+        Mining process will start from this function
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        self._hashing = {}
-        periodicFrequentItems = self._scanDatabase()
-        for i in range(len(periodicFrequentItems)):
-            itemX = periodicFrequentItems[i]
-            if itemX is None:
-                continue
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(periodicFrequentItems)):
-                itemJ = periodicFrequentItems[j]
-                if itemJ is None:
-                    continue
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._minSup:
-                    continue
-                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
-                    periodicFrequentItems.insert(j, None)
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
-                    periodicFrequentItems.insert(j, None)
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-                else:
 
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            if len(itemSets) > 0:
-                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
-            self._save([], itemSetX, tidSetX)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        global _minSup, _maxPer, _lno, _maxLa
+        self._startTime = time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
+        print(_minSup, _maxPer, _maxLa)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for i in patterns:
+            sample = self._savePeriodic(i[0])
+            self._finalPatterns[sample] = i[1]
+        self._endTime = time.time()
+        process = psutil.Process(os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
+        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
 
     def Mine(self):
         """
-        Mining process will start from here
+        Mining process will start from this function
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        self._hashing = {}
-        periodicFrequentItems = self._scanDatabase()
-        for i in range(len(periodicFrequentItems)):
-            itemX = periodicFrequentItems[i]
-            if itemX is None:
-                continue
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(periodicFrequentItems)):
-                itemJ = periodicFrequentItems[j]
-                if itemJ is None:
-                    continue
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._minSup:
-                    continue
-                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
-                    periodicFrequentItems.insert(j, None)
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
-                    periodicFrequentItems.insert(j, None)
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-                else:
 
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            if len(itemSets) > 0:
-                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
-            self._save([], itemSetX, tidSetX)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        global _minSup, _maxPer, _lno, _maxLa
+        self._startTime = time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
+        print(_minSup, _maxPer, _maxLa)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for i in patterns:
+            sample = self._savePeriodic(i[0])
+            self._finalPatterns[sample] = i[1]
+        self._endTime = time.time()
+        process = psutil.Process(os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
+        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
+
 
     def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
@@ -572,66 +533,74 @@
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
     def save(self, outFile):
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of periodic-frequent patterns after completion of the mining process
 
-        :return: returning frequent patterns
+        :return: returning periodic-frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Total number of Closed Periodic Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-        
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(sys.argv) == 5 or len(sys.argv) == 6:
+        if len(sys.argv) == 6:
+            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
+        if len(sys.argv) == 5:
+            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4])
         _ap.startMine()
-        print("Total number of Closed Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.save(sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
+        '''ap = topk('https://www.u-aizu.ac.jp/~udayrage/datasets/temporalDatabases/temporal_retail.csv', 0.001, 0.005, 0.004)
+        #ap = topk('/Users/likhitha/Downloads/contextPrefixSpan.txt', 3, 6, 2, ' ')
+        ap.startMine()
+        Patterns = ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(Patterns))
+        ap.save('/Users/Likhitha/Downloads/output')
+        memUSS = ap.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = ap.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = ap.getRuntime()
+        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/closed/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/closed/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -153,10 +153,10 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
-        """ To print the results of execution """
+        """ TO print the results of execution """
 
         pass
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/maximal/__init__.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/maximal/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/pyspark/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py` & `pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,23 @@
+# k3PMiner is and algorithm to discover top - k partial periodic patterns in a temporal  database.
+#
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
+
 #
+#             import PAMI.partialPeriodicPattern.topk.k3PMiner as alg
 #
-#             import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
-#
-#             obj = alg.TopkPFPGrowth(iFile, k, maxPer,oFile)
+#             obj = alg.k3PMiner(iFile, k, periodicity)
 #
 #             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             partialPeriodicPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of top partial periodic Patterns:", len(partialPeriodicPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -23,15 +26,14 @@
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
-#
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -46,110 +48,121 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
+from PAMI.partialPeriodicPattern.topk import abstract as _abstract
+import validators as _validators
+from urllib.request import urlopen as _urlopen
+import sys as _sys
+
+from PAMI.partialPeriodicPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-
-class TopkPFPGrowth(_ab._periodicFrequentPatterns):
+class k3PMiner(_abstract.partialPeriodicPatterns):
     """
-    :Description:   Top - K is and algorithm to discover top periodic frequent patterns in a temporal database.
+    :Description:   k3PMiner is and algorithm to discover top - k partial periodic patterns in a temporal  database.
 
-    :Reference:   Komate Amphawan, Philippe Lenca, Athasit Surarerks: "Mining Top-K Periodic-Frequent Pattern from Transactional Databases without Support Threshold"
-                  International Conference on Advances in Information Technology: https://link.springer.com/chapter/10.1007/978-3-642-10392-6_3
+    :Reference:  Palla Likhitha,Rage Uday Kiran, Discovering Top-K Partial Periodic Patterns in Big Temporal Databases https://dl.acm.org/doi/10.1007/978-3-031-39847-6_28
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of periodic frequent pattern's
-    :param  maxPer: str:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  period: str:
+                   Minimum partial periodic...
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        k: int
-            User specified counte of top frequent patterns
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        oFile : str
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+            iFile : str
+                Input file name or path of the input file
+            k: int
+                User specified count of top partial periodic patterns
+            sep : str
+                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+                However, the users can override their default separator.
+            oFile : str
+                Name of the output file or the path of the output file
+            startTime:float
+                To record the start time of the mining process
+            endTime:float
+                To record the completion time of the mining process
+            finalPatterns: dict
+                Storing the complete set of patterns in a dictionary variable
+            memoryUSS : float
+                To store the total amount of USS memory consumed by the program
+            memoryRSS : float
+                To store the total amount of RSS memory consumed by the program
 
     :Methods:
 
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Generates one frequent patterns
-        eclatGeneration(candidateList)
-            It will generate the combinations of frequent items
-        generateFrequentPatterns(tidList)
-            It will generate the combinations of frequent items from a list of items
+            startMine()
+                Mining process will start from here
+            getPatterns()
+                Complete set of patterns will be retrieved with this function
+            save(oFile)
+                Complete set of frequent patterns will be loaded in to a output file
+            getPatternsAsDataFrame()
+                Complete set of frequent patterns will be loaded in to a dataframe
+            getMemoryUSS()
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
+            getMemoryRSS()
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
+            getRuntime()
+                Total amount of runtime taken by the mining process will be retrieved from this function
+            creatingItemSets()
+                Scans the dataset or dataframes and stores in list format
+            frequentOneItem()
+                Generates one frequent patterns
+            eclatGeneration(candidateList)
+                It will generate the combinations of frequent items
+            generateFrequentPatterns(tidList)
+                It will generate the combinations of frequent items from a list of items
 
     **Executing the code on terminal:**
     -------------------------------------
-   .. code-block:: console
+     .. code-block:: console
 
 
        Format:
 
-       (.venv) $ python3 TopkPFP.py <inputFile> <outputFile> <k> <maxPer>
+       python3 k3PMiner.py <iFile> <oFile> <k> <period>
 
        Examples:
 
-       (.venv) $ python3 TopkPFP.py sampleDB.txt patterns.txt 10 3
+       python3 k3PMiner.py sampleDB.txt patterns.txt 10 3
 
 
     **Sample run of the importing code:**
-    ---------------------------------------
-    .. code-block:: python
+    --------------------------------------
+    ...     code-block:: python
 
-            import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+            import PAMI.partialPeriodicPattern.topk.k3PMiner as alg
 
-            obj = alg.TopkPFPGrowth(iFile, k, maxPer)
+            obj = alg.Topk_PPPGrowth(iFile, k, period)
 
             obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            partialPeriodicPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of top partial periodic Patterns:", len(partialPeriodicPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -160,23 +173,23 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
+    ---------------
             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _k = int()
-    _maxPer = " "
+    _period = " "
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
@@ -184,46 +197,45 @@
     _lno = int()
     _minimum = int()
     _mapSupport = {}
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
-
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+        if isinstance(self._iFile, _abstract._pd.DataFrame):
+            timeStamp, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
+                timeStamp = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
-            if 'Patterns' in i:
-                data = self._iFile['Patterns'].tolist()
             for i in range(len(data)):
-                tr = [ts[i][0]]
+                tr = [timeStamp[i]]
                 tr = tr + data[i]
                 self._Database.append(tr)
+            self._lno = len(self._Database)
+            # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _validators.url(self._iFile):
+                data = _urlopen(self._iFile)
                 for line in data:
-                    line.strip()
+                    self._lno += 1
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            self._lno += 1
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
@@ -249,65 +261,67 @@
     def _frequentOneItem(self):
         """
         Generating one frequent patterns
         """
 
         self._mapSupport = {}
         self._tidList = {}
-        n = 0
+        self._period = self._convert(self._period)
+        self._k = int(self._convert(self._k))
         for line in self._Database:
-            self._lno += 1
             n = int(line[0])
             for i in range(1, len(line)):
                 si = line[i]
                 if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
+                    self._mapSupport[si] = [1, 0, n]
                     self._tidList[si] = [n]
                 else:
                     self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    period = abs(n - self._mapSupport[si][2])
+                    if period <= self._period:
+                        self._mapSupport[si][1] += 1
                     self._mapSupport[si][2] = n
                     self._tidList[si].append(n)
         for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
-        self._maxPer = self._convert(self._maxPer)
-        self._k = self._convert(self._k)
-        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if v[1] <= self._maxPer}
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+            period = abs(self._lno - self._mapSupport[x][2])
+            if period <= self._period:
+                self._mapSupport[x][1] += 1
+        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items()}
+        #print(self._mapSupport)
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        #print(plist)
         self._finalPatterns = {}
-        #print(len(plist))
         for i in plist:
+            if self._mapSupport[i] == 0:
+                continue
             if len(self._finalPatterns) >= self._k:
                 break
             else:
-                self._finalPatterns[i] = [self._mapSupport[i][0], self._mapSupport[i][1]]
-        self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+                self._finalPatterns[i] = self._mapSupport[i]
+        #print(len(self._finalPatterns),  self._k, self._periodicity)
+        #print(self._finalPatterns)
+        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+        #print(self._minimum)
         plist = list(self._finalPatterns.keys())
         return plist
 
     def _getSupportAndPeriod(self, timeStamps):
         """To calculate the periodicity and support
 
         :param timeStamps: Timestamps of an item set
         :return: support, periodicity
         """
 
-        global lno
         timeStamps.sort()
-        cur = 0
-        per = list()
         sup = 0
-        for j in range(len(timeStamps)):
-            per.append(timeStamps[j] - cur)
-            cur = timeStamps[j]
-            sup += 1
-        per.append(self._lno - cur)
-        if len(per) == 0:
-            return [0, 0]
-        return [sup, max(per)]
+        for j in range(len(timeStamps) - 1):
+            per = abs(timeStamps[j + 1] - timeStamps[j])
+            if per <= self._period:
+                sup += 1
+        return sup
 
     def _save(self, prefix, suffix, tidSetI):
         """Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
         :type prefix: list
         :param suffix: the suffix of a patterns
@@ -316,42 +330,47 @@
         :type tidSetI: list
         """
 
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
+        #print(prefix)
+        #print(self._minimum)
         val = self._getSupportAndPeriod(tidSetI)
         sample = str()
         for i in prefix:
-            sample = sample + i + " "
+            sample = sample + i + "\t"
         if len(self._finalPatterns) < self._k:
-            if val[0] >= self._minimum:
+            if val > self._minimum:
                 self._finalPatterns[sample] = val
                 self._finalPatterns = {k: v for k, v in
-                                  sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+                                       sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+                #print(self._finalPatterns)
         else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1][0]):
-                if val[0] > y[0]:
+            #print(prefix, val)
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
+                if val > y:
+                    #print("yes")
                     del self._finalPatterns[x]
-                    self._finalPatterns[x] = y
+                    self._finalPatterns[sample] = val
                     self._finalPatterns = {k: v for k, v in
-                                          sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                    self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+                                           sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                    self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+                    #print(self._finalPatterns)
                     return
 
     def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
 
         :param prefix:  main equivalence prefix
         :type prefix: periodic-frequent item or pattern
         :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
-                            and frequent with their timestamps
+                        and frequent with their timestamps
         :type itemSets: list
         :param tidSets: timestamps of the items in the argument itemSets
         :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidI = tidSets[0]
@@ -366,89 +385,91 @@
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetI).intersection(tidSetJ))
                 val = self._getSupportAndPeriod(y)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
+                if val > self._minimum:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
             newPrefix = list(set(itemSetX)) + prefix
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetI)
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main function of the program
+
         """
-        self._startTime = _ab._time.time()
+        self._startTime = _abstract._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._k is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        _plist = self._frequentOneItem()
-        for i in range(len(_plist)):
-            itemI = _plist[i]
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
             tidSetI = self._tidList[itemI]
             itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            for j in range(i + 1, len(_plist)):
-                itemJ = _plist[j]
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetI).intersection(tidSetJ))
                 val = self._getSupportAndPeriod(y1)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
+                if val > self._minimum:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-        print("TopK Periodic Frequent patterns were generated successfully")
-        self._endTime = _ab._time.time()
-        _process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
+        print("TopK partial periodic patterns were generated successfully")
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
         self._memoryUSS = float()
-        self._memoryUSS = _process.memory_full_info().uss
-        self._memoryRSS = _process.memory_info().rss
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def Mine(self):
-        """
-        Main function of the program
-        """
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        _plist = self._frequentOneItem()
-        for i in range(len(_plist)):
-            itemI = _plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(_plist)):
-                itemJ = _plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                val = self._getSupportAndPeriod(y1)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("TopK Periodic Frequent patterns were generated successfully")
-        self._endTime = _ab._time.time()
-        _process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = _process.memory_full_info().uss
-        self._memoryRSS = _process.memory_info().rss
+            """
+            Main function of the program
+
+            """
+            self._startTime = _abstract._time.time()
+            if self._iFile is None:
+                raise Exception("Please enter the file path or file name:")
+            if self._k is None:
+                raise Exception("Please enter the Minimum Support")
+            self._creatingItemSets()
+            plist = self._frequentOneItem()
+            for i in range(len(plist)):
+                itemI = plist[i]
+                tidSetI = self._tidList[itemI]
+                itemSetX = [itemI]
+                itemSets = []
+                tidSets = []
+                for j in range(i + 1, len(plist)):
+                    itemJ = plist[j]
+                    tidSetJ = self._tidList[itemJ]
+                    y1 = list(set(tidSetI).intersection(tidSetJ))
+                    val = self._getSupportAndPeriod(y1)
+                    if val > self._minimum:
+                        itemSets.append(itemJ)
+                        tidSets.append(y1)
+                self._Generation(itemSetX, itemSets, tidSets)
+            print("TopK partial periodic patterns were generated successfully")
+            self._endTime = _abstract._time.time()
+            process = _abstract._psutil.Process(_abstract._os.getpid())
+            self._memoryUSS = float()
+            self._memoryRSS = float()
+            self._memoryUSS = process.memory_full_info().uss
+            self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -470,63 +491,64 @@
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """
-        Storing final frequent patterns in a dataframe
+        """Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a.replace('\t', ' '), b])
+            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+        """Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.replace(' ', '\t') + ":" + f'{y[0]}:{y[1]}'
+            patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        print("Top K Periodic Frequent Patterns:", len(self.getPatterns()))
+        """ This function is used to print the results
+        """
+        print("Top K Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
+        if len(_sys.argv) == 6:
+            _ap = k3PMiner(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
+        if len(_sys.argv) == 5:
+            _ap = k3PMiner(_sys.argv[1], _sys.argv[3], _sys.argv[4])
         _ap.startMine()
-        print("Top K Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Top K Partial Periodic Patterns:", len(_ap.getPatterns()))
+        _ap.save(_sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py` & `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,24 +1,28 @@
+# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
+# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
+# greater than maximum liability.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-
-#             import PAMI.periodicFrequentPattern.kPFPMiner as alg
 #
-#             obj = alg.kPFPMiner(iFile, k)
+#             from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+#
+#             obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
 #
 #             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#             obj.save(oFile)
+#             obj.save("patterns")
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -45,164 +49,205 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
+from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
 
-
-class kPFPMiner(_ab._periodicFrequentPatterns):
+class SPPEclat(_ab._stablePeriodicFrequentPatterns):
     """
-    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
+    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
+                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
+                    greater than maximum lability.
 
-    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
-                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
-                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
+    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
+                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
+                   Name of the Input file to mine complete set of stable periodic Frequent Pattern.
     :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-
+                   Name of the output file to store complete set of stable periodic Frequent Pattern.
+    :param  minSup: float or int or str :
+                    The user can specify minSup either in count or proportion of database size.
+                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                    Otherwise, it will be treated as float.
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  itemSup: int or float :
+                    Frequency of an item
+    :param maxLa: float :
+                  minimum loss of a pattern
     :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+                 This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        k: int
-            User specified counte of top-k periodic frequent patterns
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        maxLa : int or float or str
+            The user can specify maxLa either in count or proportion of database size.
+            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : str
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
+        finalPatterns : dict
+            it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Generates one frequent patterns
-        eclatGeneration(candidateList)
-            It will generate the combinations of frequent items
-        generateFrequentPatterns(tidList)
-            It will generate the combinations of frequent items from a list of items
+            Scan the database and store the items with their timestamps which are periodic frequent
+        calculateLa()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+
 
-    **Executing the code on terminal:**
-    ------------------------------------------
+
+    **Methods to execute code on terminal**
+    -----------------------------------------
     .. code-block:: console
 
 
        Format:
 
+       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
 
-       (.venv) $ python3 kPFPMiner.py <inputFile> <outputFile> <k>
+       Example usage:
 
-       Examples :
+       (.venv) $ python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
 
-       (.venv) $  python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
+               .. note:: constraints will be considered in percentage of database transactions
 
-    **Sample run of the importing code:
-    --------------------------------------
-    .. code-block:: python
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    ... code-block:: python
 
-            import PAMI.periodicFrequentPattern.kPFPMiner as alg
+                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
 
-            obj = alg.kPFPMiner(iFile, k)
+                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
 
-            obj.startMine()
+                    obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+                    Patterns = obj.getPatterns()
 
-            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 
-            obj.save(oFile)
+                    obj.save("patterns")
 
-            Df = obj.getPatternInDataFrame()
+                    Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+                    memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+                    print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+                    memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+                    print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+                    run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+                    print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
     --------------
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
-    """
+             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
 
-    _startTime = float()
-    _endTime = float()
-    _k = int()
-    _finalPatterns = {}
+       """
     _iFile = " "
     _oFile = " "
+    _minSup = str()
+    _maxPer = str()
+    _maxLa = float()
     _sep = " "
+    _SPPList = {}
+    _itemList = []
+    _last = int()
+    _finalPatterns = {}
+    _tsList = {}
+    _startTime = float()
+    _endTime = float()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
-    lno = int()
-    _maximum = int()
 
-    def _creatingItemSets(self):
+    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
+        self._iFile = inputFile
+        self._minSup = minSup
+        self._maxPer = maxPer
+        self._maxLa = maxLa
+        self._sep = sep
+
+    def _creatingItemsets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
+            if 'Patterns' in i:
+                self._Database = self._iFile['Patterns'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -215,132 +260,14 @@
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-                    
-    def getPer_Sup(self, tids):
-        tids.sort()
-        cur=0
-        per=list()
-        sup=0
-        #print(tids)
-        for i in range(len(tids)-1):
-            j = i + 1
-            #if tids[j] - cur <= periodicity:
-                #return [0,0]
-            per.append(tids[j] - cur)
-            cur = tids[j]
-        per.append(self.lno - cur)
-        return max(per)
-
-    def _frequentOneItem(self):
-        """
-        Generating one frequent patterns
-        """
-        self._mapSupport = {}
-        self._tidList = {}
-        n = 0
-        for line in self._Database:
-            self.lno += 1
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
-                    self._tidList[si] = [n]
-                else:
-                    self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = self._mapSupport[i][1]
-        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-
-    def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: list
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self.getPer_Sup(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + " "
-        if len(self._finalPatterns) < self._k:
-            if val < self._maximum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._maximum = max([i for i in self._finalPatterns.values()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
-                if val < y:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[sample] = val
-                    self._finalPatterns = {k: v for k, v in
-                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
-                                                     reverse=True)}
-                    self._maximum = max([i for i in self._finalPatterns.values()])
-                    return
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y) <= self._maximum:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def _convert(self, value):
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
         :return: converted type
@@ -348,137 +275,206 @@
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = ((len(self._Database)) * value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    def _createSPPList(self):
+        """
+        to convert the single length stable periodic patterns
+        """
+        tidLast = {}
+        la = {}
+        self._SPPList = {}
+        self._tsList = {}
+        for transaction in self._Database:
+            ts = int(transaction[0])
+            for item in transaction[1:]:
+                if item not in self._SPPList:
+                    la[item] = max(0, ts - self._maxPer)
+                    self._SPPList[item] = [1, la[item]]
+                    self._tsList[item] = [ts]
+                else:
+                    s = self._SPPList[item][0] + 1
+                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
+                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
+                    self._tsList[item].append(ts)
+                tidLast[item] = ts
+            self._last = ts
+        for item in self._SPPList:
+            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
+            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
+        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
+        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
+        self._Generation(list(self._SPPList), set())
+
+    def _Generation(self, GPPFList, CP):
+        """
+        To generate the patterns using depth-first search
+        """
+        for i in range(len(GPPFList)):
+            item = GPPFList[i]
+            CP1 = CP | {item}
+            if CP != set():
+                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
+            la = self._calculateLa(self._tsList['\t'.join(CP1)])
+            support = len(self._tsList['\t'.join(CP1)])
+            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
+                #CP = CP1
+                self._finalPatterns['\t'.join(CP1)] = [support, la]
+                if i+1 < len(GPPFList):
+                    self._Generation(GPPFList[i+1:], CP1)
+
+    def _calculateLa(self, tsList):
+        """
+        To calculate the liability of a patterns based on its timestamps
+        """
+        previous = 0
+        la = 0
+        tsList = sorted(tsList)
+        laList = []
+        for ts in tsList:
+            la = max(0, la + ts - previous - self._maxPer)
+            laList.append(la)
+            previous = ts
+            
+        la = max(0, la + self._last - previous - self._maxPer)
+        laList.append(la)
+        maxla = max(laList)
+        return maxla
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Main function of the program
+        Method to start the mining of patterns
+        """
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        self._finalPatterns = {}
+        #print(self._minSup, self._maxPer, self._maxLa)
+        self._createSPPList()
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
 
+    def Mine(self):
+        """
+        Method to start the mining of patterns
         """
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._k = self._convert(self._k)
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y1) <= self._maximum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("kPFPMiner has successfully generated top-k frequent patterns")
+        self._creatingItemsets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        self._finalPatterns = {}
+        #print(self._minSup, self._maxPer, self._maxLa)
+        self._createSPPList()
         self._endTime = _ab._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-        :return: returning USS memory consumed by the mining process
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
+        return self._endTime - self._startTime
 
-        return self._memoryUSS
+    def getPatterns(self):
+        """
+        Function to return the set of stable periodic-frequent patterns after completion of the mining process
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning stable periodic-frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-        :return: returning RSS memory consumed by the mining process
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        return self._memoryUSS
 
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def save(self, outFile):
         """
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
-        return self._endTime - self._startTime
+        :param outFile: name of the output file
+        :type outFile: csv file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
-
-        :param outFile: name of the output file
-
-        :type outFile: file
+    def getMemoryRSS(self):
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
-
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
         """
-        return self._finalPatterns
+
+        return self._memoryRSS
 
     def printResults(self):
-        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
-
-if __name__ == "__main__":
+if __name__ == '__main__':
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+        if len(_ab._sys.argv) == 6:
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
+        print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

### Comparing `pami-2024.4.24.1/PAMI/recurringPattern/basic/RPGrowth.py` & `pami-2024.4.9.1/PAMI/recurringPattern/basic/RPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/recurringPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/recurringPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py` & `pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/relativeFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py` & `pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/relativeHighUtilityPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/SPADE.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPADE.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/SPAM.py` & `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,21 +1,20 @@
-# SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
-# This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
-#  This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
-# **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# cudaAprioriTID is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+#
 #
+# **Importing this algorithm into a python program**
+# ----------------------------------------------------
 #
-#             import PAMI.sequentialPatternMining.basic.SPAM as alg
+#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 #
-#             obj = alg.SPAM(iFile, minSup)
+#             obj = alg.cuAprioriBit(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
-#             sequentialPatternMining = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
@@ -30,494 +29,573 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 
-import pandas as pd
 from deprecated import deprecated
+import abstract as _ab
+
+import os
+import csv
+import time
+import numpy as np
+import pycuda.gpuarray as _gpuarray
+import pycuda.autoinit
+import psutil
+import pycuda.driver as cuda
+from pycuda.compiler import SourceModule
+import pycuda
+
+deviceIntersection = SourceModule("""
+    __global__ void intersection(int *compareThis, int *compareThat, int *resultStart,
+                                 int *values, int *result, int resultX, int resultY){
+        const int tidX = blockIdx.x * blockDim.x + threadIdx.x;
+        const int tidY = blockIdx.y * blockDim.y + threadIdx.y;
+        int resultIndex = resultStart[tidX] + tidY;
+
+        // ignore if tidX or tidY is out of bounds or if the value comparing with is 0
+        if (tidX > resultX-1 || tidY > resultY-1 || values[compareThis[tidX] + tidY] == 0) return;
+
+        for (int i = 0; i < resultY; i++){
+            if ( values[compareThat[tidX] + i] == 0) return;
+            if ( values[compareThis[tidX] + tidY] == values[compareThat[tidX] + i]){
+                result[resultIndex] = values[compareThis[tidX] + tidY];
+                return;
+            }
+        }
+
+        //result[resultIndex] = values[compareThis[tidX] + tidY];
+
+    }
+
+"""
+                                  )
 
-from PAMI.sequentialPatternMining.basic import abstract as _ab
-_ab._sys.setrecursionlimit(10000)
 
-class SPAM(_ab._sequentialPatterns):
+class cudaAprioriTID:
     """
-    :Description:    SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
-                     This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
-                     This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
+    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
-    :Reference:   J. Ayres, J. Gehrke, T.Yiu, and J. Flannick. Sequential Pattern Mining Using Bitmaps. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Edmonton, Alberta, Canada, July 2002.
+    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
+            In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of  Sequential frequent patterns
+                   Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of  Sequential frequent patterns
-    :param  minSup: float or int or str :
-                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
-                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-            iFile : str
-                Input file name or path of the input file
-            oFile : str
-                Name of the output file or the path of output file
-            minSup : float or int or str
-                The user can specify minSup either in count or proportion of database size.
-                If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                Otherwise, it will be treated as float.
-                Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-            sep : str
-                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-                However, the users can override their default separator.
-            startTime : float
-                To record the start time of the mining process
-            endTime : float
-                To record the completion time of the mining process
-            finalPatterns : dict
-                Storing the complete set of patterns in a dictionary variable
-            memoryUSS : float
-                To store the total amount of USS memory consumed by the program
-            memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
-            Database : list
-                To store the sequences of a database in list
-            _idDatabase : dict
-                To store the sequences of a database by bit map
-            _maxSeqLen:
-                the maximum length of subsequence in sequence.
-
-    :Methods:
-
-            _creatingItemSets():
-                Storing the complete sequences of the database/input file in a database variable
-            _convert(value):
-                To convert the user specified minSup value
-            make2BitDatabase():
-                To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
-            DfsPruning(items,sStep,iStep):
-                the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
-            Sstep(s):
-                To convert bit to ssteo bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
-            startMine()
-                Mining process will start from here
-            getPatterns()
-                Complete set of patterns will be retrieved with this function
-            savePatterns(oFile)
-                Complete set of frequent patterns will be loaded in to a output file
-            getPatternsAsDataFrame()
-                Complete set of frequent patterns will be loaded in to a dataframe
-            getMemoryUSS()
-                Total amount of USS memory consumed by the mining process will be retrieved from this function
-            getMemoryRSS()
-                Total amount of RSS memory consumed by the mining process will be retrieved from this function
-            getRuntime()
-                Total amount of runtime taken by the mining process will be retrieved from this function
-            candidateToFrequent(candidateList)
-                Generates frequent patterns from the candidate patterns
-            frequentToCandidate(frequentList, length)
-                Generates candidate patterns from the frequent patterns
+        startTime : float
+          To record the start time of the mining process
+
+        endTime : float
+          To record the completion time of the mining process
+
+        finalPatterns : dict
+          Storing the complete set of patterns in a dictionary variable
+
+        memoryUSS : float
+          To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+          To store the total amount of RSS memory consumed by the program
 
+        Database : list
+          To store the transactions of a database in list
+
+
+
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
 
-    **Executing the code on terminal**:
-    ----------------------------------------
     .. code-block:: console
 
+      Format:
+
+      (.venv) $ python3 cudaAprioriTID.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
 
-       Format:
+      (.venv) $ python3 cudaAprioriTID.py sampleDB.txt patterns.txt 10.0
 
-       (.venv) $ python3 SPAM.py <inputFile> <outputFile> <minSup> (<separator>)
+    .. note:: minSup will be considered in percentage of database transactions
 
-       Examples usage:
 
-       (.venv) $ python3 SPAM.py sampleDB.txt patterns.txt 10.0
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
 
+    .. code-block:: python
 
-               .. note:: minSup will be considered in times of minSup and count of database transactions
+             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 
-    **Sample run of the importing code**:
-    -------------------------------------
-            import PAMI.sequentialPatternMining.basic.SPAM as alg
+             obj = alg.cuAprioriBit(iFile, minSup)
 
-            obj = alg.SPAM(iFile, minSup)
+             obj.mine()
 
-            obj.startMine()
+             frequentPatterns = obj.getPatterns()
 
-            sequentialPatternMining = obj.getPatterns()
+             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+             obj.save(oFile)
 
-            obj.savePatterns(oFile)
+             Df = obj.getPatternInDataFrame()
 
-            Df = obj.getPatternInDataFrame()
+             memUSS = obj.getMemoryUSS()
 
-            memUSS = obj.getMemoryUSS()
+             print("Total Memory in USS:", memUSS)
 
-            print("Total Memory in USS:", memUSS)
+             memRSS = obj.getMemoryRSS()
 
-            memRSS = obj.getMemoryRSS()
+             print("Total Memory in RSS", memRSS)
 
-            print("Total Memory in RSS", memRSS)
+             run = obj.getRuntime()
 
-            run = obj.getRuntime()
+             print("Total ExecutionTime in seconds:", run)
 
-            print("Total ExecutionTime in seconds:", run)
 
-    **Credits**:
-    ------------
-            The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
+    **Credits:**
+    -------------
+
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    _minSup = float()
-    _startTime = float()
-    _endTime = float()
-    _finalPatterns = {}
+    __time = 0
+    __memRSS = 0
+    __memUSS = 0
+    __GPU_MEM = 0
+    filePath = ""
     _iFile = " "
-    _oFile = " "
-    _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _idDatabase={}
-    _maxSeqLen=0
+    _sep = ""
+    _minSup = 0
+    Patterns = {}
+
+    def __init__(self, filePath, sep, minSup):
+        self.filePath = filePath
+        self.sep = sep
+        self.minSup = minSup
+        self.__time = 0
+        self.__memRSS = 0
+        self.__memUSS = 0
+
     def _creatingItemSets(self):
         """
-        Storing the complete sequences of the database/input file in a database variable
+        Storing the complete transactions of the database/input file in a database variable
         """
-        self._Database = []
-
+        self._Database = {}
+        lineNumber = 1
         if isinstance(self._iFile, _ab._pd.DataFrame):
             temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 temp = self._iFile['Transactions'].tolist()
-            if "tid" in i:
-                temp2=self._iFile[''].tolist()
-            addList=[]
-            addList.append(temp[0])
-            for k in range(len(temp)-1):
-                if temp2[k]==temp[k+1]:
-                    addList.append(temp[k+1])
-                else:
-                    self._Database.append(addList)
-                    addList=[]
-                    addList.append(temp[k+1])
-            self._Database.append(addList)
+
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
+                    for i in range(len(line)):
+                        if line[i] in self._Database:
+                            self._Database[i].append(lineNumber)
+                        else:
+                            self._Database[i] = [lineNumber]
+                    lineNumber += 1
+
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    temp.pop()
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
-                            temp = [i.rstrip() for i in line.split('-1')]
-                            temp = [x for x in temp if x ]
-                            temp.pop()
-
-                            seq = []
-                            for i in temp:
-                                k = -2
-                                if len(i)>1:
-                                    seq.append(list(sorted(set(i.split()))))
-
-                                else:
-                                    seq.append(i)
-
-                            self._Database.append(seq)
-
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
-        To convert the user specified minSup value
+
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    """def _readFile(self, fileName, separator):
+        
+        Reads a file and stores the data in a dictionary
+
+        Args:
+            fileName: string
+            separator: string
+
+        Returns:
+            dictionary: dictionary
+        
+        file = open(fileName, 'r')
+        dictionary = {}
+        lineNumber = 1
+        for line in file:
+            line = line.strip()
+            line = line.split(separator)
+            for i in range(len(line)):
+                if line[i] in dictionary:
+                    dictionary[line[i]].append(lineNumber)
+                else:
+                    dictionary[line[i]] = [lineNumber]
+            lineNumber += 1
 
-    def make2BitDatabase(self):
-        """
-        To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
-        """
-        self._maxSeqLen=max([len(i) for i in self._Database])
-        lineNumber=0
-        idDatabase={}
-        for line in self._Database:
-            seqNumber=1
-            for seq in line:
-
-                for data in seq:
-                    if data in idDatabase:
-                        while lineNumber+1!=len(idDatabase[data]):
-                            idDatabase[data].append(0)
-                        idDatabase[data][lineNumber]+=int(2**(self._maxSeqLen-seqNumber))
-
-                    else:
-                        idDatabase[data]=[]
-                        while lineNumber+1!=len(idDatabase[data]):
-                            idDatabase[data].append(0)
-                        idDatabase[data][lineNumber]+=(int(2 ** (self._maxSeqLen-seqNumber)))
-
-                seqNumber+=1
-            lineNumber+=1
-        for key,val in idDatabase.items():
-
-            sup=self.countSup(val)
-            while lineNumber+1!=len(idDatabase[key]):
-                            idDatabase[key].append(0)
-            if sup>=self._minSup:
-                self._finalPatterns[str(key)+self._sep+"-2"]=sup
-                self._idDatabase[str(key)]=val
-
-    def DfsPruning(self,items,sStep,iStep):
-        """
-        the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
-
-        :Attributes:
-
-        items : str
-            The pattrens I got before
-        sStep : list
-            Items presumed to have "sstep" relationship with "items".(sstep is What appears later like a-b and a-c)
-        iStep : list
-            Items presumed to have "istep" relationship with "items"(istep is What appears in same time like ab and ac)
-
-        """
-        Snext=[]
-        Inext=[]
-        ns = self.Sstep(self._idDatabase[items])
-        for i in sStep:
-            nnext=[]
-            for k in  range(len(self._idDatabase[items])):
-                nandi=ns[k] & self._idDatabase[i][k]
-                nnext.append(nandi)
-
-
-            sup=self.countSup(nnext)
-            if sup>=self._minSup:
-                key=items+self._sep+"-1"+self._sep+i
-                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
-                self._idDatabase[key]=nnext
-                Snext.append(i)
-
-        for i in Snext:
-            key = items+self._sep+"-1"+self._sep+i
-            self.DfsPruning(key,Snext,[k for k in Snext if self._Database.index(i)<self._Database.index(k)])
-        for i in iStep:
-            nnext = []
-
-            for k in range(len(self._idDatabase[items])):
-                nandi = self._idDatabase[items][k] & self._idDatabase[i][k]
-                nnext.append(nandi)
-            sup=self.countSup(nnext)
-            if sup>=self._minSup:
-                key=items+self._sep+str(i)
-                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
-                self._idDatabase[key]=nnext
-                Inext.append(i)
-        for i in Inext:
-            key = items +self._sep +str(i)
-            self.DfsPruning(key,Snext,[k for k in Inext if self._Database.index(i)<self._Database.index(k)])
-
-    def Sstep(self,s):
-        """
-        To convert bit to Sstep bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
-
-
-        :param s:list
-            to store each bit sequence
-        :return:
-            nextS:list to store the bit sequence converted by sstep
-
-        """
-        nextS=[]
-        for bins in s:
-            binS=str(bin(bins))
-
-
-            LenNum=2
-            for i in range(len(binS)-2):
-                if binS[LenNum] == "1":
-
-                    binS = binS[:LenNum] + "0" + binS[LenNum + 1:]
-                    while len(binS)-1!=LenNum:
-                        LenNum += 1
-                        binS = binS[:LenNum] + "1" + binS[LenNum + 1:]
-                    break
-                LenNum+=1
-            nextS.append(int(binS, 0))
-
-
-        return nextS
-
-    def countSup(self,n):
-        """
-        count support
-
-        :param n:list
-                to store each bit sequence
-        :return:
-            count: int support of this list
-        """
-        count=0
-        for i in n:
-            if "1" in str(bin(i)):
-                count+=1
-        return count
-
-    def startMine(self):
+        # sort dictionary by size of values
+        dictionary = dict(
+            sorted(dictionary.items(), key=lambda x: len(x[1]), reverse=True))
+        return dictionary, lineNumber
         """
-        Frequent pattern mining process will start from here
+    def getRuntime(self):
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self.make2BitDatabase()
-        self._Database = [i for i in self._idDatabase.keys()]
-        for i in self._Database:
-            x=[]
-            for j in self._Database:
-                if self._Database.index(i)<self._Database.index(j):
-                    x.append(j)
-
-            self.DfsPruning(i,self._Database,x)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Apriori algorithm ")
-
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of time taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__time
 
     def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memRSS
 
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memUSS
 
-        return self._endTime - self._startTime
+    def getGPUMemory(self):
+        """
+        To calculate the total memory consumed by GPU
+        :return: return GPU memory
+        :rtype: int
+        """
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
-        :return: returning frequent patterns in a dataframe
-        :rtype: pd.DataFrame
-        """
-
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
-
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the output file
-        :type outFile: file
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
-            writer.write("%s \n" % s1)
+        return self.__GPU_MEM
 
     def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """
+        Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.Patterns
+
+    def get_numberOfPatterns(self):
+        return len(self.Patterns)
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        dev_Intersection = deviceIntersection.get_function("intersection")
+        startTime = time.time()
+        final = {}
+
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+
+
+        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
+        for key, value in data.items():
+            final[key] = len(value)
+
+        while len(data) > 1:
+            # sort data by size of values
+            data = dict(
+                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
+
+            values = list(data.values())
+            maxLength = values[0]
+            for i in range(1, len(values)):
+                while len(values[i]) != len(maxLength):
+                    values[i].append(0)
+
+            values = np.array(values)
+            resultSize = 0
+
+            compareThis = []
+            compareThat = []
+            resultStart = []
+            counter = 0
+
+            for i in range(len(values)):
+                for j in range(i+1, len(values)):
+                    resultSize += 1
+                    compareThis.append(i*len(maxLength))
+                    compareThat.append(j*len(maxLength))
+                    resultStart.append(counter)
+                    counter += len(maxLength)
+            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
+
+            # convert all to uint32
+            compareThis = np.array(compareThis, dtype=np.uint32)
+            compareThat = np.array(compareThat, dtype=np.uint32)
+            resultStart = np.array(resultStart, dtype=np.uint32)
+            values = np.array(values, dtype=np.uint32)
+            result = np.array(result, dtype=np.uint32)
+
+            # allocate memory on GPU
+            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
+            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
+            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
+            values_gpu = cuda.mem_alloc(values.nbytes)
+            result_gpu = cuda.mem_alloc(result.nbytes)
+
+            # add all nbytes to GPU_MEM
+            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
+            if sumBytes > self.__GPU_MEM:
+                self.__GPU_MEM = sumBytes
+
+            # copy data to GPU
+            cuda.memcpy_htod(compareThis_gpu, compareThis)
+            cuda.memcpy_htod(compareThat_gpu, compareThat)
+            cuda.memcpy_htod(resultStart_gpu, resultStart)
+            cuda.memcpy_htod(values_gpu, values)
+            cuda.memcpy_htod(result_gpu, result)
+
+            blockDim = (32, 32, 1)
+            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
+
+            dev_Intersection(compareThis_gpu, compareThat_gpu,
+                             resultStart_gpu, values_gpu, result_gpu,
+                             np.uint32(resultSize), np.uint32(len(maxLength)),
+                             block=blockDim, grid=gridDim)
+
+            # copy data back to CPU
+            cuda.Context.synchronize()
+            cuda.memcpy_dtoh(result, result_gpu)
+
+            # free GPU memory
+            cuda.DeviceAllocation.free(compareThis_gpu)
+            cuda.DeviceAllocation.free(compareThat_gpu)
+            cuda.DeviceAllocation.free(resultStart_gpu)
+            cuda.DeviceAllocation.free(values_gpu)
+            cuda.DeviceAllocation.free(result_gpu)
+
+            keys = list(data.keys())
+            # convert all to string and add " "
+            for i in range(len(keys)):
+                keys[i] = str(keys[i]) + " "
+            data = {}
+            index = 0
+            for i in range(len(keys)):
+                for j in range(i+1, len(keys)):
+                    newResult = list(sorted(set(result[index])))
+                    newResult = list(filter(lambda x: x > 0, newResult))
+                    if len(newResult) >= self.minSup:
+                        keyI = keys[i].split()
+                        keyJ = keys[j].split()
+                        combinedKey = " ".join(list(str(x) for x in (
+                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
+                        if combinedKey not in final:
+                            data[combinedKey] = newResult
+                            final[combinedKey] = len(newResult)
+                    index += 1
+
+
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self.Patterns = final
 
-    def printResults(self):
+    def mine(self):
         """
-        This function is used to print the results
+        Frequent pattern mining process will start from here
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        dev_Intersection = deviceIntersection.get_function("intersection")
+        startTime = time.time()
+        final = {}
+
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+
+
+        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
+        for key, value in data.items():
+            final[key] = len(value)
+
+        while len(data) > 1:
+            # sort data by size of values
+            data = dict(
+                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
+
+            values = list(data.values())
+            maxLength = values[0]
+            for i in range(1, len(values)):
+                while len(values[i]) != len(maxLength):
+                    values[i].append(0)
+
+            values = np.array(values)
+            resultSize = 0
+
+            compareThis = []
+            compareThat = []
+            resultStart = []
+            counter = 0
+
+            for i in range(len(values)):
+                for j in range(i+1, len(values)):
+                    resultSize += 1
+                    compareThis.append(i*len(maxLength))
+                    compareThat.append(j*len(maxLength))
+                    resultStart.append(counter)
+                    counter += len(maxLength)
+            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
+
+            # convert all to uint32
+            compareThis = np.array(compareThis, dtype=np.uint32)
+            compareThat = np.array(compareThat, dtype=np.uint32)
+            resultStart = np.array(resultStart, dtype=np.uint32)
+            values = np.array(values, dtype=np.uint32)
+            result = np.array(result, dtype=np.uint32)
+
+            # allocate memory on GPU
+            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
+            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
+            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
+            values_gpu = cuda.mem_alloc(values.nbytes)
+            result_gpu = cuda.mem_alloc(result.nbytes)
+
+            # add all nbytes to GPU_MEM
+            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
+            if sumBytes > self.__GPU_MEM:
+                self.__GPU_MEM = sumBytes
+
+            # copy data to GPU
+            cuda.memcpy_htod(compareThis_gpu, compareThis)
+            cuda.memcpy_htod(compareThat_gpu, compareThat)
+            cuda.memcpy_htod(resultStart_gpu, resultStart)
+            cuda.memcpy_htod(values_gpu, values)
+            cuda.memcpy_htod(result_gpu, result)
+
+            blockDim = (32, 32, 1)
+            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
+
+            dev_Intersection(compareThis_gpu, compareThat_gpu,
+                             resultStart_gpu, values_gpu, result_gpu,
+                             np.uint32(resultSize), np.uint32(len(maxLength)),
+                             block=blockDim, grid=gridDim)
+
+            # copy data back to CPU
+            cuda.Context.synchronize()
+            cuda.memcpy_dtoh(result, result_gpu)
+
+            # free GPU memory
+            cuda.DeviceAllocation.free(compareThis_gpu)
+            cuda.DeviceAllocation.free(compareThat_gpu)
+            cuda.DeviceAllocation.free(resultStart_gpu)
+            cuda.DeviceAllocation.free(values_gpu)
+            cuda.DeviceAllocation.free(result_gpu)
+
+            keys = list(data.keys())
+            # convert all to string and add " "
+            for i in range(len(keys)):
+                keys[i] = str(keys[i]) + " "
+            data = {}
+            index = 0
+            for i in range(len(keys)):
+                for j in range(i+1, len(keys)):
+                    newResult = list(sorted(set(result[index])))
+                    newResult = list(filter(lambda x: x > 0, newResult))
+                    if len(newResult) >= self.minSup:
+                        keyI = keys[i].split()
+                        keyJ = keys[j].split()
+                        combinedKey = " ".join(list(str(x) for x in (
+                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
+                        if combinedKey not in final:
+                            data[combinedKey] = newResult
+                            final[combinedKey] = len(newResult)
+                    index += 1
+
+
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self.Patterns = final
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_Patterns))
-        _ap.savePatterns(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        _ap.mine()
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("GPU MEM: ", _ap.getGPUMemory())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/abstract.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/sequentialPatternMining/basic/prefixSpan.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/prefixSpan.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/sequentialPatternMining/closed/abstract.py` & `pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,41 +1,38 @@
-# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
-# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
-# greater than maximum liability.
+# TSPIN is an algorithm to discover top stable periodic-frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+#     from PAMI.stablePeriodicFrequentPattern.basic import TSPIN as alg
 #
-#             obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
+#     obj = alg.TSPIN(iFile, maxPer, maxLa, k)
 #
-#             obj.startMine()
+#     obj.startMine()
 #
-#             Patterns = obj.getPatterns()
+#     stablePeriodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+#     print("Total number of Periodic Frequent Patterns:", len(stablePeriodicFrequentPatterns))
 #
-#             obj.save("patterns")
+#     obj.savePatterns(oFile)
 #
-#             Df = obj.getPatternsAsDataFrame()
+#     Df = obj.getPatternsAsDataFrame()
 #
-#             memUSS = obj.getMemoryUSS()
+#     memUSS = obj.getMemoryUSS()
 #
-#             print("Total Memory in USS:", memUSS)
+#     print("Total Memory in USS:", memUSS)
 #
-#             memRSS = obj.getMemoryRSS()
+#     memRSS = obj.getMemoryRSS()
 #
-#             print("Total Memory in RSS", memRSS)
+#     print("Total Memory in RSS", memRSS)
 #
-#             run = obj.getRuntime()
-#
-#             print("Total ExecutionTime in seconds:", run)
+#     run = obj.getRuntime()
 #
+#     print("Total ExecutionTime in seconds:", run)
 
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
@@ -49,56 +46,301 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-import pandas as pd
-from deprecated import deprecated
 
-from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
+from PAMI.stablePeriodicFrequentPattern.topK import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
+
+
+_maxPer = float()
+_maxLa = float()
+_k = float()
+_lno = int()
+_last = int()
+
+
+class _Node(object):
+    """
+        A class used to represent the node of stablePeriodicFrequentPatternTree
+
+        :Attributes:
+
+            item : int or None
+                Storing item of a node
+            timeStamps : list
+                To maintain the timestamps of a database at the end of the branch
+            parent : node
+                To maintain the parent of every node
+            children : list
+                To maintain the children of a node
+
+        :Methods:
+
+            addChild(itemName)
+                Storing the children to their respective parent nodes
+        """
+
+    def __init__(self, item, children) -> None:
+        """
+        Initializing the Node class
+
+        :param item: Storing the item of a node
+        :type item: int or None
+        :param children: To maintain the children of a node
+        :type children: dict
+        """
+
+        self.item = item
+        self.children = children
+        self.parent = None
+        self.timeStamps = []
+
+    def addChild(self, node) -> None:
+        """
+        To add the children to a node
+
+        :param node: parent node in the tree
+        """
+
+        self.children[node.item] = node
+        node.parent = self
 
-class SPPEclat(_ab._stablePeriodicFrequentPatterns):
+
+class _Tree(object):
     """
-    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
-                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
-                    greater than maximum lability.
+    A class used to represent the stablePeriodic frequentPatternGrowth tree structure
+
+    :Attributes:
 
-    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
-                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
+        root : Node
+            Represents the root node of the tree
+        summaries : dictionary
+            Storing the nodes with same item name
+        info : dictionary
+            Stores the support of the items
+
+
+    :Methods:
+
+        addTransactions(Database)
+            Creating transaction as a branch in frequentPatternTree
+        getConditionalPatterns(Node)
+            Generates the conditional patterns from tree for specific node
+        conditionalTransaction(prefixPaths,Support)
+            Takes the prefixPath of a node and support at child of the path and extract the frequent patterns from
+            prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
+            Removes the node from tree once after generating all the patterns respective to the node
+        generatePatterns(Node)
+            Starts from the root node of the tree and mines the periodic-frequent patterns
+        """
+
+    def __init__(self) -> None:
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction, tid) -> None:
+        """
+        Adding a transaction into tree
+
+        :param transaction: To represent the complete database
+        :type transaction: list
+        :param tid: To represent the timestamp of a database
+        :type tid: list
+        :return: pfp-growth tree
+        """
+
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+        currentNode.timeStamps = currentNode.timeStamps + tid
+
+    def getConditionalPatterns(self, alpha) -> None:
+        """
+        Generates all the conditional patterns of a respective node
+
+        :param alpha: To represent a Node in the tree
+        :type alpha: Node
+        :return: A tuple consisting of finalPatterns, conditional pattern base and information
+        """
+        finalPatterns = []
+        finalSets = []
+        for i in self.summaries[alpha]:
+            set1 = i.timeStamps
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalSets.append(set1)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
+        return finalPatterns, finalSets, info
+
+    @staticmethod
+    def generateTimeStamps(node) -> list:
+        """
+        To get the timestamps of a node
+
+        :param node: A node in the tree
+        :return: Timestamps of a node
+        """
+
+        finalTimeStamps = node.timeStamps
+        return finalTimeStamps
+
+    def removeNode(self, nodeValue) -> None:
+        """ Removing the node from tree
+
+        :param nodeValue: To represent a node in the tree
+        :type nodeValue: node
+        :return: Tree with their nodes updated with timestamps
+        """
+
+        for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            del i.parent.children[nodeValue]
+
+    def getTimeStamps(self, alpha) -> list:
+        """
+        To get all the timestamps of the nodes which share same item name
+
+        :param alpha: Node in a tree
+        :return: Timestamps of a  node
+        """
+        temporary = []
+        for i in self.summaries[alpha]:
+            temporary += i.timeStamps
+        return temporary
+
+    @staticmethod
+    def getSupportAndPeriod(timeStamps) -> tuple:
+        """
+        To calculate the periodicity and support
+
+        :param timeStamps: Timestamps of an item set
+        :return: support, periodicity
+        """
+
+        global _maxPer, _last
+        previous = 0
+        la = 0
+        tsList = sorted(timeStamps)
+        for ts in tsList:
+            la = max(0, la + ts - previous - _maxPer)
+            previous = ts
+        la = max(0, la + _last - previous - _maxPer)
+        return len(timeStamps), la
+
+    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps) -> tuple:
+        """
+        It generates the conditional patterns with periodic-frequent items
+
+        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
+        :type conditionalPatterns: list
+        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
+        :type conditionalTimeStamps: list
+        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
+        """
+
+        global _maxPer, _minSup
+        pat = []
+        timeStamps = []
+        data1 = {}
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
+                if j in data1:
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                else:
+                    data1[j] = conditionalTimeStamps[i]
+        updatedDictionary = {}
+        for m in data1:
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[1] <= _maxLa}
+        count = 0
+        for p in conditionalPatterns:
+            p1 = [v for v in p if v in updatedDictionary]
+            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                timeStamps.append(conditionalTimeStamps[count])
+            count += 1
+        return pat, timeStamps, updatedDictionary
+
+    def generatePatterns(self, minSup, prefix, Qk) -> None:
+        """
+        Generates the patterns
+
+        :param prefix: Forms the combination of items
+        :type prefix: list
+        :returns: yields patterns with their support and periodicity
+        """
+
+        global _k
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            Qk[tuple(pattern)] = self.info[i]
+            if len(Qk) >= _k:
+                minSup = min([v[0] for v in Qk.values()])
+            if len(Qk) > _k:
+                temp = min([v[0] for v in Qk.values()])
+                res = [key for key in Qk if Qk[key] == temp]
+                for j in res:
+                    Qk[j] = None
+            patterns, timeStamps, info = self.getConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+            if len(patterns) > 0:
+                conditionalTree.generatePatterns(minSup, pattern, Qk)
+            self.removeNode(i)
+
+
+class TSPIN(_ab._stablePeriodicFrequentPatterns):
+    """
+    :Description:   TSPIN is an algorithm to discover top stable periodic-frequent patterns in a transactional database.
+
+    :Reference:   Fournier-Viger, P., Wang, Y., Yang, P. et al. TSPIN: mining top-k stable periodic patterns.
+                  Appl Intell 52, 69176938 (2022). https://doi.org/10.1007/s10489-020-02181-6
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of stable periodic Frequent Pattern.
+                   Name of the Input file to mine complete set of frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of stable periodic Frequent Pattern.
-    :param  minSup: float or int or str :
-                    The user can specify minSup either in count or proportion of database size.
-                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                    Otherwise, it will be treated as float.
-                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-    :param  itemSup: int or float :
-                    Frequency of an item
-    :param maxLa: float :
-                  minimum loss of a pattern
-    :param  sep: str :
-                 This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+                   Name of the output file to store complete set of frequent patterns
+    :param  maxPer: float:
+                   Maximum number of frequent patterns to be included in the output file.
+    :param  maxLa: str:
+                   Maximum number of frequent patterns to be included in the output file.
 
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
-        minSup : int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         maxPer : int or float or str
             The user can specify maxPer either in count or proportion of database size.
             If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
             Otherwise, it will be treated as float.
             Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         maxLa : int or float or str
             The user can specify maxLa either in count or proportion of database size.
@@ -108,360 +350,405 @@
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
-            it represents the total no of transactions
+            To represent the total no of transaction
         tree : class
-            it represents the Tree class
+            To represents the Tree class
         itemSetCount : int
-            it represents the total no of patterns
+            To represents the total no of patterns
         finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
+            To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to an output file
+            Complete set of periodic-frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent
-        calculateLa()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
-
-
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        PeriodicFrequentOneItem()
+            Extracts the one-periodic-frequent patterns from database
+        updateDatabases()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
 
     **Methods to execute code on terminal**
-    -----------------------------------------
-    .. code-block:: console
-
-
-       Format:
-
-       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
-
-       Example usage:
+    ------------------------------------------
+            Format:
+                      >>>   python3 TSPIN.py <inputFile> <outputFile> <maxPer> <maxLa>
 
-       (.venv) $ python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
+            Example:
+                      >>>  python3 TSPIN.py sampleTDB.txt patterns.txt 0.3 0.4 0.6
 
-
-               .. note:: constraints will be considered in percentage of database transactions
+                      .. note:: maxPer, maxLa and k will be considered in percentage of database transactions
 
     **Importing this algorithm into a python program**
-    ---------------------------------------------------
-    ... code-block:: python
+    ----------------------------------------------------
+    .. code-block:: python
 
-                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+                from PAMI.stablePeriodicFrequentPattern.basic import TSPIN as alg
 
-                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
+                obj = alg.TSPIN(iFile, maxPer, maxLa, k)
 
-                    obj.startMine()
+                obj.startMine()
 
-                    Patterns = obj.getPatterns()
+                stablePeriodicFrequentPatterns = obj.getPatterns()
 
-                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+                print("Total number of Periodic Frequent Patterns:", len(stablePeriodicFrequentPatterns))
 
-                    obj.save("patterns")
+                obj.savePatterns(oFile)
 
-                    Df = obj.getPatternsAsDataFrame()
+                Df = obj.getPatternsAsDataFrame()
 
-                    memUSS = obj.getMemoryUSS()
+                memUSS = obj.getMemoryUSS()
 
-                    print("Total Memory in USS:", memUSS)
+                print("Total Memory in USS:", memUSS)
 
-                    memRSS = obj.getMemoryRSS()
+                memRSS = obj.getMemoryRSS()
 
-                    print("Total Memory in RSS", memRSS)
+                print("Total Memory in RSS", memRSS)
 
-                    run = obj.getRuntime()
+                run = obj.getRuntime()
 
-                    print("Total ExecutionTime in seconds:", run)
+                print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
+    ---------------
+             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
 
-       """
+    """
+    _startTime = float()
+    _endTime = float()
+    _maxLa = str()
+    _maxPer = float()
+    _k = float()
+    _SPPList = {}
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _minSup = str()
-    _maxPer = str()
-    _maxLa = float()
     _sep = " "
-    _SPPList = {}
-    _itemList = []
-    _last = int()
-    _finalPatterns = {}
-    _tsList = {}
-    _startTime = float()
-    _endTime = float()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _rank = {}
+    _rankedUp = {}
+    _lno = 0
 
-    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
-        self._iFile = inputFile
-        self._minSup = minSup
-        self._maxPer = maxPer
-        self._maxLa = maxLa
-        self._sep = sep
-
-    def _creatingItemsets(self):
+    def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'Patterns' in i:
-                self._Database = self._iFile['Patterns'].tolist()
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                self._Database.append(tr)
+
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
+                    count = 0
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
+                            count += 1
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value):
-        """
-        to convert the type of user specified minSup value
 
-        :param value: user specified minSup value
-        :return: converted type
+    def _periodicFrequentOneItem(self) -> Tuple[Dict[str, List[int]], List[str]]:
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
 
-    def _createSPPList(self):
-        """
-        to convert the single length stable periodic patterns
+        :returns: return the one-length periodic frequent patterns
         """
+        global _last
         tidLast = {}
         la = {}
-        self._SPPList = {}
-        self._tsList = {}
         for transaction in self._Database:
             ts = int(transaction[0])
             for item in transaction[1:]:
                 if item not in self._SPPList:
                     la[item] = max(0, ts - self._maxPer)
                     self._SPPList[item] = [1, la[item]]
-                    self._tsList[item] = [ts]
                 else:
                     s = self._SPPList[item][0] + 1
                     la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
                     self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
-                    self._tsList[item].append(ts)
                 tidLast[item] = ts
-            self._last = ts
+            _last = ts
         for item in self._SPPList:
-            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
+            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
             self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
-        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
-        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
-        self._Generation(list(self._SPPList), set())
-
-    def _Generation(self, GPPFList, CP):
-        """
-        To generate the patterns using depth-first search
-        """
-        for i in range(len(GPPFList)):
-            item = GPPFList[i]
-            CP1 = CP | {item}
-            if CP != set():
-                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
-            la = self._calculateLa(self._tsList['\t'.join(CP1)])
-            support = len(self._tsList['\t'.join(CP1)])
-            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
-                #CP = CP1
-                self._finalPatterns['\t'.join(CP1)] = [support, la]
-                if i+1 < len(GPPFList):
-                    self._Generation(GPPFList[i+1:], CP1)
+        self._SPPList = {k: v for k, v in self._SPPList.items() if v[1] <= self._maxLa}
+        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: (x[1][0]), reverse=True)}
+        data = self._SPPList
+        pfList = [k for k, v in data.items()]
+        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
+        return data, pfList
+
+    def _updateDatabases(self, dict1: Dict[str, List[int]]) -> List[List[int]]:
+        """
+        Remove the items which are not frequent from database and updates the database with rank of items
+
+        :param dict1: frequent items with support
+        :type dict1: dictionary
+        :return: Sorted and updated transactions
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
+                if tr[i] in dict1:
+                    list2.append(self._rank[tr[i]])
+            if len(list2) >= 2:
+                basket = list2[1:]
+                basket.sort()
+                list2[1:] = basket[0:]
+                list1.append(list2)
+        return list1
+
+    @staticmethod
+    def _buildTree(data: List[List[int]], info: Dict[int, List[int]]) -> _Tree:
+        """
+        It takes the database and support of each item and construct the main tree by setting root node as a null
+
+        :param data: it represents the one Database in database
+        :type data: list
+        :param info: it represents the support of each item
+        :type info: dictionary
+        :return: returns root node of tree
+        """
+
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            set1 = [data[i][0]]
+            rootNode.addTransaction(data[i][1:], set1)
+        return rootNode
+
+    def _savePeriodic(self, itemSet: List[str]) -> str:
+        """
+        To convert the ranks of items in to their original item names
+
+        :param itemSet: frequent pattern.
+        :return: frequent pattern with original item names
+        """
+        t1 = str()
+        for i in itemSet:
+            t1 = t1 + self._rankedUp[i] + " "
+        return t1
 
-    def _calculateLa(self, tsList):
+    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        To calculate the liability of a patterns based on its timestamps
-        """
-        previous = 0
-        la = 0
-        tsList = sorted(tsList)
-        laList = []
-        for ts in tsList:
-            la = max(0, la + ts - previous - self._maxPer)
-            laList.append(la)
-            previous = ts
-            
-        la = max(0, la + self._last - previous - self._maxPer)
-        laList.append(la)
-        maxla = max(laList)
-        return maxla
+        To convert the given user specified value
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
+        :param value: user specified value
+        :return: converted value
         """
-        Method to start the mining of patterns
-        """
-        self.mine()
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
-    def mine(self):
+    def startMine(self) -> None:
         """
-        Method to start the mining of patterns
+        Mining process will start from this function
         """
+
+        global _maxLa, _maxPer, _k, _lno
         self._startTime = _ab._time.time()
-        self._creatingItemsets()
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._maxLa is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
         self._maxLa = self._convert(self._maxLa)
+        self._maxPer = self._convert(self._maxPer)
+        self._k = self._convert(self._k)
+        _maxLa, _maxPer, _k, _lno = self._maxLa, self._maxPer, self._k, len(self._Database)
+        if self._maxLa > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = {}
+        Tree.generatePatterns(1, [], patterns)
         self._finalPatterns = {}
-        #print(self._minSup, self._maxPer, self._maxLa)
-        self._createSPPList()
+        for x, y in patterns.items():
+            sample = self._savePeriodic(x)
+            self._finalPatterns[sample] = y
         self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
-        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
+        print("Top-K Stable Periodic patterns were generated successfully using TSPIN algorithm ")
 
+    def getMemoryUSS(self) -> float:
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
-
-        :return: returning total amount of runtime taken by the mining process
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self._endTime - self._startTime
 
-    def getPatterns(self):
-        """
-        Function to return the set of stable periodic-frequent patterns after completion of the mining process
+        return self._memoryUSS
 
-        :return: returning stable periodic-frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
+    def getMemoryRSS(self) -> float:
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self._memoryRSS
 
-    def save(self, outFile):
-        """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
+    def getRuntime(self) -> float:
+        """Calculating the total amount of runtime taken by the mining process
 
-        :param outFile: name of the output file
-        :type outFile: csv file
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
-            writer.write("%s \n" % s1)
 
-    def getPatternsAsDataFrame(self):
+        return self._endTime - self._startTime
+
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final periodic-frequent patterns in a dataframe
 
         :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
+            data.append([a, b[0], b[1]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def getMemoryRSS(self):
+    def save(self, outFile: str) -> None:
         """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        :return: returning RSS memory consumed by the mining process
-        :rtype: float
+        Complete set of periodic-frequent patterns will be loaded in to an output file
+
+        :param outFile: name of the output file
+        :type outFile: file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
+
+    def getPatterns(self) -> dict:
         """
+        Function to send the set of periodic-frequent patterns after completion of the mining process
 
-        return self._memoryRSS
+        :return: returning periodic-frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
         if len(_ab._sys.argv) == 7:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+            _ap = TSPIN(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = TSPIN(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Patterns:", len(_ap.getPatterns()))
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
+        _ap = TSPIN('/Users/Likhitha/Downloads/SPP_sample.txt', 5, 1, 1, ' ')
+        _ap.startMine()
+        print(len(_ap._Database))
+        _Patterns = _ap.getPatterns()
+        for x, y in _Patterns.items():
+            print(x, y)
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.save('/Users/Likhitha/Downloads/output.txt')
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,15 +53,14 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
 from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
-import deprecated
 
 _minSup = int()
 _maxPer = int()
 _maxLa = int()
 _last = int()
 
 
@@ -578,27 +577,19 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from this function
         """
 
-        self.mine()
-
-    def mine(self):
-        """
-        Mining process will start from this function
-        """
-
         global _minSup, _maxPer, _lno, _maxLa
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
@@ -709,15 +700,14 @@
     _ap = str()
     if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
         if len(_ab._sys.argv) == 7:
             _ap = SPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
             _ap = SPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        _ap.mine()
         print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py` & `pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/WFIM.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,22 +1,26 @@
+# WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+# It stores the database in compressed fp-tree decreasing the memory usage and extracts the
+# patterns from tree.It employs downward closure property to  reduce the search space effectively.
+#
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# ----------------------------------------------------------
 #
 #
-#             from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowthDump as alg
+#             from PAMI.weightFrequentPattern.basic import basic as alg
 #
-#             obj = alg.SPPGrowthDump(iFile, minSup, maxPer, maxLa)
+#             obj = alg.basic(iFile, wFile, minSup, minWeight)
 #
 #             obj.startMine()
 #
-#             Patterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save(oFile)
+#             obj.savePatterns(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -25,16 +29,14 @@
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
-
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -46,531 +48,676 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.stableperiodicFrequentPattern.basic import abstract as _ab
+from PAMI.weightedFrequentPattern.basic import abstract as _fp
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
 from deprecated import deprecated
 
-from urllib.request import urlopen
-import validators
-import pandas as pd
-import resource
-import time
-import sys
-import os
-import psutil
-
-_minSup = int()
-_maxPer = int()
-_maxLa = int()
-_last = int()
 
+_minSup = str()
+_minWeight = int()
+_miniWeight = int()
+_maxWeight = int()
+_weights = {}
+_fp._sys.setrecursionlimit(20000)
 
-class _Node:
 
-    def __init__(self, item, children):
-        """
-        Initializing the Node class
+class _Node:
+    """
+    A class used to represent the node of frequentPatternTree
 
-        :param item: Storing the item of a node
-        :type item: int or None
-        :param children: To maintain the children of a node
-        :type children: dict
-        """
+    :Attributes:
 
-        self.item = item
-        self.children = children
+        itemId: int
+            storing item of a node
+        counter: int
+            To maintain the support of node
+        parent: node
+            To maintain the parent of node
+        children: list
+            To maintain the children of node
+
+    :Methods:
+
+        addChild(node)
+            Updates the nodes children list and parent for the given node
+    """
+
+    def __init__(self, item: str, children: list) -> None:
+        self.itemId = item
+        self.counter = 1
         self.parent = None
-        self.timeStamps = []
+        self.children = children
 
-    def addChild(self, node):
+    def addChild(self, node: '_Node') -> None:
         """
-        To add the children to a node
+        Retrieving the child from the tree
 
-        :param node: parent node in the tree
+        :param node: Children node
+        :type node: Node
+        :return: Updates the children nodes and parent nodes
         """
-
-        self.children[node.item] = node
+        self.children[node.itemId] = node
         node.parent = self
 
+
 class _Tree:
-    def __init__(self):
+    """
+    A class used to represent the frequentPatternGrowth tree structure
+
+    :Attributes:
+
+        root : Node
+            The first node of the tree set to Null.
+        summaries : dictionary
+            Stores the nodes itemId which shares same itemId
+        info : dictionary
+            frequency of items in the transactions
+
+    :Methods:
+
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minSup
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
+    """
+
+    def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction, tid):
+    def addTransaction(self, transaction: List[str], count: int) -> None:
         """
-        Adding a transaction into tree
+        Adding transaction into tree
 
-        :param transaction: To represent the complete database
+        :param transaction: it represents the one transaction in database
         :type transaction: list
-        :param tid: To represent the timestamp of a database
-        :type tid: list
-        :return: pfp-growth tree
+        :param count: frequency of item
+        :type count: int
+        :return: None
         """
-
+        # This method takes transaction as input and returns the tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
+                newNode.freq = count
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-        currentNode.timeStamps = currentNode.timeStamps + tid
+                currentNode.freq += count
 
-    def getConditionalPatterns(self, alpha):
+    def getFinalConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
         """
-        Generates all the conditional patterns of a respective node
+        Generates the conditional patterns for a node
 
-        :param alpha: To represent a Node in the tree
-        :type alpha: Node
-        :return: A tuple consisting of finalPatterns, conditional pattern base and information
+        :param alpha: node to generate conditional patterns
+        :return: returns conditional patterns, frequency of each item in conditional patterns
         """
         finalPatterns = []
-        finalSets = []
+        finalFreq = []
         for i in self.summaries[alpha]:
-            set1 = i.timeStamps
+            set1 = i.freq
             set2 = []
-            while i.parent.item is not None:
-                set2.append(i.parent.item)
+            while i.parent.itemId is not None:
+                set2.append(i.parent.itemId)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalSets.append(set1)
-        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
-        return finalPatterns, finalSets, info
-
-    @staticmethod
-    def generateTimeStamps(node):
-        """
-        To get the timestamps of a node
-
-        :param node: A node in the tree
-        :return: Timestamps of a node
-        """
-
-        finalTimeStamps = node.timeStamps
-        return finalTimeStamps
-
-    def removeNode(self, nodeValue):
-        """
-        Removing the node from tree
-
-        :param nodeValue: To represent a node in the tree
-        :type nodeValue: node
-        :return: Tree with their nodes updated with timestamps
-        """
-
-        for i in self.summaries[nodeValue]:
-            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
-            del i.parent.children[nodeValue]
-
-    def getTimeStamps(self, alpha):
-        """
-        To get all the timestamps of the nodes which share same item name
-
-        :param alpha: Node in a tree
-        :return: Timestamps of a  node
-        """
-        temporary = []
-        for i in self.summaries[alpha]:
-            temporary += i.timeStamps
-        return temporary
+                finalFreq.append(set1)
+        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
+        return finalPatterns, finalFreq, info
 
     @staticmethod
-    def getSupportAndPeriod(timeStamps):
-        """
-        To calculate the periodicity and support
-
-        :param timeStamps: Timestamps of an item set
-        :return: support, periodicity
+    def getConditionalTransactions(ConditionalPatterns: List[List[str]], conditionalFreq: List[int]) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
         """
-        global _maxPer, _last
-        previous = 0
-        la = 0
-        tsList = sorted(timeStamps)
-        for ts in tsList:
-            la = max(0, la + ts - previous - _maxPer)
-            previous = ts
-        la = max(0, la + _last - previous - _maxPer)
-        return len(timeStamps), la
+        To calculate the frequency of items in conditional patterns and sorting the patterns
 
-    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps):
+        :param ConditionalPatterns: paths of a node
+        :param conditionalFreq: frequency of each item in the path
+        :return: conditional patterns and frequency of each item in transactions
         """
-        It generates the conditional patterns with periodic-frequent items
-
-        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
-        :type conditionalPatterns: list
-        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
-        :type conditionalTimeStamps: list
-        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
-        """
-
-        global _maxPer, _minSup, _maxLa
+        global _minSup, _miniWeight
         pat = []
-        timeStamps = []
+        freq = []
         data1 = {}
-        for i in range(len(conditionalPatterns)):
-            for j in conditionalPatterns[i]:
+        for i in range(len(ConditionalPatterns)):
+            for j in ConditionalPatterns[i]:
                 if j in data1:
-                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                    data1[j] += conditionalFreq[i]
                 else:
-                    data1[j] = conditionalTimeStamps[i]
-        updatedDictionary = {}
-        for m in data1:
-            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
-        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _minSup and v[1] <= _maxLa}
+                    data1[j] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= _minSup and v * _miniWeight > _minSup}
         count = 0
-        for p in conditionalPatterns:
-            p1 = [v for v in p if v in updatedDictionary]
-            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
+        for p in ConditionalPatterns:
+            p1 = [v for v in p if v in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                timeStamps.append(conditionalTimeStamps[count])
+                freq.append(conditionalFreq[count])
             count += 1
-        return pat, timeStamps, updatedDictionary
+        return pat, freq, up_dict
 
-    def generatePatterns(self, prefix):
+    def generatePatterns(self, prefix: List[str]) -> Generator[Tuple[List[str], int], None, None]:
         """
-        Generates the patterns
+        To generate the frequent patterns
 
-        :param prefix: Forms the combination of items
-        :type prefix: list
-        :returns: yields patterns with their support and periodicity
+        :param prefix: an empty list
+        :return: Frequent patterns that are extracted from fp-tree
         """
-
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
+        global _miniWeight, _maxWeight, _minWeight, _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
             pattern = prefix[:]
             pattern.append(i)
             yield pattern, self.info[i]
-            patterns, timeStamps, info = self.getConditionalPatterns(i)
+            patterns, freq, info = self.getFinalConditionalPatterns(i)
             conditionalTree = _Tree()
             conditionalTree.info = info.copy()
             for pat in range(len(patterns)):
-                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+                conditionalTree.addTransaction(patterns[pat], freq[pat])
             if len(patterns) > 0:
                 for q in conditionalTree.generatePatterns(pattern):
                     yield q
-            self.removeNode(i)
 
-class SPPGrowth():
-    _startTime = float()
-    _endTime = float()
+
+class WFIM(_fp._weightedFrequentPatterns):
+    """
+    :Description:
+       * WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+       * It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs employs downward closure property to  reduce the search space effectively.
+
+    :Reference :
+           U. Yun and J. J. Leggett, Wfim: weighted frequent itemset mining with a weight range and a minimum weight,
+           in Proceedings of the 2005 SIAM International Conference on Data Mining. SIAM, 2005, pp. 636640.
+           https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.76
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of weighted Frequent Patterns.
+    :param  oFile: str :
+                   Name of the output file to store complete set of weighted Frequent Patterns.
+    :param  minSup: str or int or float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+    :Attributes :
+
+        iFile : file
+            Input file name or path of the input file
+        minSup: float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        minWeight: float or int or str
+            The user can specify minWeight either in count or proportion of database size.
+            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            However, the users can override their default separator.
+        oFile : file
+            Name of the output file or the path of the output file
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        finalPatterns : dict
+            it represents to store the patterns
+
+    :Methods :
+
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Extracts the one-frequent patterns from transactions
+
+    **Methods to execute code on terminal**
+    -------------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 basic.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+
+       Example Usage:
+
+       (.venv) $ python3 basic.py sampleDB.txt weightSample.txt patterns.txt 10.0 3.4
+
+
+               .. note:: minSup and maxPer will be considered in support count or frequency
+
+
+    **Importing this algorithm into a python program**
+    -----------------------------------------------------
+    .. code-block:: python
+
+            from PAMI.weightFrequentPattern.basic import basic as alg
+
+            obj = alg.basic(iFile, wFile, minSup, minWeight)
+
+            obj.startMine()
+
+            frequentPatterns = obj.getPatterns()
+
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
+
+            obj.savePatterns(oFile)
+
+            Df = obj.getPatternsAsDataFrame()
+
+            memUSS = obj.getmemoryUSS()
+
+            print("Total Memory in USS:", memUSS)
+
+            memRSS = obj.getMemoryRSS()
+
+            print("Total Memory in RSS", memRSS)
+
+            run = obj.getRuntime()
+
+            print("Total ExecutionTime in seconds:", run)
+
+    **Credits:**
+    ----------------------
+             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
+        """
+
+    __startTime = float()
+    __endTime = float()
     _minSup = str()
-    _maxPer = float()
-    _maxLa = float()
-    _finalPatterns = {}
+    __finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _rank = {}
-    _rankedUp = {}
-    _lno = 0
-    SPPList = {}
-
-    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
-        self._iFile = inputFile
-        self._minSup = minSup
-        self._maxPer = maxPer
-        self._maxLa = maxLa
-        self._sep = sep
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __tree = _Tree()
+    __rank = {}
+    __rankDup = {}
 
-    def _creatingItemSets(self):
+    def __init__(self, iFile: str, wFile: str, minSup: str, minWeight: int, sep: str='\t') -> None:
+        super().__init__(iFile, wFile, minSup, minWeight, sep)
+
+    def __creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
-        self._Database = []
-        if isinstance(self._iFile, pd.DataFrame):
-            data, ts = [], []
+        self.__Database = []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                self._Database.append(tr)
+                self.__Database = self._iFile['Transactions'].tolist()
 
+            # print(self.Database)
         if isinstance(self._iFile, str):
-            if validators.url(self._iFile):
-                data = urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            # print(len(temp))
+                            self.__Database.append(temp)
+                except IOError:
+                    print("File Not Found")
+                    quit()
+
+    def _scanningWeights(self) -> None:
+        """
+        Storing the weights of the variables in input file in a weights variable
+        :return: None
+        """
+        global _weights
+        _weights = {}
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            items, weights = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                items = self._wFile['items'].tolist()
+            if 'weights' in i:
+                weights = self._wFile['weights'].tolist()
+            for i in range(len(weights)):
+                _weights[items[i]] = weights[i]
+
+            # print(self.Database)
+        if isinstance(self._wFile, str):
+            if _fp._validators.url(self._wFile):
+                data = _fp._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    _weights[temp[0]] = temp[1]
+            else:
+                try:
+                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            s = int(float(temp[1]))
+                            _weights[temp[0]] = s
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _periodicFrequentOneItem(self):
+    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
+        To convert the type of user specified minSup value.
 
-        :returns: return the one-length periodic frequent patterns
+        :param value: user specified minSup value
+        :return: converted type
         """
-        global _last
-        tidLast = {}
-        la = {}
-        for transaction in self._Database:
-            ts = int(transaction[0])
-            for item in transaction[1:]:
-                if item not in self.SPPList:
-                    la[item] = max(0, ts - self._maxPer)
-                    self.SPPList[item] = [1, la[item]]
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self.__Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self.__Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def __frequentOneItem(self) -> List[str]:
+        """
+        Generating One frequent items sets
+        :return: list
+        """
+        global _maxWeight
+        self.__mapSupport = {}
+        for tr in self.__Database:
+            for i in range(0, len(tr)):
+                if tr[i] not in self.__mapSupport:
+                    self.__mapSupport[tr[i]] = 1
                 else:
-                    s = self.SPPList[item][0] + 1
-                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
-                    self.SPPList[item] = [s, max(la[item], self.SPPList[item][1])]
-                tidLast[item] = ts
-            _last = ts
-        for item in self.SPPList:
-            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
-            self.SPPList[item][1] = max(la[item], self.SPPList[item][1])
-        self.SPPList = {k: v for k, v in self.SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
-        self.SPPList = {k: v for k, v in sorted(self.SPPList.items(), key=lambda x: x[1][0], reverse=True)}
-        data = self.SPPList
-        pfList = [k for k, v in sorted(data.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
-        #print(len(pfList))
-        return data, pfList
-
-    def _updateDatabases(self, dict1):
-        """
-        Remove the items which are not frequent from database and updates the database with rank of items
-
-        :param dict1: frequent items with support
-        :type dict1: dictionary
-        :return: Sorted and updated transactions
+                    self.__mapSupport[tr[i]] += 1
+        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup and v * _maxWeight > self._minSup}
+        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        return genList
+
+    def __updateTransactions(self, itemSet: List[str]) -> List[List[int]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
+        :param itemSet: list of one-frequent items
+        :return: list
         """
         list1 = []
-        for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
-                if tr[i] in dict1:
-                    list2.append(self._rank[tr[i]])
-            if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort()
-                list2[1:] = basket[0:]
+        for tr in self.__Database:
+            list2 = []
+            for i in range(len(tr)):
+                if tr[i] in itemSet:
+                    list2.append(self.__rank[tr[i]])
+            if len(list2) >= 1:
+                list2.sort()
                 list1.append(list2)
         return list1
 
     @staticmethod
-    def _buildTree(data, info):
+    def __buildTree(transactions: List[List[int]], info: Dict[int, int]) -> '_Tree':
         """
-        It takes the database and support of each item and construct the main tree by setting root node as a null
+        Builds the tree with updated transactions
 
-        :param data: it represents the one Database in database
-        :type data: list
-        :param info: it represents the support of each item
-        :type info: dictionary
-        :return: returns root node of tree
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions.
+        :return: Transactions compressed in fp-tree
         """
-
         rootNode = _Tree()
         rootNode.info = info.copy()
-        for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransaction(data[i][1:], set1)
+        for i in range(len(transactions)):
+            rootNode.addTransaction(transactions[i], 1)
         return rootNode
 
-    def _savePeriodic(self, itemSet):
+    def __savePeriodic(self, itemSet: List[int]) -> str:
         """
-        To convert the ranks of items in to their original item names
+        The duplication items and their ranks
 
-        :param itemSet: frequent pattern.
-        :return: frequent pattern with original item names
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
         """
-        t1 = str()
+        temp = str()
         for i in itemSet:
-            t1 = t1 + self._rankedUp[i] + " "
-        return t1
-
-    def _convert(self, value):
-        """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+            temp = temp + self.__rankDup[i] + "\t"
+        return temp
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
+    def startMine(self) -> None:
         """
-        Mining process will start from this function
+        main program to start the operation
+        :return: None
         """
+        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanningWeights()
+        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
+        _maxWeight = max([s for s in _weights.values()])
+        _miniWeight = min([s for s in _weights.values()])
+        self._minSup = self.__convert(self._minSup)
+        _minSup = self._minSup
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using basic algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
-        self.mine()
-
-    def mine(self):
+    def Mine(self) -> None:
         """
-        Mining process will start from this function
+        main program to start the operation
+        :return: None
         """
-
-        global _minSup, _maxPer, _lno, _maxLa
-        self._startTime = time.time()
+        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
+        self.__startTime = _fp._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._maxLa = self._convert(self._maxLa)
-        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
-        print(_minSup, _maxPer, _maxLa)
-        if self._minSup > len(self._Database):
-            raise Exception("Please enter the minSup in range between 0 to 1")
-        generatedItems, pfList = self._periodicFrequentOneItem()
-        updatedDatabases = self._updateDatabases(generatedItems)
-        for x, y in self._rank.items():
-            self._rankedUp[y] = x
-        info = {self._rank[k]: v for k, v in generatedItems.items()}
-        Tree = self._buildTree(updatedDatabases, info)
-        patterns = Tree.generatePatterns([])
-        self._finalPatterns = {}
-        for i in patterns:
-            sample = self._savePeriodic(i[0])
-            self._finalPatterns[sample] = i[1]
-        self._endTime = time.time()
-        process = psutil.Process(os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
+        self.__creatingItemSets()
+        self._scanningWeights()
+        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
+        _maxWeight = max([s for s in _weights.values()])
+        _miniWeight = min([s for s in _weights.values()])
+        self._minSup = self.__convert(self._minSup)
+        _minSup = self._minSup
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using basic algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getMemoryUSS(self) -> float:
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function.
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memoryRSS
 
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self) -> float:
+        """
+        Calculating the total amount of runtime taken by the mining process.
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._endTime - self._startTime
+        return self.__endTime - self.__startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final periodic-frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+        """
+        Storing final frequent patterns in a dataframe.
 
-        :return: returning periodic-frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+        for a, b in self.__finalPatterns.items():
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to an output file.
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+        for x, y in self.__finalPatterns.items():
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """ Function to send the set of periodic-frequent patterns after completion of the mining process
+    def getPatterns(self) -> Dict[str, int]:
+        """
+        Function to send the set of frequent patterns after completion of the mining process.
 
-        :return: returning periodic-frequent patterns
+        :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.__finalPatterns
+
+    def printResults(self) -> None:
+        """
+        This function is used to print the results
+        :return: None
+        """
+        print("Total number of  Weighted Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+
+        
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(sys.argv) == 5 or len(sys.argv) == 6:
-        if len(sys.argv) == 6:
-            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
-        if len(sys.argv) == 5:
-            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4])
+    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
+        if len(_fp._sys.argv) == 7:
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 6:
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
         _ap.startMine()
-        _ap.mine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.save(sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total number of Weighted Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        '''ap = topk('https://www.u-aizu.ac.jp/~udayrage/datasets/temporalDatabases/temporal_retail.csv', 0.001, 0.005, 0.004)
-        #ap = topk('/Users/likhitha/Downloads/contextPrefixSpan.txt', 3, 6, 2, ' ')
-        ap.startMine()
-        Patterns = ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(Patterns))
-        ap.save('/Users/Likhitha/Downloads/output')
-        memUSS = ap.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = ap.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = ap.getRuntime()
-        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py` & `pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,39 +1,40 @@
-# TSPIN is an algorithm to discover top stable periodic-frequent patterns in a transactional database.
+# WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
+# It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.stablePeriodicFrequentPattern.basic import TSPIN as alg
+#             from PAMI.weightedFrequentRegularPattern.basic import WFRIMiner as alg
 #
-#     obj = alg.TSPIN(iFile, maxPer, maxLa, k)
+#             obj = alg.WFRIMiner(iFile, WS, regularity)
 #
-#     obj.startMine()
+#             obj.startMine()
 #
-#     stablePeriodicFrequentPatterns = obj.getPatterns()
+#             weightedFrequentRegularPatterns = obj.getPatterns()
 #
-#     print("Total number of Periodic Frequent Patterns:", len(stablePeriodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 #
-#     obj.savePatterns(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
-
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -47,152 +48,151 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.stablePeriodicFrequentPattern.topK import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from PAMI.weightedFrequentRegularPattern.basic import abstract as _fp
+import pandas as pd
+from deprecated import deprecated
+from typing import List, Dict
 
 
-_maxPer = float()
-_maxLa = float()
-_k = float()
+_WS = str()
+_regularity = str()
 _lno = int()
-_last = int()
+_weights = {}
+_wf = {}
+_fp._sys.setrecursionlimit(20000)
 
 
-class _Node(object):
+class _Node:
     """
-        A class used to represent the node of stablePeriodicFrequentPatternTree
+    A class used to represent the node of frequentPatternTree
 
-        :Attributes:
-
-            item : int or None
-                Storing item of a node
-            timeStamps : list
-                To maintain the timestamps of a database at the end of the branch
-            parent : node
-                To maintain the parent of every node
-            children : list
-                To maintain the children of a node
+    :Attributes:
+        itemId: int
+            storing item of a node
+        counter: int
+            To maintain the support of node
+        parent: node
+            To maintain the parent of node
+        children: list
+            To maintain the children of node
 
-        :Methods:
+    :Methods:
+        addChild(node)
+            Updates the nodes children list and parent for the given node
 
-            addChild(itemName)
-                Storing the children to their respective parent nodes
-        """
+    """
 
-    def __init__(self, item, children) -> None:
+    def __init__(self, item: int, children: dict) -> None:
         """
         Initializing the Node class
 
         :param item: Storing the item of a node
         :type item: int or None
         :param children: To maintain the children of a node
         :type children: dict
+        :return: None
         """
 
         self.item = item
         self.children = children
         self.parent = None
         self.timeStamps = []
 
     def addChild(self, node) -> None:
         """
         To add the children to a node
 
         :param node: parent node in the tree
+        :return: None
         """
 
         self.children[node.item] = node
         node.parent = self
 
 
-class _Tree(object):
+class _Tree:
     """
-    A class used to represent the stablePeriodic frequentPatternGrowth tree structure
+    A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
-
         root : Node
-            Represents the root node of the tree
+            The first node of the tree set to Null.
         summaries : dictionary
-            Storing the nodes with same item name
+            Stores the nodes itemId which shares same itemId
         info : dictionary
-            Stores the support of the items
-
+            frequency of items in the transactions
 
     :Methods:
-
-        addTransactions(Database)
-            Creating transaction as a branch in frequentPatternTree
-        getConditionalPatterns(Node)
-            Generates the conditional patterns from tree for specific node
-        conditionalTransaction(prefixPaths,Support)
-            Takes the prefixPath of a node and support at child of the path and extract the frequent patterns from
-            prefixPaths and generates prefixPaths with items which are frequent
-        remove(Node)
-            Removes the node from tree once after generating all the patterns respective to the node
-        generatePatterns(Node)
-            Starts from the root node of the tree and mines the periodic-frequent patterns
-        """
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minSup
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
+    """
 
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction, tid) -> None:
+    def addTransaction(self, transaction: list, tid: list) -> None:
         """
         Adding a transaction into tree
 
         :param transaction: To represent the complete database
         :type transaction: list
         :param tid: To represent the timestamp of a database
         :type tid: list
         :return: pfp-growth tree
         """
-
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
         currentNode.timeStamps = currentNode.timeStamps + tid
 
-    def getConditionalPatterns(self, alpha) -> None:
+    def getConditionalPatterns(self, alpha, pattern) -> tuple:
         """
         Generates all the conditional patterns of a respective node
 
         :param alpha: To represent a Node in the tree
         :type alpha: Node
+        :param pattern: prefix of the pattern
+        :type alpha: list
         :return: A tuple consisting of finalPatterns, conditional pattern base and information
         """
         finalPatterns = []
         finalSets = []
         for i in self.summaries[alpha]:
             set1 = i.timeStamps
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
                 finalSets.append(set1)
-        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets, pattern)
         return finalPatterns, finalSets, info
 
     @staticmethod
     def generateTimeStamps(node) -> list:
         """
         To get the timestamps of a node
 
@@ -200,15 +200,16 @@
         :return: Timestamps of a node
         """
 
         finalTimeStamps = node.timeStamps
         return finalTimeStamps
 
     def removeNode(self, nodeValue) -> None:
-        """ Removing the node from tree
+        """
+        Removing the node from tree
 
         :param nodeValue: To represent a node in the tree
         :type nodeValue: node
         :return: Tree with their nodes updated with timestamps
         """
 
         for i in self.summaries[nodeValue]:
@@ -224,531 +225,595 @@
         """
         temporary = []
         for i in self.summaries[alpha]:
             temporary += i.timeStamps
         return temporary
 
     @staticmethod
-    def getSupportAndPeriod(timeStamps) -> tuple:
+    def getSupportAndPeriod(timeStamps: list, pattern: list) -> list:
         """
         To calculate the periodicity and support
 
         :param timeStamps: Timestamps of an item set
+        :type timeStamps: list
+        :param pattern: pattern to evaluate the weighted frequent regular or not
+        :type pattern: list
         :return: support, periodicity
         """
+        global _WS, _regularity, _lno, _weights
+        timeStamps.sort()
+        cur = 0
+        per = list()
+        sup = 0
+        for j in range(len(timeStamps)):
+            per.append(timeStamps[j] - cur)
+            cur = timeStamps[j]
+            sup += 1
+        per.append(_lno - cur)
+        l = int()
+        for i in pattern:
+            l = l + _weights[i]
+        wf = (l / (len(pattern))) * sup
+        if len(per) == 0:
+            return [0, 0]
+        return [sup, max(per), wf]
 
-        global _maxPer, _last
-        previous = 0
-        la = 0
-        tsList = sorted(timeStamps)
-        for ts in tsList:
-            la = max(0, la + ts - previous - _maxPer)
-            previous = ts
-        la = max(0, la + _last - previous - _maxPer)
-        return len(timeStamps), la
-
-    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps) -> tuple:
+    def conditionalDatabases(self, conditionalPatterns: list, conditionalTimeStamps: list, pattern: list) -> tuple:
         """
         It generates the conditional patterns with periodic-frequent items
 
         :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
         :type conditionalPatterns: list
         :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
         :type conditionalTimeStamps: list
+        :param pattern: prefix of the pattern
+        :type pattern: list
         :returns: Returns conditional transactions by removing non-periodic and non-frequent items
         """
-
-        global _maxPer, _minSup
+        global _WS, _regularity
         pat = []
         timeStamps = []
         data1 = {}
         for i in range(len(conditionalPatterns)):
             for j in conditionalPatterns[i]:
                 if j in data1:
                     data1[j] = data1[j] + conditionalTimeStamps[i]
                 else:
                     data1[j] = conditionalTimeStamps[i]
         updatedDictionary = {}
         for m in data1:
-            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
-        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[1] <= _maxLa}
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m], pattern + [m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _WS and v[1] <= _regularity}
         count = 0
         for p in conditionalPatterns:
             p1 = [v for v in p if v in updatedDictionary]
             trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
                 timeStamps.append(conditionalTimeStamps[count])
             count += 1
         return pat, timeStamps, updatedDictionary
 
-    def generatePatterns(self, minSup, prefix, Qk) -> None:
+    def generatePatterns(self, prefix: list) -> None:
         """
         Generates the patterns
 
         :param prefix: Forms the combination of items
         :type prefix: list
         :returns: yields patterns with their support and periodicity
         """
-
-        global _k
+        global _WS
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
             pattern = prefix[:]
             pattern.append(i)
-            Qk[tuple(pattern)] = self.info[i]
-            if len(Qk) >= _k:
-                minSup = min([v[0] for v in Qk.values()])
-            if len(Qk) > _k:
-                temp = min([v[0] for v in Qk.values()])
-                res = [key for key in Qk if Qk[key] == temp]
-                for j in res:
-                    Qk[j] = None
-            patterns, timeStamps, info = self.getConditionalPatterns(i)
-            conditionalTree = _Tree()
-            conditionalTree.info = info.copy()
-            for pat in range(len(patterns)):
-                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
-            if len(patterns) > 0:
-                conditionalTree.generatePatterns(minSup, pattern, Qk)
+            if self.info[i][2] >= _WS:
+                yield pattern, self.info[i]
+                patterns, timeStamps, info = self.getConditionalPatterns(i, pattern)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+                if len(patterns) > 0:
+                    for q in conditionalTree.generatePatterns(pattern):
+                        yield q
             self.removeNode(i)
 
 
-class TSPIN(_ab._stablePeriodicFrequentPatterns):
+class WFRIMiner(_fp._weightedFrequentRegularPatterns):
     """
-    :Description:   TSPIN is an algorithm to discover top stable periodic-frequent patterns in a transactional database.
+    :Description: WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
+       * It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
 
-    :Reference:   Fournier-Viger, P., Wang, Y., Yang, P. et al. TSPIN: mining top-k stable periodic patterns.
-                  Appl Intell 52, 69176938 (2022). https://doi.org/10.1007/s10489-020-02181-6
+    :Reference:
+           K. Klangwisan and K. Amphawan, "Mining weighted-frequent-regular itemsets from transactional database,"
+           2017 9th International Conference on Knowledge and Smart Technology (KST), 2017, pp. 66-71,
+           doi: 10.1109/KST.2017.7886090.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of Weighted Frequent Regular Patterns.
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  maxPer: float:
-                   Maximum number of frequent patterns to be included in the output file.
-    :param  maxLa: str:
-                   Maximum number of frequent patterns to be included in the output file.
-
+                   Name of the output file to store complete set of Weighted Frequent Regular Patterns.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  wFile: str :
+                This is a weighted file.
 
     :Attributes:
 
         iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Input file name or path of the input file
+        WS: float or int or str
+            The user can specify WS either in count or proportion of database size.
+            If the program detects the data type of WS is integer, then it treats WS is expressed in count.
             Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        maxLa : int or float or str
-            The user can specify maxLa either in count or proportion of database size.
-            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
+            Example: WS=10 will be treated as integer, while WS=10.0 will be treated as float
+        regularity: float or int or str
+            The user can specify regularity either in count or proportion of database size.
+            If the program detects the data type of regularity is integer, then it treats regularity is expressed in count.
             Otherwise, it will be treated as float.
-            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
+            Example: regularity=10 will be treated as integer, while regularity=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
+        oFile : file
+            Name of the output file or the path of the output file
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
-            To record the start time of the mining process
-        endTime : float
-            To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
-            To represent the total no of transaction
+            it represents the total no of transactions
         tree : class
-            To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
+            it represents the Tree class
         finalPatterns : dict
-            To store the complete patterns
+            it represents to store the patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
-        PeriodicFrequentOneItem()
-            Extracts the one-periodic-frequent patterns from database
-        updateDatabases()
-            Update the database by removing aperiodic items and sort the Database by item decreased support
-        buildTree()
-            After updating the Database, remaining items will be added into the tree by setting root node as null
-        convert()
-            to convert the user specified value
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Extracts the one-frequent patterns from transactions
 
     **Methods to execute code on terminal**
-    ------------------------------------------
-            Format:
-                      >>>   python3 TSPIN.py <inputFile> <outputFile> <maxPer> <maxLa>
+    -------------------------------------------
+    .. code-block:: console
+
+
+      Format:
+
+      (.venv) $ python3 WFRIMiner.py <inputFile> <outputFile> <weightSupport> <regularity>
+
+      Example Usage:
 
-            Example:
-                      >>>  python3 TSPIN.py sampleTDB.txt patterns.txt 0.3 0.4 0.6
+      (.venv) $ python3 WFRIMiner.py sampleDB.txt patterns.txt 10 5
+
+
+              .. note:: WS & regularity will be considered in support count or frequency
 
-                      .. note:: maxPer, maxLa and k will be considered in percentage of database transactions
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
     .. code-block:: python
 
-                from PAMI.stablePeriodicFrequentPattern.basic import TSPIN as alg
+            from PAMI.weightedFrequentRegularpattern.basic import WFRIMiner as alg
 
-                obj = alg.TSPIN(iFile, maxPer, maxLa, k)
+            obj = alg.WFRIMiner(iFile, WS, regularity)
 
-                obj.startMine()
+            obj.startMine()
 
-                stablePeriodicFrequentPatterns = obj.getPatterns()
+            weightedFrequentRegularPatterns = obj.getPatterns()
 
-                print("Total number of Periodic Frequent Patterns:", len(stablePeriodicFrequentPatterns))
+            print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 
-                obj.savePatterns(oFile)
+            obj.save(oFile)
 
-                Df = obj.getPatternsAsDataFrame()
+            Df = obj.getPatternInDataFrame()
 
-                memUSS = obj.getMemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
-                print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-                memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-                print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-                run = obj.getRuntime()
+            run = obj.getRuntime()
 
-                print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
+    ----------------
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
+        """
 
-    """
     _startTime = float()
     _endTime = float()
-    _maxLa = str()
-    _maxPer = float()
-    _k = float()
-    _SPPList = {}
+    _WS = str()
+    _regularity = str()
+    _weight = {}
     _finalPatterns = {}
+    _wFile = " "
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _rank = {}
-    _rankedUp = {}
+    _mapSupport = {}
     _lno = 0
+    _tree = _Tree()
+    _rank = {}
+    _rankDup = {}
+
+    def __init__(self, iFile, _wFile, WS, regularity, sep='\t') -> None:
+        super().__init__(iFile, _wFile, WS, regularity, sep)
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+        self._weight = {}
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                self._Database.append(tr)
+                self._Database = self._iFile['Transactions'].tolist()
 
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            _items, _weights = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                _items = self._wFile['items'].tolist()
+            if 'weight' in i:
+                _weights = self._wFile['weight'].tolist()
+            for i in range(len(_items)):
+                self._weight[_items[i]] = _weights[i]
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
-                    count = 0
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            count += 1
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
+        if isinstance(self._wFile, str):
+            if _fp._validators.url(self._wFile):
+                data = _fp._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._weight[temp[0]] = float(temp[1])
+            else:
+                try:
+                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._weight[temp[0]] = float(temp[1])
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
-    def _periodicFrequentOneItem(self) -> Tuple[Dict[str, List[int]], List[str]]:
+    def _convert(self, value) -> float:
         """
-        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
+        To convert the type of user specified minSup value
 
-        :returns: return the one-length periodic frequent patterns
+        :param value: user specified minSup value
+        :return: converted type
         """
-        global _last
-        tidLast = {}
-        la = {}
-        for transaction in self._Database:
-            ts = int(transaction[0])
-            for item in transaction[1:]:
-                if item not in self._SPPList:
-                    la[item] = max(0, ts - self._maxPer)
-                    self._SPPList[item] = [1, la[item]]
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def _frequentOneItem(self) -> List[str]:
+        """
+        Generating One frequent items sets
+        :return: list
+        """
+        global _lno, _wf, _weights
+        self._mapSupport = {}
+        _owf = {}
+        for tr in self._Database:
+            for i in range(1, len(tr)):
+                if tr[i] not in self._mapSupport:
+                    self._mapSupport[tr[i]] = [int(tr[0]), int(tr[0]), 1]
                 else:
-                    s = self._SPPList[item][0] + 1
-                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
-                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
-                tidLast[item] = ts
-            _last = ts
-        for item in self._SPPList:
-            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
-            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
-        self._SPPList = {k: v for k, v in self._SPPList.items() if v[1] <= self._maxLa}
-        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: (x[1][0]), reverse=True)}
-        data = self._SPPList
-        pfList = [k for k, v in data.items()]
-        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
-        return data, pfList
-
-    def _updateDatabases(self, dict1: Dict[str, List[int]]) -> List[List[int]]:
-        """
-        Remove the items which are not frequent from database and updates the database with rank of items
-
-        :param dict1: frequent items with support
-        :type dict1: dictionary
-        :return: Sorted and updated transactions
+                    self._mapSupport[tr[i]][0] = max(self._mapSupport[tr[i]][0], (int(tr[0]) - self._mapSupport[tr[i]][1]))
+                    self._mapSupport[tr[i]][1] = int(tr[0])
+                    self._mapSupport[tr[i]][2] += 1
+        for key in self._mapSupport:
+            self._mapSupport[key][0] = max(self._mapSupport[key][0], abs(len(self._Database) - self._mapSupport[key][1]))
+        _lno = len(self._Database)
+        self._mapSupport = {k: [v[2], v[0]] for k, v in self._mapSupport.items() if v[0] <= self._regularity}
+        for x, y in self._mapSupport.items():
+            if self._weight.get(x) is None:
+                self._weight[x] = 0
+        gmax = max([self._weight[values] for values in self._mapSupport.keys()])
+        for x, y in self._mapSupport.items():
+            _owf[x] = y[0] * gmax
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v[0] * _owf[k] >= self._WS}
+        for x, y in self._mapSupport.items():
+            temp = self._weight[x] * y[0]
+            _wf[x] = temp
+            self._mapSupport[x].append(temp)
+        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse= True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        for x, y in self._rank.items():
+            _weights[y] = self._weight[x]
+        return genList
+
+    def _updateTransactions(self, itemSet) -> List[List[int]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+
+        :Example:
+        oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+        rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
+        :param itemSet: list of one-frequent items
+        :return: None
         """
         list1 = []
         for tr in self._Database:
             list2 = [int(tr[0])]
             for i in range(1, len(tr)):
-                if tr[i] in dict1:
+                if tr[i] in itemSet:
                     list2.append(self._rank[tr[i]])
             if len(list2) >= 2:
                 basket = list2[1:]
                 basket.sort()
                 list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
     @staticmethod
-    def _buildTree(data: List[List[int]], info: Dict[int, List[int]]) -> _Tree:
+    def _buildTree(transactions, info) -> _Tree:
         """
-        It takes the database and support of each item and construct the main tree by setting root node as a null
+        Builds the tree with updated transactions
 
-        :param data: it represents the one Database in database
-        :type data: list
-        :param info: it represents the support of each item
-        :type info: dictionary
-        :return: returns root node of tree
-        """
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions
+        :return: transactions compressed in fp-tree
 
+        """
         rootNode = _Tree()
         rootNode.info = info.copy()
-        for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransaction(data[i][1:], set1)
+        for i in range(len(transactions)):
+            set1 = [transactions[i][0]]
+            rootNode.addTransaction(transactions[i][1:], set1)
         return rootNode
 
-    def _savePeriodic(self, itemSet: List[str]) -> str:
+    def _savePeriodic(self, itemSet) -> str:
         """
-        To convert the ranks of items in to their original item names
+        The duplication items and their ranks
+
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
 
-        :param itemSet: frequent pattern.
-        :return: frequent pattern with original item names
         """
-        t1 = str()
+        temp = str()
         for i in itemSet:
-            t1 = t1 + self._rankedUp[i] + " "
-        return t1
+            temp = temp + self._rankDup[i] + "\t"
+        return temp
 
-    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :return: converted value
+        main program to start the operation
+        :return: None
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+        global _WS, _regularity, _weights
+        self._startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._WS is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._WS = self._convert(self._WS)
+        self._regularity = self._convert(self._regularity)
+        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
+        itemSet = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(itemSet)
+        for x, y in self._rank.items():
+            self._rankDup[y] = x
+        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
+        _Tree = self._buildTree(updatedTransactions, info)
+        patterns = _Tree.generatePatterns([])
+        self._finalPatterns = {}
+        for k in patterns:
+            s = self._savePeriodic(k[0])
+            self._finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
+        self._endTime = _fp._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def startMine(self) -> None:
+    def Mine(self) -> None:
         """
-        Mining process will start from this function
+        main program to start the operation
+        :return: None
         """
-
-        global _maxLa, _maxPer, _k, _lno
-        self._startTime = _ab._time.time()
+        global _WS, _regularity, _weights
+        self._startTime = _fp._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
-        if self._maxLa is None:
+        if self._WS is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        self._maxLa = self._convert(self._maxLa)
-        self._maxPer = self._convert(self._maxPer)
-        self._k = self._convert(self._k)
-        _maxLa, _maxPer, _k, _lno = self._maxLa, self._maxPer, self._k, len(self._Database)
-        if self._maxLa > len(self._Database):
-            raise Exception("Please enter the minSup in range between 0 to 1")
-        generatedItems, pfList = self._periodicFrequentOneItem()
-        updatedDatabases = self._updateDatabases(generatedItems)
+        self._WS = self._convert(self._WS)
+        self._regularity = self._convert(self._regularity)
+        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
+        itemSet = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(itemSet)
         for x, y in self._rank.items():
-            self._rankedUp[y] = x
-        info = {self._rank[k]: v for k, v in generatedItems.items()}
-        Tree = self._buildTree(updatedDatabases, info)
-        patterns = {}
-        Tree.generatePatterns(1, [], patterns)
+            self._rankDup[y] = x
+        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
+        _Tree = self._buildTree(updatedTransactions, info)
+        patterns = _Tree.generatePatterns([])
         self._finalPatterns = {}
-        for x, y in patterns.items():
-            sample = self._savePeriodic(x)
-            self._finalPatterns[sample] = y
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        for k in patterns:
+            s = self._savePeriodic(k[0])
+            self._finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
+        self._endTime = _fp._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Top-K Stable Periodic patterns were generated successfully using TSPIN algorithm ")
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
-        Storing final periodic-frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe
 
-        :return: returning periodic-frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self) -> Dict[str, float]:
         """
-        Function to send the set of periodic-frequent patterns after completion of the mining process
+        Function to send the set of frequent patterns after completion of the mining process
 
-        :return: returning periodic-frequent patterns
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Frequent Regular Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
-
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 7:
-            _ap = TSPIN(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
-        if len(_ab._sys.argv) == 6:
-            _ap = TSPIN(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
+        if len(_fp._sys.argv) == 7:
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 5:
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total number of Weighted Frequent Regular Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        _ap = TSPIN('/Users/Likhitha/Downloads/SPP_sample.txt', 5, 1, 1, ' ')
-        _ap.startMine()
-        print(len(_ap._Database))
-        _Patterns = _ap.getPatterns()
-        for x, y in _Patterns.items():
-            print(x, y)
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.save('/Users/Likhitha/Downloads/output.txt')
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.24.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py` & `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/abstract.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/dfsCode.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/dfsCode.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/edge.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/edge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/extendedEdge.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/extendedEdge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/frequentSubgraph.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/frequentSubgraph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/graph.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/graph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/gspan.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/gspan.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/basic/vertex.py` & `pami-2024.4.9.1/PAMI/subgraphMining/basic/vertex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/DFSCode.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSCode.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/DFSThread.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSThread.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/abstract.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/edge.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/edge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/extendedEdge.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/extendedEdge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/frequentSubgraph.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/frequentSubgraph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/graph.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/graph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/tkg.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/tkg.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/subgraphMining/topK/vertex.py` & `pami-2024.4.9.1/PAMI/subgraphMining/topK/vertex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py`

 * *Files 1% similar despite different names*

```diff
@@ -652,17 +652,35 @@
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
         :return: None
         """
-        self.mine()
+        global minSup
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Uncertain Frequent patterns were successfully generated using CUFPTree algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
 
-    def mine(self) -> None:
+    def Mine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
         :return: None
         """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -767,15 +785,14 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
         print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -44,15 +44,14 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 """
 
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 from typing import List, Tuple
-import deprecated
 
 _minSup = str()
 _ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
@@ -282,19 +281,17 @@
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
 class PUFGrowth(_ab._frequentPatterns):
     """
     :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
-
     :Reference:
         Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
         Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
-
     :Attributes:
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
@@ -320,15 +317,14 @@
             To represent the total no of transaction
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
-
     :Methods:
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
@@ -348,53 +344,37 @@
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
-
     **Methods to execute code on terminal**
     -----------------------------------------
             Format:
                     >>> python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
             Example:
                      >>>  python3 PUFGrowth.py sampleTDB.txt patterns.txt 3
-
             .. note:: minSup  will be considered in support count or frequency
-
     **Importing this algorithm into a python program**
     -----------------------------------------------------
     .. code-block:: python
             from PAMI.uncertainFrequentPattern.basic import puf as alg
-
             obj = alg.PUFGrowth(iFile, minSup)
-
             obj.startMine()
-
             frequentPatterns = obj.getPatterns()
-
             print("Total number of Frequent Patterns:", len(frequentPatterns))
-
             obj.save(oFile)
-
             Df = obj.getPatternsAsDataFrame()
-
             memUSS = obj.getmemoryUSS()
-
             print("Total Memory in USS:", memUSS)
-
             memRSS = obj.getMemoryRSS()
-
             print("Total Memory in RSS", memRSS)
-
             run = obj.getRuntime()
-
             print("Total ExecutionTime in seconds:", run)
-
     **Credits:**
     --------------------
              The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 """
     _startTime = float()
     _endTime = float()
     _minSup = str()
@@ -585,25 +565,18 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self.mine()
-
-    def mine(self) -> None:
-        """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
-        """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         minSup = self._minSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
@@ -695,15 +668,14 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
         print("Total number of Uncertain Frequent Patterns:", _ap.getPatterns())
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/TUFP.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+# UFGrowth is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#     obj = alg.TUFP(iFile, minSup)
+#     from PAMI.uncertainFrequentPattern.basic import UFGrowth as alg
+#
+#     obj = alg.UFGrowth(iFile, minSup)
 #
 #     obj.startMine()
 #
 #     frequentPatterns = obj.getPatterns()
 #
 #     print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -40,47 +41,211 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 """
 
-
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Union
-import pandas as pd
-import deprecated
 
-_minSup = float()
+_minSup = str()
+_ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
     :Attributes:
         item : int or word
             Represents the name of the item
         probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability) -> None:
+    def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
-class TUFP(_ab._frequentPatterns):
+class _Node(object):
+    """
+    A class used to represent the node of frequentPatternTree
+    :Attributes:
+        item : int
+            storing item of a node
+        probability : int
+            To maintain the expected support of node
+        parent : node
+            To maintain the parent of every node
+        children : list
+            To maintain the children of node
+    :Methods:
+        addChild(itemName)
+            storing the children to their respective parent nodes
     """
-    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
 
-    :Reference:
-        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
-        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
+    def __init__(self):
+        self.itemId = -1
+        self.counter = 0
+        self.probability = 0
+        self.child = []
+        self.parent = None
+        self.nodeLink = None
+        self.expSup = 0
+
+    def getChild(self, id1):
+        for i in self.child:
+            if i.itemid == id1:
+                return i
+        return None
+
+
+class _Tree(object):
+    """
+    A class used to represent the frequentPatternGrowth tree structure
+    :Attributes:
+        root : Node
+            Represents the root node of the tree
+        summaries : dictionary
+            storing the nodes with same item name
+        info : dictionary
+            stores the support of items
+    :Methods:
+        addTransaction(transaction)
+            creating transaction as a branch in frequentPatternTree
+        addConditionalPattern(prefixPaths, supportOfItems)
+            construct the conditional tree for prefix paths
+        conditionalPatterns(Node)
+            generates the conditional patterns from tree for specific node
+        conditionalTransactions(prefixPaths,Support)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from
+            prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
+            removes the node from tree once after generating all the patterns respective to the node
+        generatePatterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
+    """
+
+    def __init__(self):
+        self.headerList = []
+        self.mapItemNodes = {}
+        self.mapItemLastNodes = {}
+        self.root = _Node()
+
+    def fixNodeLinks(self, item, newNode):
+        if item in self.mapItemLastNodes.keys():
+            lastNode = self.mapItemLastNodes[item]
+            lastNode.nodeLink = newNode
+        self.mapItemLastNodes[item] = newNode
+        if item not in self.mapItemNodes.keys():
+            self.mapItemNodes[item] = newNode
+
+    def addTransaction(self, transaction):
+        y = 0
+        current = self.root
+        for i in transaction:
+            child = current.getChild(i.item)
+            if child is None:
+                newNode = _Node()
+                newNode.counter = 1
+                newNode.probability = i.probability
+                newNode.itemId = i.item
+                newNode.expSup = i.probability
+                newNode.parent = current
+                current.child.append(newNode)
+                self.fixNodeLinks(i.item, newNode)
+                current = newNode
+            else:
+                if child.probability == i.probability:
+                    child.counter += 1
+                    current = child
+                else:
+                    newNode = _Node()
+                    newNode.counter = 1
+                    newNode.itemId = i.item
+                    newNode.probability = i.probability
+                    newNode.expSup = i.probability
+                    newNode.parent = current
+                    current.child.append(newNode)
+                    self.fixNodeLinks(i.item, newNode)
+                    current = newNode
+        return y
 
+    def printTree(self, root):
+        if root.child is []:
+            return
+        else:
+            for i in root.child:
+                print(i.itemid, i.counter)
+                self.printTree(i)
+
+    def update(self, mapSup, u1):
+        t1 = []
+        for i in mapSup:
+            if i in u1:
+                t1.append(i)
+        return t1
+
+    def createHeaderList(self, mapSupport, min_sup):
+        t1 = []
+        for x, y in mapSupport.items():
+            if y >= min_sup:
+                t1.append(x)
+        mapSup = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.headerList = self.update(mapSup, t1)
+
+    def addPrefixPath(self, prefix, mapSupportBeta, min_sup):
+        q = 0
+        pathCount = prefix[0].counter
+        current = self.root
+        prefix.reverse()
+        for i in range(0, len(prefix) - 1):
+            pathItem = prefix[i]
+            # pathCount=mapSupportBeta.get(pathItem.itemId)
+            if mapSupportBeta.get(pathItem.itemId) >= min_sup:
+                child = current.getChild(pathItem.itemId)
+                if child is None:
+                    newNode = _Node()
+                    q += 1
+                    newNode.itemid = pathItem.itemId
+                    if newNode.expSup == 0:
+                        newNode.expSup = pathItem.expSup
+                    newNode.probability = pathItem.probability
+                    newNode.parent = current
+                    newNode.counter = pathCount
+                    current.child.append(newNode)
+                    current = newNode
+                    self.fixNodeLinks(pathItem.itemid, newNode)
+                else:
+                    if child.probability == prefix[i].probability:
+                        child.counter += pathCount
+                        child.expSup = child.expSup * pathItem.expSup
+                        current = child
+                    else:
+                        newNode = _Node()
+                        q += 1
+                        newNode.itemId = pathItem.itemId
+                        newNode.probability = pathItem.probability
+                        if newNode.expSup == 0:
+                            newNode.expSup = pathItem.expSup
+                        newNode.parent = current
+                        newNode.counter = pathCount
+                        current.child.append(newNode)
+                        current = newNode
+                        self.fixNodeLinks(pathItem.itemid, newNode)
+        return q
+
+
+class UFGrowth(_ab._frequentPatterns):
+    """
+    :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
+    :Reference:
+        Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
+        Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
     :Attributes:
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
@@ -106,23 +271,22 @@
             To represent the total no of transaction
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
-
     :Methods:
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
+        save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
@@ -134,77 +298,69 @@
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
-
     **Methods to execute code on terminal**
-    -----------------------------------------
+    ----------------------------------------
             Format:
-                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
+                      >>>  python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
             Example:
-                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
+                      >>>  python3 PUFGrowth.py sampleTDB.txt patterns.txt 3
                       .. note:: minSup  will be considered in support count or frequency
-
     **Importing this algorithm into a python program**
-    ------------------------------------------------------
+    --------------------------------------------------------
     .. code-block:: python
-
-            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
-
-            obj = alg.TUFP(iFile, minSup)
-
+            from PAMI.uncertainFrequentPattern.basic import UFGrowth as alg
+            obj = alg.UFGrowth(iFile, minSup)
             obj.startMine()
-
             frequentPatterns = obj.getPatterns()
-
             print("Total number of Frequent Patterns:", len(frequentPatterns))
-
             obj.save(oFile)
-
             Df = obj.getPatternsAsDataFrame()
-
             memUSS = obj.getmemoryUSS()
-
             print("Total Memory in USS:", memUSS)
-
             memRSS = obj.getMemoryRSS()
-
             print("Total Memory in RSS", memRSS)
-
             run = obj.getRuntime()
-
             print("Total ExecutionTime in seconds:", run)
-
     **Credits:**
-    ---------------
-    The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+    -----------------
+             The complete program was written by P.Likhitha under the supervision of Professor Rage Uday Kiran.
     """
-
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _cupList = {}
-    _topk = {}
-    _minimum = 9999
+    _rank = {}
+    _mapSupport = {}
+    _lno = 0
+    _tree = _Tree()
+    _itemsetBuffer = None
+    _fpNodeTempBuffer = []
+    _maxPatternLength = 1000
+    _itemsetCount = 0
+    _frequentitems = {}
+    _fpnode = 0
+    _conditionalnodes = 0
 
+    def __init__(self, iFile, minSup, sep='\t'):
+        super().__init__(iFile, minSup, sep)
 
-
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
-        Scans the dataset
+        Scans the uncertain transactional dataset
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -251,265 +407,240 @@
                                 probability = float(i[i1 + 1:i2])
                                 product = _Item(item, probability)
                                 tr.append(product)
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _frequentOneItem(self) -> List[str]:
+    def _frequentOneItem(self):
         """
         takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
         """
 
         mapSupport = {}
-        k = 0
         for i in self._Database:
-            k += 1
             for j in i:
                 if j.item not in mapSupport:
                     mapSupport[j.item] = j.probability
-                    self._cupList[j.item] = {k:j.probability}
                 else:
                     mapSupport[j.item] += j.probability
-                    self._cupList[j.item].update({k: j.probability})
-        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        k = 0
-        for x, in plist:
-            k +=1
-            if k >= self._minSup:
-                break
-            self._finalPatterns[x] = mapSupport[x]
-        self._minimum = min(list(self._finalPatterns.values()))
-        return plist
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        return mapSupport, plist
+
+    def _ufgrowth(self, tree, prefix, prefixLength, prefixSupport, mapSupport):
+        if prefixLength == self._maxPatternLength:
+            return
+        singlePath = True
+        position = 0
+        s = 0
+        if len(tree.root.child) > 1:
+            singlePath = False
+        else:
+            currentNode = tree.root.child[0]
+            while True:
+                if len(currentNode.child) > 1:
+                    singlePath = False
+                    break
+                self._fpNodeTempBuffer.insert(position, currentNode)
+                s = currentNode.counter
+                position += 1
+                if len(currentNode.child) == 0:
+                    break
+                currentNode = currentNode.child[0]
+        if singlePath is True:
+            self._saveAllcombinations(self._fpNodeTempBuffer, s, position, prefix, prefixLength)
+        else:
+            for i in reversed(tree.headerList):
+                item = i
+                betaSupport = mapSupport[item]
+                prefix.insert(prefixLength, item)
+                # print prefix,betaSupport
+                self._saveItemset(prefix, prefixLength + 1, betaSupport)
+                if prefixLength + 1 < self._maxPatternLength:
+                    prefixPaths = []
+                    path = tree.mapItemNodes.get(item)
+                    mapSupportBeta = {}
+                    while path is not None:
+                        if path.parent.itemid != -1:
+                            prefixPath = []
+                            prefixPath.append(path)
+                            pathCount = path.counter
+                            parent1 = path.parent
+                            while parent1.itemid != -1:
+                                prefixPath.append(parent1)
+                                s = (pathCount * path.expSup) * parent1.probability
+                                if mapSupportBeta.get(parent1.itemid) == None:
+                                    mapSupportBeta[parent1.itemid] = s
+                                else:
+                                    mapSupportBeta[parent1.itemid] = mapSupportBeta[parent1.itemid] + s
+                                parent1 = parent1.parent
+                            prefixPaths.append(prefixPath)
+                        path = path.nodeLink
+                    treeBeta = _Tree()
+                    for i in prefixPaths:
+                        q = treeBeta.addPrefixPath(i, mapSupportBeta, self._minSup)
+                        self._conditionalnodes += q
+                    if len(treeBeta.root.child) > 0:
+                        treeBeta.createHeaderList(mapSupportBeta, self._minSup)
+                        # print(treeBeta.headerList)
+                        self._ufgrowth(treeBeta, prefix, prefixLength + 1, betaSupport, mapSupportBeta)
+
+    def _saveItemset(self, prefix, prefixLength, support):
+        l = []
+        for i in range(prefixLength):
+            l.append(prefix[i])
+        self._itemsetCount += 1
+        l.sort()
+        s = '\t'.join(l)
+        self._finalPatterns[s] = support
+
+    def _saveAllcombinations(self, TempBuffer, s, position, prefix, prefixLength):
+        # support=0
+        max1 = 1 << position
+        for i in range(1, max1):
+            newprefixLength = prefixLength
+            for j in range(position):
+                isset = i & (1 << j)
+                if isset > 0:
+                    prefix.insert(newprefixLength, TempBuffer[j].itemid)
+                    newprefixLength += 1
+                    support = TempBuffer[j].counter
+            self._saveItemset(prefix, newprefixLength, s)
 
-    @staticmethod
-    def _convert(value: Union[int, float, str]) -> Union[int, float]:
+    def _convert(self, value):
         """
         To convert the type of user specified minSup value
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
-        """
-        Saves the patterns that satisfy the periodic frequent property.
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = sum(tidSetI.values())
-        # print(prefix, val)
-        if len(self._finalPatterns) <= self._minSup:
-            sample = str()
-            for i in prefix:
-                sample = sample + i + " "
-            self._finalPatterns[sample] = val
-        if len(self._finalPatterns) == self._minSup:
-            if val > self._minimum:
-                sample = str()
-                for i in prefix:
-                    sample = sample + i + " "
-                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
-                del self._finalPatterns[index]
-                self._finalPatterns[sample] = val
-                self._minimum = min(list(self._finalPatterns.values()))
-        # print(self.finalPatterns, self.minimum, self.minSup)
-
-    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(0, len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
-                sum2 = sum(list(y.values()))
-                # print(prefix, itemJ, y, sum2)
-                # if sum2 >= self.minimum:
-                self._save(prefix, [itemJ], y)
-                classItemSets.append(itemJ)
-                classTidSets.append(y)
-            # print(itemI, tidSetI, classItemSets)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            # self.save(prefix, list(set(itemSetX)), tidSetI)
-
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self) -> None:
+    def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self.mine()
-
-    def mine(self) -> None:
-        """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
-        """
-        global _minSup
+        global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._cupList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._cupList[itemJ]
-                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
-                self._save(itemSetX, [itemJ], y1)
-                itemSets.append(itemJ)
-                tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
+        minSup = self._minSup
+        self._finalPatterns = {}
+        _mapSupport, plist = self._frequentOneItem()
+        for i in self._Database:
+            transaction = []
+            for j in i:
+                if _mapSupport.get(j.item) >= self._minSup:
+                    transaction.append(j)
+            transaction.sort(key=lambda val: _mapSupport[val.item], reverse=True)
+            o = self._tree.addTransaction(transaction)
+        self._tree.createHeaderList(_mapSupport, self._minSup)
+        if len(self._tree.headerList) > 0:
+            self._itemsetBuffer = []
+            # self.fpNodeTempBuffer=[]
+            self._ufgrowth(self._tree, self._itemsetBuffer, 0, self._lno, _mapSupport)
+        print("Frequent patterns were generated from uncertain databases successfully using UF algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self._memoryRSS = float()
+        self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
+            data.append([a.replace('\t', ' '), b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, float]:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
         """
         print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
-    if __name__ == "__main__":
-        _ap = str()
-        if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-            if len(_ab._sys.argv) == 5:
-                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-            if len(_ab._sys.argv) == 4:
-                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
-            _ap.startMine()
-            _ap.mine()
-            _Patterns = _ap.getPatterns()
-            print("Total number of Patterns:", len(_Patterns))
-            _ap.save(_ab._sys.argv[2])
-            _memUSS = _ap.getMemoryUSS()
-            print("Total Memory in USS:", _memUSS)
-            _memRSS = _ap.getMemoryRSS()
-            print("Total Memory in RSS", _memRSS)
-            _run = _ap.getRuntime()
-            print("Total ExecutionTime in ms:", _run)
-        else:
-            '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
-            ap.startMine()
-            Patterns = ap.getPatterns()
-            print("Total number of Patterns:", len(Patterns))
-            ap.save("patterns.txt")
-            memUSS = ap.getMemoryUSS()
-            print("Total Memory in USS:", memUSS)
-            memRSS = ap.getMemoryRSS()
-            print("Total Memory in RSS", memRSS)
-            run = ap.getRuntime()
-            print("Total ExecutionTime in ms:", run)'''
-            print("Error! The number of input parameters do not match the total number of parameters provided")
 
+if __name__ == "__main__":
+    _ap = str()
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:
+            _ap = UFGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = UFGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
+        _ap.startMine()
+        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
+    else:
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/TubeP.py` & `pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,105 +1,361 @@
-# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+# GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#     obj = alg.TUFP(iFile, minSup)
+#             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 #
-#     obj.startMine()
+#             obj = alg.GFPGrowth(iFile, nFile, minSup,sep, oFile)
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             Patterns = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of  Patterns:", len(Patterns))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save(oFile)
 #
-#     memUSS = obj.getMemoryUSS()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     print("Total Memory in USS:", memUSS)
+#             memUSS = obj.getMemoryUSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     run = obj.getRuntime()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
+#
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
-"""
-
 
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Union
+"""
+from PAMI.uncertainGeoreferencedFrequentPattern.basic import abstract as _ab
 import pandas as pd
-import deprecated
-_minSup = float()
+from deprecated import deprecated
+
+_minSup = str()
+_neighbourList = {}
+_ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
+
     :Attributes:
+
         item : int or word
             Represents the name of the item
         probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability) -> None:
+    def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
-class TUFP(_ab._frequentPatterns):
+class _Node(object):
     """
-    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+    A class used to represent the node of frequentPatternTree
+
+    :Attributes:
+
+        item : int
+            storing item of a node
+        probability : int
+            To maintain the expected support of node
+        parent : node
+            To maintain the parent of every node
+        children : list
+            To maintain the children of node
+
+    :Methods:
+
+        addChild(itemName)
+            storing the children to their respective parent nodes
+    """
+
+    def __init__(self, item, children):
+        self.item = item
+        self.probability = 1
+        self.children = children
+        self.parent = None
+
+    def addChild(self, node):
+        self.children[node.item] = node
+        node.parent = self
+
+
+class _Tree(object):
+    """
+    A class used to represent the frequentPatternGrowth tree structure
+
+    :Attributes:
+
+        root : Node
+            Represents the root node of the tree
+        summaries : dictionary
+            storing the nodes with same item name
+        info : dictionary
+            stores the support of items
+    :Methods:
+
+        addTransaction(transaction)
+            creating transaction as a branch in frequentPatternTree
+        addConditionalPattern(prefixPaths, supportOfItems)
+            construct the conditional tree for prefix paths
+        conditionalPatterns(Node)
+            generates the conditional patterns from tree for specific node
+        conditionalTransactions(prefixPaths,Support)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from
+            prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
+            removes the node from tree once after generating all the patterns respective to the node
+        generatePatterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
+    """
+
+    def __init__(self):
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction):
+        """
+        Adding transaction into tree
+
+        :param transaction : it represents the one self.Database in database
+        :type transaction : list
+        """
+        global _neighbourList
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                nei = _neighbourList.get(transaction[i].item)
+                l1 = i - 1
+                lp = []
+                while l1 >= 0:
+                    if nei == None:
+                        break
+                    if transaction[l1].item in nei:
+                        lp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(lp) == 0:
+                    newNode.probability = transaction[i].probability
+                else:
+                    newNode.probability = max(lp) * transaction[i].probability
+                currentNode.addChild(newNode)
+                if transaction[i].item in self.summaries:
+                    self.summaries[transaction[i].item].append(newNode)
+                else:
+                    self.summaries[transaction[i].item] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].item]
+                l1 = i - 1
+                lp = []
+                while l1 >= 0:
+                    lp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(lp) == 0:
+                    currentNode.probability += transaction[i].probability
+                else:
+                    currentNode.probability += max(lp) * transaction[i].probability
+
+    def addConditionalPattern(self, transaction, sup):
+        """
+        constructing conditional tree from prefixPaths
+
+        :param transaction : it represents the one self.Database in database
+        :type transaction : list
+        :param sup : support of prefixPath taken at last child of the path
+        :type sup : int
+        """
+
+        # This method takes transaction, support and constructs the conditional tree
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                newNode.probability = sup
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+                currentNode.probability += sup
+
+    def conditionalPatterns(self, alpha):
+        """
+        Generates all the conditional patterns of respective node
+
+        :param alpha : it represents the Node in tree
+        :type alpha : _Node
+        """
+
+        # This method generates conditional patterns of node by traversing the tree
+        global _neighbourList
+        finalPatterns = []
+        sup = []
+        for i in self.summaries[alpha]:
+            j = i.item
+            s = i.probability
+            set2 = []
+            while i.parent.item is not None:
+                if _neighbourList.get(j) is not None:
+                    #print(_neighbourList.get(j))
+                    if i.parent.item in _neighbourList[j]:
+                        set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                sup.append(s)
+        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
+        return finalPatterns, support, info
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
+
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        """
+
+        for i in self.summaries[nodeValue]:
+            del i.parent.children[nodeValue]
+
+    def conditionalTransactions(self, condPatterns, support):
+        """
+        It generates the conditional patterns with frequent items
+
+        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
+        :type condPatterns : list
+        :support : the support of conditional pattern in tree
+        :support : int
+        """
+
+        global minSup
+        pat = []
+        sup = []
+        count = {}
+        for i in range(len(condPatterns)):
+            for j in condPatterns[i]:
+                if j in count:
+                    count[j] += support[i]
+                else:
+                    count[j] = support[i]
+        updatedDict = {}
+        updatedDict = {k: v for k, v in count.items() if v >= minSup}
+        count = 0
+        for p in condPatterns:
+            p1 = [v for v in p if v in updatedDict]
+            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                sup.append(support[count])
+                count += 1
+        return pat, sup, updatedDict
+
+    def generatePatterns(self, prefix):
+        """
+        Generates the patterns
+
+        :param prefix : forms the combination of items
+        :type prefix : list
+        """
+
+        global _finalPatterns, minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+            pattern = prefix[:]
+            pattern.append(i)
+            s = 0
+            for x in self.summaries[i]:
+                s += x.probability
+            _finalPatterns[tuple(pattern)] = self.info[i]
+            if s >= minSup:
+                patterns, support, info = self.conditionalPatterns(i)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
+                if len(patterns) > 0:
+                    conditionalTree.generatePatterns(pattern)
+            self.removeNode(i)
+
+
+class GFPGrowth(_ab._frequentPatterns):
+    """
+    :Description: GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 
     :Reference:
-        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
-        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
+         Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
+         "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
+         Transactional Databases".  PAKDD 2023.
+         https://doi.org/10.1007/978-3-031-33380-4_3
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of uncertain Geo referenced Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Geo referenced frequent patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
+
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
-        minSup : float or int or str
+        minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
             To represent the total no of transaction
@@ -107,21 +363,22 @@
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
+
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
+        savePatterns(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
@@ -134,74 +391,83 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
 
-    **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
-            Example:
-                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
-
-                      .. note:: minSup  will be considered in support count or frequency
-
-    **Importing this algorithm into a python program**
-    ------------------------------------------------------
-    .. code-block:: python
-            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+    **Executing the code on terminal**:
+    ------------------------------------------
+
+    .. code-block:: console
 
-            obj = alg.TUFP(iFile, minSup)
+
+       Format:
+
+       (.venv) $ python3 GFPGrowth.py <inputFile> <neighborFile> <outputFile> <minSup>
+
+       Examples usage:
+
+       (.venv) $ python3 GFPGrowth.py sampleTDB.txt sampleNeighbor.txt patterns.txt 3
+
+
+               .. note:: minSup  will be considered in support count or frequency
+    
+    **Sample run of importing the code**:
+    ----------------------------------------
+     .. code-block:: python
+
+            from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
+
+            obj = alg.GFPGrowth(iFile, nFile, minSup)
 
             obj.startMine()
 
-            frequentPatterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of  Patterns:", len(Patterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getmemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-
-    **Credits:**
-    ---------------
-        The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
-    """
-
+        
+    **Credits**:
+    -------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+        """
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _cupList = {}
-    _topk = {}
-    _minimum = 9999
+    _rank = {}
+
+    def __init__(self, iFile, nFile, minSup, sep='\t'):
+        super().__init__(iFile, nFile, minSup, sep)
 
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
-        Scans the dataset
+        Scans the uncertain transactional dataset
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -217,14 +483,71 @@
                 self._Database.append(tr)
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
+                    temp1 = line.strip()
+                    temp1 = temp1.split(':')
+                    temp = [i.rstrip() for i in temp1[0].split(self._sep)]
+                    uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
+                    tr = []
+                    for i in range(len(temp)):
+                        item = temp[i]
+                        probability = uncertain[i]
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(tr)
+            else:
+                try:
+                    with open(self._iFile, 'r') as f:
+                        for line in f:
+                            temp1 = line.strip()
+                            temp1 = temp1.split(':')
+                            #temp1[0], temp1[1] = [i for i in temp1[0] if i], [i for i in temp1[1] if i]
+                            temp = [i.rstrip() for i in temp1[0].split(self._sep) if i]
+                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep) if i]
+                            tr = []
+                            for i in range(len(temp)):
+                                item = temp[i]
+                                probability = uncertain[i]
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
+                except IOError:
+                    print("File Not Found")
+                    
+    def _creatingNeighbours(self):
+        """
+        Scans the uncertain transactional dataset
+        """
+        global _neighbourList
+        _neighbourList = {}
+        if isinstance(self._nFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'Transactions' in i:
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
+        if isinstance(self._nFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     tr = []
                     for i in temp:
                         i1 = i.index('(')
@@ -232,283 +555,291 @@
                         item = i[0:i1]
                         probability = float(i[i1 + 1:i2])
                         product = _Item(item, probability)
                         tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._nFile, 'r') as f:
                         for line in f:
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._Database.append(tr)
+                            _neighbourList[temp[0]] = temp[1:]
                 except IOError:
                     print("File Not Found")
 
-    def _frequentOneItem(self) -> List[str]:
+    def _frequentOneItem(self):
         """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
         """
 
         mapSupport = {}
-        k = 0
         for i in self._Database:
-            k += 1
             for j in i:
                 if j.item not in mapSupport:
                     mapSupport[j.item] = j.probability
-                    self._cupList[j.item] = {k:j.probability}
                 else:
                     mapSupport[j.item] += j.probability
-                    self._cupList[j.item].update({k: j.probability})
-        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        k = 0
-        for x, in plist:
-            k +=1
-            if k >= self._minSup:
-                break
-            self._finalPatterns[x] = mapSupport[x]
-        self._minimum = min(list(self._finalPatterns.values()))
-        return plist
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        return mapSupport, plist
 
     @staticmethod
-    def _convert(value: Union[int, float, str]) -> Union[int, float]:
+    def _buildTree(data, info):
+        """
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+
+        :param data : it represents the one self.Database in database
+        :type data : list
+        :param info : it represents the support of each item
+        :type info : dictionary
+        """
+
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            rootNode.addTransaction(data[i])
+        return rootNode
+
+    def _updateTransactions(self, dict1):
+        """
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+
+        :param dict1 : frequent items with support
+        :type dict1 : dictionary
+        """
+
+        list1 = []
+        for tr in self._Database:
+            list2 = []
+            for i in range(0, len(tr)):
+                if tr[i].item in dict1:
+                    list2.append(tr[i])
+            if len(list2) >= 2:
+                basket = list2
+                basket.sort(key=lambda val: self.rank[val.item])
+                list2 = basket
+                list1.append(list2)
+        return list1
+
+    @staticmethod
+    def _check(i, x):
+        """
+        To check the presence of item or pattern in transaction
+
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    def _convert(self, value):
         """
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
+    def _removeFalsePositives(self):
+        """
+        To remove the false positive patterns generated in frequent patterns.
+
+        :return: patterns with accurate probability
         """
-        Saves the patterns that satisfy the periodic frequent property.
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = sum(tidSetI.values())
-        #print(prefix, val)
-        if len(self._finalPatterns) <= self._minSup:
-            sample = str()
-            for i in prefix:
-                sample = sample + i + " "
-            self._finalPatterns[sample] = val
-        if len(self._finalPatterns) == self._minSup:
-            if val > self._minimum:
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
+                else:
+                    s = 1
+                    check = self._check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
                 sample = str()
-                for i in prefix:
-                    sample = sample + i + " "
-                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
-                del self._finalPatterns[index]
-                self._finalPatterns[sample] = val
-                self._minimum = min(list(self._finalPatterns.values()))
-        #print(self.finalPatterns, self.minimum, self.minSup)
-
-
-    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(0, len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i+1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
-                sum2 = sum(list(y.values()))
-                #print(prefix, itemJ, y, sum2)
-                #if sum2 >= self.minimum:
-                self._save(prefix, [itemJ], y)
-                classItemSets.append(itemJ)
-                classTidSets.append(y)
-            #print(itemI, tidSetI, classItemSets)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            #self.save(prefix, list(set(itemSetX)), tidSetI)
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self) -> None:
+    def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self.mine()
-
+        global minSup
+        global minSup
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._creatingNeighbours()
+        #self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
 
-    def mine(self) -> None:
+    def Mine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global _minSup
+        global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._cupList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._cupList[itemJ]
-                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
-                self._save(itemSetX, [itemJ], y1)
-                itemSets.append(itemJ)
-                tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
+        self._creatingNeighbours()
+        # self._minSup = self._convert(self._minSup)
+        minSup = self._minSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self._memoryRSS = float()
+        self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a, b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, float]:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
+    
+    def printResults(self):
+        print("Total number of Patterns:", len(self.getPatterns()))
+        self.save("patterns.txt")
+        memUSS = self.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = self.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = self.getRuntime()
+        print("Total ExecutionTime in ms:", run)
 
-    def printResults(self) -> None:
-        """
-        This function is used to print the results
-        """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
         _Patterns = _ap.getPatterns()
         print("Total number of Patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
         _memUSS = _ap.getMemoryUSS()
         print("Total Memory in USS:", _memUSS)
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _memRSS)
         _run = _ap.getRuntime()
         print("Total ExecutionTime in ms:", _run)
     else:
-        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
-        ap.startMine()
-        Patterns = ap.getPatterns()
-        print("Total number of Patterns:", len(Patterns))
-        ap.save("patterns.txt")
-        memUSS = ap.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = ap.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = ap.getRuntime()
-        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/TubeS.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeS.py`

 * *Files 1% similar despite different names*

```diff
@@ -43,15 +43,14 @@
      GNU General Public License for more details.
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 """
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _fp
-import deprecated
 
 _minSup = float()
 _fp._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
@@ -332,19 +331,17 @@
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
 class TubeS(_fp._frequentPatterns):
     """
     :Description: TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
-
     :Reference:
         Carson Kai-Sang Leung and Richard Kyle MacKinnon. 2014. Fast Algorithms for Frequent Itemset Mining from Uncertain Data.
         In Proceedings of the 2014 IEEE International Conference on Data Mining (ICDM '14). IEEE Computer Society, USA, 893898. https://doi.org/10.1109/ICDM.2014.146
-
     :Attributes:
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
@@ -370,15 +367,14 @@
             To represent the total no of transaction
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
-
     :Methods:
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
@@ -396,53 +392,37 @@
             Extracts the one-length frequent patterns from database
         updateTransactions()
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
-
     **Methods to execute code on terminal**
     --------------------------------------------
             Format:
                       >>> python3 TubeS.py <inputFile> <outputFile> <minSup>
             Example:
                       >>>  python3 TubeS.py sampleTDB.txt patterns.txt 3
                     .. note:: minSup  will be considered in support count or frequency
-
     **Importing this algorithm into a python program**
     ---------------------------------------------------
     .. code-block:: python
-
             from PAMI.uncertainFrequentPattern.basic import TubeS as alg
-
             obj = alg.TubeS(iFile, minSup)
-
             obj.startMine()
-
             frequentPatterns = obj.getPatterns()
-
             print("Total number of Frequent Patterns:", len(frequentPatterns))
-
             obj.save(oFile)
-
             Df = obj.getPatternsAsDataFrame()
-
             memUSS = obj.getMemoryUSS()
-
             print("Total Memory in USS:", memUSS)
-
             memRSS = obj.getMemoryRSS()
-
             print("Total Memory in RSS", memRSS)
-
             run = obj.getRuntime()
-
             print("Total ExecutionTime in seconds:", run)
-
     **Credits:**
     ---------------
     The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
 """
     _startTime = float()
     _endTime = float()
     _minSup = float()
@@ -598,16 +578,16 @@
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self):
         """
-        To remove the false positive patterns generated in frequent patterns.
-        :return: Patterns with accurate probability
+        To remove the false positive patterns generated in frequent patterns
+        :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
                 if len(x) == 1:
                     periods[x] = y
@@ -625,25 +605,18 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self.mine()
-
-    def mine(self):
-        """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
-        """
         global _minSup
         self._startTime = _fp._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         _minSup = self._minSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
@@ -735,15 +708,14 @@
     _ap = str()
     if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
         if len(_fp._sys.argv) == 5:
             _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
         if len(_fp._sys.argv) == 4:
             _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
         print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_fp._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py` & `pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,333 +1,431 @@
-# UFGrowth is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
+# SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# -------------------------------------------------------
 #
+#             from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
 #
-#     from PAMI.uncertainFrequentPattern.basic import UFGrowth as alg
+#             obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, sep)
 #
-#     obj = alg.UFGrowth(iFile, minSup)
+#             obj.startMine()
 #
-#     obj.startMine()
+#             frequentPatterns = obj.getPatterns()
 #
-#     frequentPatterns = obj.getPatterns()
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             obj.save(oFile)
 #
-#     obj.save(oFile)
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             memUSS = obj.getMemoryUSS()
 #
-#     memUSS = obj.getMemoryUSS()
+#             print("Total Memory in USS:", memUSS)
 #
-#     print("Total Memory in USS:", memUSS)
+#             memRSS = obj.getMemoryRSS()
 #
-#     memRSS = obj.getMemoryRSS()
+#             print("Total Memory in RSS", memRSS)
 #
-#     print("Total Memory in RSS", memRSS)
+#             run = obj.getRuntime()
 #
-#     run = obj.getRuntime()
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
+
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
+
 """
 
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-import deprecated
+from PAMI.weightedFrequentNeighbourhoodPattern.basic import abstract as _fp
+import pandas as pd
+from deprecated import deprecated
+from typing import List, Dict, Tuple, Union, Iterable
+
+_minWS = str()
+_weights = {}
+_rank = {}
+_neighbourList = {}
 
-_minSup = str()
-_ab._sys.setrecursionlimit(20000)
-_finalPatterns = {}
+_fp._sys.setrecursionlimit(20000)
 
 
-class _Item:
+class _WeightedItem:
     """
-    A class used to represent the item with probability in transaction of dataset
+    A class used to represent the weight of the item
+
     :Attributes:
-        item : int or word
-            Represents the name of the item
-        probability : float
-            Represent the existential probability(likelihood presence) of an item
-    """
 
-    def __init__(self, item, probability):
-        self.item = item
-        self.probability = probability
+        item: str
+            storing item of the frequent pattern
+        weight: float
+            stores the weight of the item
 
+    """
+    def __init__(self, item: str, weight: float) -> None:
+        self.item = item
+        self.weight = weight
 
-class _Node(object):
 
+class _Node:
     """
     A class used to represent the node of frequentPatternTree
+
     :Attributes:
-        item : int
+
+        itemId: int
             storing item of a node
-        probability : int
-            To maintain the expected support of node
-        parent : node
-            To maintain the parent of every node
-        children : list
+        counter: int
+            To maintain the support of node
+        parent: node
+            To maintain the parent of node
+        children: list
             To maintain the children of node
+
     :Methods:
-        addChild(itemName)
-            storing the children to their respective parent nodes
+
+        addChild(node)
+            Updates the nodes children list and parent for the given node
+
     """
 
-    def __init__(self):
-        self.itemId = -1
-        self.counter = 0
-        self.probability = 0
-        self.child = []
+    def __init__(self, item: str, children: Dict[str, '_Node']) -> None:
+        self.itemId = item
+        self.counter = 1
+        self.weight = 0
         self.parent = None
-        self.nodeLink = None
-        self.expSup = 0
+        self.children = children
 
-    def getChild(self, id1):
-        for i in self.child:
-            if i.itemid == id1:
-                return i
-        return None
+    def addChild(self, node: '_Node') -> None:
+        """
+        Retrieving the child from the tree
+
+        :param node: Children node.
+        :type node: Node
+        :return: Updates the children nodes and parent nodes
+        :return: None
 
+        """
+        self.children[node.itemId] = node
+        node.parent = self
 
-class _Tree(object):
+
+class _Tree:
     """
     A class used to represent the frequentPatternGrowth tree structure
+
     :Attributes:
+
         root : Node
-            Represents the root node of the tree
+            The first node of the tree set to Null.
         summaries : dictionary
-            storing the nodes with same item name
+            Stores the nodes itemId which shares same itemId
         info : dictionary
-            stores the support of items
+            frequency of items in the transactions
+
     :Methods:
-        addTransaction(transaction)
-            creating transaction as a branch in frequentPatternTree
-        addConditionalPattern(prefixPaths, supportOfItems)
-            construct the conditional tree for prefix paths
-        conditionalPatterns(Node)
-            generates the conditional patterns from tree for specific node
-        conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from
-            prefixPaths and generates prefixPaths with items which are frequent
-        remove(Node)
-            removes the node from tree once after generating all the patterns respective to the node
-        generatePatterns(Node)
-            starts from the root node of the tree and mines the frequent patterns
+
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minWS
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
     """
 
-    def __init__(self):
-        self.headerList = []
-        self.mapItemNodes = {}
-        self.mapItemLastNodes = {}
-        self.root = _Node()
-
-    def fixNodeLinks(self, item, newNode):
-        if item in self.mapItemLastNodes.keys():
-            lastNode = self.mapItemLastNodes[item]
-            lastNode.nodeLink = newNode
-        self.mapItemLastNodes[item] = newNode
-        if item not in self.mapItemNodes.keys():
-            self.mapItemNodes[item] = newNode
-
-    def addTransaction(self, transaction):
-        y = 0
-        current = self.root
-        for i in transaction:
-            child = current.getChild(i.item)
-            if child is None:
-                newNode = _Node()
-                newNode.counter = 1
-                newNode.probability = i.probability
-                newNode.itemId = i.item
-                newNode.expSup = i.probability
-                newNode.parent = current
-                current.child.append(newNode)
-                self.fixNodeLinks(i.item, newNode)
-                current = newNode
+    def __init__(self) -> None:
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction: List[_WeightedItem], count: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return: None
+        """
+
+        # This method takes transaction as input and returns the tree
+        global _neighbourList, _rank
+        currentNode = self.root
+        for i in range(len(transaction)):
+            wei = 0
+            l1 = i
+            while l1 >= 0:
+                wei += transaction[l1].weight
+                l1 -= 1
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                newNode.freq = count
+                newNode.weight = wei
+                currentNode.addChild(newNode)
+                if _rank[transaction[i].item] in self.summaries:
+                    self.summaries[_rank[transaction[i].item]].append(newNode)
+                else:
+                    self.summaries[_rank[transaction[i].item]] = [newNode]
+                currentNode = newNode
             else:
-                if child.probability == i.probability:
-                    child.counter += 1
-                    current = child
+                currentNode = currentNode.children[transaction[i].item]
+                currentNode.freq += count
+                currentNode.weight += wei
+
+    def addConditionalPattern(self, transaction: List[_WeightedItem], count: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return : None
+        """
+        # This method takes transaction as input and returns the tree
+        global _neighbourList, _rank
+        currentNode = self.root
+        for i in range(len(transaction)):
+            wei = 0
+            l1 = i
+            while l1 >= 0:
+                wei += transaction[l1].weight
+                l1 -= 1
+            if transaction[i].itemId not in currentNode.children:
+                newNode = _Node(transaction[i].itemId, {})
+                newNode.freq = count
+                newNode.weight = wei
+                currentNode.addChild(newNode)
+                if _rank[transaction[i].itemId] in self.summaries:
+                    self.summaries[_rank[transaction[i].itemId]].append(newNode)
                 else:
-                    newNode = _Node()
-                    newNode.counter = 1
-                    newNode.itemId = i.item
-                    newNode.probability = i.probability
-                    newNode.expSup = i.probability
-                    newNode.parent = current
-                    current.child.append(newNode)
-                    self.fixNodeLinks(i.item, newNode)
-                    current = newNode
-        return y
+                    self.summaries[_rank[transaction[i].itemId]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].itemId]
+                currentNode.freq += count
+                currentNode.weight += wei
+
+    def printTree(self, root: _Node) -> None:
+        """
+        To print the details of tree
 
-    def printTree(self, root):
-        if root.child is []:
+        :param root: root node of the tree
+        :return: details of tree
+        """
+        if len(root.children) == 0:
             return
         else:
-            for i in root.child:
-                print(i.itemid, i.counter)
-                self.printTree(i)
-
-    def update(self, mapSup, u1):
-        t1 = []
-        for i in mapSup:
-            if i in u1:
-                t1.append(i)
-        return t1
-
-    def createHeaderList(self, mapSupport, min_sup):
-        t1 = []
-        for x, y in mapSupport.items():
-            if y >= min_sup:
-                t1.append(x)
-        mapSup = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.headerList = self.update(mapSup, t1)
-
-    def addPrefixPath(self, prefix, mapSupportBeta, min_sup):
-        q = 0
-        pathCount = prefix[0].counter
-        current = self.root
-        prefix.reverse()
-        for i in range(0, len(prefix) - 1):
-            pathItem = prefix[i]
-            # pathCount=mapSupportBeta.get(pathItem.itemId)
-            if mapSupportBeta.get(pathItem.itemId) >= min_sup:
-                child = current.getChild(pathItem.itemId)
-                if child is None:
-                    newNode = _Node()
-                    q += 1
-                    newNode.itemid = pathItem.itemId
-                    if newNode.expSup == 0:
-                        newNode.expSup = pathItem.expSup
-                    newNode.probability = pathItem.probability
-                    newNode.parent = current
-                    newNode.counter = pathCount
-                    current.child.append(newNode)
-                    current = newNode
-                    self.fixNodeLinks(pathItem.itemid, newNode)
+            for x, y in root.children.items():
+                #print(y.itemId, y.parent.itemId, y.freq, y.weight)
+                self.printTree(y)
+
+
+    def getFinalConditionalPatterns(self, alpha: int) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+        """
+        Generates the conditional patterns for a node
+
+        :param alpha: node to generate conditional patterns
+        :return: returns conditional patterns, frequency of each item in conditional patterns
+
+        """
+        finalPatterns = []
+        finalFreq = []
+        global _neighbourList
+        for i in self.summaries[alpha]:
+            set1 = i.weight
+            set2 = []
+            while i.parent.itemId is not None:
+                if i.parent.itemId in _neighbourList[i.itemId]:
+                    set2.append(i.parent)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalFreq.append(set1)
+        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
+        return finalPatterns, finalFreq, info
+
+    @staticmethod
+    def getConditionalTransactions(ConditionalPatterns: List[List[_Node]], conditionalFreq: List[float]) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+        """
+        To calculate the frequency of items in conditional patterns and sorting the patterns
+
+        :param ConditionalPatterns: paths of a node
+        :param conditionalFreq: frequency of each item in the path
+        :return: conditional patterns and frequency of each item in transactions
+        """
+        global _rank
+        pat = []
+        freq = []
+        data1 = {}
+        for i in range(len(ConditionalPatterns)):
+            for j in ConditionalPatterns[i]:
+                if j.itemId in data1:
+                    data1[j.itemId] += conditionalFreq[i]
                 else:
-                    if child.probability == prefix[i].probability:
-                        child.counter += pathCount
-                        child.expSup = child.expSup * pathItem.expSup
-                        current = child
-                    else:
-                        newNode = _Node()
-                        q += 1
-                        newNode.itemId = pathItem.itemId
-                        newNode.probability = pathItem.probability
-                        if newNode.expSup == 0:
-                            newNode.expSup = pathItem.expSup
-                        newNode.parent = current
-                        newNode.counter = pathCount
-                        current.child.append(newNode)
-                        current = newNode
-                        self.fixNodeLinks(pathItem.itemid, newNode)
-        return q
+                    data1[j.itemId] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= _minWS}
+        count = 0
+        for p in ConditionalPatterns:
+            p1 = [v for v in p if v.itemId in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x)), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                freq.append(conditionalFreq[count])
+            count += 1
+        up_dict = {_rank[k]: v for k, v in up_dict.items()}
+        return pat, freq, up_dict
+
+    def generatePatterns(self, prefix: List[int]) -> Iterable[Tuple[List[int], float]]:
+        """
+        To generate the frequent patterns
+
+        :param prefix: an empty list
+        :return: Frequent patterns that are extracted from fp-tree
+
+        """
+        global _minWS
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+            pattern = prefix[:]
+            pattern.append(i)
+            yield pattern, self.info[i]
+            patterns, freq, info = self.getFinalConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addConditionalPattern(patterns[pat], freq[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
 
 
-class UFGrowth(_ab._frequentPatterns):
+class SWFPGrowth(_fp._weightedFrequentSpatialPatterns):
     """
-    :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
+    :Description: SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 
     :Reference:
-        Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
-        Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
+        R. Uday Kiran, P. P. C. Reddy, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
+        "Discovering Spatial Weighted Frequent Itemsets in Spatiotemporal Databases," 2019 International
+        Conference on Data Mining Workshops (ICDMW), 2019, pp. 987-996, doi: 10.1109/ICDMW.2019.00143.
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of weighted Frequent Neighbourhood Patterns.
+    :param  oFile: str :
+                   Name of the output file to store complete set of weighted Frequent Neighbourhood Patterns.
+    :param  minSup: int or str or float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: floot :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
+
 
     :Attributes:
+
         iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Input file name or path of the input file
+        minWS: float or int or str
+            The user can specify minWS either in count or proportion of database size.
+            If the program detects the data type of minWS is integer, then it treats minWS is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minWS=10 will be treated as integer, while minWS=10.0 will be treated as float
+        minWeight: float or int or str
+            The user can specify minWeight either in count or proportion of database size.
+            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
+        oFile : file
+            Name of the output file or the path of the output file
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime : float
-            To record the start time of the mining process
-        endTime : float
-            To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
         lno : int
-            To represent the total no of transaction
+            it represents the total no of transactions
         tree : class
-            To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
+            it represents the Tree class
         finalPatterns : dict
-            To store the complete patterns
+            it represents to store the patterns
+
+    :Methods :
 
-    :Methods:
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
         frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
-        buildTree()
-            After updating the Database, remaining items will be added into the tree by setting root node as null
-        convert()
-            to convert the user specified value
-        startMine()
-            Mining process will start from this function
+            Extracts the one-frequent patterns from transactions
 
     **Methods to execute code on terminal**
-    ----------------------------------------
-            Format:
-                      >>>  python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
-            Example:
-                      >>>  python3 PUFGrowth.py sampleTDB.txt patterns.txt 3
+    -------------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 SWFPGrowth.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+
+       Example usage :
+
+       (.venv) $ python3 SWFPGrowth.py sampleDB.txt weightFile.txt patterns.txt 10  2
+
+
+               .. note:: minSup will be considered in support count or frequency
 
-                      .. note:: minSup  will be considered in support count or frequency
 
     **Importing this algorithm into a python program**
-    --------------------------------------------------------
+    ----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import UFGrowth as alg
+            from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
 
-            obj = alg.UFGrowth(iFile, minSup)
+            obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, seperator)
 
             obj.startMine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
@@ -344,334 +442,383 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -----------------
-             The complete program was written by P.Likhitha under the supervision of Professor Rage Uday Kiran.
-    """
-    _startTime = float()
-    _endTime = float()
-    _minSup = str()
-    _finalPatterns = {}
+    --------------
+    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
+        """
+
+    __startTime = float()
+    __endTime = float()
+    _Weights = {}
+    _minWS = str()
+    __finalPatterns = {}
+    _neighbourList = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _rank = {}
-    _mapSupport = {}
-    _lno = 0
-    _tree = _Tree()
-    _itemsetBuffer = None
-    _fpNodeTempBuffer = []
-    _maxPatternLength = 1000
-    _itemsetCount = 0
-    _frequentitems = {}
-    _fpnode = 0
-    _conditionalnodes = 0
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __tree = _Tree()
+    __rank = {}
+    __rankDup = {}
 
-    def __init__(self, iFile, minSup, sep='\t'):
-        super().__init__(iFile, minSup, sep)
+    def __init__(self, iFile: Union[str, _fp._pd.DataFrame], nFile: Union[str, _fp._pd.DataFrame], minWS: Union[int, float, str], sep='\t') -> None:
+        super().__init__(iFile, nFile, minWS, sep)
 
-    def _creatingItemSets(self):
+    def __creatingItemSets(self) -> None:
         """
-        Scans the uncertain transactional dataset
+        Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
-
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
+                            line = line.strip()
+                            line = line.split(':')
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [int(i.strip()) for i in line[1].split(self._sep)]
                             tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
+                            for i in range(len(temp1)):
+                                we = _WeightedItem(temp1[i], temp2[i])
+                                tr.append(we)
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _frequentOneItem(self):
-        """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-        :param self.Database : it represents the one self.Database in database
-        :type self.Database : list
-        """
+    def _scanNeighbours(self) -> None:
+        self._neighbourList = {}
+        if isinstance(self._nFile, _fp._pd.DataFrame):
+            data, items = [], []
+            if self._nFile.empty:
+                print("its empty..")
+            i = self._nFile.columns.values.tolist()
+            if 'item' in i:
+                items = self._nFile['items'].tolist()
+            if 'Neighbours' in i:
+                data = self._nFile['Neighbours'].tolist()
+            for k in range(len(items)):
+                self._neighbourList[items[k][0]] = data[k]
+            # print(self.Database)
+        if isinstance(self._nFile, str):
+            if _fp._validators.url(self._nFile):
+                data = _fp._urlopen(self._nFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._neighbourList[temp[0]] = temp[1:]
+            else:
+                try:
+                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._neighbourList[temp[0]] = temp[1:]
+                except IOError:
+                    print("File Not Found2")
+                    quit()
 
-        mapSupport = {}
-        for i in self._Database:
-            for j in i:
-                if j.item not in mapSupport:
-                    mapSupport[j.item] = j.probability
-                else:
-                    mapSupport[j.item] += j.probability
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
-        return mapSupport, plist
+    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
+        """
+        to convert the type of user specified minWS value
 
-    def _ufgrowth(self, tree, prefix, prefixLength, prefixSupport, mapSupport):
-        if prefixLength == self._maxPatternLength:
-            return
-        singlePath = True
-        position = 0
-        s = 0
-        if len(tree.root.child) > 1:
-            singlePath = False
-        else:
-            currentNode = tree.root.child[0]
-            while True:
-                if len(currentNode.child) > 1:
-                    singlePath = False
-                    break
-                self._fpNodeTempBuffer.insert(position, currentNode)
-                s = currentNode.counter
-                position += 1
-                if len(currentNode.child) == 0:
-                    break
-                currentNode = currentNode.child[0]
-        if singlePath is True:
-            self._saveAllcombinations(self._fpNodeTempBuffer, s, position, prefix, prefixLength)
-        else:
-            for i in reversed(tree.headerList):
-                item = i
-                betaSupport = mapSupport[item]
-                prefix.insert(prefixLength, item)
-                # print prefix,betaSupport
-                self._saveItemset(prefix, prefixLength + 1, betaSupport)
-                if prefixLength + 1 < self._maxPatternLength:
-                    prefixPaths = []
-                    path = tree.mapItemNodes.get(item)
-                    mapSupportBeta = {}
-                    while path is not None:
-                        if path.parent.itemid != -1:
-                            prefixPath = []
-                            prefixPath.append(path)
-                            pathCount = path.counter
-                            parent1 = path.parent
-                            while parent1.itemid != -1:
-                                prefixPath.append(parent1)
-                                s = (pathCount * path.expSup) * parent1.probability
-                                if mapSupportBeta.get(parent1.itemid) == None:
-                                    mapSupportBeta[parent1.itemid] = s
-                                else:
-                                    mapSupportBeta[parent1.itemid] = mapSupportBeta[parent1.itemid] + s
-                                parent1 = parent1.parent
-                            prefixPaths.append(prefixPath)
-                        path = path.nodeLink
-                    treeBeta = _Tree()
-                    for i in prefixPaths:
-                        q = treeBeta.addPrefixPath(i, mapSupportBeta, self._minSup)
-                        self._conditionalnodes += q
-                    if len(treeBeta.root.child) > 0:
-                        treeBeta.createHeaderList(mapSupportBeta, self._minSup)
-                        # print(treeBeta.headerList)
-                        self._ufgrowth(treeBeta, prefix, prefixLength + 1, betaSupport, mapSupportBeta)
-
-    def _saveItemset(self, prefix, prefixLength, support):
-        l = []
-        for i in range(prefixLength):
-            l.append(prefix[i])
-        self._itemsetCount += 1
-        l.sort()
-        s = '\t'.join(l)
-        self._finalPatterns[s] = support
-
-    def _saveAllcombinations(self, TempBuffer, s, position, prefix, prefixLength):
-        # support=0
-        max1 = 1 << position
-        for i in range(1, max1):
-            newprefixLength = prefixLength
-            for j in range(position):
-                isset = i & (1 << j)
-                if isset > 0:
-                    prefix.insert(newprefixLength, TempBuffer[j].itemid)
-                    newprefixLength += 1
-                    support = TempBuffer[j].counter
-            self._saveItemset(prefix, newprefixLength, s)
-
-    def _convert(self, value):
-        """
-        To convert the type of user specified minSup value
-        :param value: user specified minSup value
-        :return: converted type minSup value
+        :param value: user specified minWS value
+        :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
+                value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
+    def __frequentOneItem(self) -> List[str]:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Generating One frequent items sets
+        :return: None
         """
-        self.mine()
+        global _maxWeight
+        self._mapSupport = {}
+        for tr in self._Database:
+            for i in tr:
+                nn = [j for j in tr if j.item in self._neighbourList[i.item]]
+                if i.item not in self._mapSupport:
+                    self._mapSupport[i.item] = i.weight
+                else:
+                    self._mapSupport[i.item] += i.weight
+                for k in nn:
+                    self._mapSupport[i.item] += k.weight
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minWS}
+        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        return genList
+
+    def __updateTransactions(self, itemSet: List[str]) -> List[List[_WeightedItem]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                  rank = {'a':0, 'b':1, 'c':2, 'd':3}
+        :param itemSet: list of one-frequent items
+        :return: list
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = []
+            for i in range(len(tr)):
+                if tr[i].item in itemSet:
+                    list2.append(tr[i])
+            if len(list2) >= 1:
+                basket = list2
+                basket.sort(key=lambda val: self.__rank[val.item])
+                list1.append(basket)
+        return list1
+
+    @staticmethod
+    def __buildTree(transactions: List[List[_WeightedItem]], info: Dict[int, float]) -> _Tree:
+        """
+        Builds the tree with updated transactions
+
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions.
+        :return: transactions compressed in fp-tree.
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            rootNode.addTransaction(transactions[i], 1)
+        return rootNode
+
+    def __savePeriodic(self, itemSet: List[str]) -> str:
+        """
+        The duplication items and their ranks
+
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
+
+        """
+        temp = str()
+        for i in itemSet:
+            temp = temp + self.__rankDup[i] + "\t"
+        return temp
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
+        """
+        main program to start the operation
+        :return : None
 
-    def mine(self):
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
-        """
-        global minSup
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-        self._finalPatterns = {}
-        _mapSupport, plist = self._frequentOneItem()
-        for i in self._Database:
-            transaction = []
-            for j in i:
-                if _mapSupport.get(j.item) >= self._minSup:
-                    transaction.append(j)
-            transaction.sort(key=lambda val: _mapSupport[val.item], reverse=True)
-            o = self._tree.addTransaction(transaction)
-        self._tree.createHeaderList(_mapSupport, self._minSup)
-        if len(self._tree.headerList) > 0:
-            self._itemsetBuffer = []
-            # self.fpNodeTempBuffer=[]
-            self._ufgrowth(self._tree, self._itemsetBuffer, 0, self._lno, _mapSupport)
-        print("Frequent patterns were generated from uncertain databases successfully using UF algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self.memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
+        global _minWS, _neighbourList, _rank
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minWS is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanNeighbours()
+        self._minWS = self.__convert(self._minWS)
+        _minWS = self._minWS
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
+        _rank = self.__rank
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        _neighbourList = self._neighbourList
+        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
+        # for x, y in self._neighbourList.items():
+        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
+        #     _neighbourList[self.__rank[x]] = xx
+        # print(_neighbourList)
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+    def Mine(self) -> None:
+        """
+        main program to start the operation
+        :return : None
+
+        """
+        global _minWS, _neighbourList, _rank
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minWS is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanNeighbours()
+        self._minWS = self.__convert(self._minWS)
+        _minWS = self._minWS
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
+        _rank = self.__rank
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        _neighbourList = self._neighbourList
+        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
+        # for x, y in self._neighbourList.items():
+        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
+        #     _neighbourList[self.__rank[x]] = xx
+        # print(_neighbourList)
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.memoryRSS
+        return self.__memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._endTime - self._startTime
+        return self.__endTime - self.__startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self._finalPatterns.items():
+        for a, b in self.__finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
-        for x, y in self._finalPatterns.items():
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self.__finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, float]:
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.__finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Spatial Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = UFGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = UFGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_fp._sys.argv) == 7 or len(_fp._sys.argv) == 8:
+        if len(_fp._sys.argv) == 8:
+            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6],
+                             _fp._sys.argv[7])
+        if len(_fp._sys.argv) == 7:
+            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        _ap = SWFPGrowth('sample.txt', 'neighbourSample.txt', 150, ' ')
+        _ap.startMine()
+        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save('output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,537 +1,886 @@
-# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+# UPFPGrowth is used to discover periodic-frequent patterns in an uncertain temporal database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 #
-#     obj = alg.UVEclat(iFile, minSup)
+#             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     frequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
+
 """
 
-import operator as _operator
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-import deprecated
 
-_minSup = float()
-_finalPatterns = {}
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
+_minSup = float()
+__maxPer = float()
+__first = int()
+_last = int()
+__lno = int()
+#rank = {}
+#periodic = {}
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
+
     :Attributes:
-        item : int or word
+
+        item: int or word
             Represents the name of the item
-        probability : float
+        probability: float
             Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability):
+    def __init__(self, item: str, probability: float) -> None:
         self.item = item
         self.probability = probability
 
 
-class UVEclat(_ab._frequentPatterns):
+class _Node(object):
+    """
+    A class used to represent the node of frequentPatternTree
+
+    :Attributes:
+
+        item: int
+            storing item of a node
+        probability: int
+            To maintain the expected support of node
+        parent: node
+            To maintain the parent of every node
+        children: list
+            To maintain the children of node
+        timeStamps: list
+            To maintain the timeStamps of node
+
+    :Methods:
+
+        addChild(itemName)
+            storing the children to their respective parent nodes
+    """
+
+    def __init__(self, item: str, children: Dict) -> None:
+        self.item = item
+        self.probability = 1
+        self.children = children
+        self.parent = None
+        self.timeStamps = []
+
+    def addChild(self, node: '_Node') -> None:
+        """
+        To add the children details to parent node
+
+        :param node: children node
+        :return: updated parent node children
+        """
+        self.children[node.item] = node
+        node.parent = self
+
+
+def _printTree(root) -> None:
+    """
+    To print the details of tree
+
+    :param root: root node of the tree
+    :return: details of tree
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.timeStamps)
+        _printTree(y)
+
+
+class _Tree(object):
+    """
+    A class used to represent the frequentPatternGrowth tree structure
+
+    :Attributes:
+        root : Node
+            Represents the root node of the tree
+        summaries : dictionary
+            storing the nodes with same item name
+        info : dictionary
+            stores the support of items
+
+    :Methods:
+        addTransactions(transaction)
+            creating transaction as a branch in frequentPatternTree
+        addConditionalTransaction(prefixPaths, supportOfItems)
+            construct the conditional tree for prefix paths
+        conditionalPatterns(Node)
+            generates the conditional patterns from tree for specific node
+        conditionalTransactions(prefixPaths,Support)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from
+            prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
+            removes the node from tree once after generating all the patterns respective to the node generatePatterns(Node) starts from the root node of the tree and mines the frequent patterns
+    """
+
+    def __init__(self) -> None:
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransactions(self, transaction: List['_Item'], tid: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param tid: the timestamp of transaction
+        :type tid: list
+        :return: None
+        """
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                l1 = i - 1
+                temp = []
+                while l1 >= 0:
+                    temp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(temp) == 0:
+                    newNode.probability = transaction[i].probability
+                else:
+                    newNode.probability = max(temp) * transaction[i].probability
+                currentNode.addChild(newNode)
+                if transaction[i].item in self.summaries:
+                    self.summaries[transaction[i].item].append(newNode)
+                else:
+                    self.summaries[transaction[i].item] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].item]
+                l1 = i - 1
+                temp = []
+                while l1 >= 0:
+                    temp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(temp) == 0:
+                    currentNode.probability += transaction[i].probability
+                else:
+                    currentNode.probability += max(temp) * transaction[i].probability
+        currentNode.timeStamps = currentNode.timeStamps + tid
+
+    def addConditionalTransaction(self, transaction: List[str], ts: List[int], sup: float) -> None:
+        """
+        Constructing conditional tree from prefixPaths
+
+        :param transaction : it represents the one transaction in database
+        :type transaction : list
+        :param ts: timeStamp of a transaction
+        :type ts: list
+        :param sup : support of prefixPath taken at last child of the path
+        :type sup : int
+        :return: None
+        """
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                newNode.probability = sup
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+                currentNode.probability += sup
+        currentNode.timeStamps = currentNode.timeStamps + ts
+
+    def getConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+        """
+        Generates all the conditional patterns of respective node.
+
+        :param alpha : it represents the Node in tree
+        :type alpha : Node
+        :return: tuple
+        """
+
+        finalPatterns = []
+        finalTimeStamps = []
+        sup = []
+        for i in self.summaries[alpha]:
+            set1 = i.timeStamps
+            s = i.probability
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalTimeStamps.append(set1)
+                sup.append(s)
+        finalPatterns, finalTimeStamps, support, info = self.conditionalTransactions(finalPatterns, finalTimeStamps,
+                                                                                     sup)
+        return finalPatterns, finalTimeStamps, support, info
+
+    def removeNode(self, nodeValue: str) -> None:
+        """
+        Removing the node from tree
+
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        :return: None
+        """
+        for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            del i.parent.children[nodeValue]
+
+    def getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
+        global _lno, _maxPer
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = s
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def conditionalTransactions(self, condPatterns: List[List[str]], condTimeStamps: List[List[int]], support: List[float]) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+        """
+        It generates the conditional patterns with frequent items
+
+        :param condPatterns : conditional patterns generated from getConditionalPatterns method for respective node
+        :type condPatterns : list
+        :param condTimeStamps: timeStamps of conditional transactions
+        :type condTimeStamps: list
+        :param support : the support of conditional pattern in tree
+        :type support : list
+        """
+        global _minSup, _maxPer
+        pat = []
+        timeStamps = []
+        sup = []
+        data1 = {}
+        count = {}
+        for i in range(len(condPatterns)):
+            for j in condPatterns[i]:
+                if j in data1:
+                    data1[j] = data1[j] + condTimeStamps[i]
+                    count[j] += support[i]
+                else:
+                    data1[j] = condTimeStamps[i]
+                    count[j] = support[i]
+        updatedDict = {}
+        for m in data1:
+            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
+        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
+        count = 0
+        for p in condPatterns:
+            p1 = [v for v in p if v in updatedDict]
+            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                timeStamps.append(condTimeStamps[count])
+                sup.append(support[count])
+            count += 1
+        return pat, timeStamps, sup, updatedDict
+
+    def generatePatterns(self, prefix: List[str], periodic: Dict) -> None:
+        """
+        Generates the patterns
+
+        :param prefix : forms the combination of items
+        :type prefix : list
+        :return: None
+        """
+
+        global _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
+            pattern = prefix[:]
+            pattern.append(i)
+            s = 0
+            for x in self.summaries[i]:
+                s += x.probability
+            periodic[tuple(pattern)] = self.info[i]
+            if s >= _minSup:
+                patterns, timeStamps, support, info = self.getConditionalPatterns(i)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addConditionalTransaction(patterns[pat], timeStamps[pat], support[pat])
+                if len(patterns) > 0:
+                    conditionalTree.generatePatterns(pattern, periodic)
+            self.removeNode(i)
+
+
+class UPFPGrowth(_ab._periodicFrequentPatterns):
     """
-    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+    :Description: Basic is  to discover periodic-frequent patterns in a uncertain temporal database.
 
     :Reference:
-    Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
-    SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983984,
-    https://doi.org/10.1145/1982185.1982399
+            Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).
+            Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases. In:
+            Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
+            ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
+            https://doi.org/10.1007/978-3-030-92307-5_83
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
+    :param  minSup: float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: float :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
+
 
     :Attributes:
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
-            Name of the output file or path of the output file
-        minSup : float or int or str
+            Name of the output file or path of output file
+        minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        sep : str
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep: str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        memoryUSS : float
+        memoryUSS: float
             To store the total amount of USS memory consumed by the program
-        memoryRSS : float
+        memoryRSS: float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime: float
             To record the start time of the mining process
-        endTime:float
+        endTime: float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
-        lno : int
+        _lno : int
             To represent the total no of transaction
         tree : class
             To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
+
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
+        creatingItemSets()
             Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
+        PeriodicFrequentOneItem()
+            Extracts the one-periodic-frequent patterns from database
+        updateTransaction()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            To convert the user specified value
+        removeFalsePositives()
+            To remove the false positives in generated patterns
+
+    **Executing the code on terminal**:
+    --------------------------------------------
+    .. code-block:: console
+
+
+       Format:
+
+       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer>
 
-    **Methods to execute code on terminal**
-    ------------------------------------------
-            Format:
-                      >>> python3 uveclat.py <inputFile> <outputFile> <minSup>
-            Example:
-                      >>>  python3 uveclat.py sampleTDB.txt patterns.txt 3
+       Example Usage:
+
+       (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 0.3 4
+
+
+               .. note:: minSup and maxPer will be considered in support count or frequency
 
-                      .. note:: minSup  will be considered in support count or frequency
 
     **Importing this algorithm into a python program**
-    ---------------------------------------------------
+    ----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+            from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 
-            obj = alg.UVEclat(iFile, minSup)
+            obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 
             obj.startMine()
 
-            frequentPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getmemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    ---------------
-         The complete program was written by   P.Likhitha    under the supervision of Professor Rage Uday Kiran.
-    """
+    **Credits**:
+    -------------
+
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
+"""
+    _rank = {}
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _minSup = float()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
-    _rank = {}
+    _lno = 0
+    _periodic = {}
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
-        Scans the dataset
+        Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+            uncertain, data, ts = [], [], []
             if self._iFile.empty:
                 print("its empty..")
-            i = self._iFile.columns.values.tolist()
+            i = self._iFile._columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                data = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
+                tr = [ts[k]]
+                for j in range(len(k)):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
+                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
+                    line = line.strip()
+                    line = [i for i in line.split(':')]
+                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                    temp1 = [x for x in temp1 if x]
+                    temp2 = [x for x in temp2 if x]
+                    tr = [int(temp1[0])]
+                    for i in range(len(temp1[1:])):
+                        item = temp1[i]
+                        probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
-                    self._Database.append(temp)
+                    self._lno += 1
+                    self._Database.append(tr)
             else:
                 try:
+                    count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
+                            #count += 1
+                            line = line.strip()
+                            line = [i for i in line.split(':')]
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                            temp1 = [x for x in temp1 if x]
+                            temp2 = [x for x in temp2 if x]
+                            tr = [int(temp1[0])]
+                            for i in range(len(temp1[1:])):
+                                item = temp1[i]
+                                probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
+                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _frequentOneItem(self):
-        """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+    def _periodicFrequentOneItem(self) -> Tuple[Dict, List]:
         """
+        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        :return: Tuple
 
+        """
         mapSupport = {}
-        k = 0
         for i in self._Database:
-            k += 1
-            for j in i:
+            n = i[0]
+            for j in i[1:]:
                 if j.item not in mapSupport:
-                    mapSupport[str(j.item)] = j.probability
-                    self._tidList[str(j.item)] = {k: j.probability}
+                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
                 else:
-                    mapSupport[str(j.item)] += j.probability
-                    self._tidList[str(j.item)].update({k: j.probability})
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
-        return list(plist.keys())
+                    mapSupport[j.item][0] += round(j.probability, 3)
+                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
+                    mapSupport[j.item][2] = n
+        for key in mapSupport:
+            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
+        mapSupport = {k: [v[0], v[1]] for k, v in mapSupport.items() if v[1] <= self._maxPer and v[0] >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        return mapSupport, plist
 
-    @staticmethod
-    def _check(i, x):
+    def _check(self, i: List, x: List) -> int:
         """
         To check the presence of item or pattern in transaction
+
         :param x: it represents the pattern
         :type x : list
-        :param i : represents the uncertain self.Database
+        :param i : represents the uncertain transactions
         :type i : list
+        :return: value
+        :rtype: int
         """
 
-        # This method taken a transaction as input and returns the tree
         for m in x:
             k = 0
             for n in i:
                 if m == n.item:
                     k += 1
             if k == 0:
                 return 0
         return 1
 
-    @staticmethod
-    def _convert(value):
+    def _getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
         """
-        To convert the type of user specified minSup value
-        :param value: user specified minSup value
-        :return: converted type minSup value
+        To calculate periodicity of timeStamps
+
+        :param s: support of a pattern
+        :param timeStamps: timeStamps of a pattern
+        :return: periodicity and Support
+        """
+        global __lno, _maxPer
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = s
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def _buildTree(self, data: List[List], info: Dict) -> '_Tree':
+        """
+        It takes the transactions and support of each item and construct the main tree with setting root node as null
+
+        :param data: it represents the one transaction in database
+        :type data: list
+        :param info: it represents the support of each item
+        :type info : dictionary
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            set1 = [data[i][0]]
+            rootNode.addTransactions(data[i][1:], set1)
+        return rootNode
+
+    def _updateTransactions(self, dict1: Dict) -> List[List]:
+        """
+        Remove the items which are not frequent from transactions and updates the transactions with rank of items
+
+        :param dict1 : frequent items with support
+        :type dict1 : dictionary
+        :return: list
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
+                if tr[i].item in dict1:
+                    list2.append(tr[i])
+            if len(list2) >= 2:
+                basket = list2[1:]
+                basket.sort(key=lambda val: self._rank[val.item])
+                list2[1:] = basket[0:]
+                list1.append(list2)
+        return list1
+
+    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
+        """
+        To convert the given user specified value
+
+        :param value: user specified value
+        :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = float(value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
             else:
                 value = int(value)
+
         return value
 
-    def _removeFalsePositives(self):
+    def _removeFalsePositives(self) -> None:
         """
-        To remove the false positive patterns generated in frequent patterns
-        :return: patterns with accurate probability
+
+        :return: Removes the false positive patterns in generated patterns
         """
-        global _finalPatterns
         periods = {}
         for i in self._Database:
-            for x, y in _finalPatterns.items():
+            for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._check(i, x)
+                    check = self._check(i[1:], x)
                     if check == 1:
-                        for j in i:
+                        for j in i[1:]:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
-                            periods[x] += s
+                            periods[x][0] += s
                         else:
-                            periods[x] = s
+                            periods[x] = [s, y[1]]
         for x, y in periods.items():
-            if y >= self._minSup:
+            if y[0] >= _minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @staticmethod
-    def _Intersection(tidSetx, tidSetY):
-        """
-        This function is used to find the intersection
-        :param tidSetx: the timestamp of a patterns
-        :type tidSetx: dict
-        :param tidSetY: the timestamp of a patterns
-        :type tidSetY: dict
-        """
-        tids = []
-        support = []
-        tidDict = {}
-        for x, y in tidSetx.items():
-            for x1, y1 in tidSetY.items():
-                if x == x1:
-                    tids.append(x)
-                    support.append(y * y1)
-                    tidDict.update({x: y * y1})
-        return tidDict
-
-    def _calculateExpSup(self, tidList):
-        """
-        This function is used to calculate support of tidList
-        :param tidList: timestamp of a list.
-        :type tidList: List
-        """
-        return sum(tidList.values())
-
-    def _save(self, prefix, suffix, tidSetI):
-        """
-        Saves the patterns that satisfy the periodic frequent property.
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
-        """
-
-        global _finalPatterns
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self._calculateExpSup(tidSetI)
-        _finalPatterns[tuple(prefix)] = val
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y) >= self._minSup:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
-
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
+    def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Main method where the patterns are mined by constructing tree and remove the false patterns
+        by counting the original support of a patterns.
+        :return: None
         """
-        self.mine()
+        global _lno, _maxPer, _minSup, _first, _last, periodic
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
+        mapSupport, plist = self._periodicFrequentOneItem()
+        updatedTrans = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        Tree1.generatePatterns([], self._periodic)
+        self._removeFalsePositives()
+        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def mine(self):
+    def Mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Main method where the patterns are mined by constructing tree and remove the false patterns
+        by counting the original support of a patterns.
+        :return: None
         """
-        global _minSup
+        global _lno, _maxPer, _minSup, _first, _last, periodic
         self._startTime = _ab._time.time()
         self._creatingItemSets()
+        self._finalPatterns = {}
         self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y1) >= self._minSup:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetI)
+        self._maxPer = self._convert(self._maxPer)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
+        mapSupport, plist = self._periodicFrequentOneItem()
+        updatedTrans = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        Tree1.generatePatterns([], self._periodic)
         self._removeFalsePositives()
-        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
+        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getMemoryUSS(self) -> float:
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self) -> float:
+        """
+        Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
+        """
+        Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
-    def save(self, oFile):
-        """Complete set of frequent patterns will be loaded in to an output file
-        :param oFile: name of the output file
-        :type oFile: csv file
+    def save(self, outFile: str) -> None:
+        """
+        Complete set of frequent patterns will be loaded in to an output file
+
+        :param outFile: name of the output file
+        :type outFile: csv file
+        :return: None
         """
-        self.oFile = oFile
+        self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+    def getPatterns(self) -> Dict[str, List[float]]:
+        """
+        Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Patterns:", len(_ap.getPatterns()))
+        print("Total number of Uncertain Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
+# UPFPGrowthPlus is used to discover periodic-frequent patterns in an uncertain temporal database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
+#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowthPlus as alg
 #
-#             obj = alg.GFPGrowth(iFile, nFile, minSup,sep, oFile)
+#             obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 #
 #             obj.startMine()
 #
-#             Patterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of  Patterns:", len(Patterns))
+#             print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -28,15 +28,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,41 +46,60 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-from PAMI.uncertainGeoreferencedFrequentPattern.basic import abstract as _ab
+
+
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-_minSup = str()
-_neighbourList = {}
-_ab._sys.setrecursionlimit(20000)
-_finalPatterns = {}
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+
+_minSup = float()
+_maxPer = float()
+_lno = int()
+_first = int()
+_last = int()
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-        item : int or word
-            Represents the name of the item
-        probability : float
-            Represent the existential probability(likelihood presence) of an item
+    item : int or string
+        Represents the name of the item
+    probability : float
+        Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
+def printTree(root):
+    """
+    To print the tree with nodes with item name, probability, timestamps, and second probability respectively.
+
+    Attributes:
+
+    :param root: Node
+    :return: print all Tree with nodes with items, probability, parent item, timestamps, second probability respectively.
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
+        printTree(y)
+
+
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
@@ -98,337 +116,411 @@
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
     def __init__(self, item, children):
         self.item = item
         self.probability = 1
+        self.secondProbability = 1
+        self.p = 1
         self.children = children
         self.parent = None
+        self.TimeStamps = []
 
     def addChild(self, node):
+        """
+        To add children details to parent node
+
+        :param node: children node
+        :return: update parent node children
+        """
         self.children[node.item] = node
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
-    :Attributes:
+    Attributes:
 
-        root : Node
+        root: Node
             Represents the root node of the tree
-        summaries : dictionary
+        summaries: dictionary
             storing the nodes with same item name
-        info : dictionary
+        info: dictionary
             stores the support of items
+
+
     :Methods:
 
         addTransaction(transaction)
-            creating transaction as a branch in frequentPatternTree
-        addConditionalPattern(prefixPaths, supportOfItems)
+            creating transaction as a branch in Tree
+        addConditionalTransaction(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
-        conditionalPatterns(Node)
+        getConditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from
-            prefixPaths and generates prefixPaths with items which are frequent
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
         remove(Node)
             removes the node from tree once after generating all the patterns respective to the node
         generatePatterns(Node)
             starts from the root node of the tree and mines the frequent patterns
+
     """
 
     def __init__(self):
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction):
+
+    def addTransaction(self, transaction, tid):
         """
         Adding transaction into tree
 
-        :param transaction : it represents the one self.Database in database
+        :param transaction : it represents the one transaction in database
         :type transaction : list
+        :param tid : the timestamp of transaction
+        :type tid : list
         """
-        global _neighbourList
         currentNode = self.root
+        k = 0
         for i in range(len(transaction)):
+            k += 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
-                nei = _neighbourList.get(transaction[i].item)
+                newNode.k = k
+                newNode.secondProbability = transaction[i].probability
                 l1 = i - 1
-                lp = []
+                temp = []
                 while l1 >= 0:
-                    if nei == None:
-                        break
-                    if transaction[l1].item in nei:
-                        lp.append(transaction[l1].probability)
+                    temp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(lp) == 0:
-                    newNode.probability = transaction[i].probability
+                if len(temp) == 0:
+                    newNode.probability = round(transaction[i].probability, 2)
                 else:
-                    newNode.probability = max(lp) * transaction[i].probability
+                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
+                currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
+                currentNode.k = k
                 l1 = i - 1
-                lp = []
+                temp = []
                 while l1 >= 0:
-                    lp.append(transaction[l1].probability)
+                    temp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(lp) == 0:
-                    currentNode.probability += transaction[i].probability
+                if len(temp) == 0:
+                    currentNode.probability += round(transaction[i].probability, 2)
                 else:
-                    currentNode.probability += max(lp) * transaction[i].probability
+                    nn = max(temp) * transaction[i].probability
+                    currentNode.probability += round(nn, 2)
+        currentNode.TimeStamps = currentNode.TimeStamps + tid
 
-    def addConditionalPattern(self, transaction, sup):
+    def addConditionalPatterns(self, transaction, tid, sup, probability):
         """
-        constructing conditional tree from prefixPaths
+        Constructing conditional tree from prefixPaths
 
-        :param transaction : it represents the one self.Database in database
+        :param transaction : it represents the one transaction in database
         :type transaction : list
+        :param tid : timestamps of a pattern or transaction in tree
+        :param tid : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
+        :para probability : highest existential probability value among all periodic-frequent items
+        :type probability : list
         """
-
-        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
+        k = 0
         for i in range(len(transaction)):
+            k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
+                newNode.k = k
                 newNode.probability = sup
+                newNode.secondProbability = probability
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
+                currentNode.k = k
                 currentNode.probability += sup
+                currentNode.secondProbability = max(probability, currentNode.secondProbability)
+        currentNode.TimeStamps = currentNode.TimeStamps + tid
 
     def conditionalPatterns(self, alpha):
         """
         Generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
-        :type alpha : _Node
+        :type alpha : Node
         """
-
-        # This method generates conditional patterns of node by traversing the tree
-        global _neighbourList
         finalPatterns = []
+        finalSets = []
         sup = []
+        prob = []
         for i in self.summaries[alpha]:
-            j = i.item
+            set1 = i.TimeStamps
             s = i.probability
+            p = i.secondProbability
             set2 = []
             while i.parent.item is not None:
-                if _neighbourList.get(j) is not None:
-                    #print(_neighbourList.get(j))
-                    if i.parent.item in _neighbourList[j]:
-                        set2.append(i.parent.item)
+                set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
+                finalSets.append(set1)
                 sup.append(s)
-        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
-        return finalPatterns, support, info
+                prob.append(p)
+        finalPatterns, finalSets, support, prob, info = self.conditionalTransactions(finalPatterns, finalSets, sup, prob)
+        return finalPatterns, finalSets, support, prob, info
 
     def removeNode(self, nodeValue):
         """
         Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
         """
-
         for i in self.summaries[nodeValue]:
+            i.parent.TimeStamps = i.parent.TimeStamps + i.TimeStamps
             del i.parent.children[nodeValue]
 
-    def conditionalTransactions(self, condPatterns, support):
+    def getPeriodAndSupport(self, support, TimeStamps):
         """
-        It generates the conditional patterns with frequent items
+        To calculate the periodicity of given timestamps
 
-        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
-        :type condPatterns : list
-        :support : the support of conditional pattern in tree
-        :support : int
+        :param support: support of pattern
+        :param TimeStamps: timmeStamps of a pattern
+        :return: support and period
+        """
+        global _maxPer
+        global _lno
+        TimeStamps.sort()
+        cur = 0
+        per = 0
+        sup = support
+        for j in range(len(TimeStamps)):
+            per = max(per, TimeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = TimeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def conditionalTransactions(self, conditionalPatterns, conditionalTimeStamps, support, probability):
         """
+        It generates the conditional patterns with frequent items
 
-        global minSup
+        :param conditionalPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+        :type conditionalPatterns : list
+        :param conditionalTimeStamps : timestamps of respective conditional timestamps
+        :type conditionalTimeStamps : list
+        :param support : the support of conditional pattern in tree
+        :type support : list
+        :para probability : highest existential probability value among all periodic-frequent items
+        :type probability : list
+        """
+        global _minSup, _maxPer, _lno
         pat = []
+        TimeStamps = []
         sup = []
+        prob = []
+        data1 = {}
         count = {}
-        for i in range(len(condPatterns)):
-            for j in condPatterns[i]:
-                if j in count:
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
+                if j in data1:
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
                     count[j] += support[i]
                 else:
+                    data1[j] = conditionalTimeStamps[i]
                     count[j] = support[i]
         updatedDict = {}
-        updatedDict = {k: v for k, v in count.items() if v >= minSup}
+        for m in data1:
+            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
+        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
         count = 0
-        for p in condPatterns:
+        for p in conditionalPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
+            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
+                TimeStamps.append(conditionalTimeStamps[count])
                 sup.append(support[count])
-                count += 1
-        return pat, sup, updatedDict
+                prob.append(probability[count])
+            count += 1
+        return pat, TimeStamps, sup, prob, updatedDict
 
-    def generatePatterns(self, prefix):
+    def generatePatterns(self, prefix, periodic):
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
+        :para periodic : occurring at intervals
+        :type periodic : list
         """
-
-        global _finalPatterns, minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+        global _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
+            secProb = []
+            kk = int()
             for x in self.summaries[i]:
-                s += x.probability
-            _finalPatterns[tuple(pattern)] = self.info[i]
-            if s >= minSup:
-                patterns, support, info = self.conditionalPatterns(i)
+                if x.k <= 2:
+                    s += x.probability
+                elif x.k >= 3:
+                    n = x.probability * pow(x.secondProbability, (x.k - 2))
+                    s += n
+            periodic[tuple(pattern)] = self.info[i]
+            periodic[tuple(pattern)] = self.info[i]
+            if s >= _minSup:
+                periodic[tuple(pattern)] = self.info[i]
+                patterns, TimeStamps, support, probability, info = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
+                    conditionalTree.addConditionalPatterns(patterns[pat], TimeStamps[pat], support[pat], probability[pat])
                 if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern)
+                    conditionalTree.generatePatterns(pattern, periodic)
             self.removeNode(i)
 
-
-class GFPGrowth(_ab._frequentPatterns):
+class UPFPGrowthPlus(_ab._periodicFrequentPatterns):
     """
-    :Description: GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
+    :Description: Basic Plus is  to discover periodic-frequent patterns in a uncertain temporal database.
 
     :Reference:
-         Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
-         "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
-         Transactional Databases".  PAKDD 2023.
-         https://doi.org/10.1007/978-3-031-33380-4_3
+          Palla Likhitha, Rage Veena,Rage Uday Kiran, Koji Zettsu, Masashi Toyoda, Philippe Fournier-Viger, (2023). 
+          UPFP-growth++: An Efficient Algorithm to Find Periodic-Frequent Patterns in Uncertain Temporal Databases. 
+          ICONIP 2022. Communications in Computer and Information Science, vol 1792. Springer, Singapore.
+          https://doi.org/10.1007/978-981-99-1642-9_16
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of uncertain Geo referenced Frequent Patterns
+                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of Uncertain Geo referenced frequent patterns
+                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
     :param  minSup: str:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: floot :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
+
 
     :Attributes:
 
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup: float or int or str
+        iFile: file
+            Name of the Input file or path of input file
+        oFile: file
+            Name of the output file or path of output file
+        minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        sep : str
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep: str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        memoryUSS : float
+        memoryUSS: float
             To store the total amount of USS memory consumed by the program
-        memoryRSS : float
+        memoryRSS: float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime: float
             To record the start time of the mining process
-        endTime:float
+        endTime: float
             To record the completion time of the mining process
-        Database : list
+        Database: list
             To store the transactions of a database in list
-        mapSupport : Dictionary
+        mapSupport: Dictionary
             To maintain the information of item and their frequency
-        lno : int
+        lno: int
             To represent the total no of transaction
-        tree : class
+        tree: class
             To represents the Tree class
-        itemSetCount : int
+        itemSetCount: int
             To represents the total no of patterns
-        finalPatterns : dict
+        finalPatterns: dict
             To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+            Complete set of periodic-frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets(fileName)
             Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        updateDatabases()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
-        startMine()
-            Mining process will start from this function
+        PeriodicFrequentOneItems()
+            To extract the one-length periodic-frequent items
 
     **Executing the code on terminal**:
-    ------------------------------------------
+    --------------------------------------------
 
     .. code-block:: console
 
 
        Format:
 
-       (.venv) $ python3 GFPGrowth.py <inputFile> <neighborFile> <outputFile> <minSup>
+       (.venv) $ python3 UPFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Examples Usage:
+
+       (.venv) $ python3 UPFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 4
 
-       Examples usage:
 
-       (.venv) $ python3 GFPGrowth.py sampleTDB.txt sampleNeighbor.txt patterns.txt 3
+               .. note:: minSup and maxPer will be considered in support count or frequency
 
 
-               .. note:: minSup  will be considered in support count or frequency
-    
-    **Sample run of importing the code**:
-    ----------------------------------------
-     .. code-block:: python
+    **Importing this algorithm into a python program**
+    -----------------------------------------------------------------
+    .. code-block:: python
 
-            from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
+            from PAMI.uncertainPeriodicFrequentPattern import UPFPGrowthPlus as alg
 
-            obj = alg.GFPGrowth(iFile, nFile, minSup)
+            obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 
             obj.startMine()
 
-            Patterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of  Patterns:", len(Patterns))
+            print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -437,389 +529,372 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-        
+
+
     **Credits**:
-    -------------
+    --------------
         The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+
         """
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _minSup = float()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
-
-    def __init__(self, iFile, nFile, minSup, sep='\t'):
-        super().__init__(iFile, nFile, minSup, sep)
+    _lno = 0
+    _periodic = {}
 
     def _creatingItemSets(self):
         """
-        Scans the uncertain transactional dataset
+        Storing the complete transactions of the database/input file in a database variable
         """
+
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+            uncertain, data, ts = [], [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                data = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
+                tr = [ts[k]]
+                for j in range(len(k)):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
+                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    temp1 = line.strip()
-                    temp1 = temp1.split(':')
-                    temp = [i.rstrip() for i in temp1[0].split(self._sep)]
-                    uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
-                    tr = []
-                    for i in range(len(temp)):
-                        item = temp[i]
-                        probability = uncertain[i]
+                    line = line.decode("utf-8")
+                    line = line.strip()
+                    line = [i for i in line.split(':')]
+                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                    temp1 = [x for x in temp1 if x]
+                    temp2 = [x for x in temp2 if x]
+                    tr = [int(temp1[0])]
+                    for i in range(len(temp1[1:])):
+                        item = temp1[i]
+                        probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
+                    self._lno += 1
                     self._Database.append(tr)
             else:
                 try:
+                    count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            temp1 = line.strip()
-                            temp1 = temp1.split(':')
-                            #temp1[0], temp1[1] = [i for i in temp1[0] if i], [i for i in temp1[1] if i]
-                            temp = [i.rstrip() for i in temp1[0].split(self._sep) if i]
-                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep) if i]
-                            tr = []
-                            for i in range(len(temp)):
-                                item = temp[i]
-                                probability = uncertain[i]
+                            line = line.strip()
+                            line = [i for i in line.split(':')]
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                            temp1 = [x for x in temp1 if x]
+                            temp2 = [x for x in temp2 if x]
+                            tr = [int(temp1[0])]
+                            for i in range(len(temp1[1:])):
+                                item = temp1[i]
+                                probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
+                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    
-    def _creatingNeighbours(self):
-        """
-        Scans the uncertain transactional dataset
-        """
-        global _neighbourList
-        _neighbourList = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
-
-            # print(self.Database)
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._Database.append(temp)
-            else:
-                try:
-                    with open(self._nFile, 'r') as f:
-                        for line in f:
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            _neighbourList[temp[0]] = temp[1:]
-                except IOError:
-                    print("File Not Found")
 
-    def _frequentOneItem(self):
+    def _PeriodicFrequentOneItems(self):
         """
-        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-
-        :param self.Database : it represents the one self.Database in database
-        :type self.Database : list
+        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
         """
-
+        global first, last
         mapSupport = {}
         for i in self._Database:
-            for j in i:
+            n = int(i[0])
+            for j in i[1:]:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = j.probability
+                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
                 else:
-                    mapSupport[j.item] += j.probability
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
+                    mapSupport[j.item][0] += round(j.probability, 2)
+                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
+                    mapSupport[j.item][2] = n
+        for key in mapSupport:
+            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
+        mapSupport = {k: [round(v[0], 2), v[1]] for k, v in mapSupport.items() if
+                      v[1] <= self._maxPer and v[0] >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    @staticmethod
-    def _buildTree(data, info):
+    def _buildTree(self, data, info):
         """
-        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+        It takes the transactions and support of each item and construct the main tree with setting root node as null
 
-        :param data : it represents the one self.Database in database
+        :param data : it represents the one transaction in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
         """
-
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
-            rootNode.addTransaction(data[i])
+            set1 = [data[i][0]]
+            rootNode.addTransaction(data[i][1:], set1)
+            #printTree(rootNode)
+            #print("....")
         return rootNode
 
     def _updateTransactions(self, dict1):
         """
-        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+        Remove the items which are not frequent from transactions and updates the transactions with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
-
         list1 = []
         for tr in self._Database:
-            list2 = []
-            for i in range(0, len(tr)):
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
             if len(list2) >= 2:
-                basket = list2
-                basket.sort(key=lambda val: self.rank[val.item])
-                list2 = basket
+                basket = list2[1:]
+                basket.sort(key=lambda val: self._rank[val.item])
+                list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
-    @staticmethod
-    def _check(i, x):
+    def _Check(self, i, x):
         """
         To check the presence of item or pattern in transaction
 
         :param x: it represents the pattern
         :type x : list
-        :param i : represents the uncertain self.Database
+        :param i : represents the uncertain transactions
         :type i : list
         """
-
-        # This method taken a transaction as input and returns the tree
         for m in x:
             k = 0
             for n in i:
                 if m == n.item:
                     k += 1
             if k == 0:
                 return 0
         return 1
 
     def _convert(self, value):
         """
-        To convert the type of user specified minSup value
 
-        :param value: user specified minSup value
-        :return: converted type minSup value
+        To convert the given user specified value
+
+        :param value: user specified value
+        :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
-                value = (len(self._Database) * value)
+                value = float(value)
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self):
         """
-        To remove the false positive patterns generated in frequent patterns.
-
-        :return: patterns with accurate probability
+        To remove false positives in generated patterns
+        :return: original patterns
         """
-        global _finalPatterns
         periods = {}
         for i in self._Database:
-            for x, y in _finalPatterns.items():
+            for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._check(i, x)
+                    check = self._Check(i[1:], x)
                     if check == 1:
-                        for j in i:
+                        for j in i[1:]:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
-                            periods[x] += s
+                            periods[x][0] += s
                         else:
-                            periods[x] = s
+                            periods[x] = [s, y[1]]
+        count = 0
         for x, y in periods.items():
-            if y >= self._minSup:
+            if y[0] >= _minSup:
+                count += 1
                 sample = str()
                 for i in x:
-                    sample = sample + i + "\t"
+                    sample = sample + i + " "
                 self._finalPatterns[sample] = y
+        #print("Total false patterns generated:", len(self._periodic) - count)
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self.mine()
+        global _minSup, _maxPer, _first, _last, _lno
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._finalPatterns = {}
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        mapSupport, plist = self._PeriodicFrequentOneItems()
+        updatedTrans = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        root = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        root.generatePatterns([], self._periodic)
+        self._removeFalsePositives()
+        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def mine(self):
+    def Mine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global minSup
+        global _minSup, _maxPer, _first, _last, _lno
         self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._creatingNeighbours()
-        # self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
         self._finalPatterns = {}
-        mapSupport, plist = self._frequentOneItem()
-        self.Database1 = self._updateTransactions(mapSupport)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        mapSupport, plist = self._PeriodicFrequentOneItems()
+        updatedTrans = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(self.Database1, info)
-        Tree1.generatePatterns([])
+        root = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        root.generatePatterns([], self._periodic)
         self._removeFalsePositives()
-        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
+        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self.memoryRSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
+        self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-
+        Total amount of USS memory consumed by the mining process will be retrieved from this function.
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.memoryRSS
+        return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a, b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-    
+
     def printResults(self):
-        print("Total number of Patterns:", len(self.getPatterns()))
-        self.save("patterns.txt")
-        memUSS = self.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = self.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = self.getRuntime()
-        print("Total ExecutionTime in ms:", run)
+        """
+        This function is used to print the results
+        """
+        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
         _Patterns = _ap.getPatterns()
         print("Total number of Patterns:", len(_Patterns))
-        _ap.save(_ab._sys.argv[2])
+        _ap.savePatterns(_ab._sys.argv[2])
+        # print(ap.getPatternsAsDataFrame())
         _memUSS = _ap.getMemoryUSS()
         print("Total Memory in USS:", _memUSS)
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _memRSS)
         _run = _ap.getRuntime()
         print("Total ExecutionTime in ms:", _run)
     else:
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py` & `pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# UPFPGrowth is used to discover periodic-frequent patterns in an uncertain temporal database.
+# WUFIM is one of the algorithm to discover weighted frequent patterns in an uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
+#             from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
 #
-#             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
+#             obj = alg.basic(iFile, wFile, minSup, sep)
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of  Patterns:", len(Patterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -43,446 +43,389 @@
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
+     along with this program.  If not, see `<https://www.gnu.org/licenses/>`_.
+     
 """
 
-
-
+from PAMI.weightedUncertainFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
-from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Union
-
-_minSup = float()
-__maxPer = float()
-__first = int()
-_last = int()
-__lno = int()
-#rank = {}
-#periodic = {}
 
+_expSup = str()
+_expWSup = str()
+_weights = {}
+_finalPatterns = {}
+_ab._sys.setrecursionlimit(20000)
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-        item: int or word
+        item : int or word
             Represents the name of the item
-        probability: float
+
+        probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item: str, probability: float) -> None:
+    def __init__(self, item: int, probability: float) -> None:
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item: int
+        item : int
             storing item of a node
-        probability: int
+        probability : int
             To maintain the expected support of node
-        parent: node
+        parent : node
             To maintain the parent of every node
-        children: list
+        children : list
             To maintain the children of node
-        timeStamps: list
-            To maintain the timeStamps of node
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item: str, children: Dict) -> None:
+    def __init__(self, item, children: list) -> None:
         self.item = item
         self.probability = 1
         self.children = children
         self.parent = None
-        self.timeStamps = []
-
-    def addChild(self, node: '_Node') -> None:
-        """
-        To add the children details to parent node
 
-        :param node: children node
-        :return: updated parent node children
-        """
+    def addChild(self, node) -> None:
         self.children[node.item] = node
         node.parent = self
 
 
-def _printTree(root) -> None:
-    """
-    To print the details of tree
-
-    :param root: root node of the tree
-    :return: details of tree
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.timeStamps)
-        _printTree(y)
-
-
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
+
         root : Node
             Represents the root node of the tree
         summaries : dictionary
             storing the nodes with same item name
         info : dictionary
             stores the support of items
 
     :Methods:
-        addTransactions(transaction)
+
+        addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
-        addConditionalTransaction(prefixPaths, supportOfItems)
+        addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from
-            prefixPaths and generates prefixPaths with items which are frequent
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
         remove(Node)
-            removes the node from tree once after generating all the patterns respective to the node generatePatterns(Node) starts from the root node of the tree and mines the frequent patterns
+            removes the node from tree once after generating all the patterns respective to the node
+        generatePatterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
+
     """
 
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransactions(self, transaction: List['_Item'], tid: int) -> None:
+    def addTransaction(self, transaction) -> None:
         """
         Adding transaction into tree
 
-        :param transaction: it represents the one transaction in database
-        :type transaction: list
-        :param tid: the timestamp of transaction
-        :type tid: list
+        :param transaction : it represents the one self.Database in database
+        :type transaction : list
         :return: None
         """
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
+                if len(lp) == 0:
                     newNode.probability = transaction[i].probability
                 else:
-                    newNode.probability = max(temp) * transaction[i].probability
+                    newNode.probability = max(lp) * transaction[i].probability
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
+                if len(lp) == 0:
                     currentNode.probability += transaction[i].probability
                 else:
-                    currentNode.probability += max(temp) * transaction[i].probability
-        currentNode.timeStamps = currentNode.timeStamps + tid
+                    currentNode.probability += max(lp) * transaction[i].probability
 
-    def addConditionalTransaction(self, transaction: List[str], ts: List[int], sup: float) -> None:
+    def addConditionalPattern(self, transaction, sup) -> None:
         """
-        Constructing conditional tree from prefixPaths
+        constructing conditional tree from prefixPaths
 
-        :param transaction : it represents the one transaction in database
+        :param transaction : it represents the one self.Database in database
         :type transaction : list
-        :param ts: timeStamp of a transaction
-        :type ts: list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
         :return: None
+
         """
+        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
-        currentNode.timeStamps = currentNode.timeStamps + ts
 
-    def getConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+    def conditionalPatterns(self, alpha) -> tuple:
         """
-        Generates all the conditional patterns of respective node.
+        generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
-        :type alpha : Node
-        :return: tuple
-        """
+        :type alpha : _Node
 
+        """
+        # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
-        finalTimeStamps = []
         sup = []
         for i in self.summaries[alpha]:
-            set1 = i.timeStamps
             s = i.probability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalTimeStamps.append(set1)
                 sup.append(s)
-        finalPatterns, finalTimeStamps, support, info = self.conditionalTransactions(finalPatterns, finalTimeStamps,
-                                                                                     sup)
-        return finalPatterns, finalTimeStamps, support, info
+        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
+        return finalPatterns, support, info
+
+    def removeNode(self, nodeValue) -> None:
 
-    def removeNode(self, nodeValue: str) -> None:
         """
         Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
         :return: None
         """
+
         for i in self.summaries[nodeValue]:
-            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
             del i.parent.children[nodeValue]
 
-    def getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
-        global _lno, _maxPer
-        timeStamps.sort()
-        cur = 0
-        per = 0
-        sup = s
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > _maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-        per = max(per, _lno - cur)
-        return [sup, per]
-
-    def conditionalTransactions(self, condPatterns: List[List[str]], condTimeStamps: List[List[int]], support: List[float]) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+    def conditionalTransactions(self, condPatterns, support) -> tuple:
         """
         It generates the conditional patterns with frequent items
 
-        :param condPatterns : conditional patterns generated from getConditionalPatterns method for respective node
+        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
         :type condPatterns : list
-        :param condTimeStamps: timeStamps of conditional transactions
-        :type condTimeStamps: list
-        :param support : the support of conditional pattern in tree
-        :type support : list
+        :support : the support of conditional pattern in tree
+        :support : int
+        :return: tuple
         """
-        global _minSup, _maxPer
+        global _expSup, _expWSup
         pat = []
-        timeStamps = []
         sup = []
-        data1 = {}
         count = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
-                if j in data1:
-                    data1[j] = data1[j] + condTimeStamps[i]
+                if j in count:
                     count[j] += support[i]
                 else:
-                    data1[j] = condTimeStamps[i]
                     count[j] = support[i]
         updatedDict = {}
-        for m in data1:
-            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
-        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
+        updatedDict = {k: v for k, v in count.items() if v >= _expSup}
         count = 0
         for p in condPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
+            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                timeStamps.append(condTimeStamps[count])
                 sup.append(support[count])
-            count += 1
-        return pat, timeStamps, sup, updatedDict
+                count += 1
+        return pat, sup, updatedDict
 
-    def generatePatterns(self, prefix: List[str], periodic: Dict) -> None:
+    def generatePatterns(self, prefix) -> None:
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
         :return: None
         """
 
-        global _minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
+        global _finalPatterns, _expSup, _expWSup, _weights
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
-            s = 0
-            for x in self.summaries[i]:
-                s += x.probability
-            periodic[tuple(pattern)] = self.info[i]
-            if s >= _minSup:
-                patterns, timeStamps, support, info = self.getConditionalPatterns(i)
+            weight = 0
+            for k in pattern:
+                weight = weight + _weights[k]
+            weight = weight/len(pattern)
+            if self.info.get(i) >= _expSup and self.info.get(i) * weight >= _expWSup:
+                _finalPatterns[tuple(pattern)] = self.info.get(i)
+                patterns, support, info = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalTransaction(patterns[pat], timeStamps[pat], support[pat])
+                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
                 if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern, periodic)
+                    conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
-
-class UPFPGrowth(_ab._periodicFrequentPatterns):
+class WUFIM(_ab._weightedFrequentPatterns):
     """
-    :Description: Basic is  to discover periodic-frequent patterns in a uncertain temporal database.
+    :Description: It is one of the algorithm to discover weighted frequent patterns in a uncertain transactional database using PUF-Tree.
 
-    :Reference:
-            Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).
-            Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases. In:
-            Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
-            ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
-            https://doi.org/10.1007/978-3-030-92307-5_83
+    :Reference: Efficient Mining of Weighted Frequent Itemsets in Uncertain Databases, In book: Machine Learning and Data Mining in Pattern Recognition Chun-Wei Jerry Lin, Wensheng Gan, Philippe Fournier Viger, Tzung-Pei Hong
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
+                   Name of the Input file to mine complete set of Weighted Uncertain Periodic Frequent Patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
-    :param  minSup: float:
+                   Name of the output file to store complete set of Weighted  Uncertain Periodic Frequent Patterns
+    :param  minSup: str:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  maxper: float :
-                   where maxPer represents the maximum periodicity threshold value specified by the user.
+    :param  wFile: str :
+                    This is a weighted file.
 
 
     :Attributes:
+
         iFile : file
             Name of the Input file or path of the input file
+        wFile : file
+            Name of the Input file or path of the input file
         oFile : file
-            Name of the output file or path of output file
-        minSup: int or float or str
+            Name of the output file or path of the output file
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        sep: str
+        sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        memoryUSS: float
+        memoryUSS : float
             To store the total amount of USS memory consumed by the program
-        memoryRSS: float
+        memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime: float
+        startTime:float
             To record the start time of the mining process
-        endTime: float
+        endTime:float
             To record the completion time of the mining process
         Database : list
             To store the transactions of a database in list
         mapSupport : Dictionary
             To maintain the information of item and their frequency
-        _lno : int
+        lno : int
             To represent the total no of transaction
         tree : class
             To represents the Tree class
+        itemSetCount : int
+            To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
+        creatingItemSets(fileName)
             Scans the dataset and stores in a list format
-        PeriodicFrequentOneItem()
-            Extracts the one-periodic-frequent patterns from database
-        updateTransaction()
-            Update the database by removing aperiodic items and sort the Database by item decreased support
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
-            To convert the user specified value
-        removeFalsePositives()
-            To remove the false positives in generated patterns
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
 
-    **Executing the code on terminal**:
+    **Methods to execute code on terminal**
     --------------------------------------------
     .. code-block:: console
 
 
-       Format:
+      Format:
 
-       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer>
+      (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup>
 
-       Example Usage:
+      Example Usage:
 
-       (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 0.3 4
+      (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 3
 
 
-               .. note:: minSup and maxPer will be considered in support count or frequency
+              .. note:: minSup  will be considered in support count or frequency
 
 
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    -----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
+            from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
 
-            obj = alg.UPFPGrowth(iFile, minSup, maxPer)
+            obj = alg.basic(iFile, wFile, expSup, expWSup)
 
             obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of  Patterns:", len(Patterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -491,377 +434,419 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-
-    **Credits**:
-    -------------
-
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
-"""
-    _rank = {}
+   """
     _startTime = float()
     _endTime = float()
-    _minSup = float()
-    _maxPer = float()
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
+    _wFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _lno = 0
-    _periodic = {}
+    _rank = {}
+    _expSup = float()
+    _expWSup = float()
+
+    def __init__(self, iFile, wFile, expSup, expWSup, sep='\t') -> None:
+        super().__init__(iFile, wFile, expSup, expWSup, sep)
 
     def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Scans the uncertain transactional dataset
         :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data, ts = [], [], []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
-            i = self._iFile._columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
+            i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
-                tr = [ts[k]]
-                for j in range(len(k)):
+                tr = []
+                for j in range(len(data[k])):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
-                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.strip()
                     line = [i for i in line.split(':')]
                     temp1 = [i.rstrip() for i in line[0].split(self._sep)]
                     temp2 = [i.rstrip() for i in line[1].split(self._sep)]
                     temp1 = [x for x in temp1 if x]
                     temp2 = [x for x in temp2 if x]
-                    tr = [int(temp1[0])]
-                    for i in range(len(temp1[1:])):
+                    tr = []
+                    for i in range(len(temp1)):
                         item = temp1[i]
                         probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
-                    self._lno += 1
                     self._Database.append(tr)
             else:
                 try:
-                    count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            #count += 1
                             line = line.strip()
                             line = [i for i in line.split(':')]
                             temp1 = [i.rstrip() for i in line[0].split(self._sep)]
                             temp2 = [i.rstrip() for i in line[1].split(self._sep)]
                             temp1 = [x for x in temp1 if x]
                             temp2 = [x for x in temp2 if x]
-                            tr = [int(temp1[0])]
-                            for i in range(len(temp1[1:])):
+                            tr = []
+                            for i in range(len(temp1)):
                                 item = temp1[i]
                                 probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
-                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _periodicFrequentOneItem(self) -> Tuple[Dict, List]:
+    def _scanningWeights(self) -> None:
+        """
+        Scans the uncertain transactional dataset
+        :return: None
+        """
+        self._weights = {}
+        if isinstance(self._wFile, _ab._pd.DataFrame):
+            weights, data = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                data = self._wFile['items'].tolist()
+            if 'weights' in i:
+                weights = self._wFile['weights'].tolist()
+            for k in range(len(data)):
+                self._weights[data[k]] = int(float(weights[k]))
+
+            # print(self.Database)
+        if isinstance(self._wFile, str):
+            if _ab._validators.url(self._wFile):
+                data = _ab._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._weights[temp[0]] = int(float(temp[1]))
+            else:
+                try:
+                    with open(self._wFile, 'r') as f:
+                        for line in f:
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._weights[temp[0]] = float(temp[1])
+                except IOError:
+                    print("File Not Found")
+
+    def _frequentOneItem(self) -> tuple:
         """
-        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-        :return: Tuple
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        :return: tuple
         """
+
         mapSupport = {}
         for i in self._Database:
-            n = i[0]
-            for j in i[1:]:
+            for j in i:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
+                    if self._weights.get(j.item) is not None:
+                        mapSupport[j.item] = [j.probability, self._weights[j.item]]
                 else:
-                    mapSupport[j.item][0] += round(j.probability, 3)
-                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
-                    mapSupport[j.item][2] = n
-        for key in mapSupport:
-            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
-        mapSupport = {k: [v[0], v[1]] for k, v in mapSupport.items() if v[1] <= self._maxPer and v[0] >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
+                    mapSupport[j.item][0] += j.probability
+        mapSupport = {k: v[0] for k, v in mapSupport.items() if v[0] >= self._expSup and v[0] * v[1] >= self._expWSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    def _check(self, i: List, x: List) -> int:
+    @staticmethod
+    def _buildTree(data, info) -> _Tree:
         """
-        To check the presence of item or pattern in transaction
-
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain transactions
-        :type i : list
-        :return: value
-        :rtype: int
-        """
-
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
-
-    def _getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
-        """
-        To calculate periodicity of timeStamps
-
-        :param s: support of a pattern
-        :param timeStamps: timeStamps of a pattern
-        :return: periodicity and Support
-        """
-        global __lno, _maxPer
-        timeStamps.sort()
-        cur = 0
-        per = 0
-        sup = s
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > _maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-        per = max(per, _lno - cur)
-        return [sup, per]
-
-    def _buildTree(self, data: List[List], info: Dict) -> '_Tree':
-        """
-        It takes the transactions and support of each item and construct the main tree with setting root node as null
-
-        :param data: it represents the one transaction in database
-        :type data: list
-        :param info: it represents the support of each item
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+        :param data : it represents the one self.Database in database
+        :type data : list
+        :param info : it represents the support of each item
         :type info : dictionary
+        :return: tree
         """
+
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransactions(data[i][1:], set1)
+            rootNode.addTransaction(data[i])
         return rootNode
 
-    def _updateTransactions(self, dict1: Dict) -> List[List]:
+    def _updateTransactions(self, dict1) -> list:
         """
-        Remove the items which are not frequent from transactions and updates the transactions with rank of items
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         :return: list
         """
         list1 = []
         for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
+            list2 = []
+            for i in range(0, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
             if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort(key=lambda val: self._rank[val.item])
-                list2[1:] = basket[0:]
+                basket = list2
+                basket.sort(key=lambda val: self.rank[val.item])
+                list2 = basket
                 list1.append(list2)
         return list1
 
-    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    @staticmethod
+    def _check(i, x) -> int:
         """
-        To convert the given user specified value
+        To check the presence of item or pattern in transaction
 
-        :param value: user specified value
-        :return: converted value
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        :return: integer number
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    def _convert(self, value) -> float:
+        """
+        To convert the type of user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
-
         return value
 
     def _removeFalsePositives(self) -> None:
         """
-
-        :return: Removes the false positive patterns in generated patterns
+        To remove the false positive patterns generated in frequent patterns.
+        :return: patterns with accurate probability
         """
+        global _finalPatterns
         periods = {}
         for i in self._Database:
-            for x, y in self._periodic.items():
+            for x, y in _finalPatterns.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._check(i[1:], x)
+                    check = self._check(i, x)
                     if check == 1:
-                        for j in i[1:]:
+                        for j in i:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
-                            periods[x][0] += s
+                            periods[x] += s
                         else:
-                            periods[x] = [s, y[1]]
+                            periods[x] = s
         for x, y in periods.items():
-            if y[0] >= _minSup:
+            weight = 0
+            for i in x:
+                weight += self._weights[i]
+            weight = weight / len(x)
+            if weight * y >= self._expWSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns
-        by counting the original support of a patterns.
-        :return: None
+        startMine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
         """
-        self.mine()
+        global _expSup, _expWSup, _weights, _finalPatterns
+        self._startTime = _ab._time.time()
+        self._Database, self._weights = [], {}
+        self._creatingItemSets()
+        self._scanningWeights()
+        _weights = self._weights
+        self._expSup = float(self._expSup)
+        self._expWSup = float(self._expWSup)
+        _expSup = self._expSup
+        _expWSup = self._expWSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Weighted Frequent patterns were generated  successfully using basic algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns
-        by counting the original support of a patterns.
-        :return: None
+        mine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patternS
         """
-        global _lno, _maxPer, _minSup, _first, _last, periodic
+        global _expSup, _expWSup, _weights, _finalPatterns
         self._startTime = _ab._time.time()
+        self._Database, self._weights = [], {}
         self._creatingItemSets()
+        self._scanningWeights()
+        _weights = self._weights
+        self._expSup = float(self._expSup)
+        self._expWSup = float(self._expWSup)
+        _expSup = self._expSup
+        _expWSup = self._expWSup
         self._finalPatterns = {}
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
-        mapSupport, plist = self._periodicFrequentOneItem()
-        updatedTrans = self._updateTransactions(mapSupport)
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(updatedTrans, info)
-        self._periodic = {}
-        Tree1.generatePatterns([], self._periodic)
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
         self._removeFalsePositives()
-        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
+        print("Weighted Frequent patterns were generated  successfully using basic algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self._memoryRSS = float()
+        self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-
-        return self._memoryRSS
+        return self.memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
-
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            s = str()
+            for i in a:
+                s = s + i + " "
+            data.append([s, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
-        :param outFile: name of the output file
+        :param outFile: Specify name of the output file
         :type outFile: csv file
         :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            s = str()
+            for i in x:
+                s = s + i + "\t"
+            s1 = s.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, List[float]]:
+    def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
-            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Uncertain Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        for k in [120, 140, 160, 180, 200]:
+            _ap = WUFIM('/Users/likhitha/Downloads/uncertainTransaction_T10I4D200K.csv', '/Users/likhitha/Downloads/T10_weights.txt',
+                        k, 500, '\t')
+            _ap.startMine()
+            print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+            _ap.save('/Users/likhitha/Downloads/WUFIM_output.txt')
+            print("Total Memory in USS:", _ap.getMemoryUSS())
+            print("Total Memory in RSS", _ap.getMemoryRSS())
+            print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.24.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py` & `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-# SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
+# Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
 #
-# **Importing this algorithm into a python program**
-# -------------------------------------------------------
+# to its huge search space.we are using efficient pruning techniques to reduce the search space.
 #
-#             from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
+# **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#             obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, sep)
+#             from PAMI.fuzzyFrequentPattern import FFIMiner_old as alg
 #
-#             obj.startMine()
+#             obj = alg.FFIMiner("input.txt", 2)
 #
-#             frequentPatterns = obj.getPatterns()
+#             obj.mine()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             fuzzyFrequentPattern = obj.getPatterns()
 #
-#             obj.save(oFile)
+#             print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
 #
-#             Df = obj.getPatternsAsDataFrame()
+#             obj.save("outputFile")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -30,15 +30,15 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,745 +47,829 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-
-from PAMI.weightedFrequentNeighbourhoodPattern.basic import abstract as _fp
-import pandas as pd
+from PAMI.fuzzyFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
-from typing import List, Dict, Tuple, Union, Iterable
-
-_minWS = str()
-_weights = {}
-_rank = {}
-_neighbourList = {}
-
-_fp._sys.setrecursionlimit(20000)
 
 
-class _WeightedItem:
+class _FFList:
     """
-    A class used to represent the weight of the item
+    A class represent a Fuzzy List of an element
 
     :Attributes:
 
-        item: str
-            storing item of the frequent pattern
-        weight: float
-            stores the weight of the item
-
-    """
-    def __init__(self, item: str, weight: float) -> None:
-        self.item = item
-        self.weight = weight
+         item: int
+             the item name
+         sumIUtil: float
+             the sum of utilities of a fuzzy item in database
+         sumRUtil: float
+             the sum of resting values of a fuzzy item in database
+         elements: list
+             a list of elements contain tid,Utility and resting values of element in each transaction
 
+    :Methods:
 
-class _Node:
+        addElement(element)
+            Method to add an element to this fuzzy list and update the sums at the same time.
+        printElement(e)
+            Method to print elements
     """
-    A class used to represent the node of frequentPatternTree
 
-    :Attributes:
+    def __init__(self, itemName: int) -> None:
+        self.item = itemName
+        self.sumIUtil = 0.0
+        self.sumRUtil = 0.0
+        self.elements = []
 
-        itemId: int
-            storing item of a node
-        counter: int
-            To maintain the support of node
-        parent: node
-            To maintain the parent of node
-        children: list
-            To maintain the children of node
-
-    :Methods:
-
-        addChild(node)
-            Updates the nodes children list and parent for the given node
-
-    """
+    def addElement(self, element) -> None:
+        """
+        A Method that add a new element to FFList
 
-    def __init__(self, item: str, children: Dict[str, '_Node']) -> None:
-        self.itemId = item
-        self.counter = 1
-        self.weight = 0
-        self.parent = None
-        self.children = children
-
-    def addChild(self, node: '_Node') -> None:
-        """
-        Retrieving the child from the tree
-
-        :param node: Children node.
-        :type node: Node
-        :return: Updates the children nodes and parent nodes
+        :param element: an element to be added to FFList
+        :param element: Element
         :return: None
+        """
+        self.sumIUtil += element.iUtils
+        self.sumRUtil += element.rUtils
+        self.elements.append(element)
 
+    def printElement(self) -> None:
+        """
+        A method to print elements
+        :return: None
         """
-        self.children[node.itemId] = node
-        node.parent = self
+        for ele in self.elements:
+            print(ele.tid, ele.iUtils, ele.rUtils)
 
 
-class _Tree:
+class _Element:
     """
-    A class used to represent the frequentPatternGrowth tree structure
+    A class represents an Element of a fuzzy list
 
     :Attributes:
 
-        root : Node
-            The first node of the tree set to Null.
-        summaries : dictionary
-            Stores the nodes itemId which shares same itemId
-        info : dictionary
-            frequency of items in the transactions
+        tid : int
+            keep tact of transaction id
+        iUtils: float
+            the utility of a fuzzy item in the transaction
+        rUtils : float
+            the  resting value of a fuzzy item in the transaction
+    """
 
-    :Methods:
+    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
+        self.tid = tid
+        self.iUtils = iUtil
+        self.rUtils = rUtil
 
-        addTransaction(transaction, freq)
-            adding items of  transactions into the tree as nodes and freq is the count of nodes
-        getFinalConditionalPatterns(node)
-            getting the conditional patterns from fp-tree for a node
-        getConditionalPatterns(patterns, frequencies)
-            sort the patterns by removing the items with lower minWS
-        generatePatterns(prefix)
-            generating the patterns from fp-tree
+
+class _Regions:
     """
+    A class calculate the regions
 
-    def __init__(self) -> None:
-        self.root = _Node(None, {})
-        self.summaries = {}
-        self.info = {}
-
-    def addTransaction(self, transaction: List[_WeightedItem], count: int) -> None:
-        """
-        Adding transaction into tree
-
-        :param transaction: it represents the one transaction in database
-        :type transaction: list
-        :param count: frequency of item
-        :type count: int
-        :return: None
-        """
+    :Attributes:
 
-        # This method takes transaction as input and returns the tree
-        global _neighbourList, _rank
-        currentNode = self.root
-        for i in range(len(transaction)):
-            wei = 0
-            l1 = i
-            while l1 >= 0:
-                wei += transaction[l1].weight
-                l1 -= 1
-            if transaction[i].item not in currentNode.children:
-                newNode = _Node(transaction[i].item, {})
-                newNode.freq = count
-                newNode.weight = wei
-                currentNode.addChild(newNode)
-                if _rank[transaction[i].item] in self.summaries:
-                    self.summaries[_rank[transaction[i].item]].append(newNode)
-                else:
-                    self.summaries[_rank[transaction[i].item]] = [newNode]
-                currentNode = newNode
-            else:
-                currentNode = currentNode.children[transaction[i].item]
-                currentNode.freq += count
-                currentNode.weight += wei
-
-    def addConditionalPattern(self, transaction: List[_WeightedItem], count: int) -> None:
-        """
-        Adding transaction into tree
-
-        :param transaction: it represents the one transaction in database
-        :type transaction: list
-        :param count: frequency of item
-        :type count: int
-        :return : None
-        """
-        # This method takes transaction as input and returns the tree
-        global _neighbourList, _rank
-        currentNode = self.root
-        for i in range(len(transaction)):
-            wei = 0
-            l1 = i
-            while l1 >= 0:
-                wei += transaction[l1].weight
-                l1 -= 1
-            if transaction[i].itemId not in currentNode.children:
-                newNode = _Node(transaction[i].itemId, {})
-                newNode.freq = count
-                newNode.weight = wei
-                currentNode.addChild(newNode)
-                if _rank[transaction[i].itemId] in self.summaries:
-                    self.summaries[_rank[transaction[i].itemId]].append(newNode)
-                else:
-                    self.summaries[_rank[transaction[i].itemId]] = [newNode]
-                currentNode = newNode
+        low : int
+            low region value
+        middle: int
+            middle region value
+        high : int
+            high region values
+    """
+
+    def __init__(self, quantity: int, regionsNumber: int) -> None:
+        self.low = 0
+        self.middle = 0
+        self.high = 0
+        if regionsNumber == 3:  # if we have 3 regions
+            if 0 < quantity <= 1:
+                self.low = 1
+                self.high = 0
+                self.middle = 0
+            elif 1 < quantity <= 6:
+                self.low = float((6 - quantity) / 5)
+                self.middle = float((quantity - 1) / 5)
+                self.high = 0
+            elif 6 < quantity <= 11:
+                self.low = 0
+                self.middle = float((11 - quantity) / 5)
+                self.high = float((quantity - 6) / 5)
             else:
-                currentNode = currentNode.children[transaction[i].itemId]
-                currentNode.freq += count
-                currentNode.weight += wei
+                self.low = 0
+                self.middle = 0
+                self.high = 1
 
-    def printTree(self, root: _Node) -> None:
-        """
-        To print the details of tree
 
-        :param root: root node of the tree
-        :return: details of tree
-        """
-        if len(root.children) == 0:
-            return
-        else:
-            for x, y in root.children.items():
-                #print(y.itemId, y.parent.itemId, y.freq, y.weight)
-                self.printTree(y)
-
-
-    def getFinalConditionalPatterns(self, alpha: int) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
-        """
-        Generates the conditional patterns for a node
-
-        :param alpha: node to generate conditional patterns
-        :return: returns conditional patterns, frequency of each item in conditional patterns
-
-        """
-        finalPatterns = []
-        finalFreq = []
-        global _neighbourList
-        for i in self.summaries[alpha]:
-            set1 = i.weight
-            set2 = []
-            while i.parent.itemId is not None:
-                if i.parent.itemId in _neighbourList[i.itemId]:
-                    set2.append(i.parent)
-                i = i.parent
-            if len(set2) > 0:
-                set2.reverse()
-                finalPatterns.append(set2)
-                finalFreq.append(set1)
-        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
-        return finalPatterns, finalFreq, info
-
-    @staticmethod
-    def getConditionalTransactions(ConditionalPatterns: List[List[_Node]], conditionalFreq: List[float]) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
-        """
-        To calculate the frequency of items in conditional patterns and sorting the patterns
-
-        :param ConditionalPatterns: paths of a node
-        :param conditionalFreq: frequency of each item in the path
-        :return: conditional patterns and frequency of each item in transactions
-        """
-        global _rank
-        pat = []
-        freq = []
-        data1 = {}
-        for i in range(len(ConditionalPatterns)):
-            for j in ConditionalPatterns[i]:
-                if j.itemId in data1:
-                    data1[j.itemId] += conditionalFreq[i]
-                else:
-                    data1[j.itemId] = conditionalFreq[i]
-        up_dict = {k: v for k, v in data1.items() if v >= _minWS}
-        count = 0
-        for p in ConditionalPatterns:
-            p1 = [v for v in p if v.itemId in up_dict]
-            trans = sorted(p1, key=lambda x: (up_dict.get(x)), reverse=True)
-            if len(trans) > 0:
-                pat.append(trans)
-                freq.append(conditionalFreq[count])
-            count += 1
-        up_dict = {_rank[k]: v for k, v in up_dict.items()}
-        return pat, freq, up_dict
-
-    def generatePatterns(self, prefix: List[int]) -> Iterable[Tuple[List[int], float]]:
-        """
-        To generate the frequent patterns
-
-        :param prefix: an empty list
-        :return: Frequent patterns that are extracted from fp-tree
-
-        """
-        global _minWS
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
-            pattern = prefix[:]
-            pattern.append(i)
-            yield pattern, self.info[i]
-            patterns, freq, info = self.getFinalConditionalPatterns(i)
-            conditionalTree = _Tree()
-            conditionalTree.info = info.copy()
-            for pat in range(len(patterns)):
-                conditionalTree.addConditionalPattern(patterns[pat], freq[pat])
-            if len(patterns) > 0:
-                for q in conditionalTree.generatePatterns(pattern):
-                    yield q
+class _Pair:
+    """
+    A class to store item and it's quantity together
+    """
+
+    def __init__(self) -> None:
+        self.item = 0
+        self.quantity = 0
 
 
-class SWFPGrowth(_fp._weightedFrequentSpatialPatterns):
+class FFIMiner(_ab._fuzzyFrequentPattenrs):
     """
-    :Description: SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
+    :Description:   Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
+                    to its huge search space.we are using efficient pruning techniques to reduce the search space.
 
-    :Reference:
-        R. Uday Kiran, P. P. C. Reddy, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
-        "Discovering Spatial Weighted Frequent Itemsets in Spatiotemporal Databases," 2019 International
-        Conference on Data Mining Workshops (ICDMW), 2019, pp. 987-996, doi: 10.1109/ICDMW.2019.00143.
+    :Reference:   Lin, Chun-Wei & Li, Ting & Fournier Viger, Philippe & Hong, Tzung-Pei. (2015).
+                  A fast Algorithm for mining fuzzy frequent itemsets. Journal of Intelligent & Fuzzy Systems. 29.
+                  2373-2379. 10.3233/IFS-151936.
+                  https://www.researchgate.net/publication/286510908_A_fast_Algorithm_for_mining_fuzzy_frequent_itemSets
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of weighted Frequent Neighbourhood Patterns.
+                   Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of weighted Frequent Neighbourhood Patterns.
-    :param  minSup: int or str or float:
-                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param fuzFile: str :
+                    The user can specify fuzFile.
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  maxper: floot :
-                   where maxPer represents the maximum periodicity threshold value specified by the user.
 
 
     :Attributes:
 
-        iFile : file
-            Input file name or path of the input file
-        minWS: float or int or str
-            The user can specify minWS either in count or proportion of database size.
-            If the program detects the data type of minWS is integer, then it treats minWS is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minWS=10 will be treated as integer, while minWS=10.0 will be treated as float
-        minWeight: float or int or str
-            The user can specify minWeight either in count or proportion of database size.
-            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
-            However, the users can override their default separator.
-        oFile : file
-            Name of the output file or the path of the output file
+        iFile : string
+            Name of the input file to mine complete set of fuzzy  frequent patterns
+        fmFile : string
+            Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
+        oFile : string
+               Name of the oFile file to store complete set of fuzzy  frequent patterns
+        minSup : float
+            The user given minimum support
+        memoryRSS : float
+                To store the total amount of RSS memory consumed by the program
         startTime:float
-            To record the start time of the mining process
+               To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        finalPatterns : dict
-            it represents to store the patterns
+        itemsCnt: int
+            To record the number of fuzzy spatial itemSets generated
+        mapItemsLowSum: map
+            To keep track of low region values of items
+        mapItemsMidSum: map
+            To keep track of middle region values of items
+        mapItemsHighSum: map
+            To keep track of high region values of items
+        mapItemSum: map
+            To keep track of sum of Fuzzy Values of items
+        mapItemRegions: map
+            To Keep track of fuzzy regions of item
+        jointCnt: int
+            To keep track of the number of ffi-list that was constructed
+        BufferSize: int
+            represent the size of Buffer
+        itemBuffer list
+            to keep track of items in buffer
 
-    :Methods :
+    :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to an output file
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Extracts the one-frequent patterns from transactions
+        convert(value):
+            To convert the given user specified value
+        compareItems(o1, o2)
+            A Function that sort all ffi-list in ascending order of Support
+        FSFIMining(prefix, prefixLen, FSFIM, minSup)
+            Method generate ffi from prefix
+        construct(px, py)
+            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
+        findElementWithTID(uList, tid)
+            To find element with same tid as given
+        WriteOut(prefix, prefixLen, item, sumIUtil)
+            To Store the patten
 
-    **Methods to execute code on terminal**
-    -------------------------------------------
-    .. code-block:: console
+    **Executing the code on terminal :**
+    -----------------------------------------
 
+    .. code-block:: console
 
-       Format:
+      Format:
 
-       (.venv) $ python3 SWFPGrowth.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+      (.venv) $ python3 FFIMinerMiner.py <inputFile> <outputFile> <minSup> <separator>
 
-       Example usage :
+      Example Usage:
 
-       (.venv) $ python3 SWFPGrowth.py sampleDB.txt weightFile.txt patterns.txt 10  2
+      (.venv) $ python3  FFIMinerMiner.py sampleTDB.txt output.txt 6
 
+      (.venv) $ python3  FFIMinerMiner.py sampleTDB.txt output.txt 0.3
 
-               .. note:: minSup will be considered in support count or frequency
+    .. note:: minSup will be considered in percentage of database transactions
 
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
-    .. code-block:: python
+    **Sample run of importing the code:**
+    ----------------------------------------
 
-            from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
+        from PAMI.fuzzyFrequentPattern import FFIMiner as alg
 
-            obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, seperator)
+        obj = alg.FFIMiner("input.txt", "fuzzyMembership.txt" 2)
 
-            obj.startMine()
+        obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+        fuzzyFrequentPattern = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+        print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
 
-            obj.save(oFile)
+        obj.save("outputFile")
 
-            Df = obj.getPatternsAsDataFrame()
+        memUSS = obj.getMemoryUSS()
 
-            memUSS = obj.getmemoryUSS()
+        print("Total Memory in USS:", memUSS)
 
-            print("Total Memory in USS:", memUSS)
+        memRSS = obj.getMemoryRSS()
 
-            memRSS = obj.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
 
-            print("Total Memory in RSS", memRSS)
+        run = obj.getRuntime()
 
-            run = obj.getRuntime()
+        print("Total ExecutionTime in seconds:", run)
 
-            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
-        """
+    -------------
+        The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
-    __startTime = float()
-    __endTime = float()
-    _Weights = {}
-    _minWS = str()
-    __finalPatterns = {}
-    _neighbourList = {}
+    """
+    _startTime = float()
+    _endTime = float()
+    _minSup = str()
+    _maxPer = float()
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _sep = " "
-    __memoryUSS = float()
-    __memoryRSS = float()
-    __Database = []
-    __mapSupport = {}
-    __lno = 0
-    __tree = _Tree()
-    __rank = {}
-    __rankDup = {}
+    _fuzFile = " "
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _sep = "\t"
+
+    def __init__(self, iFile: str, fuzFile: str, minSup: float, sep: str="\t") -> None:
+        super().__init__(iFile, fuzFile, minSup, sep)
+        self._startTime = 0
+        self._endTime = 0
+        self._itemsCnt = 0
+        self._mapItemsLowSum = {}
+        self._mapItemsMidSum = {}
+        self._mapItemsHighSum = {}
+        self._mapItemSum = {}
+        self._mapItemRegions = {}
+        self._joinsCnt = 0
+        self._BufferSize = 200
+        self._itemSetBuffer = []
+        self._transactions = []
+        self._fuzzyValues = []
+        self._finalPatterns = {}
+        self._RegionsCal = []
+        self._LabelKeyOne = {}
+        self._LabelKey = {}
+        self._RegionsLabel = []
+        self._dbLen = 0
+
+    def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
+        """
+        A Function that sort all ffi-list in ascending order of Support
+
+        :param o1: First FFI-list
+
+        :type o1: _FFList
+
+        :param o2: Second FFI-list
+
+        :type o1: _FFList
+
+        :return: Comparision Value
+
+        :rtype: int
+        """
+        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
+        if compare == 0:
+            if o1.item < o2.item:
+                return -1
+            elif o1.item > o2.item:
+                return 1
+            else:
+                return 0
+        else:
+            return compare
+
+    def _convert(self, value: Union[int, float, str]) -> float:
+        """
+        To convert the given user specified value
+
+        :param value: user specified value
+
+        :type value: int or float or str
 
-    def __init__(self, iFile: Union[str, _fp._pd.DataFrame], nFile: Union[str, _fp._pd.DataFrame], minWS: Union[int, float, str], sep='\t') -> None:
-        super().__init__(iFile, nFile, minWS, sep)
+        :return: converted value
 
-    def __creatingItemSets(self) -> None:
+        :rtype: float
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._dbLen * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._dbLen * value)
+            else:
+                value = int(value)
+        return value
+
+    def _fuzzyMembershipFunc(self) -> None:
+        try:
+            with open(self._fuzFile, 'r', encoding='utf-8') as f:
+                count = 0
+                for line in f:
+                    line = line.split("\n")[0]
+                    parts = line.split(" ")
+                    lowerBound = parts[0].strip()
+                    upperBound = parts[1].strip()
+                    lb_Label = parts[2].strip()
+                    ub_Label = parts[3].strip()
+                    self._RegionsCal.append([int(lowerBound), int(upperBound)])
+                    self._RegionsLabel.append([lb_Label, ub_Label])
+                    for i in range(0, 2):
+                        if lb_Label.capitalize() not in self._LabelKey:
+                            self._LabelKey[lb_Label.capitalize()] = count
+                            count += 1
+                        if ub_Label.capitalize() not in self._LabelKey:
+                            self._LabelKey[ub_Label.capitalize()] = count
+                            count += 1
+            self._LabelKeyOne = {v:k for k,v in self._LabelKey.items()}
+            print(self._LabelKey)
+            print(self._LabelKeyOne)
+            print(self._RegionsLabel)
+            print(self._RegionsCal)
+        except IOError:
+            print("File Not Found")
+            quit()
+
+    def _creatingItemsets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
-        :return: None
         """
-        self._Database = []
-        if isinstance(self._iFile, _fp._pd.DataFrame):
+        self._transactions, self._fuzzyValues, self._Database = [], [], []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                self._transactions = self._iFile['Transactions'].tolist()
+            if 'fuzzyValues' in i:
+                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    line = line.split("\n")[0]
+                    parts = line.split(":")
+                    parts[0] = parts[0].strip()
+                    parts[2] = parts[2].strip()
+                    items = parts[0].split(self._sep)
+                    quantities = parts[2].split(self._sep)
+                    self._transactions.append([x for x in items])
+                    self._fuzzyValues.append([x for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.strip()
-                            line = line.split(':')
-                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
-                            temp2 = [int(i.strip()) for i in line[1].split(self._sep)]
-                            tr = []
-                            for i in range(len(temp1)):
-                                we = _WeightedItem(temp1[i], temp2[i])
-                                tr.append(we)
-                            self._Database.append(tr)
+                            line = line.split("\n")[0]
+                            parts = line.split(":")
+                            parts[0] = parts[0].strip()
+                            parts[2] = parts[2].strip()
+                            items = parts[0].split(self._sep)
+                            quantities = parts[2].split(self._sep)
+                            self._transactions.append([x for x in items])
+                            self._fuzzyValues.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _scanNeighbours(self) -> None:
-        self._neighbourList = {}
-        if isinstance(self._nFile, _fp._pd.DataFrame):
-            data, items = [], []
-            if self._nFile.empty:
-                print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'item' in i:
-                items = self._nFile['items'].tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for k in range(len(items)):
-                self._neighbourList[items[k][0]] = data[k]
-            # print(self.Database)
-        if isinstance(self._nFile, str):
-            if _fp._validators.url(self._nFile):
-                data = _fp._urlopen(self._nFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._neighbourList[temp[0]] = temp[1:]
-            else:
-                try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._neighbourList[temp[0]] = temp[1:]
-                except IOError:
-                    print("File Not Found2")
-                    quit()
-
-    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    def _Regions(self, quantity: float) -> None:
         """
-        to convert the type of user specified minWS value
+        :param quantity: Quantity to calculate regions
 
-        :param value: user specified minWS value
-        :return: converted type
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+        :type quantity: float
 
-    def __frequentOneItem(self) -> List[str]:
-        """
-        Generating One frequent items sets
         :return: None
         """
-        global _maxWeight
-        self._mapSupport = {}
-        for tr in self._Database:
-            for i in tr:
-                nn = [j for j in tr if j.item in self._neighbourList[i.item]]
-                if i.item not in self._mapSupport:
-                    self._mapSupport[i.item] = i.weight
-                else:
-                    self._mapSupport[i.item] += i.weight
-                for k in nn:
-                    self._mapSupport[i.item] += k.weight
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minWS}
-        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
-        return genList
-
-    def __updateTransactions(self, itemSet: List[str]) -> List[List[_WeightedItem]]:
-        """
-        Updates the items in transactions with rank of items according to their support
-        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
-                  rank = {'a':0, 'b':1, 'c':2, 'd':3}
-        :param itemSet: list of one-frequent items
-        :return: list
-        """
-        list1 = []
-        for tr in self._Database:
-            list2 = []
-            for i in range(len(tr)):
-                if tr[i].item in itemSet:
-                    list2.append(tr[i])
-            if len(list2) >= 1:
-                basket = list2
-                basket.sort(key=lambda val: self.__rank[val.item])
-                list1.append(basket)
-        return list1
-
-    @staticmethod
-    def __buildTree(transactions: List[List[_WeightedItem]], info: Dict[int, float]) -> _Tree:
-        """
-        Builds the tree with updated transactions
-
-        :param transactions: updated transactions
-        :param info: support details of each item in transactions.
-        :return: transactions compressed in fp-tree.
-        """
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(transactions)):
-            rootNode.addTransaction(transactions[i], 1)
-        return rootNode
-
-    def __savePeriodic(self, itemSet: List[str]) -> str:
-        """
-        The duplication items and their ranks
-
-        :param itemSet: frequent itemSet that generated
-        :return: patterns with original item names.
-
-        """
-        temp = str()
-        for i in itemSet:
-            temp = temp + self.__rankDup[i] + "\t"
-        return temp
+        self.list = [0] * len(self._LabelKey)
+        if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
+            self.list[0] = 1
+            return
+        elif quantity >= self._RegionsCal[-1][0]:
+            self.list[-1] = 1
+            return
+        else:
+            for i in range(1, len(self._RegionsCal) - 1):
+                if self._RegionsCal[i][0] < quantity <= self._RegionsCal[i][1]:
+                    base = self._RegionsCal[i][1] - self._RegionsCal[i][0]
+                    for pos in range(0, 2):
+                        if self._RegionsLabel[i][pos].islower():
+                            self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
+                                (self._RegionsCal[i][1] - quantity) / base)
+                        else:
+                            self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
+                                (quantity - self._RegionsCal[i][0]) / base)
+            return
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        main program to start the operation
-        :return : None
-
+        fuzzy-Frequent pattern mining process will start from here
         """
-        self.mine()
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                regions = self._Regions(float(quantities[i]))
+                print(regions)
+                item = items[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
+        listOfffilist = []
+        mapItemsToFFLIST = {}
+        self._minSup = self._convert(self._minSup)
+        # minSup = self.minSup
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                regions = self._Regions(float(quantities[i]), 3)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._FSFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def mine(self) -> None:
         """
-        main program to start the operation
-        :return : None
-
+        fuzzy-Frequent pattern mining process will start from here
         """
-        global _minWS, _neighbourList, _rank
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minWS is None:
-            raise Exception("Please enter the Minimum Support")
-        self.__creatingItemSets()
-        self._scanNeighbours()
-        self._minWS = self.__convert(self._minWS)
-        _minWS = self._minWS
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
-        _rank = self.__rank
-        for x, y in self.__rank.items():
-            self.__rankDup[y] = x
-        _neighbourList = self._neighbourList
-        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
-        # for x, y in self._neighbourList.items():
-        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
-        #     _neighbourList[self.__rank[x]] = xx
-        # print(_neighbourList)
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self._startTime = _ab._time.time()
+        self._creatingItemsets()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                regions = self._Regions(float(quantities[i]))
+                print(regions)
+                item = items[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
+        listOfffilist = []
+        mapItemsToFFLIST = {}
+        self._minSup = self._convert(self._minSup)
+        # minSup = self.minSup
+        for item1 in self._mapItemsLowSum.keys():
+            item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                regions = self._Regions(float(quantities[i]), 3)
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._FSFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
+    def _FSFIMining(self, prefix: List[int], prefixLen: int, FSFIM: List[_FFList], minSup: float) -> None:
+        """Generates ffi from prefix
+
+        :param prefix: the prefix patterns of ffi
+        :type prefix: list
+        :param prefixLen: the length of prefix
+        :type prefixLen: int
+        :param FSFIM: the Fuzzy list of prefix itemSets
+        :type FSFIM: list
+        :param minSup: the minimum support of
+        :type minSup: float
+        :return: None
+        """
+        for i in range(0, len(FSFIM)):
+            X = FSFIM[i]
+            if X.sumIUtil >= minSup:
+                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
+            if X.sumRUtil >= minSup:
+                exULs = []
+                for j in range(i + 1, len(FSFIM)):
+                    Y = FSFIM[j]
+                    exULs.append(self._construct(X, Y))
+                    self._joinsCnt += 1
+                self._itemSetBuffer.insert(prefixLen, X.item)
+                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
+
         :rtype: float
         """
 
-        return self.__memoryUSS
+        return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-
-        return self.__memoryRSS
+        return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
 
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
+        return self._endTime - self._startTime
 
-        return self.__endTime - self.__startTime
+    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
+        """
+        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
+
+        :param px:the itemSet px
+        :type px:ffi-List
+        :param py:itemSet py
+        :type py:ffi-List
+        :return :the itemSet of pxy(px and py)
+        :rtype :ffi-List
+        """
+        pxyUL = _FFList(py.item)
+        for ex in px.elements:
+            ey = self._findElementWithTID(py, ex.tid)
+            if ey is None:
+                continue
+            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
+            pxyUL.addElement(eXY)
+        return pxyUL
+
+    def _findElementWithTID(self, uList: _FFList, tid: int) -> Union[_Element, None]:
+        """
+        To find element with same tid as given
+
+        :param uList: fuzzyList
+        :type uList: ffi-List
+        :param tid: transaction id
+        :type tid: int
+        :return: element  tid as given
+        :rtype: element if exit or None
+        """
+        List = uList.elements
+        first = 0
+        last = len(List) - 1
+        while first <= last:
+            mid = (first + last) >> 1
+            if List[mid].tid < tid:
+                first = mid + 1
+            elif List[mid].tid > tid:
+                last = mid - 1
+            else:
+                return List[mid]
+        return None
 
-    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
+    def _WriteOut(self, prefix: List[int], prefixLen: int, item: int, sumIUtil: float) -> None:
+        """
+        To Store the patten
+
+        :param prefix: prefix of itemSet
+        :type prefix: list
+        :param prefixLen: length of prefix
+        :type prefixLen: int
+        :param item: the last item
+        :type item: int
+        :param sumIUtil: sum of utility of itemSet
+        :type sumIUtil: float
+        :return: None
+        """
+        self._itemsCnt += 1
+        res = ""
+        for i in range(0, prefixLen):
+            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
+        res += str(item) + "." + str(self._mapItemRegions.get(item))
+        res1 = str(sumIUtil)
+        self._finalPatterns[res] = res1
+
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
-        for a, b in self.__finalPatterns.items():
+        for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataframe
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
+
+    def getPatterns(self) -> Dict[str, str]:
+        """
+        Function to send the set of frequent patterns after completion of the mining process
+
+        :return: returning frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self.__finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
-
-    def getPatterns(self) -> Dict[str, float]:
-        """
-        Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
-        """
-        return self.__finalPatterns
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
     def printResults(self) -> None:
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of  Weighted Spatial Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Fuzzy Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 7 or len(_fp._sys.argv) == 8:
-        if len(_fp._sys.argv) == 8:
-            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6],
-                             _fp._sys.argv[7])
-        if len(_fp._sys.argv) == 7:
-            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:
+            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_fp._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS",  _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
-    else:
-        _ap = SWFPGrowth('sample.txt', 'neighbourSample.txt', 150, ' ')
-        _ap.startMine()
-        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
+        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+    else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.4.24.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/weightedFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py` & `pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.24.1/PKG-INFO` & `pami-2024.4.9.1/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pami
-Version: 2024.4.24.1
+Version: 2024.4.9.1
 Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
 Home-page: https://github.com/udayLab/PAMI
 Author: Rage Uday Kiran
 Author-email: uday.rage@gmail.com
 License: GPLv3
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python :: 3
@@ -18,20 +18,18 @@
 Requires-Dist: plotly
 Requires-Dist: matplotlib
 Requires-Dist: resource
 Requires-Dist: validators
 Requires-Dist: urllib3
 Requires-Dist: Pillow
 Requires-Dist: numpy
-Requires-Dist: sphinx
 Requires-Dist: sphinx-rtd-theme
 Requires-Dist: validators
 Requires-Dist: discord.py
 Requires-Dist: networkx
-Requires-Dist: deprecated
 Provides-Extra: gpu
 Requires-Dist: cupy; extra == "gpu"
 Requires-Dist: pycuda; extra == "gpu"
 Provides-Extra: spark
 Requires-Dist: pyspark; extra == "spark"
 Provides-Extra: dev
 Requires-Dist: twine; extra == "dev"
@@ -58,91 +56,70 @@
 [![Downloads](https://static.pepy.tech/badge/pami)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/month)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/week)](https://pepy.tech/project/pami)
 
 [Click here for more information](https://pepy.tech/project/pami)
 
 
-
-
 # Introduction
-
-***
-
 PAttern MIning (PAMI) is a Python library containing several algorithms to discover user interest-based patterns in a wide-spectrum of datasets across multiple computing platforms. Useful links to utilize the services of this library were provided below:
 
 
 1. Youtube tutorial https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ
 
 2. Tutorials (Notebooks) https://github.com/UdayLab/PAMI/tree/main/notebooks
    
 3. User manual https://udaylab.github.io/PAMI/manuals/index.html
 
 4. Coders manual https://udaylab.github.io/PAMI/codersManual/index.html
 
-5. Code documentation
-     - [HTML](https://udaylab.github.io/PAMI/html/index.html)  
-     - [Read the docs](https://pami-1.readthedocs.io)
+5. Code documentation https://pami-1.readthedocs.io 
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
-
 # Recent Updates  
 
-***
-
 - Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
 - Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
 - Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
-
 # Features
 
-***
-
 -  Well-tested and production-ready
 -  Highly optimized to our best effort, light-weight, and energy-efficient
 -  Proper code documentation
 -  Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 -  Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 -  Supports Cuda and PySpark 
 -  Operating System Independence
 -  Knowledge discovery in static data and streams
 -  Snappy
 -  Ease of use
 
-
-
 # Table of Content
 
-***
-
 - [Maintenance](#Maintenance)
 - [Try your first PAMI program](#try-your-first-PAMI-program)
-- [Evaluation](#evaluation)
 - [Reading Material](#Reading-Material)
+- [Tutorials](#Tutorials)
 - [License](#License)
 - [Documentation](#Documentation)
 - [Background](#Background)
 - [Getting Help](#Getting-Help)
 - [Discussion and Development](#Discussion-and-Development)
 - [Contribution to PAMI](#Contribution-to-PAMI)
-- [Tutorials](#Tutorials)
-
 
 # Maintenance
 
-***
-
   __Installation__
   
   1. Installing basic pami package (recommended)
 
 
          pip install pami
 
@@ -184,19 +161,16 @@
        
 
   __Information__ 
 
 
         pip show pami
 
-
 # *Try your first PAMI program*
 
-***
-
 ```shell
 $ python
 ```
 
 ```python
 # first import pami 
 from PAMI.frequentPattern.basic import FPGrowth as alg
@@ -217,95 +191,21 @@
 Frequent patterns were generated successfully using frequentPatternGrowth algorithm
 Total No of patterns: 4540
 Runtime: 8.749667644500732
 Memory (RSS): 522911744
 Memory (USS): 475353088
 ```
 
-# Evaluation:
-
-***
-
-1. we compared three different Python libraries such as PAMI, mlxtend and efficient-apriori for Apriori.
-2. (Transactional_T10I4D100K.csv)is a transactional database downloaded from PAMI and
-used as a input file for all libraries.
-3. Minimum support values and seperator are also same.
-
-* The performance of the **Apriori algorithm** is shown in the graphical results below:
-1. Comparing the **Patterns Generated** by different Python libraries for the Apriori algorithm:
-
-   <img width="573" alt="Screenshot 2024-04-11 at 13 31 31" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/fd7974bc-ffe2-44dd-82e3-a5306a8a23bd">
-   
-2. Evaluating the **Runtime** of the Apriori algorithm across different Python libraries:
-
-   <img width="567" alt="Screenshot 2024-04-11 at 13 31 20" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d615ae3-dc0d-49ba-a880-4890bb1f11c5">
-
-3. Comparing the **Memory Consumption** of the Apriori algorithm across different Python libraries:
-
-   <img width="570" alt="Screenshot 2024-04-11 at 13 31 08" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d5991ca-51ae-442d-9b5e-2d21bbebfedd">
-
-For more information, we have uploaded the evaluation file in two formats:
-- One **ipynb** file format, please check it here. [Evaluation File ipynb](https://github.com/UdayLab/PAMI/blob/main/notebooks/Evaluation-neverDelete.ipynb) 
-- Two **pdf** file format, check here. [Evaluation File Pdf](https://github.com/UdayLab/PAMI/blob/main/notebooks/evaluation.pdf)
-
 # Reading Material
-
-***
-
 For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
-
-
-# License
-
+ 
 ***
 
-[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
-
-
-# Documentation
-
-***
-
-The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
-
-
-
-# Background
-
-***
-
-The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
-has been under active development since then.
-
-
-# Getting Help
-
-***
-
-For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
-
-
-# Discussion and Development
-
-***
-
-In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
-
-
-# Contribution to PAMI
-
-***
-
-We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
-
-
 # Tutorials 
 
-***
-
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Apriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/basic/Apriori.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>              | CHARM <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/closed/CHARM.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | maxFP-growth  <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/maximal/MaxFPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | FAE <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/topk/FAE.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | cudaAprioriGCT | parallelApriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/pyspark/parallelApriori.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a>   |
@@ -494,14 +394,16 @@
 #### 5.1. Fuzzy Frequent pattern mining: [Sample](https://github.com/UdayLab/PAMI/fuzzyFrequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                   |
 |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FFI-Miner <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyFrequentPattern/basic/FFIMiner.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
+
+
 #### 5.2. Fuzzy correlated pattern mining: [Sample](https://udaylab.github.io/PAMI/fuzzyCorrelatedPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                       |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FCP-growth <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyCorrelatedPattern/basic/FCPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
@@ -599,8 +501,28 @@
 ## 11. Mining patterns from Graphs
 
 #### 11.1. Frequent sub-graph mining
 | Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-[Go to Top](#table-of-content)
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+
+# Documentation
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+
+# Background
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+# Getting Help
+For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
+# Discussion and Development
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+# Contribution to PAMI
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+[Go to Top](#table-of-contents)
```

### Comparing `pami-2024.4.24.1/README.md` & `pami-2024.4.9.1/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -11,91 +11,70 @@
 [![Downloads](https://static.pepy.tech/badge/pami)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/month)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/week)](https://pepy.tech/project/pami)
 
 [Click here for more information](https://pepy.tech/project/pami)
 
 
-
-
 # Introduction
-
-***
-
 PAttern MIning (PAMI) is a Python library containing several algorithms to discover user interest-based patterns in a wide-spectrum of datasets across multiple computing platforms. Useful links to utilize the services of this library were provided below:
 
 
 1. Youtube tutorial https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ
 
 2. Tutorials (Notebooks) https://github.com/UdayLab/PAMI/tree/main/notebooks
    
 3. User manual https://udaylab.github.io/PAMI/manuals/index.html
 
 4. Coders manual https://udaylab.github.io/PAMI/codersManual/index.html
 
-5. Code documentation
-     - [HTML](https://udaylab.github.io/PAMI/html/index.html)  
-     - [Read the docs](https://pami-1.readthedocs.io)
+5. Code documentation https://pami-1.readthedocs.io 
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
-
 # Recent Updates  
 
-***
-
 - Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
 - Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
 - Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
-
 # Features
 
-***
-
 -  Well-tested and production-ready
 -  Highly optimized to our best effort, light-weight, and energy-efficient
 -  Proper code documentation
 -  Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 -  Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 -  Supports Cuda and PySpark 
 -  Operating System Independence
 -  Knowledge discovery in static data and streams
 -  Snappy
 -  Ease of use
 
-
-
 # Table of Content
 
-***
-
 - [Maintenance](#Maintenance)
 - [Try your first PAMI program](#try-your-first-PAMI-program)
-- [Evaluation](#evaluation)
 - [Reading Material](#Reading-Material)
+- [Tutorials](#Tutorials)
 - [License](#License)
 - [Documentation](#Documentation)
 - [Background](#Background)
 - [Getting Help](#Getting-Help)
 - [Discussion and Development](#Discussion-and-Development)
 - [Contribution to PAMI](#Contribution-to-PAMI)
-- [Tutorials](#Tutorials)
-
 
 # Maintenance
 
-***
-
   __Installation__
   
   1. Installing basic pami package (recommended)
 
 
          pip install pami
 
@@ -137,19 +116,16 @@
        
 
   __Information__ 
 
 
         pip show pami
 
-
 # *Try your first PAMI program*
 
-***
-
 ```shell
 $ python
 ```
 
 ```python
 # first import pami 
 from PAMI.frequentPattern.basic import FPGrowth as alg
@@ -170,95 +146,21 @@
 Frequent patterns were generated successfully using frequentPatternGrowth algorithm
 Total No of patterns: 4540
 Runtime: 8.749667644500732
 Memory (RSS): 522911744
 Memory (USS): 475353088
 ```
 
-# Evaluation:
-
-***
-
-1. we compared three different Python libraries such as PAMI, mlxtend and efficient-apriori for Apriori.
-2. (Transactional_T10I4D100K.csv)is a transactional database downloaded from PAMI and
-used as a input file for all libraries.
-3. Minimum support values and seperator are also same.
-
-* The performance of the **Apriori algorithm** is shown in the graphical results below:
-1. Comparing the **Patterns Generated** by different Python libraries for the Apriori algorithm:
-
-   <img width="573" alt="Screenshot 2024-04-11 at 13 31 31" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/fd7974bc-ffe2-44dd-82e3-a5306a8a23bd">
-   
-2. Evaluating the **Runtime** of the Apriori algorithm across different Python libraries:
-
-   <img width="567" alt="Screenshot 2024-04-11 at 13 31 20" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d615ae3-dc0d-49ba-a880-4890bb1f11c5">
-
-3. Comparing the **Memory Consumption** of the Apriori algorithm across different Python libraries:
-
-   <img width="570" alt="Screenshot 2024-04-11 at 13 31 08" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d5991ca-51ae-442d-9b5e-2d21bbebfedd">
-
-For more information, we have uploaded the evaluation file in two formats:
-- One **ipynb** file format, please check it here. [Evaluation File ipynb](https://github.com/UdayLab/PAMI/blob/main/notebooks/Evaluation-neverDelete.ipynb) 
-- Two **pdf** file format, check here. [Evaluation File Pdf](https://github.com/UdayLab/PAMI/blob/main/notebooks/evaluation.pdf)
-
 # Reading Material
-
-***
-
 For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
-
-
-# License
-
+ 
 ***
 
-[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
-
-
-# Documentation
-
-***
-
-The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
-
-
-
-# Background
-
-***
-
-The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
-has been under active development since then.
-
-
-# Getting Help
-
-***
-
-For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
-
-
-# Discussion and Development
-
-***
-
-In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
-
-
-# Contribution to PAMI
-
-***
-
-We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
-
-
 # Tutorials 
 
-***
-
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Apriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/basic/Apriori.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>              | CHARM <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/closed/CHARM.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | maxFP-growth  <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/maximal/MaxFPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | FAE <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/topk/FAE.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | cudaAprioriGCT | parallelApriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/pyspark/parallelApriori.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a>   |
@@ -447,14 +349,16 @@
 #### 5.1. Fuzzy Frequent pattern mining: [Sample](https://github.com/UdayLab/PAMI/fuzzyFrequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                   |
 |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FFI-Miner <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyFrequentPattern/basic/FFIMiner.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
+
+
 #### 5.2. Fuzzy correlated pattern mining: [Sample](https://udaylab.github.io/PAMI/fuzzyCorrelatedPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                       |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FCP-growth <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyCorrelatedPattern/basic/FCPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
@@ -552,8 +456,28 @@
 ## 11. Mining patterns from Graphs
 
 #### 11.1. Frequent sub-graph mining
 | Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-[Go to Top](#table-of-content)
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+
+# Documentation
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+
+# Background
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+# Getting Help
+For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
+# Discussion and Development
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+# Contribution to PAMI
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+[Go to Top](#table-of-contents)
```

### Comparing `pami-2024.4.24.1/pami.egg-info/PKG-INFO` & `pami-2024.4.9.1/pami.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pami
-Version: 2024.4.24.1
+Version: 2024.4.9.1
 Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
 Home-page: https://github.com/udayLab/PAMI
 Author: Rage Uday Kiran
 Author-email: uday.rage@gmail.com
 License: GPLv3
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python :: 3
@@ -18,20 +18,18 @@
 Requires-Dist: plotly
 Requires-Dist: matplotlib
 Requires-Dist: resource
 Requires-Dist: validators
 Requires-Dist: urllib3
 Requires-Dist: Pillow
 Requires-Dist: numpy
-Requires-Dist: sphinx
 Requires-Dist: sphinx-rtd-theme
 Requires-Dist: validators
 Requires-Dist: discord.py
 Requires-Dist: networkx
-Requires-Dist: deprecated
 Provides-Extra: gpu
 Requires-Dist: cupy; extra == "gpu"
 Requires-Dist: pycuda; extra == "gpu"
 Provides-Extra: spark
 Requires-Dist: pyspark; extra == "spark"
 Provides-Extra: dev
 Requires-Dist: twine; extra == "dev"
@@ -58,91 +56,70 @@
 [![Downloads](https://static.pepy.tech/badge/pami)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/month)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/week)](https://pepy.tech/project/pami)
 
 [Click here for more information](https://pepy.tech/project/pami)
 
 
-
-
 # Introduction
-
-***
-
 PAttern MIning (PAMI) is a Python library containing several algorithms to discover user interest-based patterns in a wide-spectrum of datasets across multiple computing platforms. Useful links to utilize the services of this library were provided below:
 
 
 1. Youtube tutorial https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ
 
 2. Tutorials (Notebooks) https://github.com/UdayLab/PAMI/tree/main/notebooks
    
 3. User manual https://udaylab.github.io/PAMI/manuals/index.html
 
 4. Coders manual https://udaylab.github.io/PAMI/codersManual/index.html
 
-5. Code documentation
-     - [HTML](https://udaylab.github.io/PAMI/html/index.html)  
-     - [Read the docs](https://pami-1.readthedocs.io)
+5. Code documentation https://pami-1.readthedocs.io 
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
-
 # Recent Updates  
 
-***
-
 - Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
 - Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
 - Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
-
 # Features
 
-***
-
 -  Well-tested and production-ready
 -  Highly optimized to our best effort, light-weight, and energy-efficient
 -  Proper code documentation
 -  Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 -  Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 -  Supports Cuda and PySpark 
 -  Operating System Independence
 -  Knowledge discovery in static data and streams
 -  Snappy
 -  Ease of use
 
-
-
 # Table of Content
 
-***
-
 - [Maintenance](#Maintenance)
 - [Try your first PAMI program](#try-your-first-PAMI-program)
-- [Evaluation](#evaluation)
 - [Reading Material](#Reading-Material)
+- [Tutorials](#Tutorials)
 - [License](#License)
 - [Documentation](#Documentation)
 - [Background](#Background)
 - [Getting Help](#Getting-Help)
 - [Discussion and Development](#Discussion-and-Development)
 - [Contribution to PAMI](#Contribution-to-PAMI)
-- [Tutorials](#Tutorials)
-
 
 # Maintenance
 
-***
-
   __Installation__
   
   1. Installing basic pami package (recommended)
 
 
          pip install pami
 
@@ -184,19 +161,16 @@
        
 
   __Information__ 
 
 
         pip show pami
 
-
 # *Try your first PAMI program*
 
-***
-
 ```shell
 $ python
 ```
 
 ```python
 # first import pami 
 from PAMI.frequentPattern.basic import FPGrowth as alg
@@ -217,95 +191,21 @@
 Frequent patterns were generated successfully using frequentPatternGrowth algorithm
 Total No of patterns: 4540
 Runtime: 8.749667644500732
 Memory (RSS): 522911744
 Memory (USS): 475353088
 ```
 
-# Evaluation:
-
-***
-
-1. we compared three different Python libraries such as PAMI, mlxtend and efficient-apriori for Apriori.
-2. (Transactional_T10I4D100K.csv)is a transactional database downloaded from PAMI and
-used as a input file for all libraries.
-3. Minimum support values and seperator are also same.
-
-* The performance of the **Apriori algorithm** is shown in the graphical results below:
-1. Comparing the **Patterns Generated** by different Python libraries for the Apriori algorithm:
-
-   <img width="573" alt="Screenshot 2024-04-11 at 13 31 31" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/fd7974bc-ffe2-44dd-82e3-a5306a8a23bd">
-   
-2. Evaluating the **Runtime** of the Apriori algorithm across different Python libraries:
-
-   <img width="567" alt="Screenshot 2024-04-11 at 13 31 20" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d615ae3-dc0d-49ba-a880-4890bb1f11c5">
-
-3. Comparing the **Memory Consumption** of the Apriori algorithm across different Python libraries:
-
-   <img width="570" alt="Screenshot 2024-04-11 at 13 31 08" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d5991ca-51ae-442d-9b5e-2d21bbebfedd">
-
-For more information, we have uploaded the evaluation file in two formats:
-- One **ipynb** file format, please check it here. [Evaluation File ipynb](https://github.com/UdayLab/PAMI/blob/main/notebooks/Evaluation-neverDelete.ipynb) 
-- Two **pdf** file format, check here. [Evaluation File Pdf](https://github.com/UdayLab/PAMI/blob/main/notebooks/evaluation.pdf)
-
 # Reading Material
-
-***
-
 For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
-
-
-# License
-
+ 
 ***
 
-[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
-
-
-# Documentation
-
-***
-
-The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
-
-
-
-# Background
-
-***
-
-The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
-has been under active development since then.
-
-
-# Getting Help
-
-***
-
-For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
-
-
-# Discussion and Development
-
-***
-
-In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
-
-
-# Contribution to PAMI
-
-***
-
-We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
-
-
 # Tutorials 
 
-***
-
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Apriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/basic/Apriori.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>              | CHARM <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/closed/CHARM.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | maxFP-growth  <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/maximal/MaxFPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | FAE <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/topk/FAE.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | cudaAprioriGCT | parallelApriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/pyspark/parallelApriori.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a>   |
@@ -494,14 +394,16 @@
 #### 5.1. Fuzzy Frequent pattern mining: [Sample](https://github.com/UdayLab/PAMI/fuzzyFrequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                   |
 |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FFI-Miner <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyFrequentPattern/basic/FFIMiner.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
+
+
 #### 5.2. Fuzzy correlated pattern mining: [Sample](https://udaylab.github.io/PAMI/fuzzyCorrelatedPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                       |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FCP-growth <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyCorrelatedPattern/basic/FCPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
@@ -599,8 +501,28 @@
 ## 11. Mining patterns from Graphs
 
 #### 11.1. Frequent sub-graph mining
 | Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-[Go to Top](#table-of-content)
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+
+# Documentation
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+
+# Background
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+# Getting Help
+For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
+# Discussion and Development
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+# Contribution to PAMI
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+[Go to Top](#table-of-contents)
```

### Comparing `pami-2024.4.24.1/pami.egg-info/SOURCES.txt` & `pami-2024.4.9.1/pami.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -113,16 +113,14 @@
 PAMI/faultTolerantFrequentPattern/basic/abstract.py
 PAMI/frequentPattern/__init__.py
 PAMI/frequentPattern/basic/Apriori.py
 PAMI/frequentPattern/basic/ECLAT.py
 PAMI/frequentPattern/basic/ECLATDiffset.py
 PAMI/frequentPattern/basic/ECLATbitset.py
 PAMI/frequentPattern/basic/FPGrowth.py
-PAMI/frequentPattern/basic/_Apriori.py
-PAMI/frequentPattern/basic/_FPGrowth.py
 PAMI/frequentPattern/basic/__init__.py
 PAMI/frequentPattern/basic/abstract.py
 PAMI/frequentPattern/closed/CHARM.py
 PAMI/frequentPattern/closed/__init__.py
 PAMI/frequentPattern/closed/abstract.py
 PAMI/frequentPattern/cuda/__init__.py
 PAMI/frequentPattern/cuda/abstract.py
@@ -262,16 +260,14 @@
 PAMI/periodicCorrelatedPattern/basic/abstract.py
 PAMI/periodicFrequentPattern/__init__.py
 PAMI/periodicFrequentPattern/basic/PFECLAT.py
 PAMI/periodicFrequentPattern/basic/PFPGrowth.py
 PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
 PAMI/periodicFrequentPattern/basic/PFPMC.py
 PAMI/periodicFrequentPattern/basic/PSGrowth.py
-PAMI/periodicFrequentPattern/basic/_PFECLAT.py
-PAMI/periodicFrequentPattern/basic/_PFPGrowth.py
 PAMI/periodicFrequentPattern/basic/__init__.py
 PAMI/periodicFrequentPattern/basic/abstract.py
 PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
 PAMI/periodicFrequentPattern/closed/CPFPMiner.py
 PAMI/periodicFrequentPattern/closed/__init__.py
 PAMI/periodicFrequentPattern/closed/abstract.py
 PAMI/periodicFrequentPattern/cuda/__init__.py
```

### Comparing `pami-2024.4.24.1/setup.py` & `pami-2024.4.9.1/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import setuptools
 
 with open('README.md', 'r') as fh:
     long_description = fh.read()
 
 setuptools.setup(
     name='pami',
-    version='2024.4.24.1',
+    version='2024.4.9.1',
     author='Rage Uday Kiran',
     author_email='uday.rage@gmail.com',
     description='This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan',
     long_description=long_description,
     long_description_content_type='text/markdown',
     packages=setuptools.find_packages(),
     url='https://github.com/udayLab/PAMI',
@@ -20,20 +20,18 @@
         'plotly',
         'matplotlib',
         'resource',
         'validators',
         'urllib3',
         'Pillow',
         'numpy',
-        'sphinx',
         'sphinx-rtd-theme',
         'validators',
         'discord.py',
         'networkx',
-        'deprecated',
     ],
     extras_require={
         'gpu':  ['cupy', 'pycuda'],
         'spark': ['pyspark'],
         'dev': ['twine', 'setuptools', 'build'],
         'all': ['cupy', 'pycuda', 'pyspark', 'twine', 'setuptools', 'build']
     },
@@ -41,8 +39,7 @@
         'Development Status :: 5 - Production/Stable',      # Chose either "3 - Alpha", "4 - Beta" or "5 - Production/Stable" as the current state of your package
         'Programming Language :: Python :: 3',
         'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
         'Operating System :: OS Independent',
     ],
     python_requires='>=3.5',
 )
-
```

