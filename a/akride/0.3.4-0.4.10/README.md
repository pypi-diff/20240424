# Comparing `tmp/akride-0.3.4-py3-none-any.whl.zip` & `tmp/akride-0.4.10-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,74 +1,77 @@
-Zip file size: 70402 bytes, number of entries: 72
--rw-r--r--  2.0 unx     2119 b- defN 24-Jan-22 10:47 akride/__init__.py
--rw-r--r--  2.0 unx     2499 b- defN 24-Jan-22 10:47 akride/background_task_manager.py
--rw-r--r--  2.0 unx    34360 b- defN 24-Jan-22 10:47 akride/client.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/_utils/__init__.py
--rw-r--r--  2.0 unx     2861 b- defN 24-Jan-22 10:47 akride/_utils/background_task_helper.py
--rw-r--r--  2.0 unx     1357 b- defN 24-Jan-22 10:47 akride/_utils/class_executor.py
--rw-r--r--  2.0 unx     2624 b- defN 24-Jan-22 10:47 akride/_utils/exception_utils.py
--rw-r--r--  2.0 unx      501 b- defN 24-Jan-22 10:47 akride/_utils/file_utils.py
--rw-r--r--  2.0 unx    12897 b- defN 24-Jan-22 10:47 akride/_utils/job_creator.py
--rw-r--r--  2.0 unx      454 b- defN 24-Jan-22 10:47 akride/_utils/progress_bar_helper.py
--rw-r--r--  2.0 unx     1459 b- defN 24-Jan-22 10:47 akride/_utils/proxy_utils.py
--rw-r--r--  2.0 unx      172 b- defN 24-Jan-22 10:47 akride/_utils/resource_utils.py
--rw-r--r--  2.0 unx      595 b- defN 24-Jan-22 10:47 akride/_utils/retry_helper.py
--rw-r--r--  2.0 unx      811 b- defN 24-Jan-22 10:47 akride/_utils/store_utils.py
--rw-r--r--  2.0 unx      473 b- defN 24-Jan-22 10:47 akride/_utils/workflow_helper.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/_utils/catalog/__init__.py
--rw-r--r--  2.0 unx      779 b- defN 24-Jan-22 10:47 akride/_utils/catalog/catalog_tables_helper.py
--rw-r--r--  2.0 unx      574 b- defN 24-Jan-22 10:47 akride/_utils/catalog/dataset_tables_info.py
--rw-r--r--  2.0 unx      194 b- defN 24-Jan-22 10:47 akride/_utils/catalog/enums.py
--rw-r--r--  2.0 unx      890 b- defN 24-Jan-22 10:47 akride/_utils/catalog/pipeline_tables_info.py
--rw-r--r--  2.0 unx      541 b- defN 24-Jan-22 10:47 akride/_utils/catalog/tables_info.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/_utils/pipeline/__init__.py
--rw-r--r--  2.0 unx      171 b- defN 24-Jan-22 10:47 akride/_utils/pipeline/constants.py
--rw-r--r--  2.0 unx     1083 b- defN 24-Jan-22 10:47 akride/_utils/pipeline/pipeline_helper.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/_utils/progress_manager/__init__.py
--rw-r--r--  2.0 unx     2035 b- defN 24-Jan-22 10:47 akride/_utils/progress_manager/manager.py
--rw-r--r--  2.0 unx     1646 b- defN 24-Jan-22 10:47 akride/_utils/progress_manager/progress_step.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/_utils/rest/__init__.py
--rw-r--r--  2.0 unx      528 b- defN 24-Jan-22 10:47 akride/_utils/rest/requests_session_manager.py
--rw-r--r--  2.0 unx     3468 b- defN 24-Jan-22 10:47 akride/_utils/rest/rest_client.py
--rw-r--r--  2.0 unx      194 b- defN 24-Jan-22 10:47 akride/_utils/rest/utils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/__init__.py
--rw-r--r--  2.0 unx     1450 b- defN 24-Jan-22 10:47 akride/core/_log.py
--rw-r--r--  2.0 unx    14157 b- defN 24-Jan-22 10:47 akride/core/_pipeline_executor.py
--rw-r--r--  2.0 unx     1859 b- defN 24-Jan-22 10:47 akride/core/constants.py
--rw-r--r--  2.0 unx     1987 b- defN 24-Jan-22 10:47 akride/core/enums.py
--rw-r--r--  2.0 unx     1500 b- defN 24-Jan-22 10:47 akride/core/exceptions.py
--rw-r--r--  2.0 unx     9614 b- defN 24-Jan-22 10:47 akride/core/types.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/_entity_managers/__init__.py
--rw-r--r--  2.0 unx    18095 b- defN 24-Jan-22 10:47 akride/core/_entity_managers/catalog_manager.py
--rw-r--r--  2.0 unx    15333 b- defN 24-Jan-22 10:47 akride/core/_entity_managers/dataset_manager.py
--rw-r--r--  2.0 unx    27835 b- defN 24-Jan-22 10:47 akride/core/_entity_managers/job_manager.py
--rw-r--r--  2.0 unx     2238 b- defN 24-Jan-22 10:47 akride/core/_entity_managers/manager.py
--rw-r--r--  2.0 unx     8486 b- defN 24-Jan-22 10:47 akride/core/_entity_managers/resultset_manager.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Jan-22 10:47 akride/core/_entity_managers/subscriptions_manager.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/_filters/__init__.py
--rw-r--r--  2.0 unx      253 b- defN 24-Jan-22 10:47 akride/core/_filters/enums.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/_filters/partitioners/__init__.py
--rw-r--r--  2.0 unx     6420 b- defN 24-Jan-22 10:47 akride/core/_filters/partitioners/ingest_partitioner_filter.py
--rw-r--r--  2.0 unx      696 b- defN 24-Jan-22 10:47 akride/core/_filters/partitioners/models.py
--rw-r--r--  2.0 unx      288 b- defN 24-Jan-22 10:47 akride/core/_filters/partitioners/partitioner_filter.py
--rw-r--r--  2.0 unx     6059 b- defN 24-Jan-22 10:47 akride/core/_filters/partitioners/process_partitioner_filter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/_filters/sink/__init__.py
--rw-r--r--  2.0 unx     1247 b- defN 24-Jan-22 10:47 akride/core/_filters/sink/models.py
--rw-r--r--  2.0 unx     8936 b- defN 24-Jan-22 10:47 akride/core/_filters/sink/sink_writer_filter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/conf/__init__.py
--rw-rw-r--  2.0 unx      493 b- defN 24-Jan-22 10:47 akride/core/conf/pylogconf.yaml
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/entities/__init__.py
--rw-r--r--  2.0 unx      796 b- defN 24-Jan-22 10:47 akride/core/entities/catalogs.py
--rw-r--r--  2.0 unx      736 b- defN 24-Jan-22 10:47 akride/core/entities/datasets.py
--rw-r--r--  2.0 unx     1549 b- defN 24-Jan-22 10:47 akride/core/entities/entity.py
--rw-r--r--  2.0 unx     5133 b- defN 24-Jan-22 10:47 akride/core/entities/jobs.py
--rw-r--r--  2.0 unx      436 b- defN 24-Jan-22 10:47 akride/core/entities/pipeline.py
--rw-r--r--  2.0 unx      955 b- defN 24-Jan-22 10:47 akride/core/entities/resultsets.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-22 10:47 akride/core/models/__init__.py
--rw-r--r--  2.0 unx      398 b- defN 24-Jan-22 10:47 akride/core/models/catalog_details.py
--rw-r--r--  2.0 unx      246 b- defN 24-Jan-22 10:47 akride/core/models/progress_info.py
--rw-rw-r--  2.0 unx      130 b- defN 24-Jan-22 10:51 akride-0.3.4.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     3111 b- defN 24-Jan-22 10:51 akride-0.3.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Jan-22 10:51 akride-0.3.4.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 24-Jan-22 10:51 akride-0.3.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6499 b- defN 24-Jan-22 10:51 akride-0.3.4.dist-info/RECORD
-72 files, 223633 bytes uncompressed, 59884 bytes compressed:  73.2%
+Zip file size: 73000 bytes, number of entries: 75
+-rw-r--r--  2.0 unx     2119 b- defN 24-Apr-16 06:31 akride/__init__.py
+-rw-r--r--  2.0 unx     2499 b- defN 24-Apr-16 06:31 akride/background_task_manager.py
+-rw-r--r--  2.0 unx    34742 b- defN 24-Apr-16 06:31 akride/client.py
+-rw-r--r--  2.0 unx     1808 b- defN 24-Apr-16 06:31 akride/main.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/__init__.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-Apr-16 06:31 akride/_utils/background_task_helper.py
+-rw-r--r--  2.0 unx     1357 b- defN 24-Apr-16 06:31 akride/_utils/class_executor.py
+-rw-r--r--  2.0 unx     2624 b- defN 24-Apr-16 06:31 akride/_utils/exception_utils.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-16 06:31 akride/_utils/file_utils.py
+-rw-r--r--  2.0 unx    12931 b- defN 24-Apr-16 06:31 akride/_utils/job_creator.py
+-rw-r--r--  2.0 unx      278 b- defN 24-Apr-16 06:31 akride/_utils/platform.py
+-rw-r--r--  2.0 unx      454 b- defN 24-Apr-16 06:31 akride/_utils/progress_bar_helper.py
+-rw-r--r--  2.0 unx     1459 b- defN 24-Apr-16 06:31 akride/_utils/proxy_utils.py
+-rw-r--r--  2.0 unx      172 b- defN 24-Apr-16 06:31 akride/_utils/resource_utils.py
+-rw-r--r--  2.0 unx      595 b- defN 24-Apr-16 06:31 akride/_utils/retry_helper.py
+-rw-r--r--  2.0 unx      811 b- defN 24-Apr-16 06:31 akride/_utils/store_utils.py
+-rw-r--r--  2.0 unx      473 b- defN 24-Apr-16 06:31 akride/_utils/workflow_helper.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/catalog/__init__.py
+-rw-r--r--  2.0 unx      779 b- defN 24-Apr-16 06:31 akride/_utils/catalog/catalog_tables_helper.py
+-rw-r--r--  2.0 unx      574 b- defN 24-Apr-16 06:31 akride/_utils/catalog/dataset_tables_info.py
+-rw-r--r--  2.0 unx      194 b- defN 24-Apr-16 06:31 akride/_utils/catalog/enums.py
+-rw-r--r--  2.0 unx      890 b- defN 24-Apr-16 06:31 akride/_utils/catalog/pipeline_tables_info.py
+-rw-r--r--  2.0 unx      541 b- defN 24-Apr-16 06:31 akride/_utils/catalog/tables_info.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/pipeline/__init__.py
+-rw-r--r--  2.0 unx      250 b- defN 24-Apr-16 06:31 akride/_utils/pipeline/constants.py
+-rw-r--r--  2.0 unx     1159 b- defN 24-Apr-16 06:31 akride/_utils/pipeline/pipeline_helper.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/progress_manager/__init__.py
+-rw-r--r--  2.0 unx     2037 b- defN 24-Apr-16 06:31 akride/_utils/progress_manager/manager.py
+-rw-r--r--  2.0 unx     1646 b- defN 24-Apr-16 06:31 akride/_utils/progress_manager/progress_step.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/rest/__init__.py
+-rw-r--r--  2.0 unx      528 b- defN 24-Apr-16 06:31 akride/_utils/rest/requests_session_manager.py
+-rw-r--r--  2.0 unx     3468 b- defN 24-Apr-16 06:31 akride/_utils/rest/rest_client.py
+-rw-r--r--  2.0 unx      194 b- defN 24-Apr-16 06:31 akride/_utils/rest/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/__init__.py
+-rw-r--r--  2.0 unx     1450 b- defN 24-Apr-16 06:31 akride/core/_log.py
+-rw-r--r--  2.0 unx    15237 b- defN 24-Apr-16 06:31 akride/core/_pipeline_executor.py
+-rw-r--r--  2.0 unx     1859 b- defN 24-Apr-16 06:31 akride/core/constants.py
+-rw-r--r--  2.0 unx     2284 b- defN 24-Apr-16 06:31 akride/core/enums.py
+-rw-r--r--  2.0 unx     1500 b- defN 24-Apr-16 06:31 akride/core/exceptions.py
+-rw-r--r--  2.0 unx     9614 b- defN 24-Apr-16 06:31 akride/core/types.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/__init__.py
+-rw-r--r--  2.0 unx    18527 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/catalog_manager.py
+-rw-r--r--  2.0 unx    16073 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/dataset_manager.py
+-rw-r--r--  2.0 unx    28206 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/job_manager.py
+-rw-r--r--  2.0 unx     2238 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/manager.py
+-rw-r--r--  2.0 unx     8702 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/resultset_manager.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/subscriptions_manager.py
+-rw-r--r--  2.0 unx      140 b- defN 24-Apr-16 06:31 akride/core/_filters/__init__.py
+-rw-r--r--  2.0 unx      253 b- defN 24-Apr-16 06:31 akride/core/_filters/enums.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/__init__.py
+-rw-r--r--  2.0 unx     6412 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/ingest_partitioner_filter.py
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/models.py
+-rw-r--r--  2.0 unx      288 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/partitioner_filter.py
+-rw-r--r--  2.0 unx     6043 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/process_partitioner_filter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/_filters/sink/__init__.py
+-rw-r--r--  2.0 unx     1247 b- defN 24-Apr-16 06:31 akride/core/_filters/sink/models.py
+-rw-r--r--  2.0 unx     9178 b- defN 24-Apr-16 06:31 akride/core/_filters/sink/sink_writer_filter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/conf/__init__.py
+-rw-rw-r--  2.0 unx      493 b- defN 24-Apr-16 06:31 akride/core/conf/pylogconf.yaml
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/entities/__init__.py
+-rw-r--r--  2.0 unx      796 b- defN 24-Apr-16 06:31 akride/core/entities/catalogs.py
+-rw-r--r--  2.0 unx      736 b- defN 24-Apr-16 06:31 akride/core/entities/datasets.py
+-rw-r--r--  2.0 unx     1549 b- defN 24-Apr-16 06:31 akride/core/entities/entity.py
+-rw-r--r--  2.0 unx     5133 b- defN 24-Apr-16 06:31 akride/core/entities/jobs.py
+-rw-r--r--  2.0 unx      436 b- defN 24-Apr-16 06:31 akride/core/entities/pipeline.py
+-rw-r--r--  2.0 unx      955 b- defN 24-Apr-16 06:31 akride/core/entities/resultsets.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/models/__init__.py
+-rw-r--r--  2.0 unx      398 b- defN 24-Apr-16 06:31 akride/core/models/catalog_details.py
+-rw-r--r--  2.0 unx      246 b- defN 24-Apr-16 06:31 akride/core/models/progress_info.py
+-rw-rw-r--  2.0 unx      130 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     3475 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/WHEEL
+-rw-r--r--  2.0 unx       44 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6753 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/RECORD
+75 files, 230574 bytes uncompressed, 62086 bytes compressed:  73.1%
```

## zipnote {}

```diff
@@ -3,14 +3,17 @@
 
 Filename: akride/background_task_manager.py
 Comment: 
 
 Filename: akride/client.py
 Comment: 
 
+Filename: akride/main.py
+Comment: 
+
 Filename: akride/_utils/__init__.py
 Comment: 
 
 Filename: akride/_utils/background_task_helper.py
 Comment: 
 
 Filename: akride/_utils/class_executor.py
@@ -21,14 +24,17 @@
 
 Filename: akride/_utils/file_utils.py
 Comment: 
 
 Filename: akride/_utils/job_creator.py
 Comment: 
 
+Filename: akride/_utils/platform.py
+Comment: 
+
 Filename: akride/_utils/progress_bar_helper.py
 Comment: 
 
 Filename: akride/_utils/proxy_utils.py
 Comment: 
 
 Filename: akride/_utils/resource_utils.py
@@ -195,23 +201,26 @@
 
 Filename: akride/core/models/catalog_details.py
 Comment: 
 
 Filename: akride/core/models/progress_info.py
 Comment: 
 
-Filename: akride-0.3.4.dist-info/LICENSE.txt
+Filename: akride-0.4.10.dist-info/LICENSE.txt
+Comment: 
+
+Filename: akride-0.4.10.dist-info/METADATA
 Comment: 
 
-Filename: akride-0.3.4.dist-info/METADATA
+Filename: akride-0.4.10.dist-info/WHEEL
 Comment: 
 
-Filename: akride-0.3.4.dist-info/WHEEL
+Filename: akride-0.4.10.dist-info/entry_points.txt
 Comment: 
 
-Filename: akride-0.3.4.dist-info/top_level.txt
+Filename: akride-0.4.10.dist-info/top_level.txt
 Comment: 
 
-Filename: akride-0.3.4.dist-info/RECORD
+Filename: akride-0.4.10.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## akride/client.py

```diff
@@ -1,11 +1,12 @@
 """
  Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
+
 import json
 from contextlib import suppress
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import akridata_akrimanager_v2 as am
 import akridata_dsp as dsp
 import pandas as pd
@@ -82,15 +83,18 @@
         would be
         1. Positional params (saas_endpoint, api_key)
         2. sdk_config_tuple
         3. sdk_config
         4. sdk_config_file
 
         Get the sdk config by signing in to Data Explorer UI and navigating to
-        Utilities → Get CLI/SDK config
+        Utilities → Get CLI/SDK config.
+
+        For detailed information on how to download the SDK config refer
+        https://docs.akridata.ai/docs/download-config-saas
 
         Parameters
 
         Parameters
         ----------
         saas_endpoint: str
             Dataexplorer endpoint, if None defaults to https://app.akridata.ai
@@ -131,27 +135,27 @@
 
         dsp_conf = dsp.Configuration(
             host=f"https://{app_endpoint}/ds-core",
         )
         dsp_conf.proxy = proxy  # type: ignore
         dsp_conf.proxy_headers = proxy_headers  # type: ignore
         default_retries = get_http_retry()
-        dsp_conf.retries = default_retries
+        dsp_conf.retries = default_retries  # type:ignore
         dsp_client = dsp.ApiClient(
             configuration=dsp_conf,
             header_name="X-API-KEY",
             header_value=access_key,
         )
 
         am_conf = am.Configuration(
             host=f"https://{app_endpoint}/api",
         )
         am_conf.proxy = proxy  # type: ignore
         am_conf.proxy_headers = proxy_headers  # type: ignore
-        am_conf.retries = default_retries
+        am_conf.retries = default_retries  # type:ignore
         am_client = am.ApiClient(
             configuration=am_conf,
             header_name="X-API-KEY",
             header_value=access_key,
         )
 
         task_manager = BackgroundTaskManager()
@@ -318,28 +322,31 @@
         return self.datasets.delete_entity(dataset)
 
     def ingest_dataset(
         self,
         dataset: Dataset,
         data_directory: str,
         use_patch_featurizer: bool = True,
+        with_clip_featurizer: bool = False,
         async_req: bool = False,
         catalog_details: Optional[CatalogDetails] = None,
     ) -> Optional[BackgroundTask]:
         """
         Starts an asynchronous ingest task for the specified dataset.
 
         Parameters
         ----------
         dataset : Dataset
             The dataset to ingest.
         data_directory : str
             The path to the directory containing the dataset files.
         use_patch_featurizer: bool, optional
             Ingest dataset to enable patch-based similarity searches.
+        with_clip_featurizer: bool, optional
+            Ingest dataset to enable text prompt based search.
         async_req: bool, optional
             Whether to execute the request asynchronously.
         catalog_details: Optional[CatalogDetails]
             Parameters details for creating a catalog
 
         Returns
         -------
@@ -353,14 +360,15 @@
         )
 
         return self.datasets.ingest_dataset(
             dataset=dataset,
             data_directory=data_directory,
             featurizer_type=featurizer_type,
             catalog_details=catalog_details,
+            with_clip_featurizer=with_clip_featurizer,
             async_req=async_req,
         )
 
     def import_catalog(
         self,
         dataset: Dataset,
         table_name: str,
@@ -1052,15 +1060,15 @@
             dataset,
             left_table,
             right_table,
             join_condition,
         )
 
     def get_attached_pipelines(
-        self, dataset: Dataset, version: str = None
+        self, dataset: Dataset, version: Optional[str] = None
     ) -> List[Pipeline]:
         """Get pipelines attached for dataset given a dataset version
 
         Args:
             dataset (Dataset): Dataset object
             version (str, optional): Dataset version. Defaults to None in which
             case the latest version would be used
```

## akride/_utils/background_task_helper.py

```diff
@@ -1,11 +1,12 @@
 """
  Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
+
 import threading
 from typing import Optional
 
 from akride._utils.progress_manager.manager import ProgressManager
 from akride.core.models.progress_info import ProgressInfo
 
 
@@ -102,9 +103,10 @@
         ----------
 
         Returns
         -------
         ProgressInfo
             The progress information
         """
+        # assert self._thread is not None
         self._thread.join()
         return self._progress_manager.get_progress_report()
```

## akride/_utils/file_utils.py

```diff
@@ -1,18 +1,22 @@
 import shutil
 from pathlib import Path
 from typing import List
 
-from pyakri_de_filters.utils.file_utils import create_directory
+from pyakri_de_utils.file_utils import create_directory
 
 
 def copy_files_to_dir(files: List[str], dst_dir: str):
     for file in files:
         # file[1:] -> to get the file path without "/"
-        dest_file_path = Path(dst_dir).joinpath(file[1:])
+        dest_file_path = Path(dst_dir, *Path(file).parts[1:])
 
         create_directory(str(dest_file_path.parent))
         shutil.copy(file, dest_file_path)
 
 
 def get_file_name_from_path(filepath: str) -> str:
     return Path(filepath).name
+
+
+def concat_file_paths(*file_path_list) -> str:
+    return str(Path(*file_path_list))
```

## akride/_utils/job_creator.py

```diff
@@ -13,42 +13,42 @@
     CatalogTableResponse,
     MakeQueryTableRequest,
     QueryIdResponse,
     QueryResponse,
     Table,
 )
 from akridata_dsp import (
-    ClusterAlgorithms,
     CreateJobRequest,
     CreateJobRequestResponse,
     CreateSubmitJobRequest,
-    DatasetCreateJobRequest,
+    DatasetInfo,
     EmbedderQuality,
-    EmbeddingAlgorithms,
     RequestDataSourceCreate,
     RequestSourceMetaJobRequestCreate,
     SubmitRequestTunables,
 )
 
-from akride.core.enums import JobType
+from akride.core.enums import ClusterAlgoType, EmbedAlgoType, JobType
 from akride.core.exceptions import UserError
 from akride.core.types import AnalyzeJobParams, CatalogDetails, CatalogTable
 
 
 class JobCreator:  # pylint: disable=too-few-public-methods
     """Create a ds job"""
 
     def __init__(
         self,
-        job_req_api: dsp.RequestApi,
+        job_req_api: dsp.JobRequestsApi,
         qms_api: am.QueriesApi,
         catalog_api: am.CatalogApi,
         views_api: am.ViewsApi,
+        request_api: dsp.RequestApi,
     ) -> None:
-        self.job_req_api = job_req_api
+        self.job_req_api: dsp.JobRequestsApi = job_req_api
+        self.request_api: dsp.RequestApi = request_api
         self.qms_api = qms_api
         self.catalog_api = catalog_api
         self.views_api: am.ViewsApi = views_api
 
     def create_job(  # pylint: disable=too-many-arguments
         self,
         job_type: str,
@@ -135,15 +135,15 @@
             clusterer_algo=clusterer_algo,
             embedder_algo=embedder_algo,
             embedder_quality=embedder_quality,
             cluster_first=cluster_first,
             num_clusters=num_clusters,
             analyze_params=analyze_params,
         )
-        resp = self.job_req_api.post_request_root(
+        resp = self.request_api.create_job_request(
             create_job_request=request
         )  # type: ignore
         return request, resp  # type: ignore
 
     def _get_job_request(  # pylint: disable=too-many-arguments
         self,
         job_name: str,
@@ -154,17 +154,15 @@
         clusterer_algo: str,
         embedder_algo: str,
         embedder_quality: str,
         cluster_first: bool,
         num_clusters: Optional[int] = None,
         analyze_params: Optional[AnalyzeJobParams] = None,
     ) -> CreateJobRequest:
-        ds_req = DatasetCreateJobRequest(
-            dataset_id=dataset_id, pipeline_id=pipeline_id, frames=None
-        )
+        ds_req = DatasetInfo(dataset_id=dataset_id, pipeline_id=pipeline_id)
         req_data = RequestDataSourceCreate(catalog_filters=True)
         req_meta = RequestSourceMetaJobRequestCreate(vcs_query_id=query_id)
         submit_params = self._get_submit_params(
             job_type=job_type,
             clusterer_algo=clusterer_algo,
             embedder_algo=embedder_algo,
             embedder_quality=embedder_quality,
@@ -180,16 +178,16 @@
             submit_params=submit_params,
         )
         return request
 
     def _get_submit_params(  # pylint: disable=too-many-arguments
         self,
         job_type: str = JobType.EXPLORE,
-        clusterer_algo: str = ClusterAlgorithms.HDBSCAN,
-        embedder_algo: str = EmbeddingAlgorithms.UMAP,
+        clusterer_algo: str = ClusterAlgoType.HDBSCAN,
+        embedder_algo: str = EmbedAlgoType.UMAP,
         embedder_quality: str = EmbedderQuality.HIGH,
         cluster_first: bool = True,
         num_clusters: Optional[int] = None,
         analyze_params: Optional[AnalyzeJobParams] = None,
     ) -> CreateSubmitJobRequest:
         tunables = SubmitRequestTunables(
             cluster_algo=clusterer_algo,
```

## akride/_utils/pipeline/constants.py

```diff
@@ -1,5 +1,7 @@
 class Constants:
     # "AkridataImagePipeline"
     FULL_IMAGE_PIPELINE_INTERNAL_ID = 3101
     # "AkridataImagePatchPipelineV2"
     PATCH_IMAGE_PIPELINE_INTERNAL_ID = 3103
+    # "AkridataImageClipPipelineV2"
+    CLIP_IMAGE_PIPELINE_INTERNAL_ID = 3112
```

## akride/_utils/pipeline/pipeline_helper.py

```diff
@@ -5,14 +5,15 @@
 class PipelineHelper:
     FEATURIZER_TYPE_AND_PIPELINE_MAPPING = {
         DataType.IMAGE: {
             FeaturizerType.FULL_IMAGE: (
                 Constants.FULL_IMAGE_PIPELINE_INTERNAL_ID
             ),
             FeaturizerType.PATCH: Constants.PATCH_IMAGE_PIPELINE_INTERNAL_ID,
+            FeaturizerType.CLIP: Constants.CLIP_IMAGE_PIPELINE_INTERNAL_ID,
         }
     }
 
     @classmethod
     def get_pipeline_internal_id(
         cls,
         featurizer_type: FeaturizerType,
```

## akride/_utils/progress_manager/manager.py

```diff
@@ -50,15 +50,15 @@
             weight = step.get_weight()
             progress_report: ProgressInfo = step.get_progress_report()
             total_percentage += weight * progress_report.percent_completed
             total_weight += weight
 
             logger.debug(
                 f"Total percentage {total_percentage}, "
-                f"total_weight {total_weight}"
+                f"total_weight {total_weight}, "
                 f"Step name: {step.get_name()}, "
                 f"progress percentage: {progress_report.percent_completed}, "
                 f"weight: {step.get_weight()}"
             )
 
         percentage_completed = float(total_percentage / total_weight)
```

## akride/core/_pipeline_executor.py

```diff
@@ -7,14 +7,16 @@
     DataIngestWrapper,
 )
 
 from akride import logger
 from akride._utils.catalog.catalog_tables_helper import CatalogTablesHelper
 from akride._utils.catalog.pipeline_tables_info import PipelineTablesInfo
 from akride._utils.class_executor import ClassExecutor
+from akride._utils.file_utils import concat_file_paths
+from akride._utils.platform import is_windows_os
 from akride._utils.progress_manager.manager import (
     ProgressManager,
     ProgressStep,
 )
 from akride._utils.workflow_helper import WorkflowHelper
 from akride.core._filters.enums import FilterTypes
 from akride.core._filters.partitioners.ingest_partitioner_filter import (
@@ -60,23 +62,31 @@
     def run(self):
         logger.debug("Ingestion in progress!")
         self._progress_manager.set_msg("Ingestion in progress!")
 
         ingest_step: ProgressStep = self._progress_manager.register_step(
             "Ingest", weight=self.INGEST_WORKFLOW_PROGRESS_WEIGHTAGE
         )
-        process_step: ProgressStep = self._progress_manager.register_step(
-            "Process", weight=self.PROCESS_WORKFLOW_PROGRESS_WEIGHTAGE
-        )
+
+        process_steps: List[ProgressStep] = []
+        for pipeline in self._catalog_tables_helper.get_pipelines():
+            pipeline_id = pipeline.get_pipeline_id()
+            pipeline_name = pipeline.get_pipeline_name()
+            process_steps.append(
+                self._progress_manager.register_step(
+                    f"Process-{pipeline_id}-{pipeline_name}",
+                    weight=self.PROCESS_WORKFLOW_PROGRESS_WEIGHTAGE,
+                )
+            )
 
         self._run_ingest_workflow(ingest_step=ingest_step)
 
-        self._run_process_workflow(process_step=process_step)
+        self._run_process_workflow(process_steps=process_steps)
 
-        self._progress_manager.set_msg("Ingestion completed!")
+        self._progress_manager.set_msg(msg="Ingestion completed!")
         logger.debug("Ingestion completed!")
 
     def _run_ingest_workflow(self, ingest_step: ProgressStep):
         session_id, workflow_id = WorkflowHelper.get_session_and_workflow_id(
             workflow_id_prefix="reg", dataset_id=self._dataset.id
         )
 
@@ -88,30 +98,31 @@
             dataset_tables_info=(
                 self._catalog_tables_helper.get_dataset_tables_info()
             ),
             ccs_api=self._ccs_api,
             ingest_step=ingest_step,
         ).run()
 
-    def _run_process_workflow(self, process_step):
-        # Run process workflow
-        session_id, workflow_id = WorkflowHelper.get_session_and_workflow_id(
-            workflow_id_prefix="process", dataset_id=self._dataset.id
-        )
-
+    def _run_process_workflow(self, process_steps: List[ProgressStep]):
         for index, pipeline_info in enumerate(
             self._catalog_tables_helper.get_pipelines()
         ):
+            (
+                session_id,
+                workflow_id,
+            ) = WorkflowHelper.get_session_and_workflow_id(
+                workflow_id_prefix="process", dataset_id=self._dataset.id
+            )
             pipeline_filters_info = self._pipeline_filters_info_list[index]
             self._run_process_workflow_per_pipeline(
                 pipeline_info=pipeline_info,
                 pipeline_filters_info=pipeline_filters_info,
                 session_id=session_id,
                 workflow_id=workflow_id,
-                process_step=process_step,
+                process_step=process_steps[index],
             )
 
     def _run_process_workflow_per_pipeline(
         self,
         pipeline_info: PipelineTablesInfo,
         pipeline_filters_info: am.AkriSDKWorkflowResponse,
         session_id: str,
@@ -170,27 +181,31 @@
                 )
                 data_ingest_output_dir = filters_output_dir_map[
                     FilterTypes.DataIngest
                 ]
                 # Run Data ingest filter
                 with tempfile.NamedTemporaryFile(dir=tmp_dir) as fp:
                     logger.debug("Data ingestion filter is in progress!")
-
+                    fp_name = fp.name
+                    if is_windows_os():
+                        # windows does not allow already opened temp file.
+                        # fp will be deleted along with tmp_dir
+                        fp.close()
                     ingest_filter = DataIngestWrapper()
                     ingest_filter.init(**data_ingest_init_params)
 
                     # Featurizer output will be input for data ingest filter
                     ingest_src_dir = filters_output_dir_map[
                         FilterTypes.Featurizer
                     ]
 
                     ingest_filter.run(
                         src_dir=ingest_src_dir,
                         dst_dir=data_ingest_output_dir,
-                        tmp_file=fp.name,
+                        tmp_file=fp_name,
                     )
                     logger.debug(
                         "Data ingestion filter completed successfully!"
                     )
 
                 thumbnail_aggregator_output_dir = filters_output_dir_map[
                     FilterTypes.ThumbnailAggregator
@@ -205,28 +220,37 @@
                         session_id=session_id,
                         file_metadata_list=(process_token_info.file_info_list),
                     ),
                     workflow_api=self._workflow_api,
                     dsp_dataset_api=self._dsp_dataset_api,
                     ccs_api=self._ccs_api,
                 ).run_from_input_dir(
-                    coreset_dir=f"{data_ingest_output_dir}/{DataIngestWrapper.DEST_CORESET_SUB_DIR}",  # noqa
-                    projections_dir=f"{data_ingest_output_dir}/{DataIngestWrapper.DEST_PROJECTIONS_SUB_DIR}",  # noqa
-                    sketch_dir=f"{data_ingest_output_dir}/{DataIngestWrapper.DEST_SKETCH_SUB_DIR}",  # noqa
+                    coreset_dir=concat_file_paths(
+                        data_ingest_output_dir,
+                        DataIngestWrapper.DEST_CORESET_SUB_DIR,
+                    ),  # noqa
+                    projections_dir=concat_file_paths(
+                        data_ingest_output_dir,
+                        DataIngestWrapper.DEST_PROJECTIONS_SUB_DIR,
+                    ),  # noqa
+                    sketch_dir=concat_file_paths(
+                        data_ingest_output_dir,
+                        DataIngestWrapper.DEST_SKETCH_SUB_DIR,
+                    ),  # noqa
                     thumbnail_dir=thumbnail_aggregator_output_dir,
                     blobs_dir=metadata_output_dir,
                 )
 
                 process_step.increment_processed_steps(completed=1)
 
             #  Update dsp session
             session_create_request = dsp.PipelineSessionCreateRequest(
                 status="COMPLETE"
             )
-            self._dsp_dataset_api.put_dataset_session(
+            self._dsp_dataset_api.update_dataset_session_state(
                 session_id=session_id,
                 pipeline_id=pipeline_info.get_pipeline_id(),
                 dataset_id=self._dataset.id,
                 pipeline_session_create_request=session_create_request,
             )
 
     @classmethod
@@ -235,24 +259,24 @@
     ) -> Tuple[str, str]:
         token_number = process_token_info.token_number
         partitioner_parent_output_dir = cls._get_output_dir(
             par_dir=tmp_dir, filter_type=FilterTypes.Partitioner
         )
 
         # Prepare partitioner output directory
-        partitioner_output_dir = (
-            f"{partitioner_parent_output_dir}/{token_number}/o1"
+        partitioner_output_dir = concat_file_paths(
+            partitioner_parent_output_dir, str(token_number), "o1"
         )
         ProcessPartitionerFilter.prepare_output_dir(
             files=process_token_info.files, output_dir=partitioner_output_dir
         )
 
         # Prepare metadata dir
-        metadata_output_dir = (
-            f"{partitioner_parent_output_dir}/metadata/{token_number}/o1"
+        metadata_output_dir = concat_file_paths(
+            partitioner_parent_output_dir, "metadata", str(token_number), "o1"
         )
         ProcessPartitionerFilter.prepare_metadata_dir(
             filemeta_list=process_token_info.file_meta_list,
             metadata_dir=metadata_output_dir,
         )
         return partitioner_output_dir, metadata_output_dir
 
@@ -316,17 +340,17 @@
 
     @staticmethod
     def _get_output_dir(
         par_dir: str,
         filter_type: FilterTypes,
         token_number: Optional[int] = None,
     ) -> str:
-        output_dir = f"{par_dir}/{filter_type.value}/outputs"
+        output_dir = concat_file_paths(par_dir, filter_type.value, "outputs")
         if token_number:
-            output_dir = f"{output_dir}/{token_number}/o1"
+            output_dir = concat_file_paths(output_dir, str(token_number), "o1")
         return output_dir
 
     @staticmethod
     def _run_filter(
         filter_details: am.AkriSDKFilterDetails,
         src_dir: str,
         dst_dir: str,
```

## akride/core/enums.py

```diff
@@ -1,11 +1,12 @@
 """
  Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
+
 from enum import Enum
 
 import akridata_akrimanager_v2 as am
 import akridata_dsp as dsp
 
 
 class DatastoreType(Enum):
@@ -40,28 +41,42 @@
 
 class FeaturizerType(Enum):
     """Type of featurizer to be used for ingestion
     FULL_IMAGE: Features generated on the full image
     PATCH: Features generated on a grid of cells over image. Supports patch
     search
     EXTERNAL: Features are generated externally and registered against dataset
+    CLIP: OpenCLIP model trained on LAION dataset that generates features to
+    allow text prompt based search.
     """
 
     FULL_IMAGE = 0
     PATCH = 1
     EXTERNAL = 2
+    CLIP = 3
 
 
-class ClusterAlgoType(dsp.ClusterAlgorithms):
+class ClusterAlgoType(object):
     """Cluster algorithms supported by DataExplorer"""
 
+    HDBSCAN = "hdbscan"
+    KMEANS = "kmeans"
+    GMM = "gmm"
+    KSEGMENT = "ksegment"
+
 
-class EmbedAlgoType(dsp.EmbeddingAlgorithms):
+class EmbedAlgoType(object):
     """Embedding algorithms supported by DataExplorer"""
 
+    UMAP = "umap"
+    PCA = "pca"
+    LLE = "lle"
+    ISOMAP = "isomap"
+    GEOMETRIC_CLASS = "geometric-class"
+
 
 class JobContext(Enum):
     """Specifies the context that samples are requested under"""
 
     CONFUSION_MATRIX_CELL = 0
     SIMILARITY_SEARCH = 1
     CLUSTER_RETRIEVAL = 2
```

## akride/core/_entity_managers/catalog_manager.py

```diff
@@ -205,17 +205,20 @@
         Returns
         -------
         pd.DataFrame
             A dataframe of catalog tags.
         """
         job_id = samples.job_id
         points = ",".join(map(str, samples.get_point_ids()))
-        api_response = self.catalog_source_api.get_request_catalog_tag_source(
-            rid=job_id, points=points
+        api_response = (
+            self.catalog_source_api.fetch_catalog_db_tags(  # noqa E501
+                rid=job_id, points=points
+            )
         )
+
         columns = [item.column_name for item in api_response.column_meta]
         data = api_response.data
         df_dict = {key: [] for key in columns}
 
         # Convert from list of list of lists to a list of lists
         rows = [row for data_rows in data for row in data_rows.tags]
         # Extract the values for every row
@@ -425,15 +428,15 @@
             left_table=left_table,
             right_table=right_table,
         )
         cv_request: am.CreateViewRequest = am.CreateViewRequest(
             view_name=view_name,
             description=description,
             dataset_id=ds_id,
-            table_info=query_tables,
+            table_info=[query_tables],
         )
 
         create_view_response: am.CreateViewResponse = (
             self.views_api.create_view(create_view_request=cv_request)
         )  # type: ignore
         return create_view_response.view_id  # type: ignore
 
@@ -441,40 +444,52 @@
         self,
         left_table_info: am.TableInfo,
         right_table_info: am.TableInfo,
         join_condition: JoinCondition,
         left_table: CatalogTable,
         right_table: CatalogTable,
     ) -> List[am.QMSTableInfo]:
+        left_table_alias = (
+            left_table.alias_name
+            if left_table.alias_name
+            else str(left_table_info.name)
+        )
+        right_table_alias = (
+            right_table.alias_name
+            if right_table.alias_name
+            else str(right_table_info.name)
+        )
+        if left_table_alias == right_table_alias:
+            right_table_alias = right_table_alias + "_2"
         left_qms_tab_info = am.QMSTableInfo(
             abs_table_name=left_table_info.abs_name,
-            alias_name=left_table_info.name,  # type: ignore
+            alias_name=left_table_alias,  # type: ignore
             table_type=left_table.table_type.value,
             schema_name=left_table.schema_name,
             catalog_name=left_table.catalog_name,
             table_name=left_table_info.name,
             pipeline_id=left_table.pipeline_id,
         )
         right_qms_tab_info = am.QMSTableInfo(
             abs_table_name=right_table_info.abs_name,
-            alias_name=right_table_info.name,  # type: ignore
+            alias_name=right_table_alias,  # type: ignore
             table_type=right_table.table_type.value,
             table_name=right_table_info.name,
             schema_name=right_table.schema_name,
             catalog_name=right_table.catalog_name,
             pipeline_id=right_table.pipeline_id,
             join_conditions=[
                 am.JoinCondition(
                     type=am.JoinOperandType.BASIC,
                     left=am.JoinOperand(
-                        table=left_qms_tab_info.alias_name,
+                        table=left_table_alias,
                         column=join_condition.left_column,
                     ),
                     right=am.JoinOperand(
-                        table=right_table_info.name,  # type: ignore
+                        table=right_table_alias,  # type: ignore
                         column=join_condition.right_column,
                     ),
                 )
             ],
         )
         return [left_qms_tab_info, right_qms_tab_info]
```

## akride/core/_entity_managers/dataset_manager.py

```diff
@@ -75,15 +75,15 @@
         Returns
         -------
         bool
             Indicates whether this entity was successfully deleted
         """
         dataset_id = entity.get_id()
         api_response = self.dataset_api.delete_dataset(dataset_id)
-        return api_response.success == "True"
+        return api_response.success == "True"  # type: ignore
 
     def _create_dataset(
         self,
         dataset_name: str,
         dataset_namespace: str = "default",
         data_type: DataType = DataType.IMAGE,
         glob_pattern: str = "*(png|jpg|gif|jpeg|tiff|tif|bmp)",
@@ -133,57 +133,71 @@
             namespace=dataset_namespace,
             type=dataset_type,
             data_type=data_type.value,
             containers=containers,
             dataset_spec=dataset_spec,
         )
         api_response = self.dataset_api.create_new_dataset(am_dataset)
+        assert api_response.dataset_id is not None
         return self.get_entity_by_id(entity_id=api_response.dataset_id)
 
     def _attach_pipeline(
         self,
         featurizer_type: FeaturizerType,
         dataset_id: str,
         data_type: DataType,
+        with_clip_featurizer: bool,
     ) -> None:
         """
         Attaches a pipeline based on featurizer_type
         Parameters
         ----------
         featurizer_type: Type of featurizer to be used for attachment
         dataset_id : str
             dataset_id to attach.
         data_type : DataType
             data type of dataset
+        with_clip_featurizer: bool
+            attach clip pipeline
 
         Returns
         -------
         None
         """
-        pipeline_internal_id = PipelineHelper.get_pipeline_internal_id(
-            featurizer_type=featurizer_type, data_type=data_type
-        )
-
-        pipeline: am.PipelineItem = self._get_internal_pipeline_details(
-            pipeline_internal_id=pipeline_internal_id
-        )
+        featurizers_to_attach = [featurizer_type]
+        if with_clip_featurizer:
+            featurizers_to_attach.append(FeaturizerType.CLIP)
+
+        pipelines_to_attach = []
+        for f in featurizers_to_attach:
+            pipeline_internal_id = PipelineHelper.get_pipeline_internal_id(
+                featurizer_type=f, data_type=data_type
+            )
 
-        if not pipeline:
-            raise ServerError(
-                f"Pipeline details not found for featurizer "
-                f"{featurizer_type.value} and data type {data_type}!"
+            pipeline: Optional[
+                am.PipelineItem
+            ] = self._get_internal_pipeline_details(
+                pipeline_internal_id=pipeline_internal_id
             )
 
-        # Create attachment
-        self.pipeline_api.attach_pipeline_to_datasets(
-            pipeline_id=pipeline.pipeline_id,
-            pipeline_attach_body=am.PipelineAttachBody(
-                dataset_ids=[dataset_id]
-            ),
-        )
+            if not pipeline:
+                raise ServerError(
+                    f"Pipeline details not found for featurizer "
+                    f"{f.value} and data type {data_type}!"
+                )
+            pipelines_to_attach.append(pipeline.pipeline_id)
+
+        # Create attachments
+        for pipeline_id in pipelines_to_attach:
+            self.pipeline_api.attach_pipeline_to_datasets(
+                pipeline_id=pipeline_id,
+                pipeline_attach_body=am.PipelineAttachBody(
+                    dataset_ids=[dataset_id]
+                ),
+            )
 
     def _get_internal_pipeline_details(
         self, pipeline_internal_id: int
     ) -> Optional[am.PipelineItem]:
         # Get pipeline filter by pipeline_internal_id
         pipelines_resp: am.Pipelines = self.pipeline_api.get_all_pipelines(
             filter_by_internal_id=pipeline_internal_id
@@ -197,14 +211,15 @@
 
     @translate_api_exceptions
     def ingest_dataset(
         self,
         dataset: Dataset,
         data_directory: str,
         async_req: bool,
+        with_clip_featurizer: bool,
         featurizer_type: FeaturizerType = FeaturizerType.FULL_IMAGE,
         catalog_details: Optional[CatalogDetails] = None,
     ) -> Optional[BackgroundTask]:
         """
         Starts an asynchronous ingest task for the specified dataset.
         Attaches a pipeline based on featurizer_type and executes the pipeline
 
@@ -212,14 +227,16 @@
         ----------
         dataset : Dataset
             The dataset to ingest.
         data_directory : str
             The path to the directory containing the dataset files.
         async_req: bool
             Whether to execute the request asynchronously.
+        with_clip_featurizer: bool, optional
+            Ingest dataset to enable text prompt based search.
         featurizer_type: Type of featurizer to be used
         catalog_details: Optional[CatalogDetails]
             parameters details for creating a catalog
 
         Returns
         -------
         BackgroundTask
@@ -239,14 +256,15 @@
                 table_name=catalog_details.table_name,
             )
 
         self._attach_pipeline(
             data_type=DataType(dataset.info.data_type),
             dataset_id=dataset.get_id(),
             featurizer_type=featurizer_type,
+            with_clip_featurizer=with_clip_featurizer,
         )
 
         dataset_json = self._get_dataset_json(
             dataset_id=dataset.get_id(), version=dataset.info.latest_version
         )
 
         catalog_tables_resp: am.CatalogTableResponse = (
@@ -286,17 +304,15 @@
         if async_req:
             return task
 
         previous_completed = 0
         with ProgressBarHelper(total=100) as pbar:
             while not task.has_completed():
                 sleep(5)
-                percent_completed = (
-                    task.get_progress_info().percent_completed
-                )
+                percent_completed = task.get_progress_info().percent_completed
                 incremental_change = percent_completed - previous_completed
                 pbar.update(incremental_change)
                 previous_completed = percent_completed
         task.wait_for_completion()
 
     @staticmethod
     def _run_workflow(
@@ -389,15 +405,15 @@
             Dataset json details.
         """
         return self.dataset_api.get_dataset_json(
             dataset_id=dataset_id, version=version
         )  # type: ignore
 
     def get_attached_pipelines(
-        self, dataset: Dataset, version: str = None
+        self, dataset: Dataset, version: Optional[str] = None
     ) -> List[Pipeline]:
         """Get pipelines applicable for  dataset given a dataset version
 
         Args:
             dataset (Dataset): Dataset object
             version (str, optional): Dataset version. Defaults to None in which
             case the latest version would be used
```

## akride/core/_entity_managers/job_manager.py

```diff
@@ -1,24 +1,24 @@
+# flake8: noqa  E501
+import io
 import json
 import time
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import akridata_akrimanager_v2 as am
 import akridata_dsp as dsp
 import numpy as np
 from akridata_akrimanager_v2 import Condition
 from akridata_dsp import (
     CatalogRefineResponse,
     CatalogSourceFilter,
     CatalogTagSourceResponse,
-    ClusterAlgorithms,
     ColumnData,
     CreateCatalogSourceRefine,
     CreateSimSearchResponseV2,
-    EmbeddingAlgorithms,
     GetDatasetRequestResponse,
     RequestCatalogTagFilterSourceResp,
     RequestPlot,
 )
 from PIL import Image
 
 from akride._utils.exception_utils import translate_api_exceptions
@@ -41,49 +41,55 @@
     SampleInfoList,
     SimilaritySearchSpec,
 )
 
 from akride.core.enums import (  # isort:skip
     JobContext,
     JobStatisticsContext,
+    ClusterAlgoType,
+    EmbedAlgoType,
 )
 
 
 class JobManager(Manager):
     """Class managing job related operations on DataExplorer"""
 
     def __init__(self, cli_manager: ClientManager):
         super().__init__(cli_manager)
-        self.job_request_api: dsp.RequestApi = dsp.RequestApi(
+        self.job_request_api: dsp.JobRequestsApi = dsp.JobRequestsApi(
             cli_manager.dsp_client
         )
-        self.job_version_api: dsp.JobVersionApi = dsp.JobVersionApi(
+        self.request_api: dsp.RequestApi = dsp.RequestApi(
             cli_manager.dsp_client
         )
         self.qms_api: am.QueriesApi = am.QueriesApi(cli_manager.am_client)
         self.catalog_api: am.CatalogApi = am.CatalogApi(cli_manager.am_client)
         self.image_fetch_api: dsp.ImageFetchApi = dsp.ImageFetchApi(
             cli_manager.dsp_client
         )
         self.request_stat_api: dsp.RequestStatisticsApi = (
             dsp.RequestStatisticsApi(cli_manager.dsp_client)
         )
         self.views_api: am.ViewsApi = am.ViewsApi(
             api_client=cli_manager.am_client
         )
         self.job_creator = JobCreator(
+            request_api=self.request_api,
             job_req_api=self.job_request_api,
             qms_api=self.qms_api,
             catalog_api=self.catalog_api,
             views_api=self.views_api,
         )
         self.search_api: dsp.SimSearchRequestV2Api = dsp.SimSearchRequestV2Api(
             cli_manager.dsp_client
         )
         self.scenario_api = dsp.ScenarioApi(cli_manager.dsp_client)
+        self.scenario_exec_api = dsp.ScenarioExecutionApi(
+            cli_manager.dsp_client
+        )
         self.resultset_api = dsp.ResultsetApi(cli_manager.dsp_client)
         self.catalog_source_tag_api = dsp.CatalogSourceTagApi(
             cli_manager.dsp_client
         )
 
     @translate_api_exceptions
     def create_entity(self, spec: JobSpec) -> Optional[Entity]:
@@ -113,32 +119,29 @@
             The entity object to delete.
 
         Returns
         -------
         bool
             Indicates whether this entity was successfully deleted
         """
-        job_id = entity.get_id()
-        version_id = entity.info.version
 
-        self.job_version_api.delete_job_version_one(job_id, version_id)
         # TODO return True only if response is 200
         return True
 
     def _create_job(
         self,
         dataset: Dataset,
         job_type: str,
         job_name: str,
-        cluster_algo: ClusterAlgorithms,
-        embed_algo: EmbeddingAlgorithms,
+        cluster_algo: str,
+        embed_algo: str,
         catalog_table: CatalogTable,
         pipeline: Pipeline,
         num_clusters: Optional[int] = None,
-        analyze_params: AnalyzeJobParams = None,
+        analyze_params: Optional[AnalyzeJobParams] = None,
         max_images: int = 1000,
         predictions_file: str = "",
         filters: Optional[List[Condition]] = None,
     ) -> Optional[Job]:
         request, response = self.job_creator.create_job(
             job_type=job_type,
             job_name=job_name,
@@ -178,16 +181,16 @@
         -------
         List[Entity]
             A list of Entity objects representing jobs.
         """
         attributes = attributes.copy()
         if "data_type" in attributes:
             # the API expects this to be a list
-            attributes["data_type"] = [attributes["data_type"]]
-        api_response = self.job_request_api.get_request_root(**attributes)
+            attributes["data_type"] = attributes["data_type"].lower()
+        api_response = self.job_request_api.list_requests(**attributes)
         job_list = []
         for info in api_response.requests:
             job = Job(info)
             job.id = info.reqid
             job.name = info.reqname
             job_list.append(job)
         return job_list
@@ -206,23 +209,25 @@
         Returns
         -------
         List[Image.Image]
             A list of thumbnail images.
         """
         job_id = samples.job_id
         thumbnails = dsp.Thumbnails(samples.get_point_ids())
-        api_response = self.job_request_api.post_get_thumbnails(
+        api_response = self.request_api.get_thumbnails(
             rid=job_id, thumbnails=thumbnails
         )
         result = []
         # TODO use the async API instead
         for image_path in api_response.data:  # type: ignore
             image_path = image_path.replace("/ds/images/", "")
-            image_response = self.image_fetch_api.get_image_fetch(image_path)
-            result.append(Image.open(image_response))  # type: ignore
+            image_response = self.image_fetch_api.fetch_image(
+                image_path, _preload_content=False
+            )
+            result.append(Image.open(io.BytesIO(image_response.data)))  # type: ignore
         return result
 
     @translate_api_exceptions
     def get_fullres_images(self, samples: SampleInfoList) -> List[Image.Image]:
         """
         Retrieves the full-resolution images for the provided job.
 
@@ -236,26 +241,24 @@
         Returns
         -------
         List[Image.Image]
             A list of images.
         """
         job_id = samples.job_id
         images = dsp.Thumbnails(samples.get_point_ids())
-        api_response = self.job_request_api.post_get_images(
-            rid=job_id, images=images
-        )
+        api_response = self.request_api.get_images(rid=job_id, images=images)
         result = []
         # TODO use the async API instead
         assert api_response is not None
         for image_path in api_response.data:  # type: ignore
             image_path = image_path.replace("/ds/images/", "")
-            image_response = self.image_fetch_api.get_image_fetch(
-                image_path,
+            image_response = self.image_fetch_api.fetch_image(
+                image_path, _preload_content=False
             )
-            result.append(Image.open(image_response))  # type: ignore
+            result.append(Image.open(io.BytesIO(image_response.data)))  # type: ignore
         return result
 
     @translate_api_exceptions
     def get_fullres_image_urls(self, samples: SampleInfoList) -> Dict:
         """
         Retrieve the full resolution image paths for a given list of samples.
 
@@ -272,24 +275,25 @@
                 tag source response is empty.
         """
         points = [str(point) for point in samples.get_point_ids()]
         if len(points) == 0:
             raise UserError("Invalid input: No samples in the request")
         if not samples.job_id:
             raise UserError("Request ID cannot be empty")
-        catalog_tag_resp: CatalogTagSourceResponse = (
-            self.catalog_source_tag_api.get_request_catalog_tag_source(
-                rid=samples.job_id,
-                points=",".join(points),
-            )
+        catalog_tag_resp: (
+            CatalogTagSourceResponse
+        ) = self.catalog_source_tag_api.fetch_catalog_db_tags(
+            rid=samples.job_id,
+            points=",".join(points),
         )  # type: ignore
         if (catalog_tag_resp.data is None) or len(catalog_tag_resp.data) == 0:
             raise ValueError("Empty response from server")
         point_tags = {}
         col_of_interest = "file_path"
+        assert catalog_tag_resp.column_meta is not None
         for col_type in catalog_tag_resp.column_meta:
             if "file_path" in col_type.column_name:
                 col_of_interest = col_type.column_name
                 break
 
         for point_info in catalog_tag_resp.data:
             point_id = point_info.point_id
@@ -318,15 +322,15 @@
 
         Returns
         -------
         Image
             The requested image.
         """
         image_path = image_path.replace("/ds/images/", "")
-        api_response = self.image_fetch_api.get_image_fetch(image_path)
+        api_response = self.image_fetch_api.fetch_image(image_path)
         img = Image.open(api_response)  # type: ignore
         return img
 
     @translate_api_exceptions
     def get_job_statistics(
         self, job: Job, context: JobStatisticsContext, **kwargs
     ) -> JobStatistics:
@@ -374,15 +378,15 @@
             default_iou_thresh,
         ) = self._get_default_score_iou_thresholds(job_id=job_id)
         iou_threshold = kwargs.get("iou_config_threshold", default_iou_thresh)
         score_threshold = kwargs.get(
             "confidence_score_threshold", default_score_thresh
         )
 
-        api_response = self.request_stat_api.get_get_confusion_matrix(
+        api_response = self.request_stat_api.get_confusion_matrix(
             rid=job_id,
             score_threshold=score_threshold,
             iou_threshold=iou_threshold,
         )
         assert api_response is not None
         cells = api_response.data  # type: ignore
         class_names = np.unique([item.ground_truth_class for item in cells])
@@ -425,29 +429,32 @@
         ) = self._get_default_score_iou_thresholds(job_id=job_id)
 
         iou_threshold = kwargs.get("iou_config_threshold", default_iou_thresh)
         score_threshold = kwargs.get(
             "confidence_score_threshold", default_score_thresh
         )
 
-        api_response = self.request_stat_api.get_get_confusion_matrix_points(
+        api_response = self.request_stat_api.get_confusion_matrix_points(
             rid=job_id,
             score_threshold=score_threshold,
             iou_threshold=iou_threshold,
             prediction_class=predicted_label,
             ground_truth_class=true_label,
         )
         point_ids = api_response.points[0].points_ids  # type: ignore
         assert job_id is not None
         return SampleInfoList(job_id=job_id, point_ids=point_ids)
 
     def _get_default_score_iou_thresholds(self, job_id) -> Tuple[float, float]:
-        resp: dsp.AnalyzeMetaGetResponse = (
-            self.job_request_api.get_request_analyze(rid=job_id)
-        )
+        resp: (
+            dsp.AnalyzeMetaGetResponse
+        ) = self.request_api.get_analyze_metadata(
+            rid=job_id
+        )  # type: ignore
+        assert resp.confidence_config is not None
         iou_threshold = resp.iou_config.levels[0]  # type: ignore
         score_threshold = resp.confidence_config.levels[  # type: ignore
             len(resp.confidence_config.levels) // 2
         ]
         return score_threshold, iou_threshold
 
     def get_similar_samples(
@@ -524,19 +531,19 @@
                     "cluster_map": cluster_map,
                 },
             },
             tags=["sim_search_request_v2"],
             points_src=points_src,
         )
 
-        search_response: CreateSimSearchResponseV2 = (
-            self.search_api.post_request_sim_search_v2(
-                rid=job_id,
-                create_sim_search_v2=create_sim_search_v2,
-            )
+        search_response: (
+            CreateSimSearchResponseV2
+        ) = self.search_api.create_sim_search(
+            rid=job_id,
+            create_sim_search_v2=create_sim_search_v2,
         )  # type: ignore
         assert search_response
         sim_search_id = search_response.sim_search_rid
         assert sim_search_id
         # create a new scenario and execute
         scenario_name = "scenario" + sim_search_id
         search_configuration = {"distance_metric": "euclidean"}
@@ -550,36 +557,34 @@
                 "request": {
                     "request_id": job_id,
                     "sim_search_id": sim_search_id,
                 }
             },
         )
 
-        scenario_response = self.scenario_api.post_scenario_root(
+        scenario_response = self.scenario_api.create_scenario(
             create_scenario_request=create_scenario_request,
         )
         scenario_id = scenario_response.scenario_id  # type: ignore
         create_scenario_execution = dsp.CreateScenarioExecution(
             search_configuration=search_configuration,
             search_target={"ds_request_id": job_id},
             scenario_id=scenario_id,
         )
 
-        execution_response = (
-            self.scenario_api.post_scenario_execution_global_search(
-                create_scenario_execution=create_scenario_execution,
-            )
+        execution_response = self.scenario_exec_api.create_scenario_execution(
+            create_scenario_execution=create_scenario_execution,
         )
         execution_id = execution_response.execution_id  # type: ignore
 
         # poll for execution progress
         ready = False
         while True:
-            progress_response = self.scenario_api.get_scenario_execution_one(
-                scenario_id, execution_id
+            progress_response = self.scenario_exec_api.get_scenario_execution(
+                execution_id
             )
             ready = progress_response.status == "READY"  # type: ignore
             if ready or timeout <= 0:
                 break
             time.sleep(1)
             timeout -= 1
 
@@ -587,28 +592,27 @@
         if ready:
             resultset_id = progress_response.resultset_id  # type: ignore
             attributes = {
                 "request_id": job_id,
                 "page_number": 1,
                 "page_size": max_count,
             }
-            api_response = self.resultset_api.get_resultset_one(
+            api_response = self.resultset_api.get_resultset(
                 resultset_id, **attributes
             )
-            for sample in api_response.resultset.data[  # type: ignore
-                0
-            ].frames:
+            for sample in api_response.resultset.data[0].frames:  # type: ignore
                 samples.append_sample(sample)
 
         # delete the newly created scenario
-        self.scenario_api.delete_scenario_one(
+        self.scenario_api.delete_scenario(
             scenario_id,
         )
         if not ready:
             raise ServerError("Similarity search timed out")
+        assert job_id is not None
         samples.job_id = job_id
         return samples
 
     @translate_api_exceptions
     def get_samples_from_file_path(
         self, job: Job, file_list: List[str]
     ) -> Dict:
@@ -621,17 +625,18 @@
         Returns:
             Dict: dictionary of file_path to point_ids
         """
         job_id = job.get_id()
         assert job_id is not None
 
         ds_resp: GetDatasetRequestResponse = (
-            self.job_request_api.get_request_one(job_id)
+            self.request_api.get_request_details(job_id)
         )  # type: ignore
         file_path_col = "file_path"
+        assert ds_resp.request_source_meta is not None
         for col_type in ds_resp.request_source_meta.column_type_map:
             if "file_path" in col_type["name"]:
                 file_path_col = col_type["name"]
                 break
 
         catalog_filter = CatalogSourceFilter(
             column_name=file_path_col,
@@ -640,24 +645,27 @@
         )
 
         request = CreateCatalogSourceRefine(
             case_sensitive=True,
             filter_description="",
             filters=[catalog_filter],
         )
-        catalog_filter_resp: RequestCatalogTagFilterSourceResp = (
-            self.catalog_source_tag_api.post_request_catalog_tag_filter_source(
-                rid=job_id, create_catalog_source_refine=request
-            )
+        catalog_filter_resp: (
+            RequestCatalogTagFilterSourceResp
+        ) = self.catalog_source_tag_api.create_catalog_source_filter(
+            rid=job_id, create_catalog_source_refine=request
         )  # type: ignore
 
         filter_id = catalog_filter_resp.catalog_filter_id
-        resp: CatalogRefineResponse = self.catalog_source_tag_api.post_request_source_catalog_tag_filter_points(  # noqa: E501
+        resp: (
+            CatalogRefineResponse
+        ) = self.catalog_source_tag_api.fetch_catalog_source_filter_info(  # noqa: E501
             rid=job_id, catalog_filter_id=filter_id, request_plot=RequestPlot()
         )  # type: ignore
+        assert resp.points is not None
         res = dict(map(lambda i, j: (i, j), file_list, resp.points))
         return res
 
     @translate_api_exceptions
     def get_samples(
         self,
         job: Job,
@@ -731,15 +739,15 @@
         Returns
         -------
         IFrame
             The job panel.
         """
         job_id = job.get_id()
         assert job_id is not None
-        resp: GetDatasetRequestResponse = self.job_request_api.get_request_one(
+        resp: GetDatasetRequestResponse = self.request_api.get_request_details(
             job_id
         )  # type: ignore
 
         uri = f"/#/datavis?view=main&job_type={resp.job_type}&job_id={job_id}"
         return uri
 
     def get_cluster_samples(
@@ -771,19 +779,21 @@
                 "nclusters": 0,
                 "sample-frac": 1,
                 "sample-mode": "outlier",
                 "sample-weight": 2,
             }
         )
         job_id = job.get_id()
+        assert job_id is not None
         # TODO: request for max_count samples
-        api_response = self.job_request_api.post_request_value(
+        api_response = self.request_api.visualize_and_plot_request(
             rid=job_id, request_plot=request_plot
         )
-        data = json.loads(api_response[1:])["data"]
+        assert api_response is not None
+        data = json.loads(api_response[1:])["data"]  # type: ignore
         point_ids = []
         for item in data:
             if item["cluster"] == cluster_id:
                 point_ids.append(item["name"])
                 if len(point_ids) >= max_count:
                     break
         return SampleInfoList(job_id=job_id, point_ids=point_ids)
@@ -810,14 +820,16 @@
                 "nclusters": 0,
                 "sample-frac": sample_frac,
                 "sample-mode": "coreset",
                 "sample-weight": 2,
             }
         )
         job_id = job.get_id()
-        api_response = self.job_request_api.post_request_value(
+        assert job_id is not None
+        api_response = self.request_api.visualize_and_plot_request(
             rid=job_id, request_plot=request_plot
         )
         # exclude the prefix character
-        data = json.loads(api_response[1:])["data"]
+        assert api_response is not None
+        data = json.loads(api_response[1:])["data"]  # type: ignore
         point_ids = [item["name"] for item in data]
         return SampleInfoList(job_id=job_id, point_ids=point_ids)
```

## akride/core/_entity_managers/resultset_manager.py

```diff
@@ -1,7 +1,9 @@
+# flake8: noqa  E501
+import io
 from typing import Any, Dict, List, Optional
 
 import akridata_dsp as dsp
 from PIL import Image
 
 from akride import logger
 from akride._utils.background_task_helper import BackgroundTask
@@ -52,38 +54,40 @@
         return self._create_resultset(**spec)
 
     def _create_resultset(
         self, job: Job, name: str, samples: SampleInfoList
     ) -> Optional[Resultset]:
         logger.debug("Creating resultset: %s", name)
         job_id = job.id
-        dataset_id = job.info.dataset_id
+        if "dataset_id" in job.info.to_dict():
+            dataset_id = job.info.dataset_id  # type: ignore
+        else:
+            dataset_id = None
         request_id = job_id
         frames = [{"point_id": i} for i in samples.get_point_ids()]
         resultset_request = dsp.ResultsetCreateRequest(
             dataset_id=dataset_id,
             frames=frames,
             name=name,
             tags=None,
         )
 
-        api_response = self.resultset_api.post_resultset_root(
-            de_job_version_id=None,
-            de_job_id=job_id,
+        api_response = self.resultset_api.create_resultset(
             request_id=request_id,
             resultset_create_request=resultset_request,
         )
-        return Resultset(api_response)
+        assert api_response is not None
+        return Resultset(api_response)  # type: ignore
 
     @translate_api_exceptions
     def update_resultset(
         self,
         resultset: Resultset,
-        add_list: SampleInfoList = None,
-        del_list: SampleInfoList = None,
+        add_list: Optional[SampleInfoList] = None,
+        del_list: Optional[SampleInfoList] = None,
     ) -> bool:
         """
         Updates a resultset.
 
         Parameters
         ----------
         resultset: Resultset
@@ -103,28 +107,26 @@
                 "Either add_list or del_list must be specified."
             )
 
         job_id = resultset.job_id
         version = resultset.version
 
         request_id = job_id
-        get_frames = (
-            lambda samples: []
+        get_frames = lambda samples: (
+            []
             if samples is None
             else [{"point_id": i} for i in samples.get_point_ids()]
         )
         frames = {"add": get_frames(add_list), "del": get_frames(del_list)}
         update_request = dsp.ResultsetUpdateRequest(
             frames=frames, version=version
         )
 
-        self.resultset_api.patch_resultset_one(
-            resultset.id,
-            de_job_version_id=None,
-            de_job_id=job_id,
+        self.resultset_api.update_resultset(
+            resultset_id=resultset.id,
             request_id=request_id,
             resultset_update_request=update_request,
         )
         return True
 
     @translate_api_exceptions
     def delete_entity(self, entity: Entity) -> bool:
@@ -139,16 +141,18 @@
         Returns
         -------
         bool
             Indicates whether this entity was successfully deleted
         """
         logger.debug("Deleting entity %s", entity)
         resultset_id = entity.get_id()
-        api_response = self.resultset_api.delete_resultset_one(resultset_id)
-        return api_response.code == "RESULTSET_DELETED"
+        api_response: Dict[str, str] = self.resultset_api.delete_resultset(
+            resultset_id
+        )  # type: ignore
+        return api_response.get("code") == "RESULTSET_DELETED"
 
     def upload_resultset(
         self,
         name: str,
         store_type: DatastoreType,
     ) -> BackgroundTask:
         # TODO: Currently we do not register a datastore at DE-level, not sure
@@ -167,16 +171,17 @@
 
         Returns
         -------
         BackgroundTask
             a task object.
         """
         logger.debug("Uploading resultset to %s, %s", name, store_type)
-        task = BackgroundTask()
-        return task
+        raise NotImplementedError
+        # task = BackgroundTask()
+        # return task
 
     @translate_api_exceptions
     def get_entities(self, attributes: Dict[str, Any]) -> List[Resultset]:
         """
         Retrieves information about entities that have the given attributes.
 
         Parameters
@@ -201,15 +206,15 @@
         if "page_number" not in attributes:
             attributes["page_number"] = 1
         if "page_size" not in attributes:
             # TODO: denote a number to mean "unlimited"
             attributes["page_size"] = 1000
 
         api_response: ResultsetListResponse = (
-            self.resultset_api.get_resultset_root(**attributes)
+            self.resultset_api.list_resultsets(**attributes)
         )  # type: ignore
         assert api_response.resultsets is not None
         resultset_list = [Resultset(info) for info in api_response.resultsets]
         return resultset_list
 
     @translate_api_exceptions
     def get_samples(self, resultset: Resultset) -> SampleInfoList:
@@ -224,18 +229,16 @@
         Returns
         -------
         SampleInfoList
             A SampleInfoList object.
         """
         # TODO: denote a number to mean "unlimited" page size
         attributes = {"page_number": 1, "page_size": 10000}
-        api_response: ResultsetGetResponse = (
-            self.resultset_api.get_resultset_one(
-                resultset.get_id(), **attributes
-            )
+        api_response: ResultsetGetResponse = self.resultset_api.get_resultset(
+            resultset.get_id(), **attributes
         )  # type: ignore
         samples = SampleInfoList()
         assert api_response.resultset is not None
         for sample in api_response.resultset.data[0].frames:
             samples.append_sample(sample)
         return samples
 
@@ -257,10 +260,12 @@
             A list of thumbnail images.
         """
         urls = resultset_samples.get_thumbnail_urls()
         result = []
         # TODO use the async API instead
         for image_path in urls:
             image_path = image_path.replace("/ds/images/", "")
-            image_response = self.image_api.get_image_fetch(image_path)
-            result.append(Image.open(image_response))  # type: ignore
+            image_response = self.image_api.fetch_image(
+                image_path, _preload_content=False
+            )
+            result.append(Image.open(io.BytesIO(image_response.data)))  # type: ignore
         return result
```

## akride/core/_filters/__init__.py

```diff
@@ -0,0 +1,9 @@
+00000000: 2222 220a 2043 6f70 7972 6967 6874 2028  """. Copyright (
+00000010: 4329 2032 3032 342c 2041 6b72 6964 6174  C) 2024, Akridat
+00000020: 612c 2049 6e63 202d 2041 6c6c 2052 6967  a, Inc - All Rig
+00000030: 6874 7320 5265 7365 7276 6564 2e0a 2055  hts Reserved.. U
+00000040: 6e61 7574 686f 7269 7a65 6420 636f 7079  nauthorized copy
+00000050: 696e 6720 6f66 2074 6869 7320 6669 6c65  ing of this file
+00000060: 2c20 7669 6120 616e 7920 6d65 6469 756d  , via any medium
+00000070: 2069 7320 7374 7269 6374 6c79 2070 726f   is strictly pro
+00000080: 6869 6269 7465 640a 2222 220a            hibited.""".
```

## akride/core/_filters/partitioners/ingest_partitioner_filter.py

```diff
@@ -1,13 +1,13 @@
 import math
 from pathlib import Path
 from typing import List, Tuple
 
 import akridata_akrimanager_v2 as am
-from pyakri_de_filters.utils.file_utils import (
+from pyakri_de_utils.file_utils import (
     get_files_in_batches,
     get_input_files_batch,
 )
 
 from akride import logger
 from akride._utils.catalog.dataset_tables_info import DatasetTablesInfo
 from akride._utils.catalog.enums import TableNames
```

## akride/core/_filters/partitioners/process_partitioner_filter.py

```diff
@@ -1,16 +1,16 @@
 import math
 import os
 from dataclasses import dataclass
 from typing import Any, List, Tuple
 
 import akridata_akrimanager_v2 as am
 import pandas as pd
-from pyakri_de_filters.utils.arrow_utils import write_arrow_from_df
-from pyakri_de_filters.utils.file_utils import create_directory
+from pyakri_de_utils.arrow_utils import write_arrow_from_df
+from pyakri_de_utils.file_utils import create_directory
 
 from akride import logger
 from akride._utils.catalog.pipeline_tables_info import PipelineTablesInfo
 from akride._utils.file_utils import copy_files_to_dir, get_file_name_from_path
 from akride._utils.progress_manager.manager import ProgressStep
 from akride.core._filters.partitioners.models import ProcessFileInfo
 from akride.core._filters.partitioners.partitioner_filter import (
```

## akride/core/_filters/sink/sink_writer_filter.py

```diff
@@ -1,13 +1,14 @@
+# flake8: noqa  E501
 from datetime import datetime
-from typing import Dict, List, Tuple
+from typing import Dict, List, Tuple, Union
 
 import akridata_akrimanager_v2 as am
 import akridata_dsp as dsp
-from pyakri_de_filters.utils.file_utils import get_input_files_dir
+from pyakri_de_utils.file_utils import get_input_files_dir
 
 from akride._utils.store_utils import upload_file_to_data_store
 from akride.core._filters.sink.models import SinkWriterFilterInput
 from akride.core.constants import Constants
 from akride.core.exceptions import ServerError
 
 
@@ -17,15 +18,15 @@
         filter_input: SinkWriterFilterInput,
         workflow_api: am.WorkflowsApi,
         dsp_dataset_api: dsp.DatasetApi,
         ccs_api: am.CcsApi,
     ):
         self._filter_input = filter_input
         self._workflow_api = workflow_api
-        self._dsp_dataset_api = dsp_dataset_api
+        self._dsp_dataset_api: dsp.DatasetApi = dsp_dataset_api
         self._ccs_api = ccs_api
 
     def run_from_input_dir(
         self,
         blobs_dir: str,
         coreset_dir: str,
         sketch_dir: str,
@@ -57,23 +58,23 @@
         self,
         blobs_arrow_path: str,
         coreset_arrow_path: str,
         sketch_arrow_path: str,
         projections_arrow_path: str,
         thumbnail_arrow_path: str,
     ):
-        files_map = {
+        files_map: Dict[str, str] = {
             am.DatastoreFileType.BLOBS: blobs_arrow_path,
             am.DatastoreFileType.CORESET: coreset_arrow_path,
             am.DatastoreFileType.SKETCH: sketch_arrow_path,
             am.DatastoreFileType.PROJECTIONS: projections_arrow_path,
             am.DatastoreFileType.THUMBNAIL: thumbnail_arrow_path,
         }
 
-        self._run_from_files_map(files_map=files_map)
+        self._run_from_files_map(files_map=files_map)  # type: ignore
 
     def _run_from_files_map(self, files_map: Dict[am.DatastoreFileType, str]):
         (
             partition_start,
             partition_end,
             partition_id,
         ) = self._get_partition_details()
@@ -102,24 +103,26 @@
         list_pre_signed_response: am.ListPreSignedUrlResponseWithOperation = (
             self._workflow_api.generate_presigned_url_for_workflow_files(
                 dataset_id=self._filter_input.dataset_id,
                 file_types=Constants.FILE_TYPES,
                 partition_id=partition_id,
                 session_id=self._filter_input.session_id,
             )
-        )
+        )  # type: ignore
 
         store_path_map = {}
         # Store file metadata to sink store using pre-signed url
+        assert list_pre_signed_response.presignedurls is not None
         for pre_signed_url_resp in list_pre_signed_response.presignedurls:
             pre_signed_url_resp: am.GetPreSignedUrlResponseWithOperation = (
                 pre_signed_url_resp
             )
 
             fields = pre_signed_url_resp.fields
+            assert fields is not None
             store_path_with_dataset_id = fields.get("key")
             if not store_path_with_dataset_id:
                 raise ServerError(
                     "Key is not present in the presigned url resp"
                 )
 
             store_path_without_dataset_id = "/".join(
@@ -153,55 +156,55 @@
             sketch=store_path_map[am.DatastoreFileType.SKETCH],
             thumbnail_blobs=[store_path_map[am.DatastoreFileType.THUMBNAIL]],
             coreset=store_path_map[am.DatastoreFileType.CORESET],
             start_frame_indices=[0],
             img_end_indices=[len(self._filter_input.file_metadata_list) - 1],
         )
 
-        self._dsp_dataset_api.put_dataset_partition_update(
+        self._dsp_dataset_api.update_dataset_partition(
             partition_id=partition_id,
             session_id=self._filter_input.session_id,
             pipeline_id=(
                 self._filter_input.pipeline_tables_info.get_pipeline_id()
             ),
             dataset_id=self._filter_input.dataset_id,
             pipeline_partition_create_request=partition_create_request,
         )
 
     def _insert_entries_to_pipeline_tables(
         self,
-        store_path_map: Dict[am.DatastoreFileType, str],
+        store_path_map: Dict[Union[am.DatastoreFileType, str], str],
         partition_start: int,
         partition_end: int,
     ):
-        blobs_table_insert_values: List[List[str]] = [
+        blobs_table_insert_values: List[List[Union[int, str]]] = [
             [
                 partition_start,
                 partition_end,
                 self._filter_input.workflow_id,
                 self._filter_input.session_id,
                 store_path_map[am.DatastoreFileType.BLOBS],
             ]
         ]
 
-        summary_table_insert_values: List[List[str]] = [
+        summary_table_insert_values: List[List[Union[str, int]]] = [
             [
                 partition_start,
                 partition_end,
                 self._filter_input.workflow_id,
                 self._filter_input.session_id,
                 store_path_map[am.DatastoreFileType.CORESET],
                 store_path_map[am.DatastoreFileType.PROJECTIONS],
                 store_path_map[am.DatastoreFileType.SKETCH],
                 store_path_map[am.DatastoreFileType.THUMBNAIL],
             ]
         ]
 
         # populate insert values for primary table
-        primary_table_insert_values: List[List[str]] = []
+        primary_table_insert_values: List[List[Union[int, str, datetime]]] = []
         for file_info in self._filter_input.file_metadata_list:
             primary_table_insert_values.append(
                 [
                     partition_start,
                     partition_end,
                     self._filter_input.workflow_id,
                     self._filter_input.session_id,
```

## Comparing `akride-0.3.4.dist-info/METADATA` & `akride-0.4.10.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 Metadata-Version: 2.1
 Name: akride
-Version: 0.3.4
+Version: 0.4.10
 Summary: Data Explorer Client SDK
 Home-page: https://github.com/akridata-ai/akride-examples
 License: Proprietary
 Project-URL: Documentation, https://akridata-akride.readthedocs-hosted.com/en/latest/
 Platform: any
 Classifier: Development Status :: 4 - Beta
 Classifier: Programming Language :: Python
 Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM
 License-File: LICENSE.txt
 Requires-Dist: importlib-metadata
-Requires-Dist: akridata-akrimanager-v2 <1.22.0.dev0,>=1.21.0.dev0
-Requires-Dist: akridata-dsp <2.14.0.dev0,>=2.13.0.dev0
+Requires-Dist: akridata-akrimanager-v2 <1.23.0.dev0,>=1.22.0.dev0
+Requires-Dist: akridata-dsp <2.15.0.dev0,>=2.14.0.dev0
 Requires-Dist: requests >=2.2
 Requires-Dist: attrs >=23.1.0
 Requires-Dist: yarl >=1.9.2
 Requires-Dist: tqdm >=4.66.1
+Requires-Dist: pyakri-de-utils <1.23.0.dev0,>=1.22.0.dev0
 Provides-Extra: cpu
-Requires-Dist: pyakri-de-filters[cpu] <1.22.0.dev0,>=1.21.0.dev0 ; extra == 'cpu'
+Requires-Dist: pyakri-de-filters[cpu] <1.23.0.dev0,>=1.22.0.dev0 ; extra == 'cpu'
 Provides-Extra: gpu
-Requires-Dist: pyakri-de-filters[gpu] <1.22.0.dev0,>=1.21.0.dev0 ; extra == 'gpu'
+Requires-Dist: pyakri-de-filters[gpu] <1.23.0.dev0,>=1.22.0.dev0 ; extra == 'gpu'
 Provides-Extra: testing
 Requires-Dist: setuptools ; extra == 'testing'
 Requires-Dist: pytest ; extra == 'testing'
 Requires-Dist: pytest-cov ; extra == 'testing'
 Requires-Dist: pytest-vcr ; extra == 'testing'
 Requires-Dist: mockito ; extra == 'testing'
 
@@ -70,20 +71,20 @@
 
 ```bash
 pip install -U akride[gpu]
 ```
 
 ## QuickStart
 
-Start using Akride by copying the following code snippet into your Python terminal:
+Start using Akride by downloading the SDK configuration through the steps outlined [here]( https://docs.akridata.ai/docs/download-config-saas) and paste the following code snippet into your Python terminal:
 
 ```python
 from akride import AkriDEClient
 SAAS_ENDPOINT="https://app.akridata.ai"
-API_KEY=<your-api-key>
+API_KEY=<your-api-key-from-sdk-config>
 # API Key Configurations
 sdk_config_dict = {
   "saas_endpoint": SAAS_ENDPOINT,
   "api_key": API_KEY,
   "mode": "saas"
 }
 
@@ -102,7 +103,11 @@
 
 Check out the [akride-examples](https://github.com/akridata-ai/akride-examples)
 repository for examples of using `akride` client to interact with DataExplorer
 
 ---
 
 For more information about AkriData, please visit [akridata.ai](https://www.akridata.ai).
+
+
+## Docker command to ingest data
+docker run -it -v ${PWD}/data:/data akridata/akride:0.4.8  akride ingest -d <dataset_name> -i /data -e <de_endpoint> -a  <api_key>
```

## Comparing `akride-0.3.4.dist-info/RECORD` & `akride-0.4.10.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,72 +1,75 @@
 akride/__init__.py,sha256=_4QgfqdhPVgQ9Yl2QaPM_JYBfed0j5BxoIGN7tYkNeg,2119
 akride/background_task_manager.py,sha256=f5mMGM8Ji8VxYGV8S7JRU2dkr4ygFVwiGK_pgnVNZik,2499
-akride/client.py,sha256=fQJtasizHhrhnye8o0Sqq9sFJjdlqtn_BpaBeeUFK4U,34360
+akride/client.py,sha256=1NQBrpBi19iGO83shnh2XihV0yxZDr3m4-UHiwhmXVs,34742
+akride/main.py,sha256=qRIgKovrjcIv0eZt4Sc63S60myEHjcr98u44DnpzmFM,1808
 akride/_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/_utils/background_task_helper.py,sha256=c29VSwdDi52cYP7mzwT2AuFFBjGSAXZ4IodQcTJen7E,2861
+akride/_utils/background_task_helper.py,sha256=No8KJAeitnKAx_OkWTPk6hQlSOIbu1mdvO6WP71xv08,2904
 akride/_utils/class_executor.py,sha256=y7qV7IRDiz0Gb1BM5oI2s7pcBUObj3B46mHdiTwc62o,1357
 akride/_utils/exception_utils.py,sha256=MvimxHZts3hxRfuDjNLtPcNnXs-Vh-IR33nxD34YKYA,2624
-akride/_utils/file_utils.py,sha256=E0phexzthx3TfHj-xdgu04NqeIiclk-1E3JDiTyAshg,501
-akride/_utils/job_creator.py,sha256=gPSiTCzgmYoJ7F9XDWEdbVeEoarMx7gKW2K7BX2XuDU,12897
+akride/_utils/file_utils.py,sha256=zmiooZXaFziN6jaK4b3aI1hDKWUQqN8LWguew-I0_Gw,584
+akride/_utils/job_creator.py,sha256=J4z9V7zaFsf_jY1DJNKigRdneYvaKI0fhzDDCtmDbvE,12931
+akride/_utils/platform.py,sha256=nutg_fRstZOGVryc3L78tZeuSG3H8eH_c3Pjyt41gIA,278
 akride/_utils/progress_bar_helper.py,sha256=68mwZAUFYM372AMEbNve5r7Tip4CUHUpkxYA1amp2AA,454
 akride/_utils/proxy_utils.py,sha256=1FBXCnW11iTInCcIyhKGGe9Z8TF67WRPSZWMvgVI2pM,1459
 akride/_utils/resource_utils.py,sha256=rN00dPs3vxu73m27xTeC-KWsRg6B0jTykkb_KKemeQ0,172
 akride/_utils/retry_helper.py,sha256=qcxSfJIRLHuJy9zABkSRJXTkhxeLNJVoH4e-BMXEIGg,595
 akride/_utils/store_utils.py,sha256=8FDHFXACLAh8jNKNfjkhCyJ2j89LZ8V6uKz7zeNYAxQ,811
 akride/_utils/workflow_helper.py,sha256=9oCxx4cU1UcClMaEB5jMe5NZXRChg4gXGXVJTpsimSQ,473
 akride/_utils/catalog/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/_utils/catalog/catalog_tables_helper.py,sha256=V6CPCRLD50yTfc02dU9eB3IbvSh1vfx8rgWfqqZT3do,779
 akride/_utils/catalog/dataset_tables_info.py,sha256=em9hogSeLs1xzsQNo3ITiEGGyZzHZziJDE09LQmhZlE,574
 akride/_utils/catalog/enums.py,sha256=2pV69NLp4Z4dQHgM7prOFXyiLv1PR0Car_7fCn_6Fik,194
 akride/_utils/catalog/pipeline_tables_info.py,sha256=0HvVx9BdCgqNkESveqEcEhEsOj3BOJVrtOhanO0p5hs,890
 akride/_utils/catalog/tables_info.py,sha256=PCgCghF5RacYd6GA0YpmCwVpCavQ0M8i80F1BqmkJvY,541
 akride/_utils/pipeline/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/_utils/pipeline/constants.py,sha256=O36-tq-Hr_22xGDdRUCurcpya-GKzTPOd7bPHklsr3Q,171
-akride/_utils/pipeline/pipeline_helper.py,sha256=usiKw6Mjxrud0rzfJD6P52Mbk3Qox4kKh5oc9v9Snvg,1083
+akride/_utils/pipeline/constants.py,sha256=JkCjLeuK9RT9ubxviRDfXIjFFHxvixJNi9axBJ5kHqk,250
+akride/_utils/pipeline/pipeline_helper.py,sha256=u79lyt-TQXCvb_83uyAv3zx8PH0J-7TwMfo4DWxs0Hg,1159
 akride/_utils/progress_manager/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/_utils/progress_manager/manager.py,sha256=uUrLhzVIuIcNzwHTpQX5rxPyWNw6iN2e-OQJiBHXOqM,2035
+akride/_utils/progress_manager/manager.py,sha256=CxqdeYzLczU_DFQ7lK90AxJKX47e-QTyOVE2FsTTkoQ,2037
 akride/_utils/progress_manager/progress_step.py,sha256=FU4Ni4Qmo6GdPdrYfl2JUOZG_D-U7j0EQv7BCn6Fdy4,1646
 akride/_utils/rest/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/_utils/rest/requests_session_manager.py,sha256=4c1-g0mkLo_MvMjjKDPlMbNajeQZe8IzDcfzzV6JmEk,528
 akride/_utils/rest/rest_client.py,sha256=MNA42NaJCeDR0A85fzJUgJqFon-LxBDEseVmdR6BJeM,3468
 akride/_utils/rest/utils.py,sha256=5w20_GIqHkgvD08l1zF3xMwefbo2eiJhMiSkg4qTIGs,194
 akride/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/_log.py,sha256=LW7PWngv1SMCpzO2XgPXadiHYK1Tbs9Ji9M3eCGZ5Hc,1450
-akride/core/_pipeline_executor.py,sha256=OiHFX-MPmiBDhF8wP8wyieZaNCUquYXFbIBnH6-QoN0,14157
+akride/core/_pipeline_executor.py,sha256=CqIWR_qTVNP4FTl612mWwfVsdDmFbR1ASP0CPj4V43o,15237
 akride/core/constants.py,sha256=deN_w44brWZvvB_YAw6wQ3yfBalIL9s1IDHddIzF-wU,1859
-akride/core/enums.py,sha256=O45ba7VplGoSkhjv7vxML7C8K0EhrFupQcKlanaD4e0,1987
+akride/core/enums.py,sha256=PJk-U8KSJSTriDmx3UthV_-Bo8YmVubLM2OiwWKojxk,2284
 akride/core/exceptions.py,sha256=LZDTJQ_rDEHxqaMqrkw7PoysQPuo9CCskHo1K5luThw,1500
 akride/core/types.py,sha256=cHcC21Ks51z1t4F7eEFa3mr080FXwLzz_kohYZuNRNw,9614
 akride/core/_entity_managers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/core/_entity_managers/catalog_manager.py,sha256=5DDDPQDdj9bmaUzAwvfqrZ-ESBtCqxmMXryyGnAsze0,18095
-akride/core/_entity_managers/dataset_manager.py,sha256=GfdUDRrcXC_Ki6orQdiZ07xvYr_zvKj9PnlMJFiIUlY,15333
-akride/core/_entity_managers/job_manager.py,sha256=56fadxWi8inqbQJCbHzJ_U9rAC5EQa5whG6Gnj6xeL4,27835
+akride/core/_entity_managers/catalog_manager.py,sha256=6JyeJF-metpdxNFSA8HigdCjEgilb9TdpNVt_U43rh8,18527
+akride/core/_entity_managers/dataset_manager.py,sha256=Qxu9lStk9K4l_RLqtHxFlavybKhKn4wODsj9ksZtISo,16073
+akride/core/_entity_managers/job_manager.py,sha256=44qGFvoKFDknom5DxDLkJLbu185Q6qODGoYG3gDFuoA,28206
 akride/core/_entity_managers/manager.py,sha256=GxUe0oBhNbXyrrThs9S6j7AYF7qJI3FaXjXRbZl3MOo,2238
-akride/core/_entity_managers/resultset_manager.py,sha256=U4JbWBoLKE0bu5ci6UMuX62f5TJrwj5MUU65sgLPcGo,8486
+akride/core/_entity_managers/resultset_manager.py,sha256=ToNDisHBP6Xb-L5OSEIPDlMvoiCSSErs8b6tV6t8VBI,8702
 akride/core/_entity_managers/subscriptions_manager.py,sha256=lPmC41MYl7kg_IBp84e9Lv3waqTokRJWhJZN6CclhQE,1314
-akride/core/_filters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+akride/core/_filters/__init__.py,sha256=6CgtXVd_M0GUNJsbv9G939UenkUt91Nu28WDUq3_xwM,140
 akride/core/_filters/enums.py,sha256=NvVxgm3rkWJS0zMxbj7Cyxu2q3ExkYsUTfOedac7dQw,253
 akride/core/_filters/partitioners/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/core/_filters/partitioners/ingest_partitioner_filter.py,sha256=OJUFoRASQdIO_hRgJKTIOw14F4jXzTARHv8NdFNhsWI,6420
+akride/core/_filters/partitioners/ingest_partitioner_filter.py,sha256=P5Gc_DZUexU5i4k71HRFMXZTyGAxAp7z8aaBW_3CVKg,6412
 akride/core/_filters/partitioners/models.py,sha256=ZIqpYx-4JUfAO60YTG_AFAa6ooFFjJ9N8OpSO098bv4,696
 akride/core/_filters/partitioners/partitioner_filter.py,sha256=fgzkq45fnSWO_ctJY9wxunad3FWcpN0OtvrCV7xaY2M,288
-akride/core/_filters/partitioners/process_partitioner_filter.py,sha256=uEknU6TGvXzS1vo_mq4nxF41HZ5A0flDdwIl3CG-ldg,6059
+akride/core/_filters/partitioners/process_partitioner_filter.py,sha256=6giTjKGInXAOTiMNJ_QJxTl1pno20XYAFtXACY2U2w4,6043
 akride/core/_filters/sink/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/_filters/sink/models.py,sha256=0YEzbZ-BpAI17yP3hA8i3yNHjVOvtAk5TXLqVLODcgY,1247
-akride/core/_filters/sink/sink_writer_filter.py,sha256=Z2xJzR6inTZOYCn60mG8pPUbqTZ9AcJeeFBjPwSP2UI,8936
+akride/core/_filters/sink/sink_writer_filter.py,sha256=OHOzjNl5QMpDQFvyrF8fhKvqeTKC27YyjbkYdiJazRc,9178
 akride/core/conf/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/conf/pylogconf.yaml,sha256=fS9hD9BqxZePlDCISZUpma-7oKkFKbdhTFkm_BsbsRA,493
 akride/core/entities/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/entities/catalogs.py,sha256=RPqPbjdG79yneZ85-BcrXtBGGwoGd2kUW8hyLGdQdRI,796
 akride/core/entities/datasets.py,sha256=N9VbHNuzqTOdlazECYwcNkpbY7zf_Mr_cn25m3ZtJS8,736
 akride/core/entities/entity.py,sha256=sIRYQ69xM6Hrasn7edwgF-J6LjFG6hF7S95zTKe005g,1549
 akride/core/entities/jobs.py,sha256=7xsvcl6sjPEWWB_hUnq93nIwPVo9JI1rBrlhvxZnrDs,5133
 akride/core/entities/pipeline.py,sha256=CF9hqAUIET75JhWc0nUm9oRDFIPf7lXOnMosbrIlfyw,436
 akride/core/entities/resultsets.py,sha256=Qlq5Ze1pAgi0jHFIVSrMps106leaxmGZgdHefXIfy50,955
 akride/core/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/models/catalog_details.py,sha256=JGDYdWM5EmF0f0vRHGaH6XtixqJkfH1b93Fgj3t3A1U,398
 akride/core/models/progress_info.py,sha256=5QBfsMe74CpYmmbnjYtr8CWsjhlhgb6yuwbS1srr_YQ,246
-akride-0.3.4.dist-info/LICENSE.txt,sha256=1FQFvIUl224RHI5tv0i_Ec4Edw9VKFiMUIoM3J6fmoA,130
-akride-0.3.4.dist-info/METADATA,sha256=52k2scgDwBZ4TRsS0rnuJnonauMo9DPaYIAVRRhoBLY,3111
-akride-0.3.4.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-akride-0.3.4.dist-info/top_level.txt,sha256=qhymMhnBUjXghMz0FSS8l-t6VRF_MXcHLtW9nIP26c0,7
-akride-0.3.4.dist-info/RECORD,,
+akride-0.4.10.dist-info/LICENSE.txt,sha256=1FQFvIUl224RHI5tv0i_Ec4Edw9VKFiMUIoM3J6fmoA,130
+akride-0.4.10.dist-info/METADATA,sha256=IXnZit0jFS7MqBpGA30ikPLXHxKAVPMOkD3mBEP8-ds,3475
+akride-0.4.10.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+akride-0.4.10.dist-info/entry_points.txt,sha256=fD59ZhGa6sPkQd4H45zttMqxPXTHzJM7vQVJRLpGZns,44
+akride-0.4.10.dist-info/top_level.txt,sha256=qhymMhnBUjXghMz0FSS8l-t6VRF_MXcHLtW9nIP26c0,7
+akride-0.4.10.dist-info/RECORD,,
```

